Stepwise linear regression is a method of fitting regression models in which the selection of predictive variables is done by an automatic procedure. Starting with an empty model (no predictors), at each step it adds the most statistically significant predictor from a list of candidate variables, or removes the least significant predictor from the model. It repeats this process until a specified stopping rule is met. The end model only includes those predictors which have been identified as making a meaningful contribution to the prediction of the dependent variable. The two main types of stepwise regression are 'forward selection', where predictors are added one by one, and 'backward elimination', where you start with all predictors and remove the least significant one at each step. Although it's a powerful tool due to its simplicity, stepwise regression has been criticized for its potential to generate overfitted and unstable models, and results entirely data-dependent, ignoring conceptual issues.