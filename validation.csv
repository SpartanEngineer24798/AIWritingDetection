txt_data,no_authors
"Force detach the system volume.  Then you will be able to stop instance.  Re-attach volume and start instance.  Viola! Where the value supplied to region= is the region that contains the environment to be shut down. On that page, one can easily deactivate multiple environments swiftly: I ran into a related situation after I created an Elastic Beanstalk environment within an ec2 instance. To terminate my ec2 instance, I had to terminate my eb environment first, which I did by visiting: on reboot internal IPs and public DNS change, so if you dont have elastic IP associated with your system then this might be the reason why you cant SSH in to the system. After a routine reboot of a Linux ec2 instance with 1 small root volume and 1 small attached volume I was not able to ssh into the instance.  It is not clear why rebooting the instance caused it to be inaccessible from ssh.  The instance showed as running in the AWS console but ssh, http,etc was not responsive.  I tried to create an AMI from this running instance.  However, the AMI was never created.  Instead I just saw ""pending"" in the AMI section of the AWS console for hours.  Eventually I de-registered the AMI.  Next I tried to stop the ec2 instance.  However I am not able to stop the instance - it has been stuck in the stopping state for hours. I also tried force stopping the instance with no success.  I then tried to detach the volumes but they constantly report ""detaching""  Does anyone have any suggestions about how to handle this?  It seems that Amazon does not offer any kind of email or phone support unless you are a premium member.  Thanks very much for your help. I know this post is old, but you can also click stop again, and the dashboard will ask you if you want to do a forced stop. Sometimes, I know this from experience, you have to do two or three forced stops to get it to work.",5
"Second, by storing your code repositories outside of the /var/www/html or /srv/html, you get two nice benefits. You can create symbolic links into your repo at any level, making it easier to hide your libraries. Also, if your repository's location changes at all, you don't have to modify your virtual host configurations. Instead you just adjust your symbolic links. This might seem a bit unconventional at first but it is very reasonable as this directory is made for you (with correct permissions) when you do sudo useradd git. You can just switch to the git user, cd and immediately run: When I am downloading git repos in order to maintain site configurations which I then deploy I store them in  There is no right or wrong answer here, except the one dictated by your own personal religion and the contents of the hier(7) manpage on your system.   First, regarding the suggestion to use /srv, you are under the assumption that all git repositories are used for websites. It may be true for you, but you could have a piece of software that is not a website.",4
"In debug mode through my Azure address, the pages show up as secured. (HTTPS) However, when I access them through my domain name, I receive the ""Your connection is not private"" page where it routes me to an 'http:' address of my site if I accept. Possibly the default certificate looks nothing like your domain, but we cannot tell that without more detail. No you don't need it, but recently azure ""force"" developer to use SSL in their website, well there is a workaround but whether you need it or not that's depend on you You can also use a ""self-signed"" certificate that you generate yourself. Such a certificate will cause browsers to display the ""Untrusted certificate"" page, though most browsers (Firefox, Chrome) will allow you to accept this untrusted certificate either permanently or for the session. Down votes are for a number of reasons, but mostly that you can improve the question. Which web browser. What are the details of the cert in use (CN, subject alternative name) obfuscated if you must. How does an Azure URL differ from the domain you want to use. I looked at some of the documentation of GoDaddy and apparently they have a service I can buy to implement SSL. Is this something that is required? Or am I missing a configuration setting? First time I'm setting up a external web application for a family member. I have a Domain Name purchased through GoDaddy setup to point at my Azure website. I've added the requires SSL filter so that the pages are secure throughout the application. I'm afraid I don't know anything about Azure hosting. In general, however, in order to use SSL, you must have an SSL certificate installed. This certificate should be from a certificate authority (Comodo, Thawte, etc) in order for your browser to verify that has been genuinely issued; the certificate authority ""signs"" the certificate to ensure this trust.",4
"I am boggled when I click on an application in windows and have to wait 10 seconds for it to respond. I wonder ""you can run 16 billion instructions a second, what are you doing that keeps you from responding to my mouse click."" Because of that I would be interested in gathering together ideas on how to design an application that could diagnose these kinds of problems. I am no windows expert and wonder what methods others would suggest. Here is what I would ideally like to be able to do: Recognize any applications, drivers, or operating system states (e.g. the oft-blamed registry) that are slowing down the system and bring those to the attention of the user. Some of this could be garnered using performance counters, other aspects I am less certain of. I would be inclined to use machine learning methods to determine whether the system is bogged down and to help determine the culprit (which is workable as long as I can both contrive examples that recreate the major sources of problem, as well as garner enough data to be able to determine the culprit).",1
"Link-Local addresses are required for the operation IPv6, but not for IPv4. Microsoft, among others, has developed the APIPA (Automatic Private IP Addressing) standard using IPv4 Link-Local addressing, but it is not mandatory or supported by all OSes. I also have seen people misuse IPv4 Link-Local addresses by manually assigning and subnetting them, and that is against RFC 3927, Dynamic Configuration of IPv4 Link-Local Addresses, which is a standard. In IPv4, every interface use to have only one address whereas in IPv6, we can have multiple addresses on a single interface (like link-local, global address on an interface). In simple words, at the time of booting up, OS tries to configure an address on its interface through various methods like - When IPv6 was developed, many things were built into it that had been optional, add-on features of IPv4. Link-Local addressing was one of them. Every IPv6 interface is going to get a Link-Local address so that a device can at least communicate on the local link, and, as a built-in feature, many things, e.g. routing protocols, use it for communications on the local link. Link-local addresses are used for communication between two hosts (which are there on the same link) when no other IP address is specified. A PC automatically acquires a 169.254.x.x/16 address if it does not receive an IP address from a DHCP server. If you disable the DHCP server on your home or lab network and issue the ""ipconfig/release"" and ""ipconfig/renew"" commands, your PC will receive a 169.254.x.x address. Because of the imitation of having one address on an interface in IPv4, you may not have seen an address of range 169.254.0.0/16 on interface. And if OS doesn't able to configure an address on the interface through any of the automatic methods, then it configure an address on the interface from link-local pool.",3
"You can also define your own 'High Performance' and 'Archive' filesets, which will be moved to the beginning and end of the drive respectively, which is great for storage disks. I'd recommend UltimateDefrag Free or Defraggler. For a storage drive, the 'Fragmented files only' option in UltimateDefrag, or the 'Quick Defrag' option in Defraggler will be quite quick. The 'Quick Defrag' option would be quicker, but it doesn't do as much as UltimateDefrag's 'Fragmented files only' option. I see that you would like optimisation too. In my opinion, UltimateDefrag is the best for that. The Consolidate, Recency, Volatility or Auto strategies are great for system drive optimisation, particularly when told to respect layout.ini, 'high performance' and to move directory entries close to the MFT. I've been very satisfied lately with AusLogics Disk Defragmenter. Lots of people are also very happy with MyDefrag. Both are free, work well, fast and efficiently.  Defraggler is one good application I have used and have been satisfied with the speed. Also, it is free. I can't comment on speed since, as pointed out by Snark, ""it depends"".  I've been using PerfectDisk - http://perfectdisk.raxco.com/ - for a couple of years and ""it's fine"".  I have it setup so that it defragments in the background on my PC.  I also have it setup on my Windows Home Server, though on this one it does scheduled defragments at night.  My percentage of fragmentation is always in the neighbourhood of 0%.  It has had good ratings from publications such as CPU Magazine.  They have a free trial if you want to give it a shot.  Their ""Home Premium"" version if $29.99 (Pro is $39.99) so it's not very expensive. It's not easy to tell which one is the fastest, as it varies a lot depending on the harddisk, the file of files stored on the disk (lots of small files, some huge files, ...), the defragmentation strategy you want to use (defrag only, optimize files, ...).",4
"Neither of these have queuing or job scheduling, although you could run them via cron or at if you wanted to. Ah!  It looks like the GNU version of parallel (not the one I had installed) does do this. No load balancing, and I haven't tried it out to see what it does with each stdout and stderr, but this is precisely what I wanted.  Unfortunately I've written a script that gives status updates, has configurable output settings, and incorporates some simple load balancing, so I'll be sticking with it for now. clusterssh is another tool that might be worth looking into.  It's more interactive in that it will open and tile terminal windows for each host.  You can also run commands in each terminal separate from each other or in all (or some) at once.  For example, running top on 12 systems at a time then chasing down a process in just one of them. parallel means to start the same command on an collection of hosts (running in parallel) ... if you want to do different things on different hosts that is an sequential process  I've also seen a few talks for rshall (which despite holding RSH in the name, uses ssh natively) at the local Linuxfests, it's perl based and can use an external source for querying host lists, but it expects certain host information in specific formats. You really should look into one of the many clustering technologies out there.  Try looking at Apache Hadoop.  I recently read a great article that you may find interesting on the subject too about setting up a 10,000-core cluster to do parallel computing:  http://goo.gl/A8hgX",5
"The solution I came up with was to spin-up/down servers at the mid-point of the 2 users playing with some epsilon value (say 300miles or something). Perhaps the server would allow up to 50 users to play simultaneously and after that it would deny requests, which would then spring up another server. This should cause latency to be about equal for the 2 users playing. Furthermore, if we are matching making users then we always pair the 2 closest users together. I thought about using NodeJS + SocketIO to handle a server side for this cooperative shmup (think Raiden or Galaga) we are just starting to work on. We started to think about latency issues.  I suppose there are other factors to consider like user skill level, but I think right now we are most concerned about latency since it directly affects gameplay. Actually our game does require fairly precise timing. Also we want users to be able to see each other shoot since cool animations happen when they are in sync with each other. I had an idea for an architecture, and I was wondering if anyone has implemented something like this or has used AWS or some other service to help?",1
"Windows does not (yet) have a native Gluster client.  So you either have to connect it via samba or NFS to windows.  In other words, another server in between gluster and windows which mounts the gluster volume and re-exports it using samba or nfs. GlusterFs is good if you plan to have multiple data nodes and requirement such as HA, no single point of failure and almost linear scalability. Simply get a server with a suitable number of storage bays and RAID the disks.  In our situation we're actually using a standard PC but with a case that can take multiple drive bays (not hotswap). We're using linux software raid in raid 5 mode giving us around 8TB of storage.  It's working really well as a simple NAS for backups and basic filesharing needs.  It's not a critical server so the lack of hot swappable hardware isn't an issue at all. it basically replaces xfs+lvm, and gives you out of the box volume management and aggregation, self healing capabilities, data deduplication, data compression, snapshotting and also smb, iscsi and nfs sharing. You may want to also look into using Ceph.  But again, no native windows client and you'll have to re-export as well.  From memory Gluster has inbuilt NFS server (although it doesn't provide the clustering/failover features of gluster itself). But this it not so important. More interesting is the set of features it has. It supports file-based networking protocols NFS, SMB/CIFS, HTTP/WebDAV or FTP, block-based storage area network protocol iSCSI (initiator and target), filesystem quotas, integration with Active Directory and more. It leverages Linux LVM for volume management and all of these features are easily configurable from unified web-based management so it' not so necessary  to really understand how it works under the hood. I don't have much experience with glusterfs in production but in such scenarios I deployed  a few times community edition of Openfiler NAS appliance which is based on rPath Linux.  In your particular situation, I wouldn't be looking at Gluster or Ceph right now.  These filesystems are of more use in a large organisation or data centre where you expect to grow your storage needs very rapidly.  They are designed to run on multiple servers as well.  If you use it to serve to windows you'd end up having to run a samba server anyway and connect your windows clients to that.",3
"First step: you need to become the controller of your domain name with the registrar.  This is something people forget to take care of, and it can leave in the dust for days if you don't pay attention to this.  Where have they registered your domain?  If your domain was registered at the web hosting ISP, then perhaps that needs to be transferred to a new registrar.  You'll need to work with both the new and old registrar to get this done. Setup a server on your internal network, copy your content over (you might also need to make sure you're running the same CMS internally.)  Then figure out how to allow access to that server on your internal network (maybe even put it on a DMZ.)  Then change the public DNS record to point to your server. the company now is entering a mobile operator business which requires that the website to be moved to our internal network to let the user use our website to access some services. Given how you are asking this question, it sounds like you should hire a consultant for this.  I'd also ask this question: Do you need to move the whole website or can you create another sub-domain and use that?   www.example.com is your public website, mobileop.example.com runs your internal services.  mobileop.example.com could be run on a separate server on your internal network and wouldn't necessarily touch anything on your public one. If it's just another service, set up a new server inside a DMZ (ask your networking guy), configure it with a public IP or let your firewall do some NAT. Name that new server whatever your service will be like, e.g. access.company.com and create a A-Record for it.  Once you have control of the registrar set up and info, then you need to assign it the IP of the DNS server you will be running to handle DNS lookups of the domain.  Depending on the current type of commercial ISP you have for Internet access at your business, you might need an Internet connection upgrade to be able to host DNS and web services.  What DNS server you use and how to set it up is something I'm assuming you can handle, or we would need a small book (or a consultant) to answer or resolve your question.  From there you assign within DNS server, the IP of your web server or other services (MX) to the servers internal to your network which will run these. right now i'm still confused of what to do, what are the steps required to move the host and still alow the people when they type my company domain in the browser to be directed to the new location which is inside my company network , how the DNS NS Public IP issues has to be considered So many options.  Don't tell the consultant exactly what you want done, tell them what your end goal is.",4
"If you're working with a uniform-weighted grid and don't need to worry about dynamic pathing, Jump-Point Search is an extremely efficient pathfinding algorithm. It's extremely fast, usually ten-to-thirty times as speedy as A*. It achieves this through symmetry reduction, which is a method by which empty spaces are ignored. I'm not an expert, but I can see Floyd-Warshall being better in some situations where you can afford to just precalculate all best paths up front and then reuse them. A*, because of the closed and open lists, can be more memory intensive than a well-implemented Dijkstra.  However both are probably processor-bound anyway so you might as well go for the flexibility of A*. I'm not very familiar with many of them but I have seen the usage of Dijkstra's algorithm not for pathfinding but for finding the nearest X to you. For example, what powerup is closest to you or what enemy is closest to you out of a group of enemies. So a ""target nearest enemy"" function often uses that. I am familiar with how the common ones technically work (BFS, DFS, Dijkstra, A*) but as far as their realistic benefits I don't quite see the need for them. Considering that, given the right heuristics, A* is more performant then why bother with any others?",5
"For some reason my mailbox had been set as the Journal Report NDR destination and, beginning with Exchange 2013, mailbox and transport rules do not get processed for the mailbox with this assignment. Changing the catch all for Journal NDR fixed my problem as well.  Instead of PS I used the GUI/ECP. Compliance Managemente > Journal Rules Tab > Send undeliverable journal reports to: (Note that doing this will remove all of your rules.  You may want to just go looking for one that looks out-of-place or broken.) This has helped me out greatly and one thing to add is I am using Office 365 and these commands were exactly what I needed. This can be fixed with Exchange Management Shell and the following commands (first to verify, then to reassign): Launch MFCMAPI, and select Session -> Logon.  Browse to Top of Information Store -> Inbox and right-click and select 'Open Associated Content Table'. You're looking for messages with the message class IPM.Rule.Message.  Remove those using the DELETE_HARD_DELETE delete option. For some reason my mailbox had been set as the Journal Report NDR destination and, beginning with Exchange 2013, mailbox and transport rules do not get processed for the mailbox with this assignment. This can be fixed with Exchange Management Shell and the following commands (first to verify, then to reassign):",4
"I am trying to install windows 7 side by side with windows 10, as I am tired of headache that windows 10 gives me and also dont want to lose original windows copy from which I upgraded to windows 10.  I used Rufus program to make bootable usb disk, using FAT 32 as file system and Target platform as GPT with UEFI. But even after following all instructions, My laptop is not booting from USB disk, I have set booting order in legacy and UEFI as USB first. Even when I manually select booting device as USB drive (which is recognised and shown in available boot devices prior to boot) then it says ""No bootable device found"".  Then I tried to install windows 7 ,by running windows 7 iso while in windows 10. But no matter which windows 7 iso image I use , It always says ""Use the other installation disc that says 64-bit. When you insert it, Windows Setup will restart automatically"". I have downloaded various windows 7 iso (almost all more than 3 gb in size and claimed to be 64 bit, also checked for various signs that proves this iso was of 64 bit, e.g. size more than 3 gb, presence of bootmgr.efi file etc.), also why everyone will upload 32 bit win7 iso claiming it as 64bit as I have downloaded 5-6 such ISO and Everytime my laptop says use other 64 bit iso.  So please suggest any way to find windows 7 64 bit iso that can be installed alonside windows 10 and also reasons why windows 7 bootable usb is not booting and why it always detects every 64 bit winiso as 32 bit ? My system is 64 bit, so after searching on google I found out that I need to install 64 bit windows 7 iso on GPT partition with UEFI enabled, Legacy and CSM enabled, Secure boot disabled, USB 3.0 port status before boot set to Auto mode.  After further research someone suggested to go in sources folder in iso media and from there launch setup.exe file, to my surprise it worked and windows 7 set up started and files were loaded, but again before installation it said this is 32 bit windows 7 try other disc.",1
"Tim Berners-Lee admits that this was intended to separate the protocol for the address, but turned out to be unnecessary. It think it literally means ""Whack! Whack!"", from the really early times where actually getting a connection to a site demanded some voodoo and you had to sacrifice (whack) not one, but TWO chickens to please the WorldWideWeamons.. :) xx:// is used to specify the protocol used, it's a double / so it can't be confused with just being a subfolder. I use URLs all the time typing them in with and without ""//"" after the ""http:"" but what does the ""//"" really stand for? There is an answer for that in Tim Berners-Lee's FAQ (link found in the slashdot discussion mentioned at @Evan Anderson's answer). Basically, he copied the filename syntax from Apollo Domain, where starting a path with a double slash followed by the name of a computer was used to transparently access files in other computers (a single slash is still the root directory). He simply prepended it with the protocol, in this case http. That special case still exists in the Unix standards, where two slashes at the beginning of a file name may be interpreted in an implementation-defined manner (three or more slashes are equivalent to just one). And the same convention is widely used in Windows, only with backslashes instead of forward slashes (it is called a UNC path there).",5
"As the name implies, the location is intended for files that are only supposed to be stored temporarily. i have found programs like skype and vmware persistently holding on to files in temp folders, but i dont think they depend on them to function correctly. So, in my personal opinion, deleting the contents of the Temp folder shouldn't cause any problems. But due to the fact of how it is used, it can not be guaranteed to be a safe operation. As stated by many, it should not be a problem, but honestly just in this week i was trying to 'clean' my pc by Uninstalling programs. to my dismay i couldnt uninstall a bunch because the uninstall depended on files stored in the temp folder(since the installer extracted the setup files there), which i clear regularly.  Please also keep in mind, emptying the folder while the system in operational could cause a running application to lose a file it placed there. Emptying the folder is something best done during boot. If someone decided to place a critical file in that location, deleting that file might cause a problem to an application. So, if you write an application and you want to store something for later retrieval, this is the wrong place for you. The Temp location is only to be used for data that, if gone, wouldn't matter anyway. once on a server, after clearing the temp folder for 'Administrator' various functions in Sql Server management studio failed because of something that couldnt be found in the temp folder.",2
"Why does it need (or like) to know that? Will it result in better performance? Will some things work / not work? (Links to technical details welcome!) It's just so VirtualBox can select the optimum and supported default settings of the VM you create. You can alter these as you want in the ""Settings"" afterward. Some OS's have better support for some drivers, some doesn't have 3d Acceleration support and so on. One very useful optimization that I know of is spin-lock detection. When Windows enters a CriticalSection or Linux enters a pthread_mutex_lock, the CPU usage will briefly go to 100% while it waits for another CPU to finish. If the VM knows about that it can make sure to run the other CPU thread immediately. I've noticed VirtualBox suggests the default memory and disk size based on your OS selection. There are also additional prompts, such as one for account creation (certain OSes) before the installation to streamline the installation process. VirtualBox also provides guest additions to supported OSes. For the whole list, including caveats, take a look here. Another optimization is the virtual graphics driver. A Linux guest will get a OpenGL virtual card while Windows will get a DX9 card. Mainly so it can best select the settings it feels would be well suited for your PC plus That virtual machine. Also its used to streamline things and stuff like that. Like virtual box mainly does this for its ""guest additions"" pack which is used to help it use devices on the host machine and share folders back and forth between the host and virtual machine.",5
"Or, another better question to ask is what is a better ""best practice"" than hitting a table five times with a similar query, if any? For an example of when using a temp table is clearly better, consider a billion row table with no indexes. Out of those billion rows none of them match the initial filter condition. You don't use any space but avoid four full table scans of the large table. On a smaller data set of less than 10K records, I don't see an impact in testing this alternative; as this scales to higher amounts, where the date could be filtering a two day period out of twenty years, would this start to perform better? An alternative would be to read the table once, keeping it in a subset - like a CTE or temp table - then filtering on that subset in further queries or subqueries in the SELECT statement, which would be a fraction of the table with only one read of main table.  Temp table example: My advice would be to start with the simplest code or the code that makes the most sense to you and to measure the performance of it. Depending on the results consider adding indexes, using a temp table, or other tricks to speed up the performance of the queries. All queries hit the same exact table five times each with further filters and grabbing one record.  The starting filter is always the same though - and it generally grabs less than 1% of the total records on the table. In the systems that I work on its very common for different users to run similar or sometimes even identical queries. I'm sure that if we took all of the queries after the fact it would have been possible to create temp tables that would have decreased overall server resource utilization. However, SQL Server is designed to deal with this kind of workload. Pages from tables that are needed for queries are loaded into the buffer cache. Subsequent queries on the same tables will often run faster because data is already in memory. You're talking about trading space for time. It's very difficult for anyone to issue a statement that will be a best practice for all scenarios. The benefits of using a temp table here depend on the table size, which indexes are defined on the table, the workload, the workload on tempdb, etc. For an example of when using a temp table is clearly worse, suppose that there is a heavy workload on the server and space in tempdb is limited. Your query could fail or cause other queries to fail if it loads too much data to a temp table. As Aaron pointed out a CTE is a tool for developers to reuse code. It will not encourage the SQL Server query optimizer to spool the results to disk so they can be reused.",2
"Personally I would use Ubuntu Server so it doesn't have a GUI, then write my own scripts to, at boot up, launch a bare X server and the VM's display application (VBoxSDL) in full screen mode, so that the user first sees the Ubuntu text boot-up then it jumps straight into the Windows Loading screen.  You could also have it so that if Windows is shut down it then proceeds to close down Ubuntu. I would really think before putting a XP VirtualBox session on a computer without hardware virtualization support. Without hardware-assisted virtualization, XP is going to be VERY slow.  This leaves me with the question - which host distro would be the best choice?  I'm looking for three things - ease of maintenance on my part (updates, configuration, etc.; I have a background in slackware, but have some exposure to Ubuntu), minimalism (so that the bulk of the system's resources can be devoted to the VM), and virtualization-readiness (say, so I can have the computer automatically go from power up to virtual machine power up, thereby minimizing the user exposure to the host). I've looked at Ubuntu server JeOS, and it seems enticing, but I can't seem to find out whether it can be used on a desktop system, i.e., with a GUI, etc. I've read a pile of suggestions both on this site and elsewhere, but they all seem to be a couple years old, or not quite my situation. A family member has got a windows PC (with a Pentium 4 w/o hardware virtualization support) that keeps winding up infected and crudding up.  As I would like to minimize family IT support time, I figured the easiest thing to do was to wipe it, put Linux on it, install XP pro (they don't want a more recent one) in a VirtualBox virtual machine, and take a snapshot.  I'd set up a shared drive on the host where they could store all their stuff, so that when it inevitably gets hosed, all I have to do is roll back to the snapshot. I would recommend Windows Steady State (link). It has been discontinued by Microsoft, for XP should work fine. Steady State will allow you to keep important files and setting from being modified.  Obviously, I would like it if it has all of the standard tools that will allow me to do remote maintenance, also.",3
"The best way I've found to combat this problem is to run ethernet down the center of the rack, rather than the sides. Most of the frequently cited bad issues with patch panels can be mitigated through planning appropriately. Label both ends the same, and ensure there are no duplicates.  This makes tracing cables a whole bunch easier, and alleviates much of the concern of bundling a bunch of cables together into a neat run.  We even label our short (1 foot) cables, and then bundle them into bunches of 10-20 - and it makes it MUCH easier to identify which cable is which. As far as what to label the cables with - I would stay away from anything specific like hostnames or mac addresses, since the cables/switch ports will likely be re-purposed over time.   This is dependent on your site, but the worst cabling disasters I've dealt with are when people slavishly run power cables and ethernet cables up and down the sides of the rack. It seems like a great idea in theory, but the practice is a giant knot of either power or ethernet inside the rack post. It is a fact of life that servers move frequently comparative to other datacenter equipment. All your old cable drops just add to mess you have to deal with. Since power cables are confined to the rack and can always be reused, this keeps the clutter buildup down. alt text http://photos-h.ak.fbcdn.net/photos-ak-sf2p/v248/20/63/1067293293/n1067293293_67279_5546.jpg For LAN wiring, I've used a method of bunches of 8 cables, each a different color going from patch panel to switch.  48 port patch panels to 48 port switches.  With the different colored cables, it is VERY easy to trace a single cable without pulling it out and makes everything look very very nice. Can still be a good thing in some cases. NEVER put network drops under a raised floor unless you hate your coworkers and plan to leave soon. If you have a raised floor only use it to run power cables. These are your friend. If you have a separate networking group and they don't use patch panels, it is because they don't know how to use them. Good luck!  I think excellence in wiring is a strong trait for any sysadmin.  Presentation is a big part of any job and having well dressed racks goes a long way to showing non-technical people that you're on top of your game. Overhead cable runs are the only way to preserve your sanity or blood pressure. This is especially true if you don't use patch panels. Yes, they take up space in your rack, yes they limit the amount of available ports per rack. Putting them in is more work. I will also admit they are not appropriate for every site. Tracing cable runs of 6 feet or less is exponentially easier than tracing cable runs of 12 feet or more. Adding in new equipment is easier, no more fishing cables through 15 different runs. This can limit the amount of crap that gets shoved into one rack (sometimes this is good). One thing I forgot to mention before is that ethernet cables and power cables are different.  That means they are run differently, have different lifetimes and different uses. alt text http://photos-a.ak.fbcdn.net/photos-ak-sf2p/v248/20/63/1067293293/n1067293293_62808_2066.jpg The general good practice seems to be to identify port numbers on each jack endpoint, use bundles of short cables to patch between panels, and keep a database / text file of computer-port number assignments.",4
"Turning off xp_CmdShell is a bit like putting a veil over rotting meat. It brings a false sense of security to the table and the flies can still get at the meat.  Allow me to explain. I'll say it again.  xp_CmdShell is not a security risk.  Only bad security is a security risk.  Fix your security and then turn on xp_CmdShell.  It's a wonderful tool and you're missing out on it because of bad security practices and myth. I wouldn't do it. Especially if you plan to get any branding from Microsoft as a developer partner. We have our products certified by Microsoft, and their application checking tools will check to see if xp_cmdshell is enabled or not. Enabling xp_cmdshell sounds like a security nightmare, and something that should definitely not be done. One one hand it's ridiculous to be absolute on the idea of disabling command shell as if it's a lit stick of dynamite. However on the flip side I think it would be completely reckless not to respect the security risks of using xp_cmdshell.  However for me personally, using it to troubleshoot production jobs (on an adHoc basis) in a secure development environment, I just don't see the ""security risk"". The code isn't being stored. My account is a personal account and it does not get used in any applications public or private. That's my two cents... I just asked my DBA about this because he disabled an extremely useful function I use for troubleshooting. After doing a lot of research and reviewing answers from people significantly more experienced than me, I tend to lean toward the middle on the issue of disabling xp_cmdshell. A SQL developer on a project I am working on has asked whether it would be possible to enable xp_cmdshell on the production database as it is easier to export CSV files using xp_cmdshell than to write an SSIS package to do the same. Who can use xp_CmdShell?  That's right. Only people/app logins with ""SA"" privs or people that you made the horrible mistake of granting a proxy to can use it. So, what's the real issue with xp_CmdShell being a security risk?  The answer is xp_CmdShell is NOT a security risk.  Poor security is the only security risk.  If a hacker or an malicious internal user get's into the system with ""SA"" privs, then they can turn xp_CmdShell on in momements.  Yeah, that action gets logged but that only provides documented testimony that security was grossly lacking to begin with. Next question.  If you have xp_CmdShell turned off, who are the only people that can turn it back on?  Correct again!  Only people/apps with ""SA"" privs can turn it back on. Turning xp_CmdShell does nothing for security except to provide a chance for that part of a hackers code to turn it back on to run. That being said I look at xp_cmdshell like a loaded weapon. If you know the danger proceed with caution and if you don't, then get educated BEFORE you use it in a production environment, particularly if it's part of an application. For the most part, using xp_cmdshell in an application ""should"" probably be a last resort option.",4
"You need to update the A record  for all of your server in DNS , if you are going to change the Web Hosting , as New hosting provider will provide New IP address to your server. You need to ask your hosting provider to allocate new IP addresses (Public) to your servers which you want to access publically with DNS .  IP 198.23.52.85 is your route please do not add this IP in your DNS. Yes if you will update your DNS with this IP than you will not be able to send receive emails form your domain. Also if your goind to use your hosting provider email server than you need to modify the mx records accordingly, please ask your hosting provider for details. I am rather new to DNS, and I am not sure as to how I should transfer the hosting of a website from one server to another, as well as the email and/or without the email. I inherited the following records for the domain from the last person managing it: What do I put where for each scenario? If I put 198.23.52.86 in the 'www' host record, will I lose the old email servers' connection to the domain? It will serve you well to read all the chapters of Zytrax online 'book'; you will learn all the arcane syntax of DNS. In your screenshot, it looks like it is already set up this way.  @ refers to the ""current"" base name, and two mail exchangers are specified.  You will only change your mail exchangers if you change the A and AAAA records for those servers. Nowhere in there does www appear; the subdomain is not special.  Without an MX record, the base DNS name of your domain (eg. without the www) will be used as the mail exchanger. What do I put where for each scenario? If I put 198.23.52.86 in the 'www' host record, will I lose the old email servers' connection to the domain?  An MX record, if present for a name, indicates which DNS names route mail for the domain, and gives a priority list for their use.  It is optional, and if not present the A or AAAA record for the domain will be used. I know that I need to change the www host to point to the IP address of the new host, but I am not sure if I have the right IP. Additionally, I don't know how to change only the email, or what to put in the MX and/or records if I want to keep email at the old servers while moving the site. The new host provides the following information: If you want to have proper control over which mailserver is used, you should create an additional subdomain (eg. mail), give it the IP of your mailserver, and create an MX record (with priority 10 and that domain, like IN MX 10 mail.your.domain.net.  In that weird GUI you posted you can just give that DNS name and the priority. A good explanation on DNS Records, in fact my go-to reference, is Zytrax. Do a Google search for ""zytrax dns"" (without quotes). Now, in your situation, I assume you're changing webhosting? Then what you have to do is to contact your DNS registrant (i.e., the company where you bought your domain name from), and ask them to change the ""NS"" record for your domain to point to the two DNS servers as provided by your new hosting provider (50webs). That's it. Other configuration will be performed by 50webs. Most likely they will re-set your records (A for www, and MX for @) to some servers within 50webs. tldr; basically 50webs is asking you to delegate (not ""hand over""; you're still the domain's owner) management of all subdomains of your domain tothem",4
"In my eyes it is very unlikely that your live system with at least a two-way mirror and three offsite disks die at the same time. If one of the backups is still usable, you would have only lost the data of a week, which is much less severe (and can be mitigated by e.g. differential online backup of only newly modified files). The question is what I should put on the removable drives. Initially I thought of putting a zpool on each and using zpool send and zpool recv to update them, like here. I do however think that it is quite possible that the drives are ""damaged"" once I need to put back the backup and I would like ZFS to take care of that. Ideally all three removable drives would be in a mirror configuration up to two drive swaps ago, which will allow ZFS to use all of them to fix problems in the old data when restoring the backup. The drives will however never be connected at the same time, so this might not be possible. Does ZFS provide a means to implement what I am after, or should I use a different method? Ideally, I would have three removable hard drives of which one would sit near the server and be connected weekly or so. The others would remain off-site and they'll be rotated every now and then. I am aware that this poses a risk to new data, but I'm planning to back this up in an alternate way. I don't know your exact hardware and performance needs, but be advised that decent systems can be quite cheap if you know what you are looking for and can accept some slight downsides. Specifically: If you go for the second solution, be sure to also enable regular data snapshots to guard against things like encryption malware (which would be mitigated somewhat by having multiple disks). Also, be sure to still distribute the three disks and store them in different places to guard against fire/theft/loss. So, in summary: It depends what is more critical to your needs - multiple independent points in time, or a single one that is more resilient against failure. I am currently in the process of setting up a decent home NAS. The main concern is integrity of the data and I therefore decided on using ZFS as a filesystem. Unfortunately this does put some requirements (or recommendations to be more precise) on the hardware that make it a rather expensive solution. This prohibits me from implementing an off-site backup on a similar system and I am therefore looking for an alternative solution. But assuming it will happen, let's compare all possible backup configurations (each time assuming your live system has 2 disks as mirrors and all 5 disks are of equal size):",2
"The purpose of having two PSU is not really to protect against one of the PSU failing. It is to protect against one of the two power sources failing. And you lose all of that if both inputs are connected to the same power source. It is also possible for a failing PSU to take down everything connected to the UPS, though in principle that can be prevented if the UPS has separate fuses on each output. Connecting both PSU to the same UPS can keep the machine running in case one of the PSU fails. It does however depend on the exact circumstances of the failure. It is possible for a PSU to fail in a way that takes the machine down even if the other PSU is still in operation. So connecting both inputs to the same power source will lose you almost all of the benefits of having two PSU. If you only have access to a single UPS it will be slightly better for reliability to connect one PSU to the UPS and connect the other PSU directly to mains bypassing the UPS. If you can get a surge protector for the PSU connected directly to mains, that will be better.",1
"My best guess from the information you've provided is that you've overloaded (i.e. drawing too much current) that individual GPIO pin, OR you've exceeded the cumulative current limit for all GPIO pins. It's my understanding that the max. current all GPIO pins can source is limited to 50mA as this is the limit for the RPi's 3.3V supply. Assuming that's true, your problem may be due to loading on other GPIO pins, or if you're using the RPi's 3.3V supply to power external circuitry.   The fallout from The Foundation's paucity of hardware documentation is that ""the community"" has been motivated to conjur the missing specifications and details from observation, experimentation, comparison and yes, some speculation and guesswork. The GPIO Electrical Specifications is one source of information and documentation, there's also this on GitHub, and there's the Everything You Wanted to Know About GPIO But Were Afraid to Ask website. You may refer to these sources for the missing hardware documentation on Raspberry Pi. To flash the SleepyPi (ATMEGA328P) I have to pulldown the GPIO 22 from 3,5V to trigger the reset line for the bootloader to accept my code on the UART bus of the Raspberry Pi. TLDR; Under little load the GPIO 22 reaches 3,5v whereas under more load it only reaches ~2,4V, which is not enough to trigger the reset-line on an ATMEGA328P. UART GPIO Pins still reach 3,3V in both cases.  When I run the same code within a the larger Architecture of the my IOT Sensor (which obviously does a lot of other stuff), the GPIO unfortunately does not reach 3,3V but just 2,4V so the reset line doesn't trigger and I can't flash. You may have noticed that I used the term ""It's my understanding ..."" in my answer. You may not be aware of this, but the Raspberry Pi Foundation, a UK ""charity"", does not publish specifications or schematics for the GPIO - nor in fact for much of the hardware. If it strikes you as odd that a charity would produce and promote the RPi to hobbyists as a ""learning device"", and then withhold most of the hardware technical details as proprietary, then welcome to the club!  The code I use to do this is detailed here, although I am pretty sure it is not a code related problem: I'm not going to read through that code - leaving that to you :) It's possible to change the drive levels/pullups, so you could look for that in the code.  When I run the flashing standlone (-> e.g. the code in the Github) I works flawlessly and the Voltages of the pins looks like this: I use the Raspberry Pi as an IOT Sensor with an Arduino(-ish) that controls the power supply (specifically the board I use is the SleepyPi 2). And so it seems likely that your ""larger Architecture"" is drawing too much current from the RPi's 3.3V supply. Fix that, and I believe you'll fix your issue.",2
"$kD$-trees have a different advantage over quadtrees, in that they are guaranteed to have at most logarithmic depth, which also contributes to the time for a nearest neighbor query. But  the depth of a quadtree is at most the number of bits of precision of the input which is generally not large, and there are theoretical methods for controlling the depth to be essentially logarithmic (see the skip quadtree data structure). The cells in a $kD$-tree can have high aspect ratio, whereas octree cells are guaranteed to be cubical. Since this is a theory board, I'll give you the theoretical reason why high aspect ratio is a problem: it makes it impossible to use volume bounds to control the number of cells that you have to examine when solving approximate nearest neighbor queries. In more detail: if you ask for an $\epsilon$-approximate nearest neighbor to a query point $q$, and the actual nearest neighbor is at distance $d$, you typically end up with a search that examines every data structure cell that reaches from the inside to the outside of an annulus or annular shell with inner radius $d$ and outer radius $(1+\epsilon)d$. If the cells have bounded aspect ratio, as they are in a quadtree, then there can be at most $1/\epsilon^{d-1}$ such cells, and you can prove good bounds on the time for the query. If the aspect ratio is not bounded, as in a $kD$-tree, these bounds do not apply.",1
"It's possible. Consider KNN. When you train a model, it essentially ""remembers"" the training set. Now, when you use the model to predict an unbalanced data set, the model simply read off from the memory, so there's no problem. Or to make it more concrete, is it possible in general to train a model with a balanced training set so that we can effectively predict an imbalanced prediction set? Or both should be generarly either balanced or imbalanced? For example, imagine you train a model with an uniformly distributed age group from 10 to 80. Now you have a test set where everybody is 70-80 years old. KNN simply find out the neighbors for your old people (say 60-80 old people in your training data) and then compute a weighted average. The answer to this question is very related to the practical problem you are working with. I spent three years modifying SMOTE algorithm to use it with one problem and the next two years trying to design a biased classifier to deal with another problem. In fact, in academia we care a lot about average accuracy comparing to overall accuracy. Here it comes the point that you should deal with the imbalance data problem. But in practical, in many cases you will find that they care about overall accuracy and not average accuracy. When the cost of missing any of the samples is the same then it makes since. If you do not have any consideration,then the question is: can I maximize both average and overall accuracy?  In some cases, they cost of missing the minority samples is higher, then you need to design a biased classifier. Class balancing is necessary when the loss function you are minimising during training is not the same as the metric you are using for evaluation. The answer to the question: depends on the choice of loss function and measure of ""effectiveness"" (the evaluation metric). If both classes are given equal importance in evaluation (eg. ROC AUC), and not in the loss function, then balancing will lead to increased performance. If balancing the data can maximize both of average and overall accuracy, it is better to do it. If balancing the data can not maximize both of them , then you need to think about the problem requirements  So assume hereafter we use any of those solutions and then we train an algorithm with the new generated data set. Will this trained algorithm be useful to predict further data from this system which is in general imbalanced? One of the methods to address a classification predictive analysis on an imbalanced set consist on undersample the majority class (others approaches consist on: undersample the majority class, synthesize new minority classes...). Using a balance training set to predict a imbalanced test set is not super challenging. The other way around is much more challenging.",4
"You need to write a script in any language you know/prefer to count the number of files in your directory. In this script, you can return specific exit status based on whether you want to consider the current state as OK, WARNING, or CRITICAL in nagios terms. You can use the nrpe daemon (on the nagios server) to initiate a command on the remote host. In your Nagios services.cfg you might add a stanza which looks like this (change the admins contact group to a real contact group on your system.. or leave commented out?): Then, on my.windows.host you would have to find the nrpe.cfg file and add a line something like this (note: this is from a linux client but the configuration for windows is very similar) It is these exit coes which trigger the alert state in nagios to signal to the server somehting is amiss. It is in this batch file where you could do something like: The file check_david_files would need to be some kind of executable (batch file works apparently) which prints either ""OK', ""WARNING"", ""CRITICAL"" or ""UNKNOWN"" and then exits with the corresponding exit code:",2
"Much appreciated if somebody can throw good insight on it. Basically I am confused with the fact that action=""user.jsp"" will lead a call to server and HTTP/browser can cache web pages. Now, since I have action=""user.jsp"" so on submitting the form my web browser will send a request to server to get user.jsp. Please correct me if I am wrong. Or will be taken from HTTP cache. But lets say through some Apache setting (I have read somewhere that it is possible but don't know how to do it) I have disabled the HTTP caching of web page then user.jsp will be downloaded from server. I have read about SPA (single page application) and learned that biggest advantage of those is that save network traffic because SPA downloads all (at least most of them) application resources when loading the page. But I am not clear on this - suppose in my index.jsp I have specified all my resources and downloaded when loading index.jsp. Now my application navigation starts from index.jsp, so for navigation I submit my form and which has action=""user.jsp""",1
"Our MX record points to a frontend SMTP server, which contains aliases for actually routing the mail.  No alias, no access to the backend storage server, which is what our clients connect to. See http://www.postfix.org/ldap_table.5.html and http://www.postfix.org/VIRTUAL_README.html for more information (and what you need to put in ldap_virtual.cf). I'm upgrading the backend email server.  Currently, a user is created for every email user on the server, which creates the mailbox.  On the new server, everything autheticates through PAM to an LDAP server (all of which is working properly).  My goal is to get Postfix to create the Maildir directory for the user automatically.  This works fine when I have the /home directory with 777 permissions, but for obvious reasons, this should be avoided.  I would like to do this with 775 permissions on /home with a group owner of whatever user Postfix is running as, but I can't seem to figure out what user to use.  With the 777 permissions, the /home/$user/Maildir directory is created on message delivery.  Does anybody know how I can do this without 777 permissions? Add your domains to virtual_mailbox_domains. Create a virtual_mailbox_maps map which takes in the username as input and returns username/Maildir as output. I've been beating my head against a wall for a while now on this one.  Basically, here is the rundown: In LDAP, create your mailbox path relative to $HOME for vmail (so the maildir becomes /home/vmail/user/Maildir/). The answer to this varies depending on how your Postfix system is configured. You mention pam and LDAP, so I assume you have all your domains in $mydestination, or you use only one domain (example.com, for instance) You create a single system user, without login privileges. I shall assume a username and group of vmail:vmail here, but the specific name is not important. The general permissions for /home are 755 with owner and group both root, but the user can write within ~user. Hence Postfix will happily deliver to ~user/Maildir/, but will not be able to create ~user. Using virtual users will sidestep this issue by granting access to all mailboxes to a single uid/gid, which is then accessed only by your pop3/imap software. FWIW, Postfix runs the local delivery process as the uid/gid of the system user you are delivering to.  It may be easier for you to configure virtual_mailbox_domains with LDAP maps instead of mysql/pgsql maps (the logic is exactly the same though).",2
"To disable gdm in Ubuntu 9.10 and 10.04 rename /etc/init/gdm.conf to /etc/init/gdm.disabled. In Ubuntu 9.04 it's /etc/event.d/gdm.conf This should remove the entire GUI subsystem -- Xorg, Gnome, GDM, etc.  It should leave you with the ubuntu-minimal and ubuntu-standard packages and their dependencies (to be sure, you can pin them in Aptitude).  These are the base packages that all Ubuntu versions use (Server, Ubuntu, Kubuntu, Xubuntu, etc). Im running a virtual linux server on a low-end system, and i wold like to configure the server to boot in terminal window and not to the Gnome log in window, this is because of the low specs on the hardware. If you're intending to run Ubuntu as a server only, you should have a look at the Ubuntu Server Edition, which would eliminate all desktop-related processes and free up some resources. Mainly you need to remove the ubuntu-desktop meta-package and let Aptitude remove all its dependencies.  I think you can do this with: Why did you install Gnome in the first place if you want a headless server? Anyway, maybe this little guide on how do deactivate the GUI helps you. Basically you use rcconf stop gdm and X.org from starting. Once you've done that, fire up Aptitude to install the packages you want on your server.  If you want a lighter GUI, install Xubuntu (XFCE instead of Gnome, meta-package xubuntu-desktop) or Lubuntu (LXDE instead of XFCE, meta-package lubuntu-desktop). The Ubuntu Server Edition is probably easier to use to create a fresh install of a headless server.  But if you want to avoid reinstalling, use Aptitude to purge your desktop packages will get you there too.",5
"You could also try replacing SYSLINUX 3.86 with SYSLINUX 4.02; it is a bit tedious and you'll have to do it all again when GParted updates itself unless they upgrade to SYSLINUX 4.x. Leave a comment here and I'll write up some further instructions for doing that if you're interested. In my tests and tries I had in the last 6 months, I found that some USB sticks did not work, until I actually deleted all partitions from the device. That was done using GParted or simply fdisk. I think this is related to some partition table that GRUB could not find, or something like that. I found that for a stick that never worked in the past, manually deleting all partitions on the device, rewriting the partition table fixed the problem. I then could install Ubuntu on the device, making sure that GRUB is indeed installed on  the right device, and it worked just fine.  For a not-so-quick-and-less-likely-to-work-but-potentially-cleaner workaround, try manually telling SYSLINUX where to look by doing the following (assuming you're using Windows) - it has never let me down yet and it gets a more specific configuration for the major distributions.... IMHO Unetboot simply guesses what would be done while the universal installer has specific methods built-in. When you get that message, it means SYSLINUX can't find the configuration file, syslinux.cfg (or it is indeed not written properly - unlikely in your case). For GPartedLive, it is located in /syslinux. It looks like it's found something on the USB stick and then something is wrong. From a quick google some things that might help are: A quick workaround is to copy the contents of /syslinux (everything in it) to the root of your USB drive. SYSLINUX will always look in the root for syslinux.cfg. SYSLINUX 3.86 also seems to have some bugs with its ability to find the configuration file when it is nested inside directories - SYSLINUX 4.0 seems a lot more reliable and resilient.",4
"In the long run, a better option would be for you to switch to modern, shader-based OpenGL and use Vertex Buffer Objects instead. Performance will be much better and the restrictions of Display Lists no longer apply. You are using OpenGL Display Lists, which is a deprecated feature. One of the reasons they were dropped from the standard is that once created, a list cannot be modified. In your current setup, you have only two choices: 1) recreate the whole list every time something is changed; 2) just use direct immediate-mode drawing. I am currently working on a game and I want to know if there is any way of handling with the elements i am drawing . For example : if i draw in a loop 100 cubes , how can i show / hide the cube number 15 or 63 or n ... clicking  I thought that initializing elements in a list would work , but i didn't find any property of it that could help. I googled a while for deleting elements after displaying them , like in minecraft , but i couldn't find anything useful(e.g: When mouse - right click -> then create cube , and when mouse - left click  -> then delete the cube.. ) Some sugestions would be all i need ..  ...Thank you.",2
"Can you block them entirely? No. Spammers understand a new domain name has no history to fall back on. It's easy to set up a new domain like this, set up a server, mass phish and reap the rewards (even just 1% of the phished falling for this can be fruitful). Eventually systems on the Internet catch on, but by then the damage is done. Education is your best weapon. It could of course be a canary... Designed to call home and leave a trail of breadcrumbs back to your front door which may not be the same place you host your web site. This could then be used for a later, more targeted, approach or refined password variable dictionary attack. None of these are 100% perfect indicators of scams, but each should raise a small flag - and the presence of multiple should make you highly suspicious. If in doubt, verify the organisation is legitimate, look up a contact email address for them (not the address the email came from, usually), and use that to ask if the email is legitimate. If it is, fine - if it's not, you've done them a favour. This is spam at the least - at worst, it's a scam. Do not agree to send a read receipt. Do not download unnecessary content. Do not click links. Do not reply. Do not pass Go... etc. As others have mentioned, protecting your contact details in whois information may help eliminate these emails; I'd also like to add some common signs of spam/scam emails: There is no real way to avoid this crap, but you have your full contact information including mail in the domain record - this makes this extra easy ... I had the same email today, hence how I found this chain, and followed the crumbs the other way and checked who had been knocking on my websites door previous to the email - which lead me to Russia, Costa Rica and Panama (all IP ranges now blocked.)",4
"The most common way I've seen this done is via a second render pass on your model. Essentially, duplicate it and flip the normals, and shove that into a vertex shader. In the shader, scale each vertex along its normal. In the pixel/fragment shader, draw black. That'll give you both external and internal outlines, like around lips, eyes, etc. This is actually a fairly cheap draw call, if nothing else it's generally cheaper than post processing the line, depending on the number of models and their complexity. Guilty Gear Xrd uses this method because it's easy to control the thickness of the line via vertex color.  The second way of doing inner lines I learned from the same game. In your UV map, align your texture along the u or v axis, particularly in areas where you want an inner line. Draw a black line along either axis, and move your UV coordinates into or out of that line to create the inner line. One of a great ways of doing it is to render your scene on a Framebuffer texture, to then render that texture while doing a Sobel Filtering on every pixel, which is an easy technique for edge detection. This way you can not only make the scene pixelated (setting a low resolution to the Framebuffer texture), but also have access to every pixel values to make Sobel work.",2
"Since the Pi's other IP 192.168.1.30 is within Switch 2's subnet, by default traffic for this IP will be routed to Switch 2 causing connections to fail. Note: This setup does not allow a host on one VLAN to talk through your PI to a host on another VLAN.  That would be IP Routing and is a whole-separate question. The OS will see eth0 as the physical interface, and each VLAN will appear in the format of eth0.90 for VLAN ID number 90. More info on VLANs: https://www.reddit.com/r/homelab/comments/a94oka/learn_with_me_ep1_introduction_vlans_pfsense/ https://help.ubnt.com/hc/en-us/articles/222183968-Intro-to-Networking-Introduction-to-Virtual-LANs-VLANs-and-Tagging And this can be accomplished WITHOUT A VLAN, only using a static route. The below example is tested and known to work correctly.  If you have a couple of small switches and a Pi, you too can actually try this out quite easily. Having multiple interfaces on a host addressed within the same subnet can cause a networking issue called ""ARP Flux"" and is to be avoided. There are several ways we can address our single eth0 interface, discussed after next section ""ROUTING"" The downside is that you will need a managed switch that understands VLANs, or a direct ethernet to a firewall/router device that also understands VLANs.  A home-grade route will not be adequate, you'd need a firewall like pfsense or myriad of other soho devices. Switch 1 has a direct connection to router which routes all traffic for switch 1's subnet 192.168.1.0/28 to it. Routers route traffic from MOST specific to LEAST specific route: a static route pointing to a host will be used before one pointing to a subnet. The above host has 3 networks on the one port.  Untagged (IE normal LAN) is in the zeroconf range.   There are VLAN IDs 10 and 20, coinciding with the third octet of the IPv4 address for neatness.   So not only is it POSSIBLE to join different interfaces to different subnets, it's IMPLIED when multi-homing a host not to address multiple interfaces within the same subnet.  And as this example demonstrates, you do NOT require (2) nics each with a different MAC address.  Using (2) different /28 subnets- sensibly sized for a pair of 8 port switches- our specimen networking is organized as follows: Note we are NOT using router's interface to Switch 1 192.168.1.2, but IP of Pi within routed subnet of Switch 1. Since 192.168.1.14 is within Switch 1's routed subnet, no static route is required for this address. Once static route added, host can be accessed on either address:  192.168.1.14 (subnet 1) and 192.168.1.30 (subnet 2) This will give you anything up to 4096 different logical network interfaces which function the same as physical interfaces, from the point of view of the OS/kernel and software.",2
"Controlling territory, killing turrets, killing players, etc. Some or all of these could generate differing numbers of victory points. Collect enough victory points, and you win. Points that are further from the center get captured faster. The final point is captured in 2 or so seconds, compared to the 30 it takes to get the middle one. Consider something similar! Towers have more health if you control more of the map, meaning that having more map control makes defending a little easier, and speeding up the game once one team has obviously been put in a losing spot. I offer Team Fortress 2's design choice for the analogous mode, capture point; there are 5 points on a symmetrical map, with 1 in the middle. Each team starts controlling the two closer to their base, with the middle one neutral. You capture points by standing on them without enemies standing there. The points are in a line, it's a tug-of-war kind of deal, and you can only capture the enemy's point closest to you. A quite typical solution for ""control"" maps is this: game win condition is achieving a certain amount of points, with points being received regularly based on the number of towers/amount of territory under control. Optionally, if you make the relation between towers controlled and points gained non-linear, you can tweak the unpredictability of the end result as you see fit. Your spawn points move up as you capture points to ensure the team with advantage can actually reach the fight in a timely manner, but the losing team always spawns closer to the main ""conflict zone"". The final capture point is right next to their spawn. How is this gamestate resolved? You could allow players on the winning side to respawn in regions close to the remaining towers to be captured. This would allow the winning side to also enjoy the benefit of having all of its players spawn in close to the action and would eliminate the frustration of having to trek across the map, unable to do anything important, between respawns. This prevents any imbalance-ing that your suggestions might have. It also makes it so that the defenders don't crowd around a single objective to fortify, they have to split up. That gives the attackers the chance to form strategy, i.e. if each team has 10 tanks the defenders will need to split up 5-5 on the last two remaining sectors, so the attacking team can see this and split 7-3 on some. When that tower is destroyed, the defenders don't instantly spawn on the other objective to defend it, the attackers can make a push there while the defenders are en-route to defend. That system is more like Battlefield 1's multiplayer mode Rush, which is probably a good indication of it being a good system; if a major publisher uses it in a game that is very critically acclaimed, chances are it's at least usable. The only difference is in Rush there is 1 attacking team and 1 defending team, and the attackers have a limited number of lives to destroy all objectives. That provides a time limit without actually having a temporal time limit, which I usually don't like, but may be necessary to stop a game. You could designate the top left/bottom right corners as spawn zones, with no towers. That way, tanks that re-spawn need to travel towards towers to defend, giving the other team an opportunity to get some shots in on the tower before the defenders reach them. If a simple ""point"" game is too boring for you, you can make those points a sort of a currency, rather than the win condition. Make it so you can activate a powerful (but temporary and one-shot) power-up or ability for a certain amount of points. Then, on the one hand, the weaker side still generally enjoys numerical superiority to make it easier to push back and return in the game. On the other hand, the winning team can get the momentum required to finish off tightly respawning enemies. Additionally, or alternately you could make it so there is a no man's land between the guaranteed spawning locations and the spawning locations you earn by securing objectives. You could award points only for team kills (or devise other methods not related to tower destruction, like killing the last enemy inside of a square to 'clear the zone' or something), to give the game a multi-faceted approach for winning. Players will have to devise strategy. It won't make players too cautious, either, because they'll need points to win if they don't capture all the objectives!",5
"When the code wraps to the next line, it becomes unreadable and incomprehensible - not to mention that which line is which gets muddled. That said, :set number to turn on line numbers and :syntax on to allow highlighting will make vim a bit more friendly. Also, learn text objects. I read a book about vim (Learning the vi editor) from O'Reilly, and that was all I needed.  A couple of things that I got out of it that I use all the time... Everyone's saying that they avoid using a .vimrc because they're on different servers all the time. I've never been on a server that didn't have wget installed. Put your .vimrc on github, and it's as simple as: 2) Splitting windows.  Type in ':sp' (horizontal split) or ':vsp' (vertical split).  It'll split the window.  You can then move between windows with Ctrl+w [hjkl] to move to the right, down, up, or left, respectively.  ZZ or :quit to close the window.  :e  to open another file.  Especially useful for diffs (vertical split) or whatever else (copying / pasting for the easy). I feel that the power of vim for a sysadmin is that it's everywhere and you can instantly just start using it. That's why I have choosen not to customize my vim. This probably isn't going to be the prevailing opinion, but I use vanilla vim. I don't use any special features (other than syntax highlighting and line numbering), but it's great, because I can go to any Unix machine and feel right at home with whatever vi they happen to be running.  1) storing macros.  Hit the 'q' key, then the key you want to bind it to, then do the command, and hit 'q' again.  Then, if you hit '@key' it'll execute that command.  If you hit '@@', it'll execute the last command.  I love this for ""joining every other line"" sometimes:  your macro is '^Jj' (go to beginning of line, Join the line, move to the next one).  Then, do '@j' (j is my macro key for this).  Then, just hold down the '@' key. It'll join every line with the next one, move down, do it again, etc. 3) If you code, folding is fantastic.  Folding, basically, closes or opens segments between markers.  Determining the markers can be done manually, by regex, by... whatever.  Google ""folding in vim"" and you should get some good tutorials.",5
"Have all changes accepted, switch off tracking changes and remove hidden data and personal information by inspecting document (Click on File, Info, Check for Issues). The ONLY way to get rid of the track changes is; on the right-hand side of the document click onto the change, then a drop-down will appear, then choose delete changes. You have to sadly do this to all/ever highlighted change listed in the document. Then save the document as a test first, all ok then safe as Master Document. IT works 100% Neale The easiest way is to copy the whole text to a brand new Word document and then save it as a new file. This remains an issue 5 years later in Word 2013. The answer commonly given is, ""you must accept the changes"". My preferred solution, however, is to use Word's Document_Open event to change the initial view setting as desired: Have you ensured you 'Accpeted Changes' for these existing docs?  I had the same problem as I kept opening a doc where I turned off Track Changes, only to have it re-appear.  If you accept those changes the Track Changes setting you select will take place.  If you do not, the setting that was put in place before will persist each time you open the document. It happens because once tracking is turned on in a document you cant turn it off - you can only accept the changes but they are still there",5
"Was this value chosen randomly like explained in the website? If not and the value is not chosen randomly then how is it calculated? I don't think this is the case with any optimized way of creating a decision tree. In this image(different example) the value is 2.45 for the root node:  Consider the above picture, Here the outlook is chosen as Root node, And how is outlook chosen as root node? First, We calculate the total entropy of the data. Lets say its 0.95. Now inorder the pick the right root node, We will find weighted averages of all the subcategories. There are 4 four categories here, So we will obtain 4 weighted entropy averages. Lets say they are 0.3, 0.2, 0.4, 0.8. Now we will subtract the induvidual weighted entropy averages from the total entropy. So we will get (0.95-0.3), (0.95-0.2), (0.95-0.4), (0.95-0.8). Among all the three which ever category has the highest value that category will be chosen as the root node. These 4 values are the information gain of each of the categories i.e Whichever category has the highest information gain, we will pick it as the root node. In our case, its the outlook category/feature .Hope it helps Values chosen at the node level to split the data are determined to minimize the Gini Impurity index which represents the entropy or the chaos in your data. It chooses the value that separates your classes best. According to a website (:http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/) , these values are chosen randomly:  You have a population of 10 people. Only variable available is the age. You're predicting if the person has a certain disease. After doing some EDA , you notice that between 20-40 , both your classes , again let's assume its binary classification , are similarly present. and Exactly at the age 56 and going up, you get 4 ones and zero. Meanwhile, below the 56, you have equally distributed classes across that range. In simpler terms, you tree when splitting a population in a node, will try to make the leaves the purest they can be, and by purest, i mean containing only one class. That's how the value is decided. No, I don't think values according to which the branches are seperated are chosen at random. Instead, weighted average is calculated for each category and the category with the highest weighted average is chosen as the root node. This is also referred as Information gain You tree when considering which value to split the node on will calculate the Information Gain or Gini Impurity, and then splits your population across 2 leafs, where the entropy is minimized in those 2 leafs as much as possible. Again you will have a leaf with 4 ones a zero and the other will have, lets say, 3 zeros and 2 ones.",3
"Now in the t_Transaction table I have a Credit Account and Debit Account with 1 Amount field. My question is what is the best approach for managing the amount part? I'm developing a simple double entry accounting system for managing the payments, charges, cars expenses, etc. for car rental companies. I'm done (Development and Database wise) with the Main operation system for managing cars, customers, contracts and so on and now I'm stuck in the accounting part.I have come up with the below DB design for managing the accounting part but I'm still not 100% sure if I can use it and consider it as a go. 2- Or should I have 2 separate fields (a. Debit Amount) and (b. Credit Amount) and then based on the transaction 1 field will have the actual amount while the other will have NULL value. 1- Is it better to be logged into 1 field in the database? but then how can I distinguish weather the amount is a debit or a credit while generating reports. I'm still confused on how to develop the code for logging the transactions into the database as this is my first time development for an accounting application",1
"I'm currently seeing 150MB/s performance with my SSD and LUKS.  That is a 50% loss since without LUKS my SSD will get 300MB/s read performance. Just a tip: If we would talk about fast NVMe (3000MiB/s write) the drop down is the same, you get a few megabytes per second writes when CPU is being used intensibly. Another Tip: AES is broken and has back-doors, worst if it is the hardware builtin ""Intel"" processors AES (or also worst disk drive internal AES), to be safe do not use AES for LUKS, neither use hard disk drives (HDD and/or SSD) with built-in hardware encryption, if you do not use ATA-passwords on them any one malicious can launch a fast command (in less than 0.1s) to it and activate a ATA-password change (from empty to non empty) and on next power down your data is hijacked, you can not access that disk without that ATA-password. Our laptops and portable drives must be LUKS encrypted.  LUKS doesn't really seem to cause much performance loss on normal laptop hard drives (7200RPM) for us.  With the crazy data rates you get with SSDs, is that still true? Yes the CPU usage of the encryption will go up, however unless the machine is otherwise CPU bound I'd expect IO to still be the bottlenect as most current machines should be able to exceed 250MB/sec of AES (that being the SATA-3g bandwidth) I agree with Anonymous, i had done a test on a 64GiB RAM Ryzen 7 2700x (16 threads on 8 cores) and when Rendering (CPU is max used) the I/O on the LUKs SSD drops down a lot. Speed can go down to 15MiB/s (cascade algorithms like Serpenter and Twofish)... while two SSD in Linux Software Raid 0 can get more than 950MiB/s. Most if using not AES, using cascade (LUKS over LUKS) with algorithms that must run on the CPU while no CPU is free to be used since it is been used on Rendering. I searched around a little, but I didn't see any actual comparisons between LUKS vs. non-LUKS on SSDs. It is a Laptop where i put two Samsumng SSD 740 EVO, one in normal 2.5 slot, the other on a caddy where the dvd was... 8GIB ram and a processor quite old without AES and only 3GHz with Dual core. Identical situation for fast transcoding MPEG2 (DVD VOBs) to MP4, cpu is maxed out so there is no CPU free for LUKS.",5
"(The above reflects my personal experience, and might not be valid for all computer/mouse/driver combinations.) I just remark that a PS2 mouse is even more responsive, but very few computers today have the outlet. I think the underlying issue may be more related to Wifi interference though, as the unifying receiver transmits in the 2.4GHz frequency range used by 802.11B. moving the USB wireless dongle from the back of my PC to the USB hub I have at the other end of my desk worked for me. It's also possible that it's a power-related issue.  Disk access causes a spike in power use, potentially leaving peripheral devices (like a wireless USB mouse) with less power.  But this is very unlikely; unless you have a lot of other power-hungry peripherals and/or an undersized power supply, it would tend to point to a faulty power supply. Even though the receiver was only 2.5 feet from the mouse I was getting a lot of interference and jittering. Moving the receiver to my keyboard USB hub, which is about 0.5 feet away, fixed the problem entirely. You might have your hard drive in PIO mode. I say this because one of my friends had his hard drive in PIO mode and his mouse lagged all the time. I had a very similar issue with a Logitech Performance MX wireless mouse, which uses Logitech's Unifying Receiver. Take a look at the Windows Device Manager.  Look for any red flags/indicators on any of the device icons.  Pay particular attention to the disk drives and controllers.  A flag here could indicate a resource conflict or a driver problem.  I'm not sure whether this is still the case with Windows 7, but in earlier Windows versions, when a 32-bit disk driver somehow failed, Windows would revert to a 16-bit generic driver and the result was terrible performance and weird effects such as you describe.  The solution may be a matter of finding and installing the correct disk system driver for the affected component, which might be the motherboard.",5
"After 5 to 6 hard resets of my machine Windows would boot up quickly and properly with no errors. Some daysit would boot on the first try, and sometimes it would take up to 10 reboots. I scanned the main Windows partition on the SSD with the ""right click->scan drive for errors"" feature in Windows and it found no errors. I was tempted to say it is a problem with the SSD, but it is strange that my machine had the same trouble booting off of the USB stick as well.  Now, after a brand new Windows 10 install on my formatted SSD, I am having the same issue - Windows still refuses to start properly, instead going into ""startup repair preparation"" about every 5 out of 6 times. After reinstalling Windows and continuing to have boot issues, I updated my BIOS, which looks to have resolved my boot issues. I tried to reinstall Windows 10 from a USB stick, but even the USB stick would be stuck on a ""never-ending startup"" until after 4 or 5 reboots, at which point the Windows 10 installer ran properly and without any issues. Recently, when I boot my machine, Windows 10 would get stuck on a ""Preparing for startup repair"" screen. This screen would stay there forever, with the ""spinning loading dots"" animation.",1
"I've done this with a Linux raid-5 array, and never had any trouble from it sync-wise.  There was a noticeable lag when the drives would spin up, about 45 seconds or so.  Power-wise, though, I've decided it wasn't really worth it.  The server consumed 90 watts when the drives were spinning (it was build with power consumption in mind), and 70 when they were spun down.  That difference didn't justify the wait time from spinning them back up. I don't know what kind of power savings you'll get - I spun down disks to cut down on noise.  I wasn't using the best disks... I've done this before (5+ years ago) and never saw any problems with spinning down the disks.  Unless the disk doesn't spin back up, it should be ok.  Your access times will suffer though.  Pick a good spin-down time - something that will keep the disks spun up during the time you'll use them.  I chose 15 minutes.  It was generally good during use, and when I stopped using the disks (overnight for example) it spun down soon enough.",2
"I'd recommend Microsoft ISA Server 2006. Specifically for this requirement, it will limit to 600 HTTP requests/min per IP by default and you can apply an exception for Jon Skeet (sorry, I realise that ""joke"" has been made already!). You can even perform rudimentary rate-limiting with any regular Cisco router of any decent capacity/vintage.  Are you using a Cisco router? We use Foundry load-balancers (specifically SI850s) to handle this kind of shaping issue, it also handles quite a lot of other 'nastys' like SYN-floods etc. Might be a bit overkill for you guys though. We use a Watchguard firewall (ours is a X1000 which is end-of life now). They have many feautres revolving around blocking domains or ips who are seen time and time again or are using an obsesive amount of bandwidth.  You have the additional benefits of application-level filtering, the ability to load-balance across multiple webservers (instead of NLB on those servers), VPN termination etc. There's a number of commercial extensions available and you can even write your own ISAPI filter if you're feeling brave. This would need some tweaking because you obvisouly would not want to block Jon Skeet on stackoverflow :) If your PIX is running version 7.2 or greater of the OS, or can be upgraded to it, then you can implement QOS policies at the firewall level. In particular this allows you to shape traffic and should allow you to limit the bandwidth used by bots. Cisco have a good gudie to this here.",5
"Usually you just want to have upload folders or autogenerated files to be writable by the www-data user. Anyway, the risk you are opening here is that if your web application has any bug/vulnerability that might allow an attacker to execute code on your server, this code will execute as www-data (the user the apache process is running) and it could completely delete all your websites. In itself, no, if your web server is not configured to honor POST requests and such stuff. However, should any bug in the web server or its client scripts arise that allows to write into the filesystem (and, with the number of PHP scripts around, the probability of this happening is 1), your whole content becomes writeable. So: it's risky, and you should consider other alternatives. It's not a good idea to have write permissions on the entire folder, the most websites (for example: wordpress, joomla and magento) needs write permission on specific folders (image upload, file upload) You shouldn't allow common users access to this directory.  If a user doesn't need to write or execute from that directory they should not be allowed to do so.  At a minimum, I would limit the permissions to 755.  Only root (or a user with root access) and www-data should be able to write to this directory. And the problem to have write permissions on the website root is if someone finds an vulnerability he could use that to write a new index.php file and 'hack' your website. My question is: is this a foolish/risky permission set? Is giving www-data group those permissions opening my server up? A better way is to give write permission on folders and do not allow script (PHP, python) execution, always check if the user is uploading the right content, example, if you website allow an user to upload an image as it avatar, check if it is an image and not a fake image with PHP script inside.",5
"Or you could make it more general by introducing a type Vector2D which could be very similar to Direction enum; it would have public static final fields holding instances pointing north, south, etc: Consider placing your data in an 1D array. All it needs for it is a simple coordinate transformation like (you could also have a method that adds two vectors and this could be cleaner, but would also produce a lot more garbage). Personally, if I wouldn't want to refactor the code too much (because the project is too small or whatever the case may be), I'd go with: On a modern CPU it's good for speed as multiplication is faster than memory indirection. It also makes other operations simpler, e.g., clearing or copying data. The transition function for the game of life is a function from Bool^9 -> Bool. This is a finite and very small function to compute. If you pre-compute it, then you don't need to count anything: Just loop through the board, and replace each cell with the value of the function at that point. If you like this idea, you may enjoy pre-computing Bool^16 -> Bool^4, which is larger, but still small enough to keep in RAM, and then you can go through the board 4 times as fast, filling in a 2x2 matrix of cells at each iteration.  If checking for neighbors is all you need, this will not be a drastic improvement. However, it has the potential of making a lot of API a bit nicer; you could introduce methods with signatures more like like: Make your board slightly bigger so it gets a single tile unused border. Never examine cells on the border. This way you can omit the tests as you never land out of bounds. This makes the code a bit shorter and faster. The answer by rolfl is great, I will just add that what you have here is a nice use-case for enums. If you define an enum like:",4
"For me, choosing reputable hosts and doing regular backups - both of which you seem to be doing already - is about as well as you can do without starting to think about business continuity planning, high-availability setups, SLAs, and so on. The result of this is that I can bring up such a web site from backup in just a few minutes, as opposed to the manual way which could take an hour or more. Also - the budget for this backup should be in your pricing. It needs to be paid. And whatever infrastructure you need.... you need it. It is not ""ridiculous expensive"" then. Small businesses with small budgets, especially nonprofits, typically are not going to be able to afford high availability. The question is, if you have virtually no budget, as is commonly the case in situations like this, what is your restore strategy? I tell people that you get 99% uptime for free (ie, without spending anything extra on high availability).  That's about three and half days downtime a year.  Every extra 9 on that uptime increases the cost by somewhere between three and ten times. You say nothing about the stack, nothing about any budget - so the best and only advice here is to go to some cloud provider and start using their backup mechanisms. But start defining what you actually - need. Remember that uptime is not the same as data integrity. You can have 99.99% uptime and have lost all of your data twice in a year as long as the server was restarted ""soon enough"". Most of the VPS providers are guaranteeing that your server is running, NOT that your data is safe. Your data is Your problem :(. First, for some of them I have an incremental backup and full database dump every six hours. One client was already using CrashPlan Pro so I just used that. Whatever you do, you need to make sure you have a restorable backup. Making the business case for having 2 ""live"" servers is as simple as comparing the potential loss of revenue during a ""recovery from images"" period to the expense of another server. If people aren't ready to pay that kind of money, it is in my opinion a mistake to mislead them into thinking they can get any extra protection of any significance. The complexity of the implementation depends on the application stack, but ideally you'd want to setup a ""hot standby"" (at a different provider), with data being replicated in real-time (or as close to real time) as possible. The answer totally depends on you architecture and requirements. Some time ago 3 discs failed on a server of mine, taking down 20+ vm's when a Raid 6 failed. But: Because this is critical, we had backups - daily for non-important stuff, 15 minutes for databases and emails. Heck, now I added a server that gets replicated to another machine every 30 seconds. What you're looking for is something that will store your backups on a separate server and (IMHO) not even in the same provider. Depending on the data size that you're talking about, a portable hard drive could be used as a third line of offline defence. Backup your data as you have been doing and then regularly copy that (or just the changes if possible) to the portable hard drive or even a local computer. There are also reasonably cheap options like Backblaze for backup solutions, but the price will depend on the amount of data you're talking about. If you can do incremental backups it will be much cheaper than full backups, but incremental backups can be very difficult depending on where the data is stored (flat files = easy, database = not so easy). I have a simple ansible playbook I put together in about an hour (not having previously worked with ansible) that installs nginx, php-fpm and MariaDB and prepares them to host a web site or sites. Running this playbook results in a server (or servers) that are ready to host a typical web application, and I can simply restore the nginx virtual host, application files and database to it.",5
"one another if data in a chunk should be included in new images derived from this image. The idea being will define if it's private or public. Public means it's part of the PNG specification, and private means Does anyone know of a program?  I would download a trial of Fireworks but that section of adobe's website is ""experiencing difficulties"". recommedation appears to be PSD, and to my knowledge that's the only layered format that Fireworks supports. Macromedia/Adobe Fireworks is stored in chunks that are marked as ancillary, private, and possibly unsafe-to-copy. The PNG format consists of signature, and a series of data chunks. The signature lets the program know You will need to download Macromedia/Adobe Fireworks, install it, open the image files, and export them that this is a png file, and chunks store data. Chunks have properties to further define them. Byte 1 While I cannot find any information from Adobe/Macromedia I would assume that the layer data created by the image-software cannot understand the chunk, or has an error it is allowed to skip the chunk. Byte 2 and page data. I would assume this is probably why in the export options it's called a Fireworks PNG. Since the image-editor may not be able to understand the chunk this property lets it know how to use the will define if it's data is critical, or ancillary. Critical means it contains data that is important, display an error message. Ancillary chunks contain data that isn't important to regular presentation, if in a different format that supports layers; it should be a format that GIMP also supports. The common that some data may be dependent on other data in the image, and therefore would not work with a new image. and if the image-software has issues with a critical chunk, it is required by the PNG specification to format. Byte 4 defines if it's safe-to-copy, or unsafe-to-copy. This byte is for image-editors to tell the data is specific to a third party. Byte 3 is just a reserved property for future revisions of the PNG",2
"Once you've got that done, you should be set to start developing. I've written a blog post on how to use github to effectively develop a game, including using their wiki features, issues for bug tracking, and just managing your project. You can find that here: Using Git and Github for Game Development My answer is going to focus on using the proper tools, as if you're serious about making a game in your free time, you hopefully already have the motivation down. :) If you need more examples of GDDs and TDDs, I have mine in Google Doc's, and I wouldn't mind sharing them! Next is being able to well define a backlog of tasks and features that need to get done. The best process for this is up to you, the developer. Some options include creating a Game Design Document (GDD), a Technical Design Document (TDD), and a Feature Backlog. A good sample for a GDD can be found here: http://www.runawaystudios.com/articles/chris_taylor_gdd.asp For the TDD, model it similarly to the GDD, but focus it more around the technical detail of programming. So what language you are going to use, framework, engine, and other technical specifics. Both the GDD and TDD will help you better understand the scope of the project and figure out what the list of to-do is. You can do this in the middle of the project if you've already started it, as it's always important to rescope and know what exactly your goals and features are. I'd recommend using some form of version control for your game. Sites such as Github or Bitbucket provide hosting for git and mercurial (bitbucket support mercurial, not github). This is your first step to organizing your code and assets. I have a ritual of disconnecting my network cable before I go to bed. The next morning - I'm not online, so I have no sites to visit, no emails to read, no people to chat to, etc. So I naturally get straight to work on my game. I don't connect again until I absolutely have to. If there's something I need online - I write it down to look up later, or use my iPhone's browser. Defining soft deadline for your project may or may not help push you to stay on track. Every individual is different, and only you will know how to motivate yourself. Try out different techniques and see what works for you. Game development (and software development) typically take much longer than people initially estimate. So don't get discouraged and stick with it.",2
"What seems to be happening with Windows 10, is that after a month or so, it is automatically cleaning these stale printers up, assuming that they are no longer needed.  I have found that even when it does this, in a few cases I have had to run the devnodeclean tool from Microsoft to clean up shadow copies of printers it tried to remove because it tends to cause the Devices & Printers menu to take up to 10 minutes to load.  The shadow copies show up in Device Manager but do not actually appear in the Devices & Printers screen in that case. We have several remote/mobile users who travel between numerous locations and install local printers specific to those locations.  These users might exceed 50 printers on their system.  With Windows 7, it was never an issue.  The OS left them alone and so when they returned to that location, they were able to simply resume printing.  Additionally, they would send jobs to one of these offline printers so that when they arrived at the site, they would print automatically upon connection.  This enabled them to set these printers up to print which saved them a lot of time, allowing them to focus on other responsibilities. Is there a way to disable this ""feature"" and prevent the cleanup of these printers?  They are offline but there is nothing wrong with them; they just aren't connected for a few weeks.  I have tried finding information on this without any luck.  I also looked through local group policy but I can't find any particular setting that might control cleaning up stale local printers.",1
"If your usage only takes advantage of a few GPU features, you might well run at 100% of the throughput of those features, while leaving half the chip asleep.  But the half that's asleep couldn't be used for that type of work at all. The load factor shows how much more of the same computation could be done, not how much of the chip's total processing capability is being used for that computation. Usually, it means that your CUDA program is suboptimal. I'm now optimizing my CUDA program. I wrote several iterations of it, improving the performance in each iteration. So surprisingly, in each iteration, it was reporting 100% of GPU Load. But power consumption was different in each iteration. In the latest iteration, with the increase of power consumption from 40% to 70%, my program has been improved 7 times (!!!) in terms of the wall time it takes to compute what I need. For example, your 92% shows that on average, the GPU did something during 920,000 out of every 1 million clock cycles.  It doesn't mean that 92% of every single circuit of every single shader processor was active, let alone 92% of every single circuit on the whole board (VRAM controller, DAC, shaders and raster units and branch predictors and texture lookup units and so on). 250 watts is the maximum power consumption for this video card model, not this particular video card. Unless you have the very worst instance of that video card model ever made, you'll never use 250 watts. GPU mostly stalls on memory operations. I optimized for better caching (i.e. less global memory hits), and I got the following changes of sensors: Unfortunately, the source code is proprietary, so I can't give it to you to try yourself. However, you can get some idea of what the bottleneck in my program does: it is a loop with one memory read from an array (ith item), an addition and a multiplication, and an assignment of float. E.g. in the screenshot below, GPU 2's utilization is 92%, while the power usage is 129 watts out of 250. Why isn't the power usage around 250 * 0.92 = 230 watts?",4
"in short, rsync works really, really well.  Any error could better be attributed to your hardware and/or filesystem.  bpbkar wouldn't perform any better facing the same failures. If your backups take 18 hours to run normally, deprioritising them probably isn't going to solve the problem (unless you want to run your backups for a couple of days at a time).  I'd be inclined to setup a disk replication mechanism to another machine (I like DRBD, myself) and then use LVM to take a point-in-time snapshot, backup that, and move on.  Because it's running on a separate machine, (a) it can hammer as hard as it likes without affecting the live app, and (b) it won't be contending with the live app for disk IO, meaning it'll probably run a whole lot faster as well. bpbkar is Veritas Netbackups backup client. It supports throttling, so the combination of normal I/O and backup I/O doesn't saturate your disks. Look at here: We have a system wher we rsync live servers to backup servers (which are built out of cheap 1TB SATA discs) then take full tape backups of the backup servers. It's excellent: You might also want to look into synthetic full backups so you don't need to do as many full backups. Another vote for rsync.  I use it to daily backup 9TB of a very heavy used fileserver.  never had an issue. If you're concerned about 'point in time', create an LVM snapshot, mount, rsync, umount, destroy.  Somewhat higher load on the server, but still far (far!) less time than a full copy. An anectode from testing: when we approached the 8TB limit of ext3, made some 'pull the plug' tests to determine how possible is to corrupt a file by hardware failure while copying.  pulled the plug on the server, the storage boxes, and the SAN wiring.  copied tens of millions of files. One thing I can say for sure: anything you do on the same machine is going to completely bone your disk cache -- as the backup process reads all the data off the disk to be backed up (even if it just checks mtimes rather than reading and checksumming all the files), that's still a lot of metadata blocks running into your cache, and those will be kicking out useful data from the cache and causing more disk IO than is otherwise warranted. If the administrator says that it must positively, absolutely be bpbkar, first do an rsync to a less used system, and then run bpbkar from it.  No need to hog your production system. Get your NetBackup admins to schedule the backups better - do full backups on alternating weeks for each RAID array.  Is there anything stopping you doing full backups at the weekend, as you say the system is mostly busy weekdays, and incremental backups during the week? That'd help you get the backup done during the quiet slot between 2300 and 0900",5
"A common cause for this is a hung ""sudo"" process. If you run sudo and it prompts for your password, but you close the terminal, sudo will hang forever waiting for the password, and this blocks any other logins until you kill it. After installing MySQL 5.1.50 64-bit and running the package that configures MyQL to run at startup, the Terminal app now sporadically display a blank window, like so: Try running jobs at the Terminal to see if that shell has any child processes in the background.  If there is something super heavy running in the background maybe it is causing the shell to become unresponsive? I opened the Activity Monitor and selected to show all processes. I noticed root was running several (> 10) login processes, few sh processes and a sudo process. I force-quitted them all, though some login processes didn't quitprobably the sudo kept them hanging. After this, Terminal worked normally and the excessive login processes I couldn't kill quitted. I managed to get the command prompt back after following the instructions from the MacFixIt column at CNET : OS X Terminal displays a blank window instead of a command prompt I just had the exact same problem, though it occurred after installing Git (or at least that is when I noticed the issue). I think the trick is to look for login and shell related processes which could hang new ones. Probably, in my case, killing the sudo would've been sufficient enough. However, the Terminal will intermittently go blank and I have to repeat the process again and it's driving me nuts. The CNET article cures the symtoms but the cause of this problem is still unknown. Does anyone has any theories or experiences to share in order to fix this annoying problem permanently? In my case, somehow git diff head had got put in the background in one of my shells, so git and less appeared under a shell in which I thought there was just a bash process. When I did fg it fixed the problem. If most of your windows/tabs are just ""login -> bash"" then it should be easy to spot.",5
"(You can see this effect exaggerated if you connect to an OS X system from Windows using the RealVNC client over a higher latency connection. It will take a long time to build up the screen output.) OS X Leopard with its own ScreenSharing client, connecting to another OS X native screen sharing server, does some negotiation on colour depth, etc., so it's transferring less data. I've switched from UltraVNC (back) to RealVNC, primarily because UltraVNC was not handling my multi-monitor server right. I experienced the same thing, but could not get UltraVNC or TightVNC to match the performance I expected. I was tipped of TeamViewer and it worked like a charm. The setup configured the computers for access and after letting them accept connections over LAN I got speedy remote control with full colours. I couldn't find a way to make RealVNC request a lower colour depth so what I did, to work around this, was run a copy of VineServer on OS X with the '-maxdepth 8' option (and '-rfbport 5901' to run it as an alternate server), and connect to that when I connect long distance (which I always do through an SSH tunnel, before anyone states the obvious.)",3
"Note that I've emphasized the word ""usually"" several times in the previous paragraph. That's because there's no standardization of user interfaces or how BIOS/CSM/legacy options are activated and interact with boot media. Thus, one computer may offer relatively straightforward options for controlling the boot mode of external media and another one may leave you looking like Lex Luthor or Doctor Evil by the time you get it to do what you want. (You'll be equally ill-tempered, too!) As a general rule, though, you want to get the installer to boot in the same boot mode (BIOS/CSM/legacy vs. EFI/UEFI) used by whatever OS(es) already exist. This goal can usually be accomplished by using the computer's built-in boot manager, which is usually accessed by hitting Esc or a function key early in the boot process (before any boot loader or OS-specific stuff appears on the screen). With BIOS/CSM/legacy support active in the firmware, EFI boot managers usually provide two options to boot external media. One option includes the string ""UEFI"" and the other doesn't. Select the ""UEFI"" option to boot in EFI/UEFI mode, and select the option that lacks that string to boot in BIOS/CSM/legacy mode. For this reason, my general advice is to not use BIOS/CSM/legacy support if you can help it. The boot process on most EFI/UEFI-based computers is much more straightforward if BIOS/CSM/legacy support is not activated. Of course, going EFI-only is not always possible. In your case, Windows XP doesn't support EFI-mode boots (at least, not on x86 or x86-64 computers), so you pretty much have to use BIOS/CSM/legacy support to dual-boot it. I mention my advice to go EFI-only for the benefit of others who might be using Windows 7 or later rather than XP, or in case you want to make changes to your setup in the future. The workaround you found will result in a successful installation, but without a copy of GRUB. This is probably fine, since you can then run update-grub in Mint to have Mint's GRUB detect your Red Hat installation. The error message (""For a UEFI installation..."") clearly indicates that your installer booted in EFI/UEFI mode, rather than in the BIOS mode that Windows XP certainly uses and that Mint probably also used.",1
"Power Users also allows an account to create shares. I tested it by creating shares in a cluster with new-smbshare command where I only added the domain account to Power Users. Without being in Power Users, the domain account could not create the share. Useful if you do not want to make an account an Admin just to be able to create shares. Caveat: This was tested in a Windows 7 environment with O365 only for Outlook, so I can't swear it will work for the other apps, but VBA permissions have never varied per-application before, so I'm assuming it will be the same for the Excel, Word, and PowerPoint macros.  Not quite true - if you use Office 2016/O365, all macros are disabled until you are at least a power user.  You can enable anything you like and there is no error - it just doesn't work.  But after I upgraded myself to the Power Users group (I had already enabled the right options in Trust Center), I can record, write, or run vba solutions as I do when logged in with my administrator account.",2
"Only with Linux is not possible. With SeLinux or GrSecurity you can ""hide""(in fact, you don't give it access) processes between users. It's a little bit complicated, but not impossible. How would I do this? I've seen it in action on many shared hosting services that offer shell accounts, but haven't come across a way to do it myself. I'm running a Debian server and giving out shell access for a handful of friends. I'd like to hide the running processes from those normal users, so that for example top only shows their own processes, and not anything started by anyone else. Obviously root should see all processes, though. A quick and easy way for top, but not bullet-proof, is to give them an alias in their ~/.bashrc (or the shell-specific appropriate startup/alias file) that just runs top looking at their own username: This doesn't do anything for ps or course, but aside from putting users in a chroot or jail environment, or selinux as Sacx mentioned, there's not a lot that you can do to hide the system from them. If you aren't interested in SELinux, you could possibly get some traction with this problem by getting creative with permissions on the PID subdirectories in /proc/, but some massive testing would have to be done. How you would effectively enforce this on users who have command line access I don't really know. Given that users have access to the command line they could use a variety of other tools to see other peoples processes anyway.",5
"For performance, start measuring application performance. Western Europe to China through the great firewall will perform poorly. Try different regions until you find a close one by network latency. Data sovereignty, compliance, and other political concerns may require determining where it physically is. Ask your compliance person whether the API telling you which region is sufficient. Do not rely upon or trust geolocation services for IP addresses. Today, IP addresses are logical entities (I am oversimplifying) and can be assigned to different data centers, in some cases to multiple data centers at the same time (IP ANYCAST). They can also be reassigned (Software Defined Networks) with an API call. Geolocation of IP addresses is unreliable. Sometimes the correct city, sometimes the incorrect continent. All Google IP addresses are owned by Google and typically point to one of Google's business addresses which does not indicate the physical location. If you are using a Regional IP address then you know what region it is in by selecting the region when you create it. Regional IP addresses can only be used in the same region.",2
"I was thinking about getting a dedicated server (I may need the extra power that a VPS can't provide) from The Planet but I don't know to much about how you would operate one. I have experience in setting up multiple VPS's on Linode and Slicehost, I just select my OS in their CP and connect via SSH in putty and do my thing. Is it the same with dedicated servers (just chose you OS from the CP and connect via SSH and put on whatever crap you want)? The only difference is my colo facility where I own the physical machines. I'm responsible for racking them and the guys on site replace hard drives for me if anything breaks. But other than that you can move your knowledge from a VPS environment to leased server to colocation and back again very easily.  Most dedicated server environment's where you aren't managing the hardware yourself are very similar to VPS's in terms of build process and rebooting etc.  Yes it's the same. I use slicehost's VPS's, Serverbeach's dedicated servers and I also have a cluster of 20 physical Dell 2950 and 1950 servers at a hosting facility where I lease a rack. I run Ubuntu and CentOS. They're all very similar to operate. You SSH in, do your thing and sign out.",2
"I would suggest digging into the tcpdump output manually.  It is tedious, and the protocol is hard to decode manually, but if you dump some data to a file and then mk-query-digest it, mk-query-digest will tell you the byte offset in the file where it finds the sample query it prints out.  This should let you narrow in on one packet, and with the protocol docs from the MySQL internals manual, you ought to be able to see if that packet really is the alleged RESET.  I believe that the RESET is probably related to the binary (prepared statement) protocol, if it's real; if it's spurious, I have no guesses. I suspect that this is mk-query-digest misinterpreting the TCP traffic.  Reconstructing the traffic as an observer on the sidelines, due to out-of-order packets, dropped packets, retransmits, and so on, is never an exact science.  When there are a lot of errors in what mk-query-digest sees, it can sometimes interpret the traffic in spurious ways and appear to find things that might not exist.",1
"I want to create a bar-code for each person. When that person scans their bar-code at the end of the line that meal is recorded for that day. I think it should be easy! I'm not an Excel power user by any description, please pardon my lack of knowledge. I need to create a spreadsheet to track who eats meals, we serve two meals each day, breakfast and lunch.  I can create the bar-codes with the ID Numbers. I can make it put the #1 in a cell for that meal on that day. I can not get it to go to the right row when the bar-code is scanned it just jumps down the next cell directly below the last cell. If that option is not available, you can change the setup in Excel to tell it where to move when Enter is pressed. This will change the behavior to move in the direction you like. Go to File > Options > Advanced . Editing options. The bar code scanner you are using is likely adding a carriage return after the scan. Depending on the scanner, somewhere in the setup you should be able to change this to a TAB.",2
"I modified my 2950 as follows. I pulled the SAS controller, Backplane, all cables associated with. Then added 4 port SATA controller in PCIe riser. Ran sata cables to front and attached to drives. ( have to move drives to sata position to get room for cables from fans) Tapped power from old backplane board connector (20 pin molex connector) which has +3,+5 and +12. Made my own Sata power harnes out of sata connectors. The system screams. Using NOVABENCH it scored 1106 without the Graphics check. NOTE: my system has 32gb ram and 2 3ghz CPUs. For the application we used these drives for this was acceptable, but there are definitely applications and data that we would not be comfortable with a similar situation. With the supported drives (that were smaller) everything worked fine.  We contacted the server/raid manufacturer and the issue may be addressed in a future firmware update to the RAID, but we aren't sure. The hard drives that you purchase from Dell are no different from those purchased from Newegg/CDW/etc. The only difference is that if you purchase from Dell, your likelihood of being able to make a warranty claim on the drive is much higher than if purchased elsewhere. I used some unsupported SATA drives in a RAID enclosure and while they work, we have issues booting the server sometimes.  When the server boots, the RAID initializes and sometimes not all of the drives turn on.  It usually requires a cold boot and then a warm reboot (or two) to get it going.  Which drive(s) don't turn on is pretty random.  Has been as many as 3 that don't turn on initially. So really it depends on how important the service and data is that are relying on those drives.  What kind of compatibility and support requirements do you have?",3
"BTW - keep in mind that you should not worry about CPU being consumed unless it is being consumed and not doing something useful, or not doing enough because you have more stuff to do. 20% is only a lot relative to total CPU available to the VM its running on, and only important if you have better things to do with that CPU. Every time I've seen ridiculous values for the task manager load it turned out to be swapping on the VM host due to memory shortage. Check your memory statistics on the host, especially the values of assigned and used memory and the swap used. The % CPU used is therefore relative to the available CPU. In the case of a VM, it is relative to the physical capacity of the CPU its running on minus the capacity of the CPU consumed by the other VMs.  Check if you are not over-commitning the memory and the host server is not swapping. This will flush your performance down to the drain. Make sure the VMware tools are installed and the paravirtualization (direct I/O, baloon) drivers are running. Using < 100% CPU is not good, especially when you are trying to get work done. Keep in mind that CPU is a temporal resource. Unused cycles cannot be stored and have no value, they simply represent wasted opportunities to get things done. After long time investigating it, I figured that that Webroot Secure Anywere (software for virus and other types of protection)  WRSA.exe was the culprit. Once I terminated that process,  everything started to be fast again like magic!  Not sure yet what to use instead of Webroot for protection. Task Manager attempts to do things in real-time. It is polling resources at a set frequency. For any given set of metrics and load being monitored by task-manager, the number of instructions are per-determined at some fixed value by the algorithm at work. I had a sort of similar issue with a 2003 standard VM on vSphere 5.1. Initially I had configured a very simple VM (4GB vRAM, 20GB vDisk) with 1 socket with 2 cores. At idle, the VM CPU was constantly in the 30-50% range, even though there were no programs installed & it was fully up to date etc I tried different CPU configurations and when I changed the # of vCores from 1 socket & 2 cores to 1 socket and 4 cores, (and restarted), everything calmed down - idling at 99%. The performance on a virtualization server should be assessed first on the physical server, then on the VMs. 100% cpu consumption is not bad, it is desirable. It means that for a given workload, you are running as fast as you can. Monitor the CPU queue, if this is > 0, then you need more CPU. Also the CPU time spend in system time could affect the performance very badly. Even a 10% could double the response time of a VM even if the CPU in total is less than 15% used. If its using 20% CPU for a given set of metrics, its because there is not a lot of CPU to start with. If you run the same load on a machine with 10X the CPU, it will only take 2% of the CPU. I had similar symptoms, not on a VM but a Win7  Laptop. Taskmgr.exe and other processes (like Winmerge.exe) were taking 13% of cpu (one out of 8).  The issue was temporarily alleviated by restart, so I was doing that a lot.",5
"Another thing to take account of is the terminal emulator you are using. Most probably it's a GUI one like konsole or GNOME's Terminal. If it is vulnerable it can also be exploited with a program on the compromised server (e.g. the remote server could send a character sequence which causes a buffer overflow and allows the attacker to execute code on your client computer). Also, if you enable agent forwarding to that host an attacker could use that combined with any ""known hosts"" files already on the compromised host to potentially extend their reach. This sort of thing is purely theoretical.  While it's certainly possible for something like that to exist, I've never heard of it. Plain SSH client is generally safe from anything on the server at least for as long as no security vulnerabilities are found in it. But the following SSH features will pose security problems: However, if it happens that someone has a vector for exploiting some bug in your SSH client through the action of the server to which you connect, you could be somehow compromised.  For instance, an attack might exploit a buffer overflow in the key negotiation process or something. X essentially requires fully trusting all clients (applications), and can scrape data off your client using an invisible window. Intrinsically this does not pose any security threat (except insofar as you are using a compromised machine), unless you are using X forwarding also (X windows are not sandboxed). I've never heard of a worm or an automated attacking software use these attack vectors, but if it's a targeted attack, these (especially the Agent Forwarding) may be used against you.",3
"There are a limited set of TLDs defined and recognized by IANA consisting of every two-letter country code and certain names with three or more letters. Originally, the generic names were just .gov, .com, .net, .org, .edu, and .mil (along with .arpa for historical reasons, as well as defined unassigned names such as .test and .example). Which should you select?  The one or ones that best fit your site's purpose and content. Pragmatically, if you pick anything other than COM or NET, you will probably want to consider also capturing the same name in COM and/or NET to prevent user confusion. First, a nitpick: The .com part of a domain name is called the ""top level domain"" or TLD, not ""extension"". I can't ever remember deliberately going to a .biz or a .info, so if you're looking for TLDs to avoid, they'd be right at the top of my list.  And probably .cn should be avoided, too, since when I see that I immediately assume spam.  That last one might be different if I actually read chinese, but assuming your site will be in english, that pretty much leaves .com or some sort of clever name/country TLD. I'd say that the conventions as initially planned are mostly ignored. Originally .cx was meant for sites in the Christmas islands! And .com for commercial entities.   In the long run, the idea is that organizations would pick the best name and stick to it, and end users would be educated that not everything ends in COM.  Avoid any of the ones spammers seem to love. For instance I tend to be dubious of anything ending in .biz Recently, the TLD name space was expanded with the names .biz, .info, and .name among others, and a mechanism was created for sponsoring organization to apply for and create additional names. A complete list with notes can be found at Wikipedia, and IANA maintains the official list. Well .net would be the next major one you would want to grab. .org is suppost to only be for non-profit type organizations, so keep that in mind.  When I bought a domain, I just bought the main 3 you mentioned. .com .net and .org.",4
"But where does this path come from? This is the reason so much contradictory advice occurs: You must use the path shown in the Explorer window title bar after a successful connection. This will vary depending on dozens of imponderables. This is a pure example of Microsoft's famous ""FUD"" technique. They want people who connect to Apache servers to experience ""Fear Uncertainty and Doubt."" The fix for this is a bit involved and a great deal of contradictory advice exists on the web. The first step is generally agreed: The AuthDigestDomain directive should list all the locations protected by the ""davusers"" realm in the davusers.digest file. It tells the client browser to let users access the other locations if they have provided authentication for one of them. In the example above, we created an exception on the local ""intranet"" (LAN), but you can do the same thing for remote servers if you feel daring. In your case, the server name in the tile bar may be shown as an IP address, or perhaps without @SSL: Just enter what you see. Back out of the configuration windows and the nagging will stop. Both of these support mounting a WebDAV server. But I'd recommend to test them first; the Rclone devs say their mount option is experimental, and in my experience Mountain Duck still has some bugs. If you really hate the idea of dealing with SSL, you can turn this requirement off using a registry hack on the Windows client. Put the following lines in a plain text file ""Install - Enable BasicWebdav.reg""  When you are able to mount your WebDAV share, you will be assaulted by another annoyance. Every time you drag a file from the remote folder to a local folder, you will get a menacing warning: Finally, and this is very important, WebDAV access from Windows will be insanely slow (as in glacial) if you don't make this adjustment: Every version of windows has required a new set of hacks and patches to make WebDAV work. One might get the impression that Microsoft feels threatened by this technology or, perhaps more likely, only wants to see it work with their servers. The answer by @user2152363 is very good. Just for completeness, here are two alternative solutions you could look into:",2
"However, I've already heard rumors of ISPs which continue with their IPv4 practice of giving out only a single IP address per customer, and selling /64 blocks as an extra feature/addon. So if you end up in that situation, you still need NAT... All I seek to know is that, would NAT be required if after ipv6 is implemented, would NAT be eliminated or be used in a different way(Transformed or Upgraded into a new version with some added abilities to tackle any situations or problems that the network specialists or researchers foresee). NAT is a wonderful concept used in ipv4. It is the major reason for the existence on internet, in my concerns. There are no networks in the world that doesn't work without employing NAT. If your home network gets a /64 subnet from the ISP, you don't need NAT. And ISPs usually get a /32 allocation (even tiny organizations get a /48 by default), so technically they have no reason not to give out /64's or even /56's (good enough for 256 subnets per customer) to every single customer they have. IPv6 doesn't have an address shortage problem, and there is currently no NAT for IPv6, although there is an experimental RFC for people who just can't give it up. NAT is a kludge in IPv4 to get around the IPv4 address shortage while waiting for IPv6 to become ubiquitous. I certainly wouldn't use the word wonderful. NAT breaks the original IP paradigm of end-to-end connectivity, and many workarounds have been needed due to NAT.",3
"It is a pretty long road if you start out only knowing how to program. It is also a field with a lot of current activity and a lot will change before you get very far along that road. It is good to have a goal, but if you start late in a hot area you need to keep a flexible outlook so that you have options if that field cools off while you study. But those same things needed for ML are also good for other things as well.  The first two paragraphs of the Wikipedia article on Machine Learning will give you a good outline of the things you need to know. Algorithms and Data Structures are key from CS, but you also need quite a lot of things like Statistics, Mathematics, Pattern Recognition and such.  So, the courses you mention probably aren't especially necessary but your statement that you have trouble with documentation is a bit worrying. Perhaps that will change with practice.  If you are a young person starting out, don't commit too deeply to any one path. Learn the basics so that you can choose later when you know more, both about yourself and about the state of research at that time.",1
"Either the upload script is changing the permissions when it saves the file, or the NTFS permissions aren't setting to inherit the permissions correctly. I have an asp.net application on IIS 7 and Windows Server 2008. I have a folder where the administrator can upload mp3s through a asp.net form. The folder that the mp3s are stored has permissions under IIS_IUSRS (IIS USERS) to write, read, etc. When he upload a file, automatically the new file loses all the permissions and consequently this mp3 cannot be played by the website either from an authenticated user nor from an anonymous user. If I change the permissions manually the problem is fixed, but it starts again in a new uploading. Any ideas? To rule out (or in) the upload script, try dropping a file in that folder manually (copy, not move) and make sure that it obtains the correct permissions.  If it does, then the issue is most likely with the asp.net form. If the issue happens manually from Windows Explorer too, then check the Advanced NTFS permissions.  It's possible that the permissions for the IIS_IUSERS user are set for 'this folder only' and aren't inheriting for new files.",2
"Why does the native resolution differ from the supported resolution? when I input the 1280 x 1080, does the resolution turn to a true 1280 x 1080, or is it simply spoofing me? Are there any other performance quirks to turning the resolution up? In the title, it advertises 1080p resolution, however, if you scroll down, you'll see in the description: The input (aka ""support"") resolution(s) indicate what the projector/display will accept.  Naive users only look for the maximum input resolution, but you should check for support of every possible resoluion that you might use.   You can mirror output from a 1280x1080p display so you can work in the higher resolution while the projector puts out a reduced resolution version of what the presenter is seeing rather than having to reduce both screens to the projector's native resolution. When the input resolution matches the native resolution, no scaling is performed and you get an accurate 1:1 mapping. You actually need to know all the specs, not just one number.  The native resolution may be considered more important to picture quality, but the quality of the scalers and support for the input resolution(s) that you want to use are just as salient. Because there is more than just one type of resolution.  You're conflating the different types of resolutions.",3
"Piriform makes a program called Recuva, I have recovered some deleted files I thought were gone. It might work for you.  In the future I would encourage(force) employees to use a network location with backup on it rather then letting them store information like that in a non-company cloud In any case, do not use your computer as new data might be overwriten on the lost ones which will make harder to recover the lost data. If you want to continue using your computer, then take image of your hard-disk, so that you can continue using your desktop or if you try to recover the data by yourself, you can attempt to recover data from the image file which will ensure that your PC's file system will not be damaged . You can use Winhex to take the image and it also provides data-recovery options. but it is a basic program. More professional (and commercial) tools would be EnCase, Forensic Toolkit, DMZ F.I.R.E., Maresware Well, we had an emplyee who used to store all of his business accounts in Dropbox. Now he left our company turned off the sync and uninstalled dropbox so all of his files went poof! His Dropbox account was not registered to his work email so we can't access the account. Now, we all are trying to recover those files, we tried Pandora Recovery but no success. You could also try data recovery service provided by the harddrive manufacture's data recovery service. I know Seagate does this. 2) If your ex employee would not provide the log-in info. Then you can recover the data even if they are deleted and new data is overwritten on the old ones. However, the digital forensics may not be easy if you are new on it. Then I would strongly suggest you to contact with companies that does data-recovery, I think it would be the easiest option so that they can recover all lost file as they use good tools. It is even possible to recover the data with a professional tools even your hard disk is physically crashed. If you do Google search, you should see the available companies as Google provides result (or ads) considering your location. If the data is really vital, and for some reason you are not able to get hold of your ex-employee, I suggest you go for professional data recovery service like SalvageData Recovery. But as I said it would save you from a lot trouble if you contact with a company which does data-recovery.",4
"Click of death? Whirring sound? (I feel like a car mechanic ;) ) Most likely the drive is coming to the end of its life. Does it have SMART? See if that's tossing any errors as well as backup all your data. Probably going to have to get a new one soon. Remember though, SMART is a quickie...  If it fails, then your drive is surely toast, but if it passes, keep an eye on it.  It just might have not reached that SMART test thresholds... Yes, back up your data immediately.  If you don't have somewhere to put backups, go buy a new harddrive and clone everything onto that.  You'll need the new drive soon anyway. As for the sound... it's most likely the hard drive resetting itself and trying to re-read data as a result of errors.  As others have said, these errors should show up in SMART info, but don't even waste time looking at that, as your HD could die at any time. Then you can safely diagnose it...  But the first poster (MrStatic) is correct, what is the noise?  Have you tested it with a smart test..? If it's an external drive, it might be the fan that stops periodically spinning. If that's the case, the hard drive itself may be rescued yet. I'm not going to suggest you stop the fan by hand (if that even applies) because that's dangerous (for the drive)",4
"A static IP makes everything a lot easier to get and keep working. Because the address remains constant you can simply leave [your VoIP service]1 to get on with it. If you have a major installation this is the only way to go.  With a dynamic address every time your address changes the server needs to re-register with the service provider, or it won't work. It is not the end of the world, there are any number of scripts available that check your current address and force registration, or there is dyndns.org which will match a domain name to your continuously changing address, either method is adequate for SOHO use.  But you do need to be more careful over how you secure the server. Make sure there is nothing open to the Internet that is not absolutely essential to operation etc.  my girlfriend will begin working from home, and she needs to hook her Cisco 7000 series Voice over IP phone up to our existing DSL account. The phone provider's tech support said that there was no other way to configure the phone to work on our network unless we had a static IP. This sounds unnecessary, but I'm no VoIP expert. Our Internet service provider is AT&T, and the service is standard DSL. Is this a common thing to install VoIP phones? Has anybody configured phones to work off of a standard dynamic IP? What about with AT&T? AT&T is stating that they only sell static IPs in blocks of 8 which is totally unnecessary for me.",2
"I could create a row of buttons as keyboard keys, but seems like it can be a better solution to use some modern simple and compact MIDI controller that is connected to the Pi via USB and then somehow my app (Python or anything) would catch its signals. Is it possible to connect it without having to modify hardware part of the MIDI keyboard or create a middleman block, just by setting up necessary libs on the Pi and linking them to my app? I want to build a kind of a synthesizer, with a set of potentiometers connected to Pi via ADC, a set of momentary buttons that would toggle the modes and so on. I ended up using PYO, an audio synthesis framework for Python, to handle all the signal generation and MIDI signal handling.  It was relatively simple to work with, but seems to be a fairly flexible model.  I only used some very basic functionality. I just did something similar with my Pi in order to put together a simple audio synthesis demonstration for a local elementary school. I used an existing MIDI control surface (one of these) as the input device.   I'd recommend using Csound or Pure Data, they both have fully functional (USB)MIDI, OSC, and Audio processing/generation functions. Csound has a bit of a steep learning curve, and not too many people use it with real time midi, but once you get the hang of it there's a massive codebase to work with and most generators / effects sound better than a lot of VSTs. There's also the csound~ object in Pure Data for interfacing with your Csound code. Pure Data is pretty easy to get into, patches run the same on linux or other OS's, and you can run it on a headless Pi by using the no-gui option if you so choose.",3
"I understand that this would give a very minimal level of protection; the idea is to prevent random browsing from search engines or easy access for anyone who knows the URL(s). That is, we don't need or expect to protect against informed or motivated attackers this way. I can't find any precedent for this online. Are there any similar examples out there, or any better ways to achieve something similar to our goals? (Not meaningful security, but a first modicum of confidence in the request's origin) On the other hand if your testers come from known locations you should simply have a whitelist of approved IPs on the test stack and reject anyone coming from elsewhere.  What you are describing is essentially Bearer token authentication - the client will have to present a valid token or be refused access. Needless to say this should only be used over HTTPS to prevent anyone from tapping on the plaintext traffic. However all sites these days should only use HTTPS so it's a moot point ;) We thought of doing this using a custom 'X-' request header, and then using AWS WAF's regex match conditions to check it against a 'non-trivial-to-guess' pattern. There are pretty much no rules around what that token is - it can be a base64 encoded random string that both the server and client know, or it can have some auth data encoded inside like JWT tokens. In your case you can simple genarate and encode a random string (32 chars or so) and use that for basic authentication. My organisation is adding a firewall to our test stacks, using AWS WAF. We'd like to whitelist all traffic from the SDKs we've built to facilitate requests between our services.",2
"Originally I assumed it was some bad code -- everything is running PHP scripts -- but after awhile we saw hung processes resulting from a variety of different requests, many of them simple, but always showing POST as the last type of request handled. 27-31   7629    1/34/107074 W   0.96    4480    0   0.0 0.39    1394.10 xxx.xxx.xxx.xxx www.mysite.com  POST xxxxx HTTP/1.1 This combination lead to a death-spiraling experience: a seemingly innocent page was causing Apache to recursively call itself and Apache became totally unresponsive. If apache still manages to hang then problem with POST method. If this works then problem with large number of scripts may be due to common included file in all scripts. Probably that war story does not help you at all, so I point you to couple of other debugging techniques: The process itself is always doing something, usually consuming significant CPU resources until we kill it. We are having Apache processes get hung up after seeing a POST request, and wondering what the best path might be to troubleshooting this. When I look at the extended server status for the requests that hang, I see entries like: I don't expect that the community would instantly be able to solve this without seeing the actual code running; what I am really looking for is the best path to debug this as I am out of ideas. Some things that do come to mind: Create a simple php page which takes just one post variable and prints it on screen. Block all clients so that only you can access server. Restart web server. Now use your simple script and send only one post variable.  53-31   28616   9/13/74232  W   0.71    7174    0   7.6 0.01    961.47  xxx.xxx.xxx.xxx www.mysite.com  POST xxxxx HTTP/1.1 Don't know what's going on in your case, but I've spotted some broken rewrite rules with Apachetop. For example, in one case a rewrite rule was pointing to a local, but non-existing page, and did so by performing a redirect (R in RewriteRule line). Unfortunately also the Apache's Error Document was misconfigured -- ErrorDocument 404 http://thesiteimtalkingabout.com/404.html and that 404 page was missing, too.  One very useful utility is apachetop. It's like top, but is keeping its eye on your Apache's access log in real time. You can sort the results by many different criterias -- amount of requests, amount of kilobytes transferred and so on.",3
"Because I use this setup at work, I immediately think of a digital signage solution - a free one (budgets are tight at work) which works well enough with elderly or cheap hardware  - specifically Concerto (not associated other than as a user.) There are plenty of non-free ""digital signage"" solutions, but they are very non-free, so unless your budget for this is higher than one would expect for a home project, probably not an option. But if you are a high-budget operation, there's plenty of options. It has all 4 of your points, in some way, but how you'd be able to do 1 & 2 on the stock software is probably not what you are thinking of, at a guess. But there are ways to do those things - I just suspect you are looking for a method which allows more manipulation based on them than I'm aware of being built in. You could of course, write something that does what you want and submit it to the project, but that assumes that you want to get under the hood. That assumes you are willing to use a ""digital picture frame"" that will run Debian (well, some folks run other systems - I find Debian the least trouble) which won't (as far as I know) be an off-the-shelf DPF. But it can be a display connected to a Pi, or to a computer that is past updates (though there you have to be careful, as if you get too old the power bill will be enough to afford something newer and less power hungry.) Some tablets would also work, I believe. The machines speced at http://www.concerto-signage.org/deploy are all far above what I actually use in production (I don't run their ""Player"" - I just have a browser in kiosk mode running.)",1
"I have an older version of AI running on Windows, but with it the way I do what you want is by selecting Save for Web... on the File menu which opens a dialog of the same name. In that over on the right you can select the PNG output format desired, and then on the Image Size tab you can set the exact horizontal and vertical resolution of the image created. Here's an illustration of using Save for Web... to create a 24-bit PNG file showing where to set the pixel resolution in the dialog: This can still be a little tricky because when either using Export... or Save for Web..., the overall image dimensions/aspect ratio are determined by an imaginary bounding-box that encloses all the objects in the drawing. Because of this, it is sometimes necessary to add an empty rectangle around the (larger) area desired to force the proportions to be what is desired. Alternatively you can sometimes do this afterwards in Photoshop (or other image-editor). Playing with the PPI around to get the right size could probably work, but it's annoying. There must be a better way, since resolution independence is the entire point of vector graphics. When I try to export a png I get the following dialog, where I can choose the PPI, but not the resolution.",2
"It is not possible.  The Microsoft stand on security has always been that administrators can access everything, even things they can't access (whether that is through changing permissions or taking ownership of an object). That said, I have seen examples where the top level users had their own single IT person just for them. They maintained separate email environments, and were the person who handled desktop IT for the C-suite. The rest of the hoi polloi IT had to work through that one person. It can really work when that one person is a nice guy and is willing to work with the rest of the company. It can be downright evil when that one person lets the power go to their head and they start going their own way just to go their own way. Then the password for the ""real"" Administrator account (which still has Domain Admin and Exchange OU membership) is known only to the company directors. At the end of the day though, if the directors don't trust the sysadmins then the business either needs less paranoid directors or more trustworthy sysadmins, depending on whether or not the directors are right to be worried. Of course this will limit the debugging and configuration capabilities of the actual admins; but when the extra access is needed, one of the directors will have to get involved. So, unless your executives want to manage their own Exchange server (and domain), you are going to have to live within Microsoft's bounds. What can be done, of course, is events on those mailboxes can be audited and the event logs on the exchange servers can be secured. As Erik points out, even this won't help if a sysadmin takes a backup tape home and restore it. What about if the actual admins (the guys who do the day-to-day work) are removed from the Domain Admin and Exchange Organization Unit groups?  My understanding[*] is that these are the necessary permissions needed to view a mailbox. I have the utmost sympathy for the original poster: the head of my company insists on the same kind of policies.  His organization has been painstakingly created to segregate administrative power among different individuals.  It's frustrating, but the owner of the business gets to make the rules. The only way this is possible is to not keep the emails in Exchange, which in tern exempts them from any email archiving system in place. Depending on what kind of regulatory environment you live in, that can be a very very bad idea.  Now others, like Novell, have taken a different approach in the past where admins could have access removed from objects with no way to gain it back.  The big downfall of that is that portions of your file system, directory store, or mailboxes could easily become totally inaccessible with no recourse. But ultimately it comes down to the trust issue. If they can't trust their own highest level Administrators to not poke their nose in areas they haven't been invited in to (perhaps they've read a bit too many BOFH stories), then that admin doesn't need to work there. It's called professional ethics, and one of the top ones for sysadmins is to not go info-hunting for curiosity. This is why I got a solid background check for this and my last sysadmin job.",4
"AltShiftNumLock gives you a dialog box asking you whether you want to turn on Mouse Keys. Once enabled, the numpad's / and - keys can be used to switch between the left and right mouse button (or * for both), 0 will press and hold the mouse button, and . releases it. As an alternative to the already posted answers, you can turn on Mouse Keys. Mouse Keys lets you use the numeric keypad to control the mouse pointer and buttons. You can combine this with a real mouse or trackpad, so that you use the keypad to control the buttons, but a mouse or trackpad for positioning. If you are using a Microsoft Mouse you can download the control software from hear,  sorry I cant find a list of what it allows you to defined each mouse button to do on each version of windows, but worth a try.    There are many mouse on the market with additional buttons, often these come with software that let you define what you wish the additional buttons to do.      XMouse Button Control (XMBC) is free software that claim to work with most mouse with additional buttons and allows you to control what these buttons do, one option it gives is Click-Drag [Sticky buttons/keys] After suffering from severe back-pain (not caused by an injury) while doing mouse-intensive work I've found using a vertical mouse a very big relief. It might take a few days 'til you are familiar with the altered position of your mouse-hand but after a while using a conventional mouse felt even more awkward, not to speak of using a trackpad. As for a key toggle, I think that some 3rd party mouse/trackball drivers offer that.  I don't recall the current kensington software, but as you note its other balls that need this kind of thing for a larger number of users.  With 4 or 6 buttons, it might be a choice to set one to latch left button down.  I saw that many years ago but it could have been Logitech.  Ps: I am not affiliated with any of the companies mentioned nor do I sell their products or benefit from mentioning them in any way. Use a trackball, but rig up the button to be pressed with the other hand.  This is a feature I look for in a tiny (for HTPC) keyboard+ball device.  But for a good quality Kensington trackball, it would be a simple mod will wire a switch in parallel to the existing one.  Put it on the other side of the keyboard or use a foot switch.  The company that makes XKeys sells foot switches and such. While i think that keltari's and and mioxlav's answers are probably what you were aiming for I yet would like to bring vertical mice into the game. If you use ""ClickLock"" (Windows 7 and above) you have to be able to press the mouse button for a very short time to start a drag, however a lot of people can't do this....",4
"Traditionally, floppy images use the FAT12 format, which have a limit of 16MB and 8k clusters. There appear to be extensions to FAT12 that allow 32MB or even 256MB (64k clusters) but I am uncertain on the specifics. Now you have a PXE-bootable 16MB FAT12 floppy image with FreeDOS. At this point, you can re-mount the image and copy any additional files you need. Now we have a 16MB floppy image (FAT12_16MB.img) containing the FreeDOS operating system. However, if you try to boot this via PXE, you will get the following error: However, that command can take a minute to complete. Using a higher block size, we can accomplish the same thing in less than a second: you need to try vmlinuz kernal to load the bigger images, you can find it in the linux mirrors online.. If we target the maximum FAT12 size (16,736,256 bytes) for a bootable 'floppy' image, we can do the following: Now that we have a 'blank floppy disk', we need to format it as FAT12 (if you are interested in the available parameters for this command, run man mkfs.fat to view the man page): I've been through this process a few times, and although it is extremely easy, I keep forgetting the exact steps I used, so I am documenting it here for my own reference as well as others. For the record, I am using Slackware Linux 14.2+ as the PXE server, and booting to several different Dell Optiplex models for BIOS updates. As mentioned in other answers, you can create bootable ISO images for large filesystems. However, since you specifically asked about bootable floppy images, here are the steps I use for that. From the MEMDISK documentation, there are some specific floppy disk geometries that memdisk will attempt to guess: First, some background information on MEMDISK and FAT12 to put your question in context. There is a lot of conflicting and ambiguous information out there, so hopefully this clears things up a bit. In practice, I haven't had any luck with the APPEND floppy parameter; I get a MEMDISK: No ramdisk image specified! error when I use it. And now we can start copying files to it. I downloaded the FreeDOS OEM Boot Disk from fdos.org and copied it to my larger image: There are some other helpful step-by-step resources out there that I consulted while creating this answer. Important: Since we will be writing directly to the image file, make sure your floppy image is not mounted prior to the next step: A syslinux thread from 2003 indicates that MEMDISK supports a maximum image size of 4GB, depending on the hardware (i.e., memory-limited, but possibly additional hardware-specific considerations). An earlier thread from 2002 also indicates a separate PXELINUX limitation (Linux kernel space limited to ~1GB, but this information is over 15 years old).",2
"With the typical non-transparent proxies these new packets contain a different source and target IP and port than the original packets. They also might packetize the application payload in a different way. For example the original client might have send a small HTTP POST request in one packet while the proxy might make two packets out of it: one containing the HTTP header and one the HTTP body.  Contrary to this DPI (Deep Packet Inspection) as found in IDS/IPS works at the packet level. It will analyze the application layer payload (like a proxy does) but then forward the original packet (unlike a proxy), usually unmodified or at most marginally modified (changing some bytes but keeping the size). Some proxies can run in a destination transparent mode. In this case the original destination IP and port is kept, but source IP and port changes. And some proxies can run in a full transparent mode where additionally the source IP and port are kept. But in all of these cases the proxy might packetize the data in a different way. And even if the payload of the packets is kept the same the TCP sequence numbers will very likely differ. A proxy is usually a user space program which maintains independent TCP connections to the client and to the server. While the application layer payload might be left unchanged by the proxy (or not, proxies often add some HTTP header like Via and/or X-Forwarded-For) the original TCP packets are not simply passed through but new packets are created.",1
"You could use your /root/install.log and /var/log/yum.log to get a list of installed RPMs, or, if you have a similar system you could get the RPM list from there.  I've never tried to, but you can try to set up a new empty RPM database (copy from fresh install?) and reinstall all currently installed packages in the current installed versions. You may have a look into your rpm (or maybe yum, if you're using it) logs to determine currently installed packages. Sometime ago I managed to erase the /var/lib/rpm folder which basically contains information about all the rpms in the System You said you already did this first step, but for future reference (and for anyone else that runs into this) That said, if this system can suffer downtime, I'd suggest re-installing as the best way. Otherwise, see the comment above. Its my pure curiosity to know of a method by which we can restore the database of the installed rpm's If you've kept a backup, then you might be able to restore it. Otherwise, you've wiped the only copy of the RPM database. (The rpm --rebuilddb command rebuilds some files in that directory from others.) Your system is basically hosed. It's not really a big deal, your system isn't hosed.  Just run the above command(s) and you should be back in business.",5
"After successfully moving to a new hoster, I'm having a weird issue on just my computer (Windows 10). After propagation was complete and I removed the temporary entries in my hosts file, it looked fine for about a minute (apparently some sort of local cache), and then it wasn't. Checking nslookup, I saw this strange scenario: mail. and www. were correct, and they even claimed to simply be aliases of the base domain, but a query on the base domain itself pointed to the old server. What??? Then, after a couple hours I noticed that the previously behaving mail. and www. versions had also reverted to the old IP! Then a little later they got well again. Then after an hour or so away from my computer I checked again, and www. is correct, but mail. (and of course the naked domain) were wrong. A few minutes after that, they swapped (mail. was correct but www. was wrong). I'm going mad! The naked domain doesn't come and go, but is consistently wrong. Re-adding the hosts entry 192.145.233.49 proverbs2525.org fixes it, but when I remove the hosts entry again the naked domain goes back to 192.232.219.88, possibly taking the subdomains with it. It's as if I have some secondary, hidden hosts file (shall I call it a ""ghosts file""?) that still has 192.232.219.88 in it and wants to assert itself when I'm not looking. I moved two other domains at the same time, and they are working fine. It's definitely not a propagation problem - I set all the TTLs to 10 minutes the previous day, and it has been about 8 hours since I changed the nameserver entries. My husband's computer on the same LAN, my phone, geopeeker, etc. all see everything pointing to 192.145.233.49 as they should. Can someone help me find the ghost?",1
"Third, before enabling aging on the zone and scavenging on the server make sure to export the zone so that it can be restored in the event that something goes wrong. First, understand that an update and a refresh are not the same thing. An update is an actual update of the record, such as a host getting a new ip address and updating it's DNS record with the new ip address. A refresh is a refresh of the timestamp of the record with no other change to the record itself. Updates are always accepted regardless of the No-refresh interval. The No-refresh interval is meant to tamp replication traffic in the case that a record is not updated but the client is merely attempting to refresh the timestamp of the record. There's no need to replicate every refresh of the timestamp, especially considering that Windows clients attempt to refresh their timestamp every 24 hours by default. Second, you can enable aging on the zone without enabling scavenging on the server so that the DNS zone has time to ""shake off the dust"", meaning the timestamps of the records for real, honest to goodness clients can be refreshed. You can then see which records are being refreshed and which are truly stale. I'd recommend setting the No-refresh and the Refresh intervals to the default of 7 days each. This should have the effect of allowing all records with timestamps older than 7 days to be refreshed. Then wait 14 days (the No-refresh interval + the Refresh interval) and enable scavenging on the server (I recommend the default scavenging interval of 7 days). Then wait 7 more days to see which records have been scavenged (this will be records that haven't been refreshed in 14 days + 7 days (the No-Refresh interval + the Refresh interval + the scavenging interval).",1
"In an ideal world, you should be able to publish your root passwords on the internet and it wouldn't matter, even if you gave people access to your machine (but didn't put them in the /etc/sudoers file).  Of course, you shouldn't publish the root password, but the idea is that you protect your systems with concentric layers of protection.   Disallow remote root access (or at least root access via passwords).  (if you allow root access via keys, carefully control those keys, or better yet use something like kerberos that allows centralized revocation of keys). sudo also automatically logs every command to syslog and you can define the commands each user can user. You should disable root access from remote so an attacker can't compromise root without first compromising a user then escalating to root. We enable root access at the console only. sudosh is a logging shell, and you can replay the whole terminal session at a later date, showing exactly what the user saw on their terminal. If you want to create an audit trail, and don't want to fall victim to the ""sudo su -"" or ""sudo vim foo.txt"" problems mentioned here, you can use ""sudosh"". Hand out root access via sudo, but make the only allowable command to run ""sudosh"". I'd disable su and use sudo.  That way, users use keys (preferably encrypted) to access the system then they use their password only for privilege escalation.  You can restrict which programs people access with sudo, but mostly you just restrict access to who knows the root password. Good permissions schemes are something even long-time Linux users get wrong, or don't realize the scope they have. If you are using root on a regular basis then your permissions may be wrong or you may need to grant sudo rights or a new group rights to r/w/x the files or directories.",5
"If it's none of the pedestrian/common options, it could be you have the drive listed as 'shared' on a network and have accessed it via another computer. In my case, even though the remote computer was off, it still had a lock on the directories. Unshare and it should work if that's the case. For me changing the drive letter did it. I could not kill the accessing process cause it was PID 4 (system itself), which opened H:\$EXTEND..... (did see it in Process Explorer). This will show you all the open files on your removable volume, the processes that own the file handle and the PIDs of the processes.  Double click to highlight the process in the main window (top) and file (bottom).  From there you can right-click the process to kill it or right-click the file to close the file handle. Often this will be because the drive is being indexed; right click the drive>properties, and uncheck index. Or the AV is monitoring the drive - turn off AV to test this. The 2 most reliable ways are ones that nobody has suggested yet because they are sometimes the most inconvenient:",5
"The plan is to shut down Oracle before the sysadmin copies over the data to the new SAN.  Since we are using NFS, when I start up Oracle again, the change should be transparent to Oracle. The sysadmin has been told that the redo logs do not need to be copied.  It seems to me that the redo logs should be archived before the copy.  Should I force a log switch (alter system switch logfile)before the shutdown and then copy the archive logs?  Or will a shutdown trigger the redo logs to be archived?  I have to migrate all data for an Oracle server from  the present SAN to a new SAN.  It is an 11g database in production on a Linux server.  Database software is on local disk but all data (including datafiles, archive logs, control files, etc) are on a SAN via NFS. FWIW: some applications check the controlfile and complain if it is changed. This is part of a check for licenses. If you are using ASM, you could even do this online by first adding the new disks to the current diskgroups, rebalance the disk groups and drop the old SAN disks from the diskgroups. During this drop, the first thing that happens is - again - a rebalance action in which the data that is on the dropping disks is moved from those disks to the new disks. A very powerful feature, not sure if it can help you. Short answer for you question, you need the online redologfiles. The archives are only needed for recovery operations so they should remain accessible, not perse on the new SAN. A regular backup would be ok. Copy the online redo logfiles and use RMAN for the copy of the database since it handles everything that needs to be done in a good way. If it all has to be done using FS copies, it still can be done online, if you are running archive log mode. Since 11g RMAN can clone from an active database, also very powerful.",2
"While changing some connection in the server room and simultaneously in a wiring closet, I hooked the switches up in a loop.  It took a few seconds for the ports to saturate, and a few more to unplug the last few connections.   Luckily we only setup LOCAL authentication for serial line access (hey if you have access to the box to plug into the serial port ... well we all know the rule about physical access and security)  Almost 20 years ago, I was helping set up a new office and was getting the Internet connection going, setting up an HP/Apollo workstation as the main DNS server.  I learned the hard way about the need to include the . at the end of FQDNs. I dont know if this counts or not.  We were on a very tight budget and needed to run 10 new CAT5 runs to a location in the middle of a concrete floor.  There wasnt any under-floor conduit available and the ceilings were cathedral with no crawlspace.  There was a row of low-hanging florescent light fixtures directly over the location where the CAT5 needed to be installed.  I proposed that we run the cat5 out of a wall at fixture-level, then run a piece of conduit 25' across the top of the fixtures to a column.  Then down the outside of the column to a box. You are a System Admin/Consultant and you have to deploy an application working fine on dev machine, but as soon as you deploy it on the production machine it does not work. What is the silliest configuration mistake you ever made? This looked and worked beautifully for years....until maintenance was called in to change one of the florescent tubes....access to the light was via the top of the fixture...which is where I had mounted 25' of conduit.",4
"Which happens exactly how often? Remember - DNS answers are cached, possibly on multiple levels. I.e. I would bet most end users use their provider's DNS. You REALLY overthink the estimate of that 1ms (max) in the whole larger amount of work it takes to get to the host name and then make something with it. Seriously, you totally overestimate the issue. If you use CloudFlare than this is good enough - especially if your content is behind CloudFlare. No. You have no clue how many name servers. What you see as one server can be a cluster using the same IP address. You only have a minimal amount. Let's be clear. Ever since the outage of a low cost low end provider of whatever you do not trust a technology that is proven. Why? You think everyone is as incompetent as GoDaddy? Never had a DNS issue in years. Should I limit that amount? Main concern is adding a tiny bit of latency each time someone does a lookup on my domain name. Does the lookup download the list of 13 name servers? I just created a domain name and next step is setting up name servers. I'm thinking of using Route 53, CloudFlare DNS, and another provider. This means I'd probably end up with about 13 name servers attached to my domain name.  Overengineering showing a paranoia mostly built on not knowing technology AND Professional providers. Get rid of that.",2
"Yes; I believe this is already how the above container is configured, for example. In any case, it would simply be a matter of configuring your live linux environment to run the LAMP stack and whatever else you need to run at start up by changing what is in the init.d directory (for the above solution; other linux distros may have different ways of handling automatic script/program execution). Also, would it work fine between computers? It is possible to make the server run once it is connected to a computer? And it is possible to make some python scripts run automatically once the the server starts up? Yes. The above Turnkey Linux container, for example, already features Python, PHP, and MySQL. LAMP is a common package and is very viable to use for small servers AFAIK. There is the small issue of write wear on flash media like USB drives to contend with. Yes. You can use something like Turnkey Linux's LAMP Container to make a bootable Live image on a USB stick. This is a little harder to answer with any degree of completeness, but generally if the computers are properly configured, reasonably modern and reasonably-specced, yes.  4&5. It is possible to make the server run once it is connected to a computer? And it is possible to make some python scripts run automatically once the the server starts up? I would like to know if it is possible to create a web server in a pen drive. The Web Server must have python, PHP, MySQL etc... installed on it and run without any trouble. I saw a few articles about XAMPP running on a pen drive, but I would like to know if it is possible to add python and if it is viable to do such thing.",2
"You can right-click on the mounted partition in Finder and select ""encrypt"" to retroactively encrypt a partition that has been created unencryptedly without the need to delete the partition. Alternatives to that solution, to decouple it from google's endpoint, are in use inside certain organizations who have not open-sourced their solution to date. I described using the central tool to CV in a standalone mode, which can be driven by solutions like Puppet or Casper with the end state being logging the recovery key into a secured database. So you may have a suitable answer to your question, but if you're still looking at the automation behind directly addressing your need, and if this is for a business, may I suggest http://code.google.com/p/cauliflowervest/ (CV for short,) an anagram for 'FileVault Escrow'. It uses googleApps account auth to secure individual recovery keys behind an organization, so that appropriate access can be granted. I know this is about diskutil, but people will get here looking for Disk Utility app solutions as I also couldn't find an encryption option for existing partitions. No, their bootloader is about DRM, not about customer-friendly features. You can still have fully truecrypt-ed windows around.",3
"The layer is set as visible per the layers window/list.  Is there a faster way to locate where the missing layer is? I finally realized this happens when I duplicate a layer from one canvas size to another, where the first canvas is larger in a significant way. The above answers were correct, zooming out works, but I wanted to clarify what the cause is, at least for me. Also helps to pay attention to where on the canvas the layer being duplicated from is located. This way you know which direction to drag from ;-) I restore the ""layer"" view by going to ""Essentials"" at top right hand corner, and ""Reset Essentials"". I use the ""Edit --> Transform"" method myself. Have also found out that if doing that doesn't show the bounding box, select ""View--> Fit On Screen"" to rectify. I've had this issue too. Check to see if your workspace is in ""analysis"" (upper right hand corner). That's the workspace I use most often. Good luck!! This happens to me quite often, where a layer will ""disappear"" off canvas and I spend precious time trying to ""drag"" it back into view.  Its swimming around in the vast gray expanse somewhere.",5
"If they do not go high and low on command then there is something wrong with the pins, if they do but you still cannot get I2C to work then your problem will need further investigation. Are you looking at the correct I2C bus, between rev 1 and rev 2 the GPIO header was changed from using I2C 0 to I2C 1, see eLinux - RPi Low-level peripherals for details of the various pin changes. should scan both buses separately, if your devices show up after one of the calls then that is the bus they are on. If you do not know how to drive the GPIO pins manually then see here for a Python example and here for a BASH example. Failing that if you have access to an oscilloscope you can monitor the pins and run i2cdetect again. You are looking for the voltage level to pulse a digital sequence between high (3.3v) and low (0v) on both pins. If you are fairly sure the pins are not working then try to use them as simple GPIO pins (0 & 1 for rev 1, 2 & 3 for rev 2) and manually drive them high, check they are high (3.3v) using a multimeter, then drive them low and check they are low (0v).",1
"I've created a script which helps me auto-generate a GameObject prefab. It uses some UnityEditor, so it can't be used at runtime (and its not meant to be). You could use Platform dependent compilation blocks in your scripts, so certain parts would be compiled just if they are in a specific platform. Example: That way when I compile a build, the compiler sees the file as effectively empty and skips over it, even if it's not sitting in an Editor folder. However, since it needs to be attached to a GameObject as a Component to do its thing, I'm not sure where I should put it. I can't store it in the Editor folder, because Unity yells at me and says I'm not allowed. I use this a lot to write Gizmos' attributes for the Editor without generating unnecessary data types on the final build. Myself I'll usually just wrap the whole file in a preprocessor conditional directive block like this: You could also use Inspector classes that will only be compiled on Editor mode, this scrips must be on Editor's folder of course. With this solution you separate the Game's functionality from the Editor's (on this case, I have created the PrefabUtilities MonoBehaviour class). Example:",3
"In short: I want to install apache2, php5, mysql-server without using lamp, but configured in a way so I can open up netbeans, start a project with root in /var/www/, and run every single function without permission faults. Does anyone have experiences or workarounds to this? At last change your umask in the file /etc/profile to 002, umask should be the last line of that file Now change the owners of the /var/www/ folder, as the owner I wouldn't take root, but rather www-data, and change the group to the group just made: www-pub I've been searching a long time, but I can't figure out how to configure apache the best way for a developer computer. The plan is simple, I want to install apache 2 + mysql server so I can develop some php website. I.e. you can set a default ACL on a directory and that ACL will be applied to every object created in that directory. I don't want to install lamp though, just the apache2, php5 and mysql. The problem that I've been looking an answer for is the permissions on the /var/www/ folder. I've tried making it my folder using the chown command, followed by a chmod -R 755 /var/www. Most things work then, but fwrite for example won't work, because I need to give write permissions to everyone, unless I change my global umask to 000 I'm not sure what I can do. Modern linux you can add ACLs over and above,the old user:group:others model, using setfacl and getfacl. We still have to change the permissions so that we can create file on /var/www/. If you don't know what the ""2"" means, this stands for SGID, information about this can be found at http://www.codecoffee.com/tipsforlinux/articles/028.html It is much easier to enable userdirectories (see UserDir) and let apache serve stuff from your home directory, like it does on most hosting services.",3
"Now connect your device again manually. Then in Credential Manager click ""Add a Windows Credential"": enter device name (LS-CHL, in my case) as internet/network address, user name (without domain name) and password. Save. If you are like our company, some internal sites cross boundaries, and it will always prompt for credentials when crossing boundaries like firewalls, because windows authentication credentials are not being passed automatically across the boundary.   If you are on a company Intranet, check to see if you are in the correct OU , and make sure that Group policy is set up correctly Let me know if this works for you. I've used it on about 5 workstations and it has worked for me, but I forgot the solution, which turned out to be easy. Log off from you Windows account and then log in again. Windows will again say it cannot connect network drive. Open Credential Manager again and you will notice that your device is not there under windows credential. Is it possible that you already saved the wrong password for this account? (Maybe the password changed?) Try going into the password settings and deleting any saved credentials for the account and starting over: We are working too hard to fix this... after exhaustive searches, I ran across my note where I have fixed this problem multiple times. It's real easy. I think this has been everyone's problem including mine since we think the syntax that windows creates must be the right syntax. It doesn't need the machine name, only the user.  If I try to log in (simply log in Sam and then my password) and check the remember, it saves in the credentials as: http://windows.microsoft.com/en-US/windows7/Remove-stored-passwords-certificates-and-other-credentials You may or may not get the network guys to update this.  If not you can use a browser that remembers credentials independently.  I have the same issue in chrome, but at least chrome remembers the credentials, and I just have to submit them with a button click instead of entering them again from scratch. Go to Credential Manager under Control Panel-User Accounts. Look for your network storage device under windows credential and open it. Then select Edit. Replece the domain name in front of your user name (LS-CHL, in my case). Save. I had the same problem for a long while, but tonight I seem to have solved it. This is how I did it. Hope it works for you and many others. My network storage is Buffalo Linkstation LS-CHL (I will call it LS-CHL below).",5
"Ok so can a computer from a TCP/IP network communicate with a computer from a IPX/SPX network? If so how will the routers handle the addressing scheme? And what if i wanna telnet that computer from the IPX/SPX network, should I use the IPX addressing scheme even if my network is using the IP addressing scheme. And anyone please give an example of a IPX address? Don't know what it looks like. As for IPX addresses they consist of 12 bytes, 4 network, 6 host, 2 socket.  The following is an example address:  01a83f03 c42c03087a07 10a9.  The host portion can be the MAC address of the host.  The network segment can also be written as 00000000 which would mean the current network the host is attached to. Ok all joking aside no you cannot directly communicate from an IPX/SPX host and an IP host without first going through either a proxy or a tunnel.  Much like how you cannot have an IPv4 host talk to an IPv6 host.  You can however have IPX and IP traffic coexist on the same network as they can both run on top of Ethernet.",2
"If you think the hacker hacked her way into the server and then edited some files you should block the hackers way in to the machine, not just preventing her from editing certain files. Do you have any idea how I secure those directories so the server can write in them but malicious javascript code injection is prevented? I created several websites where some directories are set to the permission 755 and set the owner to ""www-data"" and group to ""www-data"". This is so that I could prevent setting permisson to 777 for those directories and still make them writeable by the webserver for stuff like cache files, user uploads etc. Schedule regular malware scans for the directories that can be written to by the web server.  This should only be the cache and upload directories.  The culprit is probably in your web application. There are lots of interesting things to read at BadwareBusters.org. What I think after some log research is that the hacker scanned my server and found files with those exact permissions and the where able to edit these files and inject code into them. Setting owner to www-data make the directories and files writable by your web server.  Set this ownership only for directories you want the web server to write to.  Configure the server not to run active code from these directories.   The injection likely comes from the active code you are running on your system.  You may want to limit access to javascript files only to directories that can NOT be written to by the web server.  It seems my method is VERY insecure since these websites are very different in terms of which applications runs on each website. That makes me think it is not a particular upload script that is insecure, but more something to do with my permission setting.",3
"If you only have one IK animation computation happening and it is not happening at every frame, then it should be fine in real-time -- especially if you are using FinalIK or BioIK. However, if you are worried about the performance of your IK solver, you can easily build your own FABRIK IK Solver: Build Your Own IK in Unity3D Having that said I'm not sure if you approach the problem from the good side or I didn't understand your description. What you wrote seems like you have the current position of hand and the target position, and you want to 'drag' your character by hand from start to end.  So what can we do with IK? On jump you set simple solver, only for the raised hand. The solver have the task to not to allow the hand to move above brick level. Then when you're movig up and touch the brick then you hand will start to bend, so it never goes above the brick. Given that all the details were done right it should improve the look of you character. In old Mario game you could jump under the brick to hit it with your fist but because of the character proportions its hand ended at the same level as head so it looked fine. Now let's assume you put a typical human in place of Mario. So we create a trajectory from initial to final position in realtime and the main character moves accordingly in the trajectory. As you can see in videos you've provided it is possible even for more characters. In the second video you have more than 10 models, each of them is computed in realtime and I suppose they don't share the same IK results (the reason for this demo is to show it can solve foot and hand contacts for different character proportions, so each character is computed separately). for example in this video of final IK https://www.youtube.com/watch?v=ZuWvGq3yV44 the characters are following the trajectory in real time I suppose. But what if we were to create the trajectory realtime depending on player positions, would that be a CPU intensive task to NOT work on mobile devices?  The IK itself doesn't need to be so heavy. There are many levels of IK - you can compute a hand or leg made of just two segments or you can compute entire body at once. The first one is very simple computationally while the second is almost never needed in a game. This could work but you may end up with other problems like breaking of the rest of the body movement. Instead, I would move the character by his center as always, and IK would just respond to this movement. Now I can understand this would work fine in editor but if we have exported the game and this computations happen realtime for every object, how much CPU intensive would this be? Do all the existing IK assets provided on store  I am looking at an implementation as such demonstrated in the video. https://www.youtube.com/watch?v=Gyakxv34404&feature=youtu.be At some point Mario will touch the brick and you need to decide if it keeps moving or stops - either way may look odd. Assuming a game of mario, where mario is a 3D Character. And there is a coin placed at a height. So when mario comes into area of effect for the coin, we know the initial position of his arms, and we know the expected final position which is the position of the coin.",3
"Yes, hard drives store data by flipping poles in magnetic domains on the disk--at first glance this means nothing is added or subtracted and thus no weight. However, that's not the whole picture.  The orientation of those domains matter.  There is less total field energy when the domains are 1010101010 than when they are 11111111 or 00000000.  I'm sure everyone is familiar with e=mc^2.  Putting energy into the domains DOES mean mass, albeit an incredibly small amount of it. It depends on what font size your text is saved in. 24-point font is very heavy, whereas 8-point is quite light. Bold text also increases the weight, and you should avoid saving lots of text in italics, because all the characters lean to the right, which changes the way the disk spins. My physics isn't up to even trying to estimate the mass but I'm sure it's beyond anything the most sensitive scale could possibly measure. Depends on where you're doing the weighing.  One of the answers immediately jumps into discussing femtograms, which are not a measure of weight, but instead measure mass.",3
"If the text was copied from a website, maybe it's inside a table. If that's so, try copying only the text and pasting it outside the table, and see if it gets wrapped properly. This happens to me sometimes when I copy and paste email contents from Gmail into a Microsoft Word document. All the column/table formatting is coming along for the ride.  You can avoid the problem entirely by being careful how you choose to paste the text into Word. Don't go with the default.  Choose a Paste Option that doesn't retain the source formatting. On the ribbon is the ""Paste"" button, if you click the down arrow the ""Paste Special"" option is there.  From the paste special window you want to use the ""Unformatted text"" option. If you copy the entire contents of your document, then ""Paste Special"" that will clear any and all formatting that is currently on the text.  If this fixes it then it is a page/paragraph/etc formatting problem.  Which is probably the case, it's just hard sometimes to find what or where the formatting issue is at. Try to select the table and then check Table Properties and set the ""preferred width"" to 7.5"" or whatever size will fit with your margins.",4
"As soon as I connect it to the first switch, my lights in first switch start blinking like crazy and my entire network simply stops working. The minute I remove the second switch's wire, its all fine again. Usually when I connect switches, I just use a regular cable because most of them switch automatically, although they should understand a crossover as well. If you're using a crossover cable make sure you're not plugging it into an uplink port on the switches. Edit: Just saw your comments about the brands of the products. For clarifications sake, you actually have three switches in your scenario (your DIR-300 has a router AND a switch. It's the switch that's causing the issue here). Not sure if this is the right place to ask this question. But I have a router which connects to the internet. Now I have a switch connected to this router. I added a lot more computers so I added another switch and connected it to the first switch using a cross-over cable.  This sound suspiciously like you have the switches plugged into eachother in a circle. E.g. Switch A is plugged into Switch B which is plugged into Switch C which is plugged into Switch A. So broadcasts go A -> B -> C -> A -> B -> C -> A -> B -> C so on and so on at the maximum speed the switch will support, thus rendering the network unusable. Connect modem to router via uplink port then run a crossover cable from 4 port switch in router to switch1/portX, then run a crossover cable from switch1/portY to switch2/portX then plug in devices as needed. Make sure that none of the switches are plugged into any other switches except for that central switch (the router. Which I'm guessing is a consumer grade router that also has a switch in it). I would suggest that you use the older switch for the lest amount of traffic and/or as the switch2 in this example.",5
"The outer archive can be some custom format.  If you're a programmer, you can easily create a custom archive format.  Maybe your format simply reverses every 5th bit, which would effectively destroy 7-Zip's ability to make sense of the file.  Then, when someone runs the custom extractor you create, you flip every 5th bit, and run the resulting file (which is the 7-Zip executable).  Your custom extractor can also have a GUI and do whatever else you're seeking to have happen. The only way to do this is to have the file extract the data in a way that 7-Zip cannot read.  If 7-Zip can read the data, then 7-Zip can show the contents, because that is what 7-Zip does. The inner archive can be created with 7-Zip and provide desirable features like compression and keeping track of multiple files.  By using a standard format, you don't need to learn how to implement all of the quirks of a standard format, but you get the benefits provided by the common support of that format. Consider making multiple archives.  One way you can probably achieve your goals is to place an archive within an archive.  Make the inner archive perfectly accessible by 7-Zip, but that won't matter because nobody can see the inner archive until they first crack open your outer archive.",1
"About Office 2007, it shouldn't be any issue installing this software except if that DVD is broken, scratched or files corrupted. You're probably dealing with a bad combination of drive and disk. Windows 7 and Office 2007 work fine together. If you have a second machine you can try these options to try and get the software installed:  I was trying to install Office 2007 on my new Win 7 box, but after inserting the DVD (original Microsoft disk), the drive kept trying to read the disk and nothing happened at all. I couldn't browse it nor start manually. After removing the disk I got the error message that the format operation failed . The disk seems fine, because I was able to mount it correctly under Linux on the same machine. Any ideas? I thought Win 7 was supposed to be better than XP an Linux? Sounds to me like the disc is damaged or your DVD drive/drivers are bad. Try initializing the installation process on another (Windows) computer. If it allows you to initialize the installation, then it is the drive or that machine. If not, then the disc is bad. Just because you are able to mount the disc in Linux doesn't mean that the disc isn't too damaged to install properly in Windows. Same for me as GAThrawn. You also can try to install Office 2010 Beta that is now available and I use it, so far so good!",4
"Works great for me with smaller numbers of tabs, but there is no reason why you can't have many many folders saved Take a look at Session Manager. It may have the options you need. Maybe not exactly the way you want it, but pretty close. I bounce back and forth between three to ten projects on which I am doing research, and I often have 20 to 50 tabs open on each project. As a result I usually have 100 to 200 tabs open at a time. This is a major drain on memory and creates instability. I would like to find a way in Firefox 3 or 4 to quickly and easily save all the tabs in a single window without saving all the other tabs, assign that group of tabs a name, and reopen named groups of tabs in a single window - as if the tabs in a given window were in a folder that you could save and reopen. Ideally, I would like to be able to open and close tabs in a window and then save the group to the same name and have the altered group open next time. And it should not interfere with the ability of the session manager to save and reopen all my tabs in every window when I quit for the day or if the session crashes. If the dynamic session requirement can be worked around, saving all Tabs to a bookmark folder, and then re-opening them with ""Open All in Tabs"" is a no-extension alternative. Is there any way to do this in a standalone copy of Firefox 3 or 4? If not, is there an add-on or extension that allows you to do it?",3
"The only change I had to make was to change the instances of 'eth0' to the new format of Ubuntu device naming. My devices are enp0s31f6 (primary) and enp3s0 (secondary, unused). And here is the output of ifconfig on the VPN/SMB server when I'm connected to the VPN from another computer in a remote location: Initially I had Firewall issues, where I could not connect to the VPN at all and in fact the SMB Shares were not visible. I essentially turned off the firewall temporarily to debug. The server, which has the VPN and the SMB share is running Ubuntu Server 15.10. The clients are Windows 7, 8 and 10, as well as OSX El Capitan. I have tried expanding and changing the hosts and interfaces in SMB.conf, and I had to some of the rules it added to iptables to use enp0s31f6 instead of eth0. I can connect to the VPN from my house or anywhere else just fine, but I never am able to connect to the shares on the VPN server. From my samba logs, I see nothing about my computer trying to connect, which makes me think SAMBA is not accessible from the VPN connection? I set up a VPN using L2TP and IPSEC (with LibreSwan) using this script: https://github.com/hwdsl2/setup-ipsec-vpn If I navigate to \192.168.42.10 from my Windows 7 machine while I'm on the VPN, it shows MY shares. Going to any of the other IPs doesn't do anything, or immediately says it is inaccessible.",1
"When moving from Location A to Location B, the laptop wakes up from sleep, displaying the screen on the internal monitor. I switch to the external monitor display (using Fn+F8 on this laptop), and only after that do I unlock my session with my password. However, Windows has crammed all my nicely arranged windows into the upper left corner as if it were trying to fit them all on the laptop internal screen resolution. Longer version: I have a laptop that I move between two locations. I have one docking station, and the same kind of monitor configured for 1600x1200, in both locations. The internal laptop screen is awful so I don't use it. When moving from Location B to Location A, I have the laptop lid closed when using the docking station so Windows apparently concludes the screen resolution is 1600x1200 and doesn't resize any windows. Short version: When moving my laptop and sleeping between using different monitors, all my open windows are crammed into the upper left corner as if they tried to fit on the laptop internal screen resolution. I plug in and switch to the external monitor before unlocking my session. Is there a way to prevent this automatic resizing?",1
"Any ideas about how to get into the drive outside of windows? I have seen virus protectors that will boot there own OS to fix a virus so I assume some software must be available that will do the same to recover data. Ok , it took a while but I have now tested a new adaptor and also tried the hard drive on 2 windows 7 PCs and a widows XP. The result is the non windows TV runs the disk (in read only mode) and on all windows systems it will install the drivers sucsessfully but after that the drive is not visable/accessable. Divice manager will find it but not access it, and disk manager will not see it at all. My conclussion is the MBR is broken or missing. I can connect and read the harddrive on my smart TV from Philips, but cant recover the files from there. Any ideas how I can copy the data that I need off of this drive? Is there a third party tool available that I can use? I am recieving a message that the drive is not initialized. When I go into Computer Management I can see the drive (as ""Disk 2, Unknown, Not Initialized""), but when I attempt to initialize it or right-click and select ""Offline"" It tells me that ""the device is not ready"". I have an old Seagate Barracuda 7200.7 hard drive that I am attempting to connect to a Windows 7 machine by using a USB 2.0 to IDE/SATA Adapter.(it was previously connected to an internal IDE connection untill it disapeared from the disc manager) I'm connecting things in the right order (ide, power, then usb) and the jumper settings on the drive are neutral.",1
"There are a lot of tricks which can be sold as a service. For example, there could be a transparent proxy gateway between you and a server farm with a really good network bandwidth. Some tricky coding-recoding of the communication (for example, forcing caching/compression, or caching parts of html pages) were also possible. In the dial-up and Windows 98-days, I used some software that preemptively followed every link in the background for every page I opened in a web-browser.  The results were stored in the browser's local cache, like any other page I had previously visited.  It was still a look-ahead caching proxy of sorts, but it worked on my computer without the need of a separate accelerator proxy, and it worked really well for surfing web-pages.  It made dial-up feel instantaneous; however, long downloads (audio and video) still took normal speeds, because they were simply too big to preemptively download and cache.  And, it could not help AJAX and other dynamic pages.  It eventually failed, because they could not keep up with browser development, and they slipped over the event horizon like most every other dot-bomb company. 99% of all ""Speed up/Tweak your PC"" are fake or do nothing but the very basics that every good SysOp will do anyways. There are however some tweaks for unusual system settings which in fact might result in a speed advantage. For example Windows isn't really prepared for 100 mb/s DSL connections and is noticeably slow (like ~80 mb/s only) unless you tweak the Registry properly. In this case a tweaking tool might save you some hassle, but usually appropriate tools are freely available either directly from Microsoft or from third parties. If someone re-invented that wheel, they could make a lot of web-surfing feel faster, although you can never exceed the ISP-provided bandwidth, and AJAX pages would be a hairy thing to try to accelerate (i.e., look-ahead cache). The main problem with these solutions, that these goodstanding people before me simply don't understand, how they could work. One example would be, you send a request to a server with a fast connection to download the content you want and relay it to you in a low bandwidth friendly way. Essentially zipping the internet.",4
"This often happens when the catalog cache is corrupted. Boot into safe mode and delete the %SystemRoot%\System32\catroot2 folder ( do not delete the ""catroot"" folder! only ""catroot2""!). The system was SP2, I have installed SP3 in hopes that would help, and I've also downloaded and installed the mouse drivers from Dell's site (there are no specific drivers for the keyboard), with no change. I've tried manually searching for the driver (using the Have Disk option) - the first file it's looking for is in the c:\i386 directory, but that installs a generic HID mouse device; the system then runs the hardware wizard for a new ""unknown"" device. I have a PC that up until a couple days ago was working fine.  I moved it from one site to another and now when I plug in the USB mouse or keyboard (the same ones that were working previously) XP brings up the New Hardware Wizard.  Going through it, the correct keyboard and mouse are identified, but XP can't find the drivers. If you have the drivers and utilities disc. Then install all the chipset drivers for this system, once this installation is complete, restart the system and connect the keyboard and mouse and check the functionality. Go to the device manager > choose on which device you are facing problem > go to update driver > and then click browse from the computer and choose any suitable driver.  This happens when you connect one USB device to a different USB port on your machine. If you want, you can try to plug the keyboard and mouse into their original USB ports which would bypass all of this mess.",5
"I think you described the job of a Network Administrator.  Not a System administrator because you never mention any type of system that you would administer, except for your comment that you ""patch security holes"" which is a pretty vague statement.  I really wouldn't consider you a consultant either because consultants typically are hired on a short contract or temporary basis, where you specialize in a specific area (HP hardware tech, Dell hardware tech, Terminal Server tech, VMware tech, etc.), and you didn't describe anything in your job which fits you into that position. A consultant is, in my view, someone hired by the company to accomplish a specific task, where as a full time position of monitoring and maintaining the infrastructure would be considered outsourcing.  The job you have described is that of a sysadmin. A consultant is usually someone with a large amount of expertiese and experience who will advise a client on a particular task or project. The consultant may do the work, but often is just there to advise and provide a high level overview. I call myself an IT Administrator, because I do servers (~100 total) and networks (~6/7 WAN links) across 4 physical sites.  I'm not sure how big of an IT department you have where you are, but typically in larger environments IT is broken up into multiple areas Network Admins, Network Engineers, System Admins, System Engineers, Help Desk, etc. But when it comes to smaller environments you can play the roles within IT, for example the Help Desk, Network Admins and System Admins could be coupled into one job.  Where I work at we have around 30-40 IT guys so we are broken up into the different titles that I listed above, which keeps us focused on certain specialties within our infrastructure.",3
"The reason for this distribution discrepancy is that I can't just do a random split on my images, since they are taken by some 10 cameras all taking pictures of the same respective static scenery (highways). This means that I have to hold out all images of about 2/3 cameras as validation and use the other cameras for training, otherwise the model will be too optimistic, since validation images come from the same camera as the train images.  This makes the weighted loss uninterpretable for the validation data. Is there a way to counteract this? For instance, would giving separate weights for the train/validation sets (based on their respective distributions) help? I have a 3-class image classification problem. The classes are highly unbalanced (with about 98% of the images beloning to one class). To counteract this unbalanced data, I weight the losses by using the inverse of the class probabilities (1/(Proportion of class)). This means that the loss gets multiplied a great lot for the minority classes. This does help in counteracting the class imbalance. If you wanted to compare the training loss to the validation loss, you cannot use the same weights if the distributions differ.  Instead, you can simply compare the average unweighted loss per set.  The weighted loss is for penalization. The validation set is used to estimate how well the model is fitting to your solution space.  For instance, if your training loss continues to decrease but your validation loss remains constant or increases, then you should employ early stopping as to avoid overfitting.  This does not require a weighted loss. However, for this weighted loss I use the proportions in the training dataset. In the validation dataset, the distribution is somewhat different (96.66%, 2.59%, 0.75% in val set as opposed to 98.28%, 0.98%, 0.73% in the training set). This means that if I use the weights based on the training set, mainly misclassified class 1 images gets punished a lot more than they should according to the distribution in the validation set.",2
"Assuming that's the case, and that the work you're doing will end up on a Web server/hosting provider eventually, I'd recommend getting into the habit of setting up test sites (with or without authentication) on the Web server that will host the site eventually or a dedicated test or staging server.   As ErikA mentioned though, it would be much better to just use the firewall rules or Apache config and limit access to the appropriate users. If you had a dialogue opened up for every time someone hit your IP on port 80, you'd have a never-ending flood of dialogue boxes. The real solution is to either require authentication on your site (.htaccess style) or alternatively, set up your firewall to only allow connections to port 80 from a pre-determined list of IP addresses. You could keep a terminal open and tail -F /var/log/apache2/access.log (or equivalent). You could also tail -F /var/auth.log | grep -i port 80 (or equivalent) in another window. Or you could use multitail to watch both files in the same window. The reason for doing this is a) you don't have to open up the world to your workstation b) you can leave it up 24/7 c) you won't have any ""surprises"" when promoting your changes live (like broken paths or absolute URLs that you forgot about) because you've already been doing this all along. Is there a reason why you have port 80 open to your development machine in the first place?  i.e. are you letting clients view the work in progress of the site off of your Mac? This can be as simple as setting up another virtual host in Apache on a sub-domain such as dev.example.com or if you don't have control over their DNS, setting up client.yourdomain.com and having that on a separate virtual host somewhere. You could just use tcpdump/wireshark and watch eactly what is happening.  Add the appropriate filters so you don't get flooded with more information then you need. You could also check out the app, Little Snitch. It's pretty great at catching all kinds of traffic, and let's you decide which bits you want to allow and deny. I think it's exactly the tool you are looking for.",5
"Ultimately.. installed opera 28, installed extension ""Disable Youtube HTML5 Player"" and it's good now.  The firefox extension Flash Control always worked for me. It disables autoplay of both flash and html5 media. It provides placeholders F for flash and V for html5 media that can be clicked to get played. But now that HTML5 video is getting popular, I see a lot of ads popping into view again. They are not only a distraction, they hog resources on my computer and make the fans spin full speed. The bad thing about Flash video was that it required a third-party plugin to play the content. The good thing was I could select which content to play; using the click-to-play feature in Firefox and Chrome. For Chrome/Chromium, there is HTTP Switchboard (available in Chrome store or github). There is a column in the matrix which can be selectively be turned on or off, which turn on/off HTML5 video/audio/SVG/fonts -- for whatever hostnames you wish. In Firefox you can set media.autoplay.enabled to false in about:config though as of now this may not be sufficient for every case. An extension like NoScript (default settings) could be used to complement the missing functionality or even otherwise used by itself without the about:config setting. In NoScript it's also possible to extend the functionality to whitelisted sites (NoScript Options > Whitelist) via NoScript Options > Embeddings: Apply these restrictions ...",5
"I was very impressed at the power saving for a better machine! And as I run Hyper-V on it I was able to run the old server image, my build machine, a Home Server virtual machine and others all on the one box so it was well worth the upgrade. Give Edison a shot.  I never actually figured out what it does for you, but it does tell you the energy consumption of your computer, how much it costs, etc... You may be interested in a Kill-A-Watt device; it allows you to directly measure the electricity usage of any device. Any specific reasons for windows home server?. Linux is a good choice for home server if you have little experience with pc's as you may have to do some customization. You can build a custom pc with 45w processors. If you have a budget around $500+  then you could build a Windows Home Server with this and it only consumes around 80W (http://www.mini-box.com/PicoPSU-120-WI-25-12-25V-DC-DC-ATX-power-supply ). I believe the dedicated home servers are designed to be low power but if you add lots of external HDDs with their own power supplied add about 15w (0.36 Kwh a day) for each one. I recently upgraded a server I had running at home, it was a Windows 2k3 install which I believe is what is under the covers of Home Server.",5
"However, there is an extension to BGP, typically known as additional-paths which puts an identifier on each route sent to a peer and thus enables BGP peers to exchange multiple routes to a path, thereby giving you not only a greater scope for traffic engineering, but also a reduced convergence time in the event of path withdrawal since the non-bestpath prefix(es) can be installed in the FIB as a backup route(s) that will be used the moment the bestpath gets withdrawn - this is particularly significant if you're running full internet tables as the BGP scanner on a Cisco router will run every 30 seconds at best and walking over 300,000 routes takes its toll. Since it is an extension, both routers must support it and negotiate the feature at connection time (or alternatively, have negotiated dynamic capability). It is usable with both eBGP and iBGP. For EBGP if you have multiple sessions to the same peer on a single router you should be using BGP ""multipath"" to allow ECMP. Depending on how and where route reflection is done in an SP network this can result in using all sessions quite evenly. There's so much filtering between you and the LG (looking glass) that can get your prefix removed from on path or another.  What's the length of your advertised prefix?  Do you have PI or PA space?  Since the bestpath is advertised to peers, you two paths can be competing with one another upstream and only one is making it to the LG.  If you shutdown one of your paths, does it then show up in the LG after convergence?  That will tell you you're losing one to bestpath.  Try LG's with your providers to confirm your prefix is there first, then try to work your way up to the next SP. Note that BGP additional-paths has nothing to do with ECMP since obviously, we're talking about receiving multiple prefixes with the same next-hop - with the only exception being cases where you're running iBGP and not using next-hop-self (or doing dirty things with eBGP where the nexthop is preserved) Typically, BGP peers can only send a single path to eachother, with any updates for that path replacing the existing one. One aspect not yet covered by these answers is BGP ""add-path"" which lets a BGP peer send not only their best route, but some or all of their alternates as well. Trying to look for your own routes in looking glasses does not always work the way you expect, particularly if you buy from someone who purchases a lot of transit. For example, Internap buys transit from many other large ISPs. If you buy from Internap and then someone else, there is a good chance that your route on the other provider would be hidden by virtue of not being best. Most ISPs prefer their own customer routes. Seeing or not seeing your own routes in a looking glass has very little to do with if your route is propagating correctly or not.",4
"I don't want to answer definitively, as I'm not a hardware expert, but I've swapped PS/2 mice and keyboards hundreds of times with the power on and never had a problem.  One thing you may notice is that any custom settings for the keyboard may get reset; for instance, if you've increased the key repeat rate in the O/S settings, it will probably revert to the default. I'm unclear whether there is a possibility of hardware damage (I've never done any damage, and it's just a serial protocol, so I THINK it's safe hardware wise).  The problem that I've found is that when hot-swapping PS/2 devices, I often lose control. That is, I take one keyboard out, put the new one in, and it won't work until I reboot the device. I'm unclear whether this is an interface level problem (something gets out of sync in the protocol, or a resettable fuse trips somewhere), or whether it's a driver level problem (lost comms mid-packet and never re-syncs), but I know that swapping PS/2 keyboards is a very hit-or-miss proposition.",2
"NTFS-3G (Fuse), was slapped together and is fully functional. The project has the driving commercial force of Tuxera.  This project addresses the original problem of utilizing NTFS from Linux.  Tuxera offers a premium proprietary NTFS kernel driver, which highlights why the community needs to complete Linux-NTFS.   So with the original problem addressed, the community outcry cooled off.  Which can be unfortunate, as many times the correct implementation never gets completed.  When I think about it, Tuxera actually protected its proprietary kernel NTFS implementation.  Creating an inferior FUSE driver, cooled the push for a solid performing GPL kernel driver. Linux-NTFS (kernel FS driver), Was created first, and after a while development stalled.  IMO a bad choice, it deserved priority and still does.  This driver has been stable, read only, for as long as I've done Linux (over half a decade).  This only addresses half of the problem, so the community looked anywhere they could. My question then, is if the NTFS filesystem has been successfully reverse engineered, why have the kernel NTFS team not implemented the changes in their driver? At the moment it is still marked as experimental, and there is a good chance it will destroy your data. Once the community identifies a significant problem, projects pop up to address it.  In this case, the problem is NTFS FS utilization.   ...that was literally just a conclusion based on one sentence i read. how does that sound to anyone who's actually educated on the subject? =) Now just to clarify, I am a huge community project supporter/enthusiast.  I just also happen to be a critic, with no kernel programming ability.  FUSE has many merits, especially for specialty FS drivers. The cold hard facts still stand, Kernel FS Drivers provide much stronger performance.  Writing kernel drivers takes much more time/talent, then a comparable FUSE implementation.  Both of which (Time from Talented community Programmers), have always been in short supply. It's a matter of priority. Choosing to do one thing means that something else won't get done. ntfd-3g works well, so touching the kernel driver is very low priority. ntfs3g isn't really a driver, it's an application. it uses FUSE (filesystem in userspace) for an interface and is cross-platform. so, while the kernel ntfs driver could possibly implement the methods used by ntfs3g (could they? i'm not sure), it'd be operating in userspace, which isn't the kernel's jurisdiction. I would ask that the other one be deleted, not this one, as it should not have been migrated in the first place. i just asked myself this question today, actually. here's my really hazy and non-expert understanding of it. Captive NTFS (Driver Wrapper for NTFS.SYS), was comparably easier to create.  As much code already existed in other projects.  The main reason the community looked on, was because NTFS.SYS is not Free Software.",4
"If I have two switches connected redundantly and both are capable of using STP, RSTP, or MSTP, how quickly can an alternate route be detected and normal communication resume if the main route is disconnected? If there is break not caused by link down (eg: configuration change, intermediate device failure etc), then RSTP and MSTP will wait for 3x Hello Interval (3x2 (default) = 6 seconds by default) before re-converging. Is there a theoretical limit to the three protocols and what are some practical delays that add to it? Bear in mind though that these figures are for a single switch - if you have a large network diameter, downstream switches may start detecting failure at slightly later times, meaning they will start this process later and ultimately add more time to a complete topology re-convergence. Added to this is when you're using protocols like VSTP/PVST/PVST+ the switch needs to do this re-convergence for every VLAN, which if there are a lot can be quite taxing on the CPU thus slowing down the r-econvergence further. STP on the other hand is a lot slower because it has a slightly different state machine - it will wait 10x Hello Interval (20 seconds) for a BPDU timeout, and then sits in listening State for another 15 seconds, followed by the learning state for another 15 seconds, giving you somewhere around 50 seconds to converge. In an RSTP or MSTP environment, if an interface goes hard down (eg: it's disconnected or otherwise shut down), then the topology change will trigger immediately - between only two switches, a new tree should establish in less than a second and forwarding will recommence. RSTP is going to be much faster than STP during failover. STP is going to have a failover time between 30-50 seconds. STP Timer = Max Age 20sec. Forward Delay 15 sec. maximum delay in STP = 20 + 15 + 15 = 50 sec.  (Bridge Protocol Data Units) BPDUs are sent to all the switches to notify that there is a change and in STP this is done by the root switch. The root switch is selected by lowest priority or lowest MAC address. The problem with this is that if something fails far away from the root switch then the convergence will be very slow. In RSTP, all the switches are allowed to send BPDUs and are able to converge faster. In STP, BPDUs are RELAYED by the switches but in RSTP are sent every hello time(2s by default).",3
"Although I haven't implemented it yet, I'm planning on moving all of my log-generating machines to rsyslog, and implementing a bastion-type server which will function as the collector of syslogs. From there, I think the free version of Splunk can do everything I need to pull out information.  Then on the loghost we have some custom scripts that are similar to logcheck that basically watch the incoming logs for anything suspicious. I use a central syslog host.  Each edge system sends *.debug to the central loghost.  The central syslog host runs syslog-ng, and has rules to split logs so that each machine generates its own files named for that day.  It also dumps everything into a single file, against which I run a descendant of logcheck.sh. Each of my application servers runs a small perl script to send their logs to syslog, which then forwards on to the loghost (perl script below). Once a day I run a log compacter, which zips up any logs older than 7 days, and deletes anything older than 28 days.  Between the two, it gives logs an expected life of 35 days on the server, which means that all logs should make it to monthly backups, where they can be recovered for up to two years. I have 20-30 Linux servers and a few Windows boxes (most of them virtualized). We utilize a lot of Perl and Bash scripts to do most of our automated jobs and I'm trying to standardize their logging. I've got about 30 servers, and I just use straight up syslog to send all the logs to a single logging server.  For backup, all of the machines are also configured to store their own logs locally for a few days, using logrotate to take care of the rotation and deletion of old logs. Here is my logging perl script.  It works by piping the program's output into it, and then it syslogs the output and spits it back out so you can send it elsewhere (I send to multilog).  You can also give it the -q option to just go to syslog. We also have all of the email from every host going to one place, so that if any program complains that way, we get all the messages.  This could theoretically go to a single mailbox that a program could act on and analyze. I've seen other tools like swatch and logcheck, but I'm not quite sure how all these pieces fit together...  Any recommendations would be greatly appreciated! I've been looking at log4perl and log4sh for logging of scripts and syslog-ng to get all the logs on a centralized logging server. I've also read up on splunk, even though is sounds like the enterprise edition is pretty pricey and I might go over the free license limit with all my servers.",4
"Admittedly, xslx files really are zip files, but we don't want that behavior. Just open in Excel, please. I doubt it's an apache config issue.  Internet Explorer has it's own MIME Type Detection algorithm.  You can circumvent this behavior by right-clicking and selecting 'File  -> Save As' or just use MS 2003 .doc format. Background: For some reason, whenever a user tries to open an xslx (excel 2007) file from our intranet using MSIE, the file download dialog interprets it as a ""zip"" file.  Firefox, OTOH, opens the files normally. Is it possible that the fault is my apache configs? or is this a client-browser-only issue? You can indeed fix this in Apache's configuration.  Add the following lines (and ensure that mod_mime is enabled): if its an apache config issue then there is the file that contains the mapping of extensions to mime types that you can quickly examine to rule out this possibility. This will ensure that Apache sends the appropriate MIME type to the client, and IE will understand that the files are Office documents and not zips.",4
"To the OP, there is indeed currently no way to rearrange jump list items without unpinning all and re-pinning in the order you prefer. MS's decision to disable drag-and-drop functionality here is curious. And annoying for many users. I personally don't use the quick access feature (formerly known as favorites), as I dislike the extra steps to open explorer and having another window to close. Jump lists are more efficient to many users' workflow. Removing drag-and-drop functionality diminished their usefulness, IMHO. 'Hoping MS returns this functionality to Win10 in the near future via updates. I found I was able to do it as with Windows 7 through 8.1.  Drag it as usual but don't expect a visual indication that it is moving.  When you release the mouse button it will indeed appear in the new location.  If you do it slowly (as I had been because I was expecting that visual clue) you'll get a menu instead.  Just quickly drag and drop. The original question pertains to pinned jump list items off the task bar. Quick access and jump lists are independent of one another, therefore rearranging quick access items/lists does not affect jump lists, nor vice-versa. This is currently not possible. You can only do this by un-pinning every item and then re-pin them again in the correct order. I have only been able to figure out how to do this using folders in Explorer. But if you open a folder and go the the left pane, there is a Quick Access shortcut at the top. These are your pinned folders. You can rearrange them in any order you want here. http://www.addictivetips.com/windows-tips/how-to-rearrange-pinned-items-on-the-taskbar-jump-lists-in-windows-10/ Unfortunately many folks are confusing pinned task bar items, pinned jump list items off the task bar, and quick access lists in explorer, and thus, giving irrelevant answers.",4
"There are many factors that go into whether or not bounce back will happen; transmission signal strength, wire resistance, cable length, etc. So, whatever the minimum is, it's long enough to not receive bounce back. In the early days of Ethernet networking, for twisted pair it was one meter. Today equipment is usually manufactured with a number of counter measures to reduce or eliminate the effects of bounce back. I've reliably used cables as small as one foot without problems. But again, that will depend on whether the rest of the physical equipment and software can compensate adequately for the short length. I'm not sure if the minimum cable length was ever listed in an RFC. Whatever the minimum is depends on several factors. For a networking signal, if the signal bounces back and is strong enough to reach the sender, then the sender has to figure out if the signal is new, or a reflection of its own transmission. A basic description of the problem caused by cables being too short is that the electrical signal, when transmitted will bounce off the end of the wire and reflect back in the other direction. Just like throwing a ball at a wall if you're close enough to the wall when the ball bounces back it may reach you.",1
"It is possible to configure IP and Domain Restrictions in the web site features of IIS Manager. But it could also be an application functionality.   Is there anything in the configuration to allow IIS to process all requests from the same IP at the same time no matter how many User X sends (This is all an internal system) If you check in IIS Manager > Sites > [Web site] > IP Address and Domain Restrictions and there aren't any entries, it may be an application configuration.  It may also be present at the server level node. However, User X cannot access the webpage as the report is still running and it appears IIS won't allow more than 1 request from the same IP address to be processed. It will wait for the latest request to finish before it moves on to the next one.  If there are entries, you can right-click on them and select Edit Dynamic Restriction Settings to view or configure the maximum concurrent requests. In our internal network, we have many applications hosted on different pools. User X might load a report which sends a request to the web server to run. Whilst this request is running, user X may also try and load the home page in order to perform another task.",2
"Imagine that your data is not easily separable. Your classifier isn't able to do a very good job at distinguishing between positive and negative examples, so it usually predicts the majority class for any example. In the unbalanced case, it will get 100 examples correct and 20 wrong, resulting in a 100/120 = 83% accuracy. But after balancing the classes, the best possible result is about 50%.  I have created a synthetic dataset, with 20 samples in one class and 100 in the other, thus creating an imbalanced dataset. Now the accuracy of classification of the data before balancing is 80% while after balancing (i.e., 100 samples in both the classes)  it is 60%. What are the possible reasons for this? To tackle an imbalanced dataset, first you have to choose which question you want to answer. Then, what's the good metric for this question. Answer these 2 question first before deciding which technique you should use. That's because this technique puts more weight to the small class, makes the model bias to it. The model will now predict the small class with higher accuracy but the overall accuracy will decrease. For the original dataset, if the model just makes a dummy prediction that all samples belong to the bigger class, the accuracy will be 83% (100/120). But that's usually not what we want to predict in an imbalanced dataset. The problem here is that accuracy is not a good measure of performance on unbalanced classes. It may be that your data is too difficult, or the capacity of your classifier is not strong enough. It's usually better to look at the confusion matrix to better understand how the classifier is working, or look at metrics other than accuracy such as the precision and recall, $F_1$ score (which is just the harmonic mean of precision and recall), or AUC. These are typically all easy to use in common machine learning libraries like scikit-learn. Let's take a fraud detection problem. The probability that a transaction is a fraud is very small (let's say 0.01%) but the loss of an undetected fraud transaction is enormous (e.x. 1 millions dollars). On the other hand, the cost of manually verifying if a transaction is relatively small. In that case, we would like to detect all possible frauds, even if we have to make a lot of false positive predictions.",3
"Consider the BitTorrent approach: When a Torrent is created, all of the files are split into 'blocks', each block individually hashed, and each of those hashes recorded in the .torrent file. This is what allows a peer to incrementally verify incoming data, without having to wait for the entire file to finish downloading first. Errors can also be corrected on a per-block basis, instead of requiring re-transmission of the entire file. Aside from the logistical benefits, this approach also allows hashing to scale across multiple cores - if 8 cores are available, 8 blocks can be simultaneously hashed. You can use md5deep for this and hashdeep for other hashes. It supports multi threading with the -j flag. By default it will create a hashing thread for each core. It also has a flag to break files into pieces before hashing but will not use multiple threads on a singe file. I've used this for getting sha256 of half a million files and it worked great. It also has a recursive flash which makes handling large directory trees easier. Here is the manpage for it http://md5deep.sourceforge.net/md5deep.html and git repo https://github.com/jessek/hashdeep If you engineer your verification process to work on some subset of the data, e.g. blocks of some fixed size, you can hash each block on a separate core, thus eliminating a large amount of delay in the pipeline. Obviously, this approach has a small time/memory trade-off: each additional instance of hashing has some overhead associated with it, mostly in the form of memory, although this is minimal unless you're running hundreds of instances. Most of the answers here have addressed the linear nature of most hashing algorithms. Although I'm sure there exists some true scalable hashing algorithms, an easier solution is to simply split the data up into smaller pieces, and hash each individually. I'm working on a tree hashing project, which is designed for exactly this problem: off-the-shelf parallel hashing of large files. It works now, though it hasn't been reviewed, and there's a good chance that changes from review will result in changes to the final digest. That said, it's very fast: https://github.com/oconnor663/bao",3
"Also, is it safe to keep both the interfaces enabled (USB wifi + onboard wifi) on static IP address connected to the same ssid.? Will they create any clash.? I was planning to keep both the interfaces enabled, just in case if one fails for some reason, another interface can keep the things going.? To use two interfaces for failover it isn't done to give both an ip address. Wifi will not switch over automatically. You have to reboot. To avoid this you must use bonding. Here is a suggestion to use dynamic failover. I have a raspberry at a very remote site where the wifi range is not that great. I am using a USB wifi with the antenna to connect to the wifi. Should I disable the onboard wifi in this case as I am using external USB wifi.? It doesn't matter if both interfaces are activated when then build in isn't configured in /etc/wpa_supplicant/wpa_supplicant.conf. But to avoid possible confusion I would prefer to disable the build in wifi interface with putting this into the /boot/config.txt:",2
"If I understand you correctly, you want to set your BIND server as the source of your zones but you want to use other 3rd party servers as the authoritative servers for the domains, acting as slaves to your BIND host? You will need to have A records set to resolve to these 3rd party server IPs as your ns1/ns2 and also register the vanity NS. I have my domains then pointed to these slave servers as the authoritative servers (both primary and secondary) for these domains. All this requires is a configuration change at your registrar, eNom. I have my host server hosting all my zones and providing AXFR zone transfers to a bunch of slave DNS services (there are many available on the internet). You will be able to find guides on how to do this with bind. This can definitely be done and I have a similar setup. This is my setup (kind of like a CDN for DNS). I also have a vanity NS setup so that ns1.mydomain.com actually points to the authoritative server IPs and not my host. eNom should be able to setup a vanity NS setup for you.",1
"In the good old days your program was responsible for doing everything that needed to be done during the execution of your program, either by you doing it yourself or by adding library code others wrote to your program.   The only thing running beside that in the computer was the code to read in your compiled program - if you were lucky.   Some computers had to had code entered through switches before being able to do more (the original ""bootstrap"" process), or even your whole program entered this way. I experimented with a Microkernel capable of running a Java Virtual Machine, but found later that the sweet spot for that is Docker. Kernels, in addition to mediating shared access to RAM and hardware, also perform a loader function.   Finally the terminal program means that your program doesn't need to know how to draw a window, nor how to talk to the kernel graphics card driver, nor all of the complexity to do with dealing with screen buffers and display timing and actually wiggling the data lines to the display.  Doing I/O in user-mode (without the kernel) is not possible, if you access I/O ports or registers for devices, or addresses connected to devices (one or both needed to perform any I/O), these trigger an exception in the same way. In typical desktop OSes, the kernel itself is an executable. (Windows has ntoskrnl.exe; Linux has vmlinux, etc.)  If you needed a kernel in order for an executable to run, then those OSes could not exist. Say you have a terminal program running on a desktop window manager, running on your kernel which in turn is running on your hardware. If you load this executable somehow from an operating system somehow (i.e. if it allows raw code to be loaded and executed), it will still be in user mode.  If your code accesses things that are prohibited in user mode, as opposed to kernel mode, such as unallocated memory or I/O device addresses/registers, it will crash with privilege or segment violations (again, exceptions go to kernel mode and are handled there) and still won't work. If your compiler produces a file that's meant to be processed by an operating system loader, and that loader is not there, it won't work. The kernel does not process every instruction in a program.  It just does the system calls and switches between running programs to share the CPU. These executables also reference libraries (Windows .dll or Linux shared object .so files) - their code has to be included. What you need a kernel for is to do the things a kernel does. Allow multiple executables to run at once, referee between them, abstract the hardware, etc. Most programs aren't capable of doing that stuff themselves competently, and you wouldn't want them to even if they could. In the days of DOS -- which could barely be called an operating system itself -- games often used the OS as little more than a loader, and directly accessed the hardware much like a kernel would.  But you often had to know what brands and models of hardware were in your machine before you bought a game. Many games only supported certain families of video and sound cards, and ran very poorly on competing brands if they worked at all.  That's the kind of thing you get when the program controls the hardware directly rather than through the abstraction typically provided via the kernel.) Your program ""needs"" the kernel in just the same way it needs the standard C libraries in order to use the printf command in the first place. The actual code of your program runs on the CPU, but the branches that code makes to print something on screen go through the code for the C printf function, through various other systems and interpreters, each of which do their own processing to work out just how hello world! actually gets printed on your screen. And that is where we are today.  Your programs run full speed but whenever they need something managed by the operating system they call helper routines provided by the operating system, and that code is not needed and not present in the user programs themselves.  This included writing to the display, saving files, accessing the network, etc. Sure.  You need to convince the OS to somehow run your raw code without processing any metadata.  If your code calls kernel APIs, it still won't work. The kernel is ""just"" more code. It's just that that code is a layer that lives between the lowest parts of your system and the actual hardware. Effectively everything you do that needs hardware access, be it display, blocks of memory, bits of files or anything like that has to go through some device driver in the kernel to work out exactly how to talk to the relevant device. Be it a filesystem driver on top of a SATA hard disk controller driver which itself is sitting on top of a PCIe bridge device.  The kernel knows how to tie all these devices together and presents a relatively simple interface for programs to do things without having to know about how to do all of these things themselves.  All this resulted in a large amount of helper code being moved out of the individual programs and into the ""operating system"", with a standardized way of invoking the helper code from user programs.   Essentially, only system calls go to the kernel.  Anything to do with I/O or memory allocation/deallocation typically eventually results in a system call.  Some instructiosn can only be executed in kernel mode and will cause the CPU to trigger an exception.  Exceptions cause a switch to kernel mode and a jump to kernel code. It was quickly found that it was nice to have code running capable of loading and executing program.   Later it was found that computers were powerful enough to support running several programs at the same time by having the CPU switch between them, especially if the hardware could help, but with the added complexity of the programs not steppings on each others toes (for instance, how to handle multiple programs trying to send data to the printer at once?). Doing memory allocation in user-mode (without the kernel) is not possible, if you access memory you don't have permission to access, the MMU, previously programmed by the kernel, notices and causes a CPU-level ""segmentation fault"" exception, which triggers the kernel, and the kernel kills the program. Desktop window managers provide a layer that means that programs don't have to know how to draw windows and play well with other programs trying to display things at the same time. Many ""executable formats"", like ELF or PE, have metadata in the executable file in addition to the code, and its the loader's job to process that.  Read the gory details about Microsoft's PE format for more information. Microkernels have been written that provide just what is needed for a given program to run without a full operating system.  This has some advantages for the experienced users while giving away most others.   You may want to read the Wikipedia page about it - https://en.wikipedia.org/wiki/Microkernel - if you want to know more.",4
"I'd like to use the ruby_block to determine the url for the remote_file to download from, so the order is important. The first output we see is ""in remote_file"", as the puts statement is executed when Chef compiles the remote_file resource.  On the next line, we set the source parameter to the value of node.default['test']['foo'], which is apparently {}.  That's not a valid value for source, so the Chef run terminates at that point - before the code in the ruby_block ever runs. Chef has a compile and a run phase. I assume that the code inside the ruby_block is not executed during compile phase, because it is inside the block statement (which would then be executed during the run phase). The puts inside the remote_file block however is at the ""attribute level"" inside the resource definition, which is really executed by chef (in fact, afaik the source node.default... is a function call). Hopefully that helps you to understand the behaviour you're seeing, but we still have a problem to solve. As with the first example, we don't expect anything to be printed while the ruby_block is compiled - the whole ""block"" is saved, and its contents won't run until that resource is converged. As we've seen above, resources aren't ""run"", they're first ""compiled"" and then ""converged"".  With that in mind, what you need is for the the remote_file resource to use some data that is not known when it is compiled, but will be known when it is converged.  In other words, something like the ""block"" parameter in the ruby_block - a piece of code that doesn't run until later.  Something like this: Also mention that I don't read the attribute through node.default[], but directly through node[]. Otherwise the attribute precedence feature of chef would not make sense.  If it wasn't for my puts() statements I'd assume that these are getting run in the expected order, because the log says: As StephenKing wrote in his response, the first thing to understand is that recipes are compiled (to produce a set of resources), and then resources are converged (to effect changes to your system).  These two phases are often interleaved - some of your resources might be converged before Chef has finished compiling all of your recipes.  Erik Hollensbe covers this in some detail in his post ""The Chef Resource Run Queue"". Good question - both of your examples work the way that I would expect, but it isn't immediately obvious why. Although you asked ""how can I force the ruby_block to run first?"", your comment to StephenKing suggests this isn't really what you want - if you really wanted that block to run first, you could put it directly into your recipe code.  Alternatively, you could use the .run_action() method to force the resource to be converged as soon as it is compiled - but you say that there are still more resources that need to converge before the ruby_block can be useful. Fortunately, such a thing does exist - it's called Lazy Attribute Evaluation.  Using that feature, your second example would look like this:",3
"I've noticed this both with PostgreSQL streaming replication, 5 minutes like clockwork, as evidenced by the log entries that the connection was lost and reestablished.  Also with ssh, but I haven't timed it there.  Go make a cup of tea, come back and your connection is gone. This is also what compose.io docs recommend for connection debugging https://docs.compose.io/common-questions/how-to-connect-to-mongodb.html If you are already doing that, maybe you can debug the connection using the mongo shell to connect to your DB instances. I have no experience with MongoDB specifically, but I have noticed that AWS will sever TCP connections (each end thinks the other end closed it) after 5 minutes of idleness, and that the keepalive setting does not prevent this. First a probably silly question, are you replacing all the , , etc placeholders with the actual values from your compose.io database? As per MongoDB BOL Here if you shall connect to a ReplicaSet consisting of one primary and 1 or more secondaries. To Do this we need to supply the driver with a seedlist of servers and the name of the ReplicaSet we wish to connect to. Lets take a look at a code example.",3
"Most continuous integration tools have a git plugin or native feature that allows you to filter on a path. If you were using Jenkins, you could set up a few jobs, each with the included region to be be the pathway that should be triggered -- so, e.g. changes to App1/* would trigger jobs/app1/ which executes make -f App1/Makefile or similar. The fact that all the code is in a single repository doesn't mean that all the code is changing every time a commit is pushed. I would first make the ""pathways"" in the code explicit. E.g. perhaps you have a few subdirectories: The trick then to maintaining this project is really in the Makefiles, or other build tool of your choice. You would write the dependencies in there. You could also have a single global Makefile and use the git diff-tree trick in the question mentioned above: Assuming that App1 is independent from App2, you could trigger it's build by checking if anything in that directory changed (see https://stackoverflow.com/questions/424071/how-to-list-all-the-files-in-a-commit).",1
"When I make this change, the entire scene becomes shadowed. I'm sure I'm misunderstanding something somewhere, but I'm not sure where. I have tried varying the constant term (70) between 1 and 70, though this doesn't appear to make a difference. I have no errors reported by KHR_debug However, my understanding is that the whole point of this, is that I can move exp(70 * lightDepth) out of this shader, and into the shader when I generate the depth texture at the light. That is: I'm playing around in my little toy project to see if I can understand how exponential shadow mapping works. To begin, I have the following two fragment shaders: This works fine - I have shadows where I expect them, and I'm using the approximation to the boolean function by taking the product of exponentials, as shown in the ESM paper. It is unclear if you can call glClearDepth(INFINITY) or need to clear the depth using a full-screen quad as the ARB_depth_buffer_float extension says ""ClearDepth takes a floating-point value that is clamped to the range[0, 1]."" You are using GL_DEPTH_COMPONENT24 which is a fixed-point format clamped to the range [0, 1] inclusively. I don't know how supported it would be to clear the depth with a quad at depth near-infinity regarding clipping against the far plane: The clipping calculation might go nuts with rounding errors. Your best bet is to draw a fullscreen quad at a reasonable depth (1.0) with depth-test disabled and FORCE the depth at INFINITY (or a very large non-infinity value) in the pixel shader so this will happen after primitive clipping.",2
"Most sites have a favicon to leave an impression on visitors. It's not needed, but helps distinguish from other sites (say in your bookmarks or something similar). You don't, of course, need to do this.  It just gives a web site a little bit more personality and recognizability.  I suppose you could do as you suggest and put up a blank icon file, but if you'll go to that much trouble, you should just put up a real one. Why don't you just create a small icon that suits your web site and use that?  It's trivial to do.  Most modern browsers look for this file.  I haven't tried it myself, but Googling, I found the website favicon.ico Generator that presents a simple image editor interface. I had this clogging up the Apache logs and wanted to clear it out. For one of my websites, I created a valid favicon.ico. For the rest of my virtual hosts, I simply ""touch favicon.ico"" and created an empty one [Note: Apache running on Linux]. No more errors in the error logs. I thought about the waste of resources, but only for a short time. If I provided a real favicon.ico, then I would be transferring a lot of bytes very frequently. If I provided nothing, I would be sending back a 404 response. Sending back a zero length file appears to me to be the least waste of resources and bandwidth. And my error logs are clear of a spurious problem, so I can focus on real problems with my webserver. While not required, having a favicon.ico will improve the user's experience on your webpage.  Most modern browers request one, and not having one requires your server to generate a 404 error and send this back to the browser.  404 errors can degrade the user performance because of how slow they are. In these days of tabbed browsing, the favicon is more important to me than ever - I wouldn't consider running a site without one.",5
"Thus, your component container should be the abstract ""Component"" type. Behavior specific to each component should be contained entirely within the specific component class. The more important thing to cache friendliness is combining components that need each other's data. Position and speed will usually be accessed very close to each other. Same with position and, say, rotation. So put them all that data in a physics component with the physics behavior. So rather than trying to optimize all the components in aggregate, focus on the actual function of each component. Basically, assume the cache will be wiped between each component update and structure your data accordingly. The main purpose of the ECS pattern is to be programmer friendly, not speed. So this is essentially an architecture problem, although I will also address your cache-friendliness concerns. It seems like you're setting your components up like data records, whereas they should contain all the relevant behavior for their subject. Think",1
"You want a similar user with a different name and different privileges somehow linked to an original user. AD can't do this. A user account is a user account and privileges or group membership are granted to the user account/SID. I have a set of users in Active Directory. They are technical users, ie not associated to a real person. I need to associate them with some password different from the one they already have. Indeed this password will give them only limited rights as compared to their full rights. But doing this I will end up with twice many users to manage. Is there a way to link two users ? For instance, if I delete username, then restricted-username is deleted as well ? Or linking properties of the two users (some properties of restricted-username would points to the ones of username)? I understand that an appropriate way to do this is to create new users with similar names (for example, if I have an account username, I will create restricted-username). This way I can give the restricted rights to restricted-username and use this user.",2
"The easiest and most common solution would be to take a live snapshot of the VM (fsfreeze/flush/use the qemu-ga to maintain fs consistency before taking the snapshot), back up the underlying image, while the VM writes to the snapshot, and then merge the two when the backup is complete.  If you are using LVM for the VM's storage you can also use LVM snapshots, wich are quick and not disruptive. This has the disadventage of not having the VM's RAM into account so it MIGHT have invalid data on any point in time. So where question is, is there a way to do full VM backups that can be done consistently without freezing the VM and restore of backup should work aswell :) I Found several ready scripts which i have tested, but they all either stop or ""freeze"" the VM for a little while, to save memory state. But this is no good for us. We have few custom application that write logs, that need to be consistent and we cannot have 10-60second pauses while system is ""freezed"" for the duration of backups, also this ""freeze"" causes the VM's clock to be out of sync ( clock is behind by the duration of the ""freeze) which in term messes with our custom sofware, where one of its functions is to monitor some measuring equipments timestamps. I'm trying to setup a KVM backup solution for our servers that would take live backups without stopping the system. I'd go with a different approach: instead of saving the whole VM's state, I'd keep a pristine image of a freshly installed VM just in case, and do routine backups of the data I'm interested to keep safe. The specific techique to do such a backup depends on the tools you are using (MySQL? InfluxDB?).",3
"Do this three times or until you get the Automatic Repair screen, let repair load and run its Diagnosis, when that is done click the Advanced Options button when it appears, then Troubleshoot button, then Advanced Options, then System Restore button. When restore loads hit Next button, now you will see your restore points, highlight either the first one or second it the date is not over 10 days old (make note of the name and date of the restore point), once selected hit next. This process takes a long while depending on the speed of your hard drive, be patient and Do Not interrupt the process. My Windows 10 desktop has begun displaying a completely gray screen immediately after login. I recently installed new graphics drivers for Ubuntu and I suspect that my graphics drivers for Windows 10 were deleted during this process. Is there any way for me to reinstall graphics drivers when I cannot see the screen, or possibly install into the Windows partition through Ubuntu? Graphics card is an RTX 2080. I have tried booting into Safe Mode, but the result is the same, with the addition of a non-functional flashing taskbar. Power down the PC, start the PC, before you get to the login screen when Windows is booting press and hold the Power Button until the PC shuts off.",2
"When I was hired, I was told that the LAN was ""running out of scope"". The network had a 24-bit subnet mask and uses DHCP for client addressing. The DHCP server wasn't set to check DNS before handing out addresses. DNS wasn't set to scavenge old records. RRAS was set to get addresses for VPN clients from DHCP, and by default it grabs blocks of 10. So it would grab blocks of 10 and start handing them out to clients, but as our address space ran out, some of them were ""good"" and some were already registered to other clients in DNS. I shortened the subnet mask to 22-bit and the problem was solved. And/or try plugging your internet directly into a 2nd NIC on the vpn server, and use RRAS Basic Firewall + NAT, and change your DHCP so new default gateway is that server. Does anyone have any ideas about what might be going on here, or suggestions for things we might troubleshoot? OR.. you could code either a simple batch script, or pretty simple c# app to automatically redial the vpn endpoint if ping fails after x times, and ship a copy to all laptops (.msi?). Here's a suggestion: Set up another RRAS server and direct the incoming VPN conections to this new server. If the problem persists then you'll have ruled out the server as the problem and can focus on the upstream devices, most likely the Astaro appliance, as the problem. It could be down to PPTP Passthrough not being supported or not working correctly on your outbound router. You also might want to read Multiple VPN Connections  Why It Isnt Possible which explains why this happens and ways around it. It's a good read! Here's the really strange part:  every time users connect to the VPN connection, it appears to work.  On the client machine, the VPN connection shows ""connected"" (and their ipconfig shows a PPP adapter with a DHCP-issued internal IP address), and I can see their connection on COMPANY-VPN01 as well).  However, more than half the time, these connections are no good -- users can't access any internal resources.  Since I've experienced this myself, I've tested some of the obvious things -- tried using FQDNs of internal resources instead of NetBIOS names, tried using IP addresses, tried pinging internal resources by name and IP address.  Nothing goes through.  But if I disconnect and reconnect the VPN connection a few times, I'll eventually get a connection that works flawlessly -- and will continue to work flawlessly, no matter how long I leave it connected.  So, whatever the problem is, it's a) intermittent and b) occurs when the VPN connection is being set up -- because if you get a ""good"" connection, it remains good.  I'm two weeks in as the System Administrator at a midsize manufacturing company with executives and sales staff who make heavy use of remote access.  From almost the day I arrived, I started getting complains about VPN performance problems.  Here's our setup:  Our gateway is an Astaro appliance that is configured with DNAT rules that forward PPTP and GRE traffic to an internal server we'll call COMPANY-VPN01.  COMPANY-VPN01 is a Windows Server 2003 box configured with RRAS.  Our client computers (mostly XP, but some Vista and 7) have Windows PPTP VPN connections set up to point to vpn.company.com, which resolves to the public IP of the Astaro box's external interface.",4
"It sounds like a memory leak. To confirm, use the task manager to check for memory usage right after startup. Then check again when it gets slow. If you do find a process causing a memory leak you can try terminating it.  It can be solved by using an application that forces the clocks to stay at one level. But you'll have to manually put it back to low power mode to save on power when you're done. It is likely a memory leak.  Rebooting would flush the memory, thus fixing it.  Check to see if there is a patch for any software you added recently.  If the software's creator is aware of the leak they would eventually create a patch to fix it.   To determine if this is the issue:  With the same things running check free memory a few times a day to see if it is decreasing. Alright, I found the  responsible. Turns out ATI/AMD graphics drivers have a bug that causes the graphics card to spontaneously go into low power mode during gaming in some situations. Probably triggered by a media application overriding the high performance game trigger. I noticed my clocks go down when I feel the slowdown.",3
"Doing it this way also means you have a PST file with your mail backed up, so if your import is unsuccessful for any reason, you still have all your mail in the PST and can try again. If your company uses Exchange, you will be able to download your mail from the server just by connecting with your outlook client. However, if you have archived any of your mail to a local PST file, you will need to copy that. Just use Outlook to import the .PST/.OST file.  Click File, Import or something like that.  Been a while since I used it.  I assume you were using Exchange?  If the .OST file is bigger, convert it to .PST then import it:  http://www.convertost.com/ On your old laptop, open Outlook, then go to Tool -> Optoins -> Other Tab -> AutoArchive. In the middle of that tab will the the path to your Archive file. Copy this file to your new laptop and point the AutoArchive path on the new laptop to the copied archive file. If Outlook can't read the archive file for some reason, you can import this file into Outlook.  If you do not have Exchange, or you you just want to make sure you've got everything, your best bet is to make a new PST file. To do so go to file-> export. Then select export to a file. In the next menu, select Personal Folder File (PST). On the next screen select the root of your mailbox (Often personal folders), and then export it, to a location you want it to end up in.",3
"You see that await is 119.61 while svctm is 2.44 and %util is 100.20. Note that the avrqu-sz is large, quite large at 141.31. So, the IOs are waiting in the block layer before being serviced and we see a high value of avgqu-sz also meaning that there are pending IOs also meaning to go to block layer. Again, an indication that we are not seeing a sequential stream of IOs but random chunks and that is why, svctm is perfectly fine while await and avgqu-sz are high. Here's my suspicion of what is happening, though I don't know enough about ImageMagick to confirm it.  By limiting the amount of memory to 512 bytes, in order to process the image ImageMagick is forced to use the disk as an intermediary storage method since it cannot use memory.  As a result, it ends up consuming a huge amount of I/O throughput as it pages chunks in and out, causing the system to lock up. With this command I get about the same real time as when not using limit and I have more or less the same IO activities than without the limit options. And btw,good job with the blktrace output. You might also want to go ahead and make a pdf using seekwatcher utility. You set 256B for the maximum memory and 512B for the file mapping in memory. Thus instead of having nice big chunk of buffer read, you read the size of FS block (or even less if you have a 4K hard disk). This is something that would generate a lots of unnecessary IOs. Also, can you please tell what is the throughput of the disk, in disk vendor's term, IOPS. So that I can also tell whether you are saturating your disks. I have run your exact command line (although with a different picture I presume ;-) ) with and without the limit options. And I understood the problem: the limit options unit are in bytes. What does it mean? So, it boils down to how the application is issuing IO. I don't think you can control too much of that about. So, try without limiting the convert command and see whether you can get a stream of sequential IO. From the arguments you have to ImageMagick, you look to be desiring a 200x200 TIFF file... yet your iotop output shows a 70MB/s write out to your RAID array and an absurdly high number of I/O transactions per second. You haven't shown a comparative blktrace without the --limit option but I would assume that when used with limit, the convert command isn't doing random IO or at least the randomness reduces to some extent. Why are you setting the limits so low - in fact, why do you have them at all?  Setting them higher will reduce the amount of paging that ImageMagick has to do significantly.  If you don't trust the images coming in and you are attempting to keep the images from consuming too much resources, put a cap on the size of the images you permit conversion of, set the memory limit to, say, 2-4X the cap you specify, limit the number of simultaneous conversions, and for extra paranoia, limit the command execution time to prevent an intentionally malformed image from cycling forever. sdb3              0.00  1138.00  0.00 410.00     0.00 8700.00    42.44   141.31  119.61   2.44 100.20 When the Q2Q goes high, it means that there are time gap between the successive requests coming to the device queue from the application. Application issues an IO, it doesn't issue the next IO from the next sector of the disk, disk head goes to seek somewhere else and when it finds the proper one sector, application issues the next IO. This kind of IO is called random IO. Random IO increases the Q2Q time or time between requests sent to the block layer.",3
"Temporary removal seems a little risky in view of possible interruptions (eg to power!) so Id suggest working on a copy would be safer. How to copy visible cells only is explained here. However if the hidden rows are all at the top, you could select the first not hidden row down to the last and just copy that area. Then export from copy. If you save once the csv file from excel sheet, excel generates the csv file full of blank spaces, but if you open the csv file and you save it again as csv file using the ""Save as"" menu, this new csv file won't contain the blank spaces but only the used cells. Caveat: this answers the question by way of another spreadsheet program. The following LibreOffice extension, after installation, allows this functionality: 1) Select the desired cell range 2) Click the extension's toolbar button 3) Paste the data into a new worksheet, and save as .csv. My best idea would be to remove the cells temporarily and then revert back to a previous state of the file.",4
"Your next step is to reduce the moisture in the trapped air. Low moisture means less chance of condensation and with no condensation, you get no frost.  For this, I prefer to use silica packs.  You can buy these cheaply, but even better they come in the packaging with many items (shoes, clothes, electronics, and so on).  Most people throw them away, but I recommend saving them as they have many uses. Before you close you enclosure, add some silica packs.  However, before you do that, make sure the packs are ""fresh"" by heating them.  Heating them (120-150C/250-300F) will cause them to ""release"" the moisture they have gathered.  For this, I use either an oven (longer, but you can easily set the temperature) or microwave on 1-2 minute cycles (faster, but you need to figure out a way to determine if you have heated them enough and some outside packages won't do as well). Now, silica packs will not remove all the moisture from the air, but you don't really need to remove all the moisture.  The waste heat from your electronic devices will help to prevent the moisture from condensing on them and most of the condensation (if you have any) would be on the inside of the enclosure itself (which will tend to be colder).  Make sure you account for this by not setting any electronics or power cords directly on the bottom of the enclosure. Whenever I am deploying something outside in an enclosure, this is the procedure I use.  First, make sure you start with the proper enclosure (for example NEMA 3 or 4).  If you penetrate the enclosure, make sure the penetrations are properly sealed to maintain the rating.  (I assume this would be the case as you need electricity and because you mention that the enclosure is made of metal and no one wants their antennas inside a metal box, right?) Buy an inexpensive cooler -- a big plastic or styrofoam one like you'd use for a picnic -- remove the lid, place it open side down over the equipment. As long as there's no drafts in the container/house-unit, the heat from the equipment will stay up inside the cooler. If it's really cold, move the wall-wart power supply/ies inside the cooler too. Since your enclosure is going to be restricting air from flowing into/out of the enclosure, you now only need to contend with any moisture that is trapped within the enclosure with your devices.",2
"I faced the same issue and since I did not find any (for me) suitable solution, I began playing and trying around a little... If you like to put shortcut icons for your virtual drives on the desktop, don't make the shortcut of the virtual drive letter, make it of the actual folder like above: C:\DRIVES\DRIVE-S. You can right-click on the shortcut folder icon, select: PROPERTIES, then select: CHANGE ICON, and choose an icon that looks like a drive or whatever you want. That way if you delete a file using the shortcut, the file will go to the recycle bin. If you want to delete files in SUBST drives, but want the peace of mind that you can undelete it if needed, instead of deleting the file from the virtual drive, example: Drive ""S"", delete it from the actual folder, example: C:\DRIVES\DRIVE-S. It will go to the Recycle bin. I've created a substed drive on Windows 7. When I delete a file, it doesn't go to the recycle bin, instead it is deleted permanently. I copied the $Recycle.bin folder from one drive to my substed drive and this seems to be an easy and working solution. According to various sources, the recycle bin indeed seems to not be available for that kind of drive.",4
"Because you are on free trial, you have the ability at any time within 90 days after your hitting your spending limit to re-enable your subscription and have it automatically upgrade to our standard Pay-As-You-Go offer.  When your usage results in charges that exhausts the monthly amounts included in your offer, the services that you deployed will be disabled for the rest of that billing month. For example, Cloud Services that you deployed are removed from production and your Azure virtual machines are stopped and de-allocated. To prevent your services from being disabled, you can choose to remove your spending limit. When your services are disabled, the data in your storage accounts and databases will be available in a read-only manner for administrators. At the beginning of the next billing month if your offer includes credits over multiple months, your subscription will be re-enabled and you can re-deploy your Cloud Services and have full access to your storage accounts and databases. I'm curious about Azure virtual machines and sharepoint farm solution.after configure sharepoint for 3 days after i had problem,Failed to start virtual machine. While i press restart button it shows error.",2
"If the command window has a specific name when it waits for input its easy to do with autoit. You can check for the name or title of the cmd window an then press y or n. As well it should be possible in different other ways. Im not sure you can accomplish this with batch file but there is a easy 3rd party scripting language that would fit you task. The programm is called AutoIT. You could include your batch script into the AutoIT and then do the y pressing with autoit. To use approach that Ivan Viktorovic suggested (about the window name), you can still use VBScript. It's easily done by checking if the window with that name exists, and then sending window name to ""AppActivate"" and performing Enter key with ""SendKeys"", and sending appropriate keys to send Y or N. This one would be better if you don't want to download additional software. You could always try looking at the ""sendkeys"" interface.  Have a look at tcc from jpsoftware, which has the ability to send keys to other windows, to see if this is what you want.",3
"Also, in the past DFSR has been incompatible with backup, A/V, and other software.  In 2008-2009 we found that Netbackup didnt like DFSR and was reporting success with our file server backups but was actually backing up NOTHING.  Only on testing restores did we discover this horrible, horrible fault in Netbackup.  If theres one thing you never want backup systems to do, is report a successful backup but truly have an empty tape. Windows 2008 R2 does have DFS, but microsoft does not apparently support this large of a dataset (or more accurately, more than 8 million files - according to the docs I could find). Now, I am assuming this is some sort of MSA attached to a physical machine, but in the event you are using a SAN, Check with your SAN vendor for async replication options. The other important things to look at are - are the file permissions maintained? Are the other file attributes maintained eg. what happens to compressed folders? What happens to encrypted files? Are open files replicated? etc etc. DFSR in 2003 R2 (2008 and 2008 R2 are likely much more scalable, exp from being x64) worked so wonderfully for us with 75 servers with all on different WAN links (1-10MB) syncing files totaling at least 500GB (per server times 75) on bad, slow, or saturated WAN links that I would just recommend doing what you can to get DFSR to work.  We found it easier to manage org wide then Veritas, but this was 2006-7 after 2003 R2 came out.  GPOs controlling BITS bandwidth is your friend. Re-indexing databases, defrags, and bulk file moves/adds/deletes have all caused me headaches in the past. The second important feature to look at is - the time required to recovery in case there is a drop in connection. Does the replication start from where it left? Does it re-start the replication? There are numerous ""back-up"" software which makes a 1-1 backup, which would be ok, but since it will be transfering over internet, I'd like something that has compression during transfer like DFS has.   I have a Customer using the SureSync product to replicate data to a ""hot standby"" file server. They're replicating just under 2TB and it's worked very well. It uses the NTFS change journal to track file updates and performs delta replication. Give all the above, block-based replication solutions are much better than file based replication. Host block-based replication would be cheaper than ""off-host"" block-based replication. I'm not finding published product limitations, so you'd probably better check in with the ""manufacturer"" to see. The third - how well (or worse) do they perform on varying network condition and what is the bandwidth utilization. I've used Doubletake Move to move large datasets across the Internet. They use byte level replication, and keep track of changes to files. There is also a nice bandwidth throttling scheduler to use less during the day, and crank it up at night and on the weekend. It also recovers pretty well in the event of a connection break of some kind. I also looked at Veritas Volume Replication, but this seems almost too much as I would also require Veritas Volume Manager.   One of the things with DFSR is it keeps a folder with copies of all the blocks that were changed that need to be sent out, so storage is something you want to be liberal with.  I am curious about the limits you quote, as I would think that 8M files would be fine in context of plenty of resources (RAM, CPU, Disk).  I don't know our file count. The key for looking at replication is to look for a ""consistent set"" of data that gets replicated. For example: Db & corresponding Log files should be replicated in a ""consistent"" manner such that the data is use able on the other replicated site. I want to assynchronously replicate Server A's volume to a volume on Server B for data redundancy.  Something that I can say to my users, ""go here for data"" when/if Server A goes belly up due to machine problems, disaster, etc. Anyway I give a vote of confidence to DFSR, especially in its 3rd version with 2008 R2 as something you should give a shot if you can't find a vendor's product that specifically says they've tested your scenario.  Often what Microsoft officially supports is much more conservative to what they know will work.  Obviously your mileage may vary, and you have to determine your own level of risk you're willing to take. If your rate of change is too high on the source side, and you don't have enough bandwidth to overcome it, you will never get a good replication.",5
"A bit of background, my organization has 160 locations across the state that host a server at each location. These locations do not have any consistent network connections or a steady connection to a data collection or reporting server, they do however have access to the internet and so can report over email or other indirect reporting tool. I am looking for recommendations for good software that can report the status of hardware it is installed on through an email or IP Connection. Solution Found, also by IBM, specifically the company that made the RAID array in our model of servers. ""Simplied interface to trigger events automatically, such as e-mail notication or task execution. "" (page 9). We currently use IBM System x3200 and System x3200 M2 servers in our environment. We have looked at IBM's web page, but we have not found any good monitoring software that meets our needs. I am looking for some kind of software that can monitor the hardware status, particularly the health of the mirrored raid arrays on the server and report back when that hardware shows danger levels or the raid array becomes unhealthy. We currently use Windows Server 2003 enterprise in our environment. Seems like this capability is included with the servers.  You probably just need to install and configure the agents on each server, and setup a management console on one workstation",2
"5/ I presume it is not a hot-swappable RAID controller. So, turn off the system, swap the failed drive for the new drive, and start the system. Go into whichever tool you used to create the RAID set, and confirm that it sees the new drive and is rebuilding. Actually, your answer is not completely correct. in all likelihood you do not want to break the RAID. 4/ If you haven't already, figure out specifically which drive has failed. The error codes might make it clear, or the raid array management utility might tell you. I would not reuse the bad drive in any production system. You could run SpinRite on it and possibly resolve the issue and keep it around a cold spare, but I wouldn't. 1/ Get a replacement drive. If the system is under warranty, HP should send one. If not, go buy one or order one online. The replacement should be the same disk if at all possible. If not, it needs to the the same size or bigger. First, if the system is under warranty, call HP. If it is close to still under warranty, I would call and see if they will cut you a break. Make sure to tell them that you are actually using the hardware RAID. 2/ If you haven't already, take a backup.  At the very least, get a Dropbox or SugarSync account and get a copy of your important stuff off the machine.",1
"For database backups (and backups in general) I would always prefer to use ""real"" backup software that can handle all of these. We do a full back up once a day and then have a per-customer policy of weeding out old backups, generally we keep 4 last daily backups (to survive the weekend ;), 4 last sundays and 4 last first sundays in a month. After that one or two dumps a year are going to the archive to be kept forever. We also keep the replication logs for however long we can afford to spare the diskspace. They are also pretty useful as the debugging tool as they record exactly who changed what and when. Our standard setup is a HA cluster with two databases one replicating to the other which is read-only. Most recently, I've managed MySQL servers in EC2. We set up EBS snapshots on a 15 minute cron job, 3-5 snapshots were kept. The advantage of this method is that you can restore from one of the twice daily backup and then apply the log files up until the last 15 minutes at the latest. This way you are minimising the amount of data that you can lose. How often you backup your data is up to you however. How much data are you comfortable with losing? If you can afford to lose a days worth of data then backup once a day. Data never changes? Then you only need one copy! Theoretically all you need is one full backup and all the replication logs to be able to do a point in time restore, but more frequent full backups will speed up the restore. One neat trick with backup is to use innodb tables and --single-transaction paramater for mysql dump, that way the backup won't block the database while it runs. I wouldn't advocate a CSV dump as a backup solution; all it will give you is the raw data.  There is a lot more beside that, particularly with a database.  Table descriptions, views, stored procs, you name it.  If you don't have these as well you won't be able to get it back successfully.  There is also the RDBMS application and config to consider.  You might have a large number of patches on, which you'll need to also put on your recovery environment to get it to the same level.  You might be running some special configuration dictated by the requirements of your applications.  You might even have a specific set of OS settings required for your database to run optimally.  All of these will also need to be got back, and unless you have a backup solution that's capable of doing them, it's further delays in your recovery time, not to mention no guarantee that you'll get it back the same as it was. For MySQL I use automysqlbackup (http://sourceforge.net/projects/automysqlbackup/), since my backup software (Backup Exec) don't support snapshots on Linux systems. When we did 'traditional' MySQL servers, we backed up daily via MySQl-ZRM. The backups are essentially mysqldumps, which get sent off to tape, SAN, etc, depending on customer's needs.",5
"Or instead of creating truly private properties and functions, you can simply prefix them with an underscore to denote they are private and should never be accessed from outside. While nothing really stops you from accessing them, nothing also stops you from redefining the Team class and accessing anything you want. It's a bit cumbersome. Plus, why should I care that the members are in some array held by TeamClass? In fact, do I need to care about MemberClass at all? Why not further abstract it: Answering your question, how can you pass some variable that can't change outside of the class. Create a getter method: Teams have members, so it almost seems natural to put the Member class into the Team class. But projects also have members. How are you going to implement that? You can't nest Member in both classes. Another problem with nested classes is that you have to traverse through Company -> Department -> Team -> Employee hierarchy just to get everyone's name or email. Of course, it's hard to tell out of context if this is really the best solution to your problem, but it's something to consider. Ideally, how you choose to represent the members inside TeamClass shouldn't matter to the outside world. Your solution is fine, but I think I'd get tired of accessing the members like this:",2
"I'd almost go the other route and redo the other server to only use one partition instead to solve both your path problem and the issue with tampering with a running production server? Or invest in a pair of extra drives for a separate operating system mirror array and wipe, reload them both ^^ Your best bet is to back up everything, wipe the drive and re-partition it, and restore data from the backup.  Yeah, as Bart said, move the load over to the other server and then wipe and reload the first one in peace would be the simplest option. But I don't see much point either in having two partitions if they're both on the same RAID-5 array, fully extended - it's not like it will help performance or that inetpub is hard to find on the system volume. Despite the foregoing, you haven't given a real reason for such an operation, which can cause downtime lasting anything from half an hour to whatever time it takes to restore from the backup. Even when it all goes well the amount of downtime will depend on a number of factors, mainly the size of the partition, the amount of data on it and the level of fragmentation. First off, repartitioning is no big deal, with a number of readily available products to do the job, but there is always a possibility that something can go wrong. Murphy's Law ensures that the more you need this system up and running the higher the probability that it will all go pear shaped. You therefore absolutely must have backups that can be used from the cold metal state, as others have already said. Downtime-it could take an hour, or if something goes wrong, anywhere from half a day to a day or more. You won't know until you find out if something goes kaput. If there's no space to shrink (love Server 2008+ volume management btw, it's finally beginning to do useful stuff like extend and shrink system volumes) you're out of online options. Use a defrag tool that will move files to the beginning of the disk - perhaps the built-in defragger would help you enough. Then re-run the shrink check. Have a well-tested backup and do it during a maintenance window. Better yet switch your standby server into your production server, then turn your production server into a standby and repartition it with the technique above. The old idea of separating the OS and data into separate partitions, at least on Windows machines, is only followed by a very small number of admins these days, as most of us learned a long time ago that there are numerous disadvantages and no real advantages. If they must be separated do so on separate drives, not partitions on the same drive. The use of RAID doesn't change this in any way. You might be able to use a Linux bootdisk with gparted to edit the partition, but you're still taking quite a risk, esp. with the RAID controller. You would need backups. Tested that you can restore it from bare metal. I.e., you are playing with a partition. One mistake = lots of downtime. Primarily I'd worry about migrating it off of being a production server before trying anything with it. You're kind of asking for trouble by playing with a live production server's partitions. Switch your standby server into your production server, then turn your production server into a standby and repartition it, while it might be junk it only has to work while you are rebuilding the old primary.  If this is your bosses idea get him to approve what option he wants- use some planned downwtime and hope that the standby can be brought up if the server rebuild takes longer than expected or swap machines and take potentially degredated services while it's being built.  This shouldn't be your call per se- present the options to the business and let them decide",4
"The actual throughput of the network shouldn't be a factor here, as you won't even come close to saturating a 1Gbps link.  Now that your application must now travel down through layers 1-6 of the OSI model, and then back up to the application layer at the receiving end, each step along the way will be adding a very small amount of latency to the connection.  But to be honest, there's so many other factors at play here, and running 100 instances of your application at once is only making things more complicated. How long does it actually take when you run a single instance? It doesn't feel right that this is a network capacity problem (unless of course something else is eating the bandwidth). When all is said and done your total data transfer for the 100 transactions is 2.2MB which is nothing really. However, that being said - 150 milliseconds is quite a while for such a small amount of data (20KB burst with 2KB return), so unless your network is congested, or has some strict QoS, then there's no reason that the addition of the network should cause it to take that long. You can test this yourself if you like - send an ICMP packet 20KB in size, and see how long it takes to get a response. (ping X.X.X.X -l 20480).",2
"As for backing up the site source code - wouldn't that be something better left to version control software? There are two types of files that will be backed up. The first is the video files described above. These only need to be backed up once per file, as they will never change. The second type to backup is files from the site itself. These should be backed up regularly and tracked for revisions. Most of the changes here will not be coding changes, and the staff making the changes are 1) not technically inclined and 2) distributed throughout the US. I don't think that an svn-based solution will work well given these facts. This gets you a simple duplicate backup of the videos and a more complex timeline-based backup of the site.  And of course, though I use S3, you could equally well use Dropbox or a remote host or whatever. Do you see any problems with this approach? Will I run into issues the first time the script executes, due to the large amount of data to be transferred during the first go-round? Sounds like a decent idea to me, although I think you might be reinventing the wheel here a bit since I'm sure there is backup software out there that would cover your needs. It does everything you listed above with the added bonus of you not having to write a single line of code. It only backs up changed files-so after your initial huge backup it will only backup new/changed files. It runs very fast and is easy to get up and running. If you want to be a bit more bleeing-edge, you might give FSVS (Fast System VerSioning) a look. It's a backup system that uses the Subversion back-end to store files and track versions, but doesn't require end users to interact with Subversion.  Your application sounds common enough that I wouldn't recommend investing the time in rolling your own solution. Second, put all the files 'from the site itself' into a git repo, and whenever you want to do a backup, do a commit and then put a copy of the .git dir on S3 as well.  Note that no one but you has to know how to work git. First, sync all the videos to S3.  Note that this also provides some amount of backup to your website since you can serve the files straight from S3 as well, should the need arise. Something like rsnapshot could take care of your versioning needs (provided the destination machine has enough disk space, of course) w/o having to reinvent the wheel as you are re: your ""backup database"". You'd need to use the rsync protocol, rather than FTP, but you'll more-than-likely end up with less data traversing the wire using rsync anyway. I run a website which currently hosts approximately 300GB (over 1000 files) of training videos on a shared hosting provider. We increase that by ~50 video files per month (~20GB). Currently our backups have been on the desktop machines of our staff, however I'd like to set something up that is more automated. I will be looking into other hosting options, but in the meantime, I would like opinions/improvements regarding the following plan for backups on this server.",5
"Deduplication option is available under Linux, on filesystems BTRFS and ZFS. BTRFS is natively developed under linux and has off-line deduplication tool. I aren't thinking 'offline', you must umount fs. Offline means, actively writed data isn't deduplicated. But later you run tool for deduplicate thinks stored now. Actually probably tool is in beta. See doc available on the page. For speed up writings and readings, you can add faster devices to the storage pool (specially SDD drives or maybe faster flash USB, pay attention on device reliability). Other way is inside ZFS. Avaliable as FUSE and natively: http://zfsonlinux.org/  . This do online deduplication, unfortunately this slow down writes because all must be calculated on the fly. You can online off and on this behavior. After you off deduplication, all deduplicated data will be still stored as deduplicated. New writes will be stored as 'duplicated'. If you want deduplicate that data in the future, you must turn on deduplication and rewrite all 'duplicated' files.",1
"Well, possibly, but it would extremely unlikely... like getting struck by lightning while being eaten by shark unlikely.   First off, the virus is most likely intended for a Windows machine, putting it on an Android device would likely have no effect even if it would executed. Second, Android sandboxes applications, essentially putting up security walls to limit what they can do without your permission even as a root user, which would mitigate much of what malware could do without you giving it permission to do things like change system settings or send SMS messages. Third, is that by default 3rd party applications are disabled, so it if didn't come from the Play Store, it isn't going to be installed, as long as Unknown Sources is turned off in security settings (never just leave this on, if you need it, turn it on as needed and turn it off right afterwards). Although the chances your phone could be infected are nearly zero, if you have concerns, at minimum factory reset the device, but since you are rooted I would assume you have custom recovery (and hopefully a nandoid backup), you could wipe all partitions and restore a nandroid or reflash the ROM image.",1
"The /var permissions are either a red herring or incidental. For the uid-to-name lookup to work, the following must be correct, in order: Turns out that my password had expired. It was odd that I didn't get any previous warnings that it was going to expire. After a couple of reboots and logging in did a statement finally indicate that my password expired. I had this problem once, a misbehaving script had modified permissions of several files in /etc/ on my machine. If that doesn't help (and even if it does), please provide more information about your auth system - LDAP, samba, AD (via what?), or something else. You should also chown the folder to ensure the permissions are back to what is needed for your implementation. More than likely you will need you set the permissions to something like 600/700 with root as the user. But once again, I don't know what you're doing in that directory so it depends on what you need to do. I encountered the 'I have no name!' prompt on my laptop (Ubuntu 14.04) at the office today. My laptop uses pbis (Likewise) to connect to AD.",4
"Using n+1 redundant datacenters and quality hardware may be crucial to how much uptime you will have and how long the lifespan of a server will be. A single request is absolutely no work (relatively) for current hardware.  5,000 domains worth of zonefiles would fit on your average thumb drive. I would recommend that you get yourself 4 mid-range servers with redundant disks and set up a DNS infrastructure in two locations.  This way you have redundancy at each site, and geographically as well.  Consider this:  If your location were go to dark, how would you serve DNS for the 5,000 domains? Obviously one was in a location, and the other in another geographical location, for redundancy purposes. RAID-1 may be sufficient, as long as it's hardware RAID. The only stats I have on DNS are from my personal server farm (personal site, projects, hosting for freelance clients) as I don't run the DNS machines at work. This is for about 6 domains, with about 15,000 WWW views per month. The servers are both older HP Proliants, dual 1.4GHz pentium, 2GB RAM, and both are hosting more than DNS (also DHCP and a slew of other services). I have a script that pulls query stats every hour. I'm seeing an average of 21M queries on my primary server and 100k queries on my secondary each day. The BIND process doesn't even show up in top. Also, considering that ISPs offer DNS servers to their clients, the number of requests that actually make it back to the SOA is far lower than the number of people who are visiting your site.  Say, for example, 50,000 people in Eastern MA on Verizon DSL visit your site in the space of 1 hour, that's only likely to generate a handful of DNS requests which you have to respond to.  Verizon would be doing it's own level of caching. DNS is a very simple protocol that incorporates a TTL (Time to Live) into it's response data.  That being said, a single user browsing www.mysite.com will only ask for DNS once every X minutes (Default is usually 1 hour, or so).  I'm only pointing this out so that it's clear that the number of hits per second isn't so important as the number of unique hits per second. Personally I'm running DNS for about 40 domains on 3 VMs behind a load ballancer (sadly all in one site).  The machines have basically 0 CPU and memory load on them besides the basic OS. We've got MRTG style reports for the bandwidth for each VIP, and the two DNS VIPs show less then 1k of bandwidth average (we've got about 40k customers hitting us daily).",4
"I've looked at http://search.cpan.org/~ken/xls2csv/script/xls2csv and it's just about right except that I don't want to have to use CPAN on every machine I need this on to build test and install the requirements. Is there an easy to compile plain C XLS (Excel sheet(s)) to CSV (or other text delimited format) converter for Linux or the BSDs that doesn't rely on Perl and installing a bunch of module perl modules or using X, any kind of GUI, or Windows? You can use pyExcelerator a python module to read/write excel files. As a bonus it comes with a nice converting tool: py_xls2csv. It is also packaged in debian (so probably in all debian-like and other distributions too) and freeBSD under the names python-excelerator and py-excelerator respectively. If you're happy with xls2csv as it runs in one machine, you can use PAR::Packer to build a distributable package for all other machines with the same architecture. You can include as much of perl and its modules and dependencies as you want, to the point of creating a stand-alone executable with no perl dependencies whatsoever. Quoting from the forementioned perldoc:",3
"If you right click on the Mint Menu applet, and select Preferences, then on the first tab, you can select another hotkey to replace <Super_L>, or if you just want to remove that hotkey at all, you can press backspace. Now I can still open the start menu if I want to via Super+M, but the Super key does not interfere with Super+T anymore. (If you don't need a shortcut for the start menu at all, just remove the entire line.) The shortcut works every other key press which is very annoying. Is there a way to reliably use such shortcuts? Now I want to use for example the shortcut ""Super_L+1"" to open a terminal window. I went to Menu->Control Center->Keyboard Shortcuts and assigned the proper key combination to ""Open a terminal window"". It shows up as ""Mod4+1"". I'm using Linux Mint 15. I'm using the keyboard shortcut ""Super_L"" to open the Mint menu of the Mate desktop. I don't think you can do that. What I've done is mapped the Menu key (the one that opens context menus) to open the Mint menu. I don't use that key anyways, and you can always use Shift + F10 to get the same effect. This problem was annoying me as well. I never use the Super key for the start menu, anyway, and opening a terminal is much more important to me. I solved the annoyance by opening the following file:",4
"This night, we went through two hours of downtime on one of our site (yes, it hurts..), and I am now trying to identify the problem:  However, I can't see any operation of that type in my ""journal"", so I really don't know where should I look to check that information.  I need to insist on the fact that their was no human action to end this downtime and that the problem was solved ""by himself"".  First of all, I looked at the website logs. I found the following error that triggered more than a thousand times during the downtime period: Another clue is that my ""maintenance window"" is set to Thursday between 1AM and 1:30 AM, and that this maintenance window corresponds approximately with the beginning of the downtime period.  Another strange thing is that two of our websites are running on this RDS instance, and that only one was impacted by the problem.  That was a DNS issue in AWS services in the region eu-west-1. You can check that in AWS Personal Health Dashboard https://aws.amazon.com/premiumsupport/phd",2
"Is this copy operation just via a normal network share (i.e. copying via Windows Explorer or similar)? I experienced a bug like this in under Win2K but that was fixed as far back as SP1 if I remember rightly. If you are copying via another method then that application/service may be trying to hold the file in RAM until it has it all, in which case you need to upgrade or replace that app/service. Try to use a protocol like FTP - if it still happens, you should probably look into some networking problems.  Based on my work this AM ESEUtil seems to be the way to go.  It wasn't as fast as I'd expected, but it didn't freak out Windows either. Trying to do a P2V conversion onto a 64-bit server running Windows Server 2008.  Any of the normal file transfer methods for the VMDK file (which is 44GB) cause Windows on the destination server to run out of its 14GB RAM after a few minutes due to the file system caching. In any case I would check that your RAID 10 was actually healthy - the symptoms here would make me suspect it wasn't able to write as fast as it should be. Also, from a logical point of view, the server does not actually need to store a file in memory when copying from file system to file system; it just allocates a buffer in memory the file passes through. Depending on how you copy files, though, some applications will first store the file completely in memory, and then write it to the disk. Running the P2V conversion or file copy on a 32-bit server doesn't have this problem and memory usage stays reasonable. Seems to happen quite a bit in 2003/XP x64 actually. In fact I just realized one of my servers has been experiencing this same issue for a while now and I haven't had time to troubleshoot it.  There are known issues with the Network File Copy processes on W2K - if the remote system can't empty the write cache faster than the rate the file data is arriving in over the network then it will steadily consume all physical memory on the server if the file is big enough. Mark Russinovich has some details on the ways this might happen in this article on changes made in Windows Vista's file copy mechanisms. The performance graph you posted looks like this issue and I have seen exactly this sort of behavior in the past where I had a target system with very slow disks and a fast network.  Maybe you need to beef up your virtual memory on your system hard drive, where low hard drive space could have caused said problem. The interesting question here would be how the server actually stores the files - as you can see, the I/O load is just way down, which means it's not actually writing the file anywhere, just buffering it in memory. Are you using Server 2003 64-bit? There is some information on fixing this issue with x64 on Server 2003 here -> http://www.techspot.com/blog/224/slow-system-performance-when-copying-large-files-in-xp-x64-server-2003-x64/ The Windows FTP client uses a Temp file on C:\ before moving the file to the target destination.  Beware! :-) However even though your target OS is a bit old the hardware isn't all that weak and a RAID 10 setup with 4x7.2K SATA drives should be good for somewhere between 60 and 120Meg/sec write speed which is significantly higher than the 39Meg/sec Vista is reporting for your copy. The odd thing here is that if it is a solid, well-configured GigE link then you could hit network transfer rates reaching 70Meg/sec (and maybe a bit higher) for a sustained copy of a large file like this. That said 38Meg/sec isn't abnormal either if there's any other traffic flowing in or out of either the client or server or (as is more likely) that rate is mostly limited by the speed of your local laptop hard drive.",5
"In deciding where on the front & back to actually stick the labels, I favor locations that are more permanently associated with the chassis itself. I avoid sticking computer labels on removable modules in general & hard-disk carriers in particular because I don't want people thinking I've labelled the disk itself or getting confused if the carrier's pulled out and moved around. I label them whereever the label will fit.  Most of my servers have room on the front where it doesn't cover anything important.  These ones just have the server name. Servers vary so much that it's hard to standardize on locations unless you're only buying from one manufacturer; that said, I feel it's very important to label both the front and the back. When you're staring at a bunch of identical chassis backs, it's nice to have another way to double-check that you're about to pull the right power cord. We started to use a laminated sheet of paper cut in half length-wise with an excel spreadsheet with two columns. One with the ""U"" numbers filled in and one for server information. We attach these to the rack doors, front and back. For more permanent enclosures we would fill in the information before laminating, but for racks that change often we leave them blank and have fine point dry erase markers available. I've used Dell servers exclusively throughout my career and I always put the label on the front bezel and on the front of the chassis behind the bezel. I try to put the label in the same spot on all the severs when and where it's possible (upper left, upper right, etc.). We used to label each server individually, but with the increasing ability to stuff more into the chassis we had to find a better way. There simply weren't places to stick a label in some cases and waiting for the small lcd to scroll through was annoying. I use the rack and put the labels sideways.  Then you also notice when a server doesn't have a label. The main use of labels for me is so that ""remote hands"" operators can quickly identify and locate a piece of equipment if I need them to. I simply tell them ""It's the server labeled CORPDC1 in Rack 35 position U 25."". Our minimal label is our institution's name & the server's property number, but if there's space, I'll add the hostname & IP addresses to save a trip back to look it up. On a few systems I've also stuck labels above individual ports to note their purpose, but it's not something I do for most servers. In addition (or if there really is no room on the server itself) they get a label on the rack, both front and back.  There is plenty of room for a large label here, so they include the server name, IP of its management interface, and a QR code that links to our documentation wiki, so I can pull up all the server's details from my phone instead of heading back to my desk. As for labeling other components and for the amount of detail to include, that's a personal preference. I've seen people label hard drives, optical drives, etc. and I've seen them put an awful lot of detail on the label (server name, ip address, specs, etc.). I've very rarely used any information other then the name. I can get everything else I need from my asset management system, management software or from the vendor.",5
"It's pretty easy to just add a WriteNetworkUpdate method that writes out all the data you need to replicate and a corresponding read method.  This isn't much different than your saving/loading code (it might even be the same set of methods for simpler games). I am building a software system (game engine with networking support ) that is made up of (roughly) these layers: As it looks now, the Messaging layer sits between both layers (game and network) and ""knows"" both of them (it contains Converter objects that know how to translate between data objects of both layers back and forth). Game related data is passed to the messaging layer (this could be anything that is game specific), where they are to be converted to network specific messages (which are then serialized to byte arrays). Is there a good design for passing objects between layers? I'd like to learn more about the different options. I have done two things in the past, the first using converters like you mentioned (AutoMapper), the second is to have another layer which both projects have a reference to. The second layer would contain the contract between the the objects you want map. For example: Your Networking project would then send/receive IMonsterData types. No one outside of Domain (Game) would have access to the Monster class. A more advanced way to handle things is to use reflection (especially easy in C#).  You can write some network-specific reflection attributes to attach to class fields that the network layer uses to generate and decode update or event packets.  We do this in C++ via a custom reflection system and most other big engines I'm familiar with do something similar. Typically we just allow some level of coupling.  Academic purity is really, really far down on the list priorities worth caring about out in the real world, especially in cases where none of the theoretical down-sides are actually coming up in practice (what exactly would said coupling prevent or make harder to maintain than the contortions it imposes do?). I'm looking for a way to be able to convert ""game"" data into ""network"" data, such that no strong coupling between these layers will exist. In a data driven engine you don't have hard coded classes which represent your states of the objects, you basically do have only a list/hashtable with names of the variables, and types and the data for the variable itself, maybe with metainformation too (for serilization).",4
"2) If you find that your missile is at a right angle to your target, this could be the point where the lock 'breaks', and the missile just moves straight unless the target gets 'in front of' the missile again. As the other answers by Martin and Nicol point out, you probably want to guide your missile not directly at the target, but in a way which will make it collide with the target later on. However, the method described by Martin is complicated and the one described by Nicol is inefficient. Here is a worked example I implemented this morning, in my case for a player AI in a sports simulation game, where the player is trying to chase their opponent. The movement is governed by a standard 'kick-drift' model where accelerations are applied at the start of a timestep to update velocities and then objects drift at that velocity for the duration of the timestep. I know this is an oldish question, but I think there is something that has been missed in the answers given so far. In the original question, the missile (or whatever) was told to accelerate towards the position of the target. Several answers pointed out that this was wrong, and you should accelerate towards where you think the target will be at some later time. This is better but still wrong. LOS Rate can be easily derived by measuring the LOS vector (target position - missile position), and storing its variable.  The LOS vector from the new frame (LOS1) is subtracted by LOS vector from the old frame (LOS0) to generate a delta of LOS -- now you have a primitive LOS rotation rate. N is the navigation constant -- in the real world, it is typically set between 3 to 5, but the actual workable figure in game is somewhat dependent upon the sampling rate at which you're deriving the LOS rate/delta.  Try a random number (start from 3) and increase up to 1500, 2000, etc until you see desired leading effect in game.  Note that higher the navigation constant, the faster the missile will react to LOS rate changes early on in the flight.  If your homing rocket simulation model is somewhat realistic, excessive navigation constant could overload your missile's aerodynamic capability, so you should use a balanced number based on trial and error. However, I disagree with his solution to the problem.  Instead, I would program the missiles thusly: What you really want to do is not accelerate towards the target but move towards the target. The way to think about this is to set your desired velocity pointed at the target (or a projection of the targets location) and then figure out what acceleration you could best apply (given whatever restrictions you have, i.e. a missile probably can't accelerated directly in reverse) to achieve your desired velocity (remembering that velocity is a vector). The most simplest and advanced method to use for this in games (and real life) is Proportional Navigation. A couple of simple options that have been found to be 'good enough' for games I've worked on in the past: Under Proportional Navigation (PN), missile accelerates ""N"" times faster than the LOS Rotation Rate.  This will force the missile to lead the target until LOS Rotation Rate becomes zero -- that is, the missile and target appear frozen in state as sightline no longer changes -- they are now on collision course.  The variable ""N"" is known as Navigation Constant (a constant multiplier). A simpler way - but still pretty efficient - to guide a missile is by adjusting its angle according to the angle change between the missile and the target. On every tick you calculate the angle from the missile to the target, and compare it with the angle from the previous tick. The difference is the exact difference you want to make on the missile's angle. So if the angle was 0.77 in one tick and 0.75 in the next, you want to adjust the missile's angle by -0.02. This method is simple, and as long as the target is ""in front"" of the missile, it's very efficient in terms of route chosen. It also applies to any number of dimensions, not just in 2d. I would post the derivation of this, but I've found there is no math markup supported on this site. Boo! You will just have to trust that this is the optimal solution, bearing in my that I have no restrictions on the acceleration direction, which is not the case for a missile type object, so that would require some extra constraints. As has been said you should aim the missile for where the target is expected to be when you get there rather than for where the target is right now.  This will stop MOST missiles from going into orbit but an orbit is still possible if the target evades just right.  This is a legitimate tactic used by aircraft pilots to dodge incoming missiles--since the missile is going much faster than you it will have a larger turning radius and a sharp jink at the right instant causes it to go on by.  (Although you could still be at risk from a proximity detonation.) Since we are dealing with a missile that can still track and still has thrust you get an orbit situation if the target evades into one of the zones that FxIII's post talks about. I also check the new speed against a player dependent max speed and cap it at that. In the case of a missile, car or something with a maximum turning rate (in degrees per tick) you could simply look at the current angle of motion versus the calculated ideal and if this change is greater than allowed, just change the angle by as much as possible towards the ideal. Sightline, or Line of Sight (LOS) is imaginary line between missile and the target -- the vector between missile position and target position.  The rate of angular change of this LOS is the LOS Rotation Rate. When LOS Rotation Rate becomes zero, then the sightline no longer changes -- the two objects are now on a collision course.  Think of yourself as chasing someone whilst playing football/soccer.  If you lead him in a way that his body looks ""frozen"" in your field of vision (sightline between you and him no longer change), you will collide with him as long as you maintain your running acceleration to keep his body appear frozen in your view. If your end goal in your solution is simply to make sure your missile hits the target, then I am all for just Making it hit the target. Again, this will just depend on how the solution looks. Code is in python, but should be readable with any language background. For simplicity, I assume each time step has a length of 1 and express the velocity and acceleration in appropriate units to reflect that. Under the Constant Bearing Decreasing Range (CBDR) logic, when two objects (missile and target) are traveling in same direction without change in sightline between each other, they will collide. if the missile has been thrusting at 90 degrees to it's line of motion for 360 degrees of movement you are in orbit.  Adjust the thrust to 120 degrees from the line of motion.  The missile's orbit will widen as it's not turning as hard but the missile will also slow, thus allowing it to maneuver better.  When the range to target opens to 1.25x the diameter of the dead zone (note that this diameter is based simply and only on the missile's speed, no complex calculation is required at runtime) the missile returns to it's normal tracking behavior. For anyone interested in the derivation of this, I wrote down the distance between the player and target after the timstep, in terms of the initial position, velocity, acceleration rate and acceleration angle, then took the derivative with respect to the acceleration angle. Setting that to zero finds the minima of the player-target distance after the timestep as a function of the acceleration angle, which is exactly what we want to know. Interestingly, even though the acceleration rate was originally in the equations, it cancels out making the optimal direction independent of how much you are actually able to accelerate. Note that the atan2(a,b) function computes the inverse tan of a/b, but ensures the angles sits in the correct quadrant of a circle, which requires knowing the sign of both a and b. 1) If the resolution of the scene you are looking at allows it, then the object can explode when it is Near the target (Which is how I believe most common day homing missiles actually work in any case). If your orbiting range is about twice the size of the object away then this will likely not work for you as it would just end up looking bad. I always prefer simple solutions whenever possible. If you are making a game where the homing missile is just one of the weapons being used then  you can likely get away with these as players are likely to fire off a salvo and then swap back to their constant engagement weapons as soon as possible. If you are making a missile simulation however, then clearly one of the other answers is the better choice.",5
"The problem is that actually computing the DamerauLevenshtein distances is tricky, and you may see algorithms that compute something similar (optimal string alignment, for example), but that don't actually find the minimal number of steps. If DamerauLevenshtein with adjacent transpositions is OK for your application, you can use that. You'll find algorithms for computing it online (even in Wikipedia), and as long as your code computes the minimum number of steps, the triangle inequality is obeyed. (But note that, as stated, algorithms based on finding the DamerauLevenshtein distance through optimal alignment will not obey the triangle inequality, but that is because they don't really find the true DamerauLevenshtein distance.) Actually, the DamerauLevenshtein distance is a metric. (See, for example, 11.1 of Encyclopedia of Distances, by Deza & Deza, Springer, 2009.) That is, it does obey the triangle inequality. This can be seen quite easily if you view every possible string as a node, with the edit operations and transpositions between them as edges. The DamerauLevenshtein distance is then a shortest-path distance between the two strings in question, and such distances (graph geodesics) are always metric (i.e., triangular): You can't take a shortcut (breaking the triangle inequality) without simultaneously discovering a shorter path (a contradiction). If you don't want to get into the details of that, you could simply use a heuristic algorithm such as $A^*$ to compute the distance, with plain Levenshtein distance as the lower-bounding heuristic, for example. Should be efficient enough for reasonably-sized strings, and will give you the corret DamerauLevenshtein distance. And it will work with your BK-trees. If you want the unrestricted DamerauLevenshtein distance (without necessarily using adjacent transpositions): Fear not, there are ways of computing that as well! You could have a look at the work of Lowrance & Wagner, for example.",1
"My only other advice would be to watch a high-def movie file while playing Crysis, editing a huge project in photoshop, and running Chrome with 128 tabs open. It's really hard for even Vista to use more than 6GB of RAM. What you can do is use a Ramdrive program to create a virtual hard drive (ie R:) and put cache in there like firefox/photoshop/swapfile. You can also install your most launched programs there for a slight speed boost. Just don't put anything super important. I myself use ramdisk plus as it can use the memory above the 3.2 addressable in XP. I just put together an i7 and mapping out how to use 12GB. I generally run lots of VM for testing so it shouldn't be too hard ;) If you really want to use up a lot of RAM just set up several virtual machines using VirtualBox or a similar program.  You basically set up a separate virtual computer with its own operating system.  When you create the VM you can allocate a specific amount of memory for it to use. The end result is that (with your amount of RAM) you could easily have a Windows Vista and two Linux distros running simultaneously. Or, if you're feeling froggy, you could be running Windows AND Linux AND OS X at the same time.  Look out.",2
"Having said that, generally it's considered best practice to separate your ""Sign-In"" accounts (ie. those you log into via the AWS console) from your ""Access Credential"" accounts (ie. where the API keys are used within other software).  Your ""Sign-In"" accounts should have MFA enabled but no Access Credentials. You use this account to create other ""Access Credential"" users which have limited access to only the services/functions you specifically need to access. Note: I haven't found a way to get the CLI to ask for MFA when calling a user profile (--profile my_iam_user) only calling a role profile triggers the MFA request. However, this user can also access all services via CLI (by access key and secret key), does that mean the CLI will always bypass the MFA, even it is enabled for this user? Adding a new answer as this situation has changed somewhat. The CLI can manage a lot of this for you if you're using roles. Described here: http://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html You can enable MFA on API access but any services using those credentials including the CLI has to use temporary security credentials (ie. you make an AssumeRole call providing your access key, secret and MFA token and it returns temporary access keys which are valid for as long as the MFA auth is valid). When I call aws s3 ls --profile my_admin_role it says Enter MFA code:, after I paste in the code it returns the listing.  Note the mfa_serial entry. You can get this value from your user details in the AWS IAM console. This entry tells the CLI that MFA is required for that role.",3
"I'm sure that there are many interesting reasons for having separate partitions, but I've found it easiest (and more flexible) to just create / and nothing else. A swap partition might be OK if your machine is low on RAM (2GB is on the edge) and/or you anticipate heavily using the machine. I've found that 1.5x RAM size is a good swap size. If you have neither a special memory-intensive task in mind nor a serious shortage of disk space, I would accept the default size for the swap partition. If you don't care about hibernating or you are very enterprising and don't mind some hacking to make hibernation work, you can forgo the swap partition entirely and use a swap file. I don't see a particular reason to have a separate boot partition, unless you're doing encryption or something like that. Unless you exactly know what you're doing and have some real reason to do that, splitting your filesystems into multiple places does not make much more sense than a all-in-one-place approach for the typical home use.  Ultimately, I've chosen to have a separate home partition, and I've made sure to use only fully resizable (both shrinkable and growable) filesystems (ext4), so that if I ever need to change my disk allocation, I can just boot a live CD and move partitions around without having to copy to and from a removable disk. The ramzswap device is a nifty feature called compcache that is apparently included in Ubuntu's stock kernel. Ultimately, the best long-term solution is probably something like ZFS, which assimilates all disk space and allows you to dynamically allocate it by creating filesystems out of the pool. But linux doesn't have that yet. (If you're interested in when it will hav this, keep an eye on things like btrfs and tux3, and also possibly zfs-fuse.) Speaking of the /swap partition:2GB of installed RAM make very unlikely that you ever need a swap space, except when you will put your PC in suspend / sleep mode. In this scenario the RAM image (part of it?) will be dumped on disk, so you can live happy with a swap partition = RAM size. A clean installation of Ubuntu 9.10 requires an odd 2.5 GB on disk. Is up to you to decide how much space you need for extra programs and data. Create swap at the beginning of your drive, 2x your physical RAM is a good guideline.  Boot can be pretty small if you want to make it a separate partition, a gig or less.  You want make your root partition big enough that you have space to install more applications later but not so big that you feel cramped in your home partition. In the past, I've done both separate home setups and ""one big filesystem"" setups. I can't really say that either approach is more flexible than the other, because they have different flexibilities. If you have one big partition, you never have to adjust your partitions to deal with mis-allocated disk space, but if you want to install a new OS and keep your home directory, you need some extra space lying around (or you need to do some creative repartitioning and bind mounting). On the other hand, with separate partitions, keeping your home through upgrades and OS switches is easy, but you run the risk of running out of disk space on one partition with lots of room on others.  You can easily install GRUB on the MBR to have a dual-boot machine and you can create a single partition (mounted as /) for both system files and users home directories.",4
"All of which can be gleened from the fine manual. It took two tries to find out simply using """" (empty destination mailbox) was what we really wanted ;-) The letters FS after the comma mean Flagged and Seen. To mark the message as ""unseen"", either remove the S flag... Maildir actually uses a unique format that makes this quite easy. Simply place the mail in new/ directory inside the mail users mailroot if you want it to show up as new. Otherwise it goes in the cur/ directory. The Maildir format consists of a series of directories - matching the IMAP folder structure, within which are the emails, one file per email. The new folder contains any emails that have not been seen by a client, and the cur folder contains current emails.  The tmp folder should be empty if the mail server is not operating. This shows the INBOX folder (cur, new, tmp) and another folder called ""personal"".  Note the dot prefix showing that this folder is hidden, so this should be accounted for in your transfer. I tried the accepted answer and it failed - the dovecot versions in play are probably too far apart and we also switched the underlying MTA. So here is a more robust solution: doveadm import Assuming you're hosting emails for the domain hosted.tld and an account exists for the login local.account@hosted.tld (or maybe simply ruth) and the backup from the previous server is in MailDir format and available inside a folder structure like /tmp/TRANSFER/domain/account/Maildir then you can import them to your new dovecot hosting with In order to copy emails from one email system to another, you can simply copy the directories and files, and ensure the permissions for those directories and files are correct at the destination. The migration went fine, I just wanted to know how I could ""show"" all the old mails in UA Clients. I mean, is there any dovecot index file or such that could be re-created, destroyed, so that all old mails are ""flagged"" as new ? It's already running for other users, I just want to merge old accounts from the old server to the new one. I'm migrating my mails from an old server to a new one, the configuration is good (mysql virtual accounts & imap only), with dovecot and postfix. You may even have more luck than us without needing to transfer the files beforehand - if old and new server are running at the same time and your accounts are set up appropriately using doveadm sync.",5
"the problem is that you can not copy the file because you will get an error stats the file is being used... and soon after you close Firefox the file will be disappeared. So what to do? I am having the same problem as the OP for a long time. At least since 2011 youtube changed the way its videos are cached and since then its hard to find them. just write %temp% in the address bar then there should be a file which its name contains ""fla"" and had a reasonable size (depends on the opened video). My guess is youtube is using some nonstandard Flash cache method, probably on the Flash cache folder itself but I only found that folder once or twice, shouldnt be hard though. The video will be there in the cache only as long as you are seeing it in the browser, after which it is deleted automatically. I'm trying to find youtube video (just watched) in Firefox cache folder, but I cant find the folder. The old way it used to be stored on the regular Windows Temp folder on the Windows use folder. Sometimes, I don't know when, you can indeed find it there, for that I usually use this really lightweight freeware called VideoCacheView because it searches all IE and windows related folders really quick and can copy those nasty 'locked', in use files with just a right click instead of me having to use the command line with hobocopy tool. to solve this problem you should switch your user and log off the user from another user using task manger. then com back and copy the file and change the extension from .tmp to .flv then open the file. I know that km player can read the file. It gives ""videos from Youtube"" as an example, but the same applies to any other content you might find in a webpage.",5
"People often over estimate what it takes to write the ""engine"" part of a game. If you only do what you need it won't take that long. The hard part is to not get stuck writing infrastructure and to only write what you absolutely must to solver your problem. So for example, say you don't have the money to spend on Unity.  Or, you'd rather spend it on new hardware instead.  Or, there are performance issues with the free engines available to you, or you need to have more control at a low level. Avoid using a library if your primary goal is to learn from the experience of implementing the concepts that are solved by the library. Whenever I develop a game (part time), my goal is purely learning. I don't care how long it takes, thats why I'm doing it all from scratch! Now, you decide. Don't roll you own engine. Roll you own game. If you happen to write an engine at the same time then good for you, if not you can always refactor whatever parts you might want to reuse to make it more ""engine"" like. If you love writing software for the sake of writing software, then you're going to love writing a game engine.  A common trapping of beginning game programmers is to get caught up writing an engine as some kind perpetual project.  I honestly believe that's how all these open source game engines came about -- game programmers who actually didn't",3
"Besides, I'm testing some OSs to absolutely clean install a new system and the more I'm into it, the more it catches me.  Using the bridged apapter gives me 169.254.95.179, so no connection to the internet at all. VMnet0 (auto-bridging) lets the guest imitate a second computer in the usual home network 192.168.0.x. Via VMnet1 (host-only) again there's nothing.   Is this possible without installing a VPN between host and guest or using a second VM with a server connecting them? I found out that the host cannot access the guest using NAT. Somehow I could access guest's shared folders, although ping did work only from guest to host. Further I found out (in the documentations and in fora) that the host can route its traffic through the guest.  When I choose NAT in VMware as network adapter traffic obviously isn't channeled through the proxy connection since the IPs are located in different cities. Assuming the connection is:  The only entry in netstat -n showing VM IPs is the one from the VM to the guest's SMB port 445 (in netstat -f it says microsoft-ds).  Of course I simply could install the VPN & Proxy software on the guest OS, but this solution seems not clean enough to me. I'd like to have one straight line of data traffic. Somehow VMware and Virtualbox haven't considered implementing this. In Virtualbox I only found how to set up the NAT configuration to redirect traffic from host to guest. My host is Windows 7 Professional 64bit using VMware Player or Virtualbox (more familiar with VMware) and the software shared by the VPN provider. [Means: no PuTTY or FoxyProxy things, no shell code (in Win7 at least). Easy software which monkeys like me are able to use without any solid knowledge base in network engineering.] In other words and according to netstat -n, I connect from my 192.168.host:port to VPN:22. On the VPN server I get a new internal IP, let's say 172.168.host. Then 172.168.host connects to Proxy:22.",1
"Fiber is going to be expensive. I would recommend going with copper as a first step. With copper (ethernet) you've already got the infrastructure in place (I'm assuming that the other building already has plant wiring, a switch, etc). The only implementation costs is going to be for a copper cable run from one building to the other. If you go with fiber, you're going to have to pay for the fiber cable, installation and qualification, new switches and\or fiber interfaces for the current switches if they support fiber, etc. A physical connection fiber/wire will almost always be better from the perspective of functionality and performance. The two buildings most likely don't have a common ground.  Running anything with an electrical connection such as a CAT5 cable across that takes a risk of electrical surges due to the difference in grounds.  I've seen a lightning strike one building and take out everything connected to a network in another building b/c they were linked via a cat5.  It killed a lot of switches and network cards.  Maybe modern switches do more to protect against this as this was a long time ago.  But I wouldn't risk it. Also while you're running the lines run a couple of spares if you can.  No matter what you choose to go with. OTOH another inexpensive option might be to just setup a point to point wireless connection.  A wireless connection using recent technology should be able to provide between 50-150 MB of bandwidth and seems like it should be enough for 10-15 people using computers for typical office functions.",3
"This can also be caused by having both click-to-run (e.g., Office 365 downloaded) and MSI (regular installer) installed Office applications on a computer at the same time. For example, if you install Visio from a volume license ISO download and then install Office 2013 Pro Plus using an Office 365 E3 license by downloading it from the portal, you can get this error. My user had this issue because they had both Office 2013 ProPlus and Office 365 installed.  We fixed it by uninstalling both Office 2013 and 365 and reinstalling Office 365. To resolve it uninstall the stand alone OneNote 2013 install and then reinstall OneNote from the Office Suite in Programs and Features (in my case Microsoft Office Professional Plus 2013). Recently OneNote 2013 was made free, on a new Development Machine I had installed OneNote, however a few months later I realized that I needed the rest of the Office Suite on this machine. Installing from Media I opted not to install OneNote (because it was already installed of course!).",3
"So, I went ahead and created an AD group that I've added myself to. Then I added this AD group in Microsoft SQL Server Management Studio under Security/Logins using just one snippet of SQL Query code: You've created the login, which tells the server who you are.  Now, you need to tell the individual databases who you are and what rights you should get.  You'll do this by adding a user to individual databases and then adding rights to the user.  Something like: That will add the user to the database ""db"" and give it read permission on all objects in the dbo schema. I've got this SQL Server 2005 Express installation that's got a Database I'm trying to connect to through a software that gives me the error ""cannot connect using the user 'mydoimain\myuser'. Meaning, it's trying to connect using my domain user as a default. Which is ok. This is actually what I want. But I want to be able to add users to a group and grand that group permissions to the database instead of specifying each user. This added the group under the Security tab. But I still get this error when trying to connect using the software from my client. What else do I need to set? I even tried to click on the newly added group that appeared after running the above query snippet and added every server roles as well. The user has been granted access to the database engine(done by default) and Enabled under Login (done by default).",2
"There is another alternative that has not been discussed here.  SomeClown has alluded to it but did not specifically recommended it.  This will be not only the most expensive option but far and away the simplest and most reliable.  I simply use a separate ""box"" for each OS that I use or play with and use a KVM switch to swap between them.  While this requires a much greater footprint and greater initial expense as well as the stand-by use of power for each box involved, it is clearly the simplest and most reliable solution.  Not even Windows 7 XP Mode works as well with legacy hardware (HP Photosmart P1100 printer) as a dedicated XP machine.  With this setup, you can leave a program in one OS running while you work on a program in another OS and have no delay or slow-down as a result of switching between OSs.  This just works much better for me. You do not need any special hardware to dual-boot, but I would recommend searching for the computer you are thinking about in the context of Linux issues to see if there are any common issues with it.  Even brands that (at least in my experience) provide decent enough Linux support (in my case, Toshiba), you could still end up wit a laptop whose ACPI won't work with Linux, effectively preventing you from using it as a laptop. I know you can get virtual linux in things like cygwin, or you can get two whole operating systems installed on the one machine (""dual boot""?).  One more option you have is to install linux in a Virtual Machine in Windows using for example Virtualbox -- this is nice in the way that you can run both windows and linux ""at the same time""; however, in a VM the performance will always be a little decreased, so if you need to run programs requiring a lot of CPU/memory, this solution might not be for you. It seems you don't need any specialized hardware for it? but does separate drives or something like that help? As @houbysoft mentioned, partitioning (meaning breaking up one drive into two segments that will appear as different drives on your computer) is the most common option for dual-booting, but the Wubi Installer for Ubuntu provides an effective alternative by creating virtual partitions on your disk.  It also uses the Windows Boot Manager rather than the GRUB Menu that most distributions default to. It is therefore not even comparable to installing both linux and windows side by side. You don't need to have any special hardware, not even separate drives : you will simply divide your (one) drive into two or more partitions and install linux on one of them. Note that for some of the more newbie-friendly linux distributions such as Ubuntu, this is usually done automatically and is really simple. Dual-booting will provide you with faster Linux than a true virtual solution (which Cygwin is not) because you will be dedicating your computer's entire processing power, etc to it.  However, you will have to put up with the time it takes to shut down one operating system and boot the other whenever you need to switch. Running Linux in a virtual machine (such as VirtualBox) will typically result in fewer issues simply because your hardware will be emulated so you will not likely have any driver incompatibilities. Cygwin is not really used as a ""full linux"" in windows, rather, it is usually used to compile linux software under windows, hardly like a full linux replacement. I'm a genetics PhD student. The word-processing and referencing software available for windows is more suitable (and more compatible with computers in the library etc.), but there's some genetics software that runs better on linux. (Currently i'm shopping for a laptop.)",4
"You could run Chrome Canary at the same time as Chrome; and then use the Windows volume mixer to turn up or down the volume for either app, independently.  Unless you break each tab into a separate browser, it is not possible, as windows sound control will just see chrome as one browser, not the multiple tabs within the browser. Sorry, but them's the breaks. But what you can do (at least in windows) is individually control the volumes of different running programs. So if instead of visiting different websites on different tabs, you visit them with different applications, you can control the volume by clicking the ""speaker"" icon on the windows taskbar and then clicking ""mixer"". For example, visit one website in chrome, and the other in firefox or IE. Rowe suggested installing chrome canary as your second browser, but any browser application will work. Sounds like Volume Master (Chrome extension) does exactly what you want: https://chrome.google.com/webstore/detail/volume-master/jghecgabfgfdldnmbfkhmffcabddioke This doesn't address your question exactly, but it's as close as I can get. Considering it's been over three years since this was posted and there isn't a better solution, this is probably the best we can do. This is, essentially, an expansion on Rowe's solution.",4
"You can set up rules to send off alerts, fire up scripts, whatever.  If you're using a hosting provider that provides an API which will allow you to scale up resources when you go down, you could write a script to temporarily increase server resources until load goes back below a certain threshold. Get Munin installed. It will track your system resources (RAM, CPU, Disk), but can also monitor more detailed apache stats if you enable mod_status. By default, Munin just graphs the data it collects, but you can also configure it to alert on threshold exceeded events. That ought to give you a good start towards understanding how your system behaves as a baseline.  Look for abnormalities and then figure out a way to detect that condition and alert you going forward.  Repeat. But I don't know why. It's not getting high traffic, not even from bots crawling the site.  It's not happening at the same time every night, so it doesn't seem to be related to any specific cron job that might be firing off. Is there any way to log what's going on with the system when this happens so I can look at it the next morning? I've got a pretty typical LAMP server running a web site but sometimes, during the night, resource usage skyrockets and the site becomes unresponsive.  There are a lot of logged errors about being unable to connect to MySQL, and the MySQL process gets automatically restarted, according to /var/log/messages ..",4
"I run Monit to keep check on processes on a Debian server. It's working as normal for all other services (Apache SSL, Postfix, SSH etc.) but Monit's checks on Dovecot are failing repeatedly. I think this may have started after installing some package updates, but I'm not sure when.  Dovecot is working correctly for client connections as email is still working. I've tried testing with open_ssl s_client and all seems fine for SSLv3, TLS1.1 and TLS1.2 from there too. Option 1 is the only real solution as the other two force the you to test something different that you were hoping to test, but they do at least easily provide a similar test while you wait for the update. 'dovecot' failed protocol test [IMAP] at INET[localhost:993] via TCPSSL -- IMAP: error receiving data -- Success If you want to monitor dovecot in general, you might consider monitoring port 143 without SSL by using the following configuration: The problem is caused by http://osdir.com/ml/ubuntu-security-announce/2014-05/msg00023.html as TomDogg mentioned. I thought it might be something like this https://secure.kitserve.org.uk/content/ssl-tls-version-conflict-zarafa-monit but I've tried replacing 'sslauto' with all of these options in turn, but with no luck: SSLV2|SSLV3|TLSV1|TLSV11|TLSV12",3
"Still, its better to have another NAT router behind the phone, because they cant handle multiple clients very well IIRC. But why even bother with two routers? OpenWrt can do it all and you can just use a switch at the second location. Or set the second router to be a wireless access point, if thats your goal. So you can of course still talk to the router. Even in regular WiFi routers, the WiFi AP interface is bridged to the wired network, so it would be fatal if this wouldnt work. (Please note that the diagram above is tremendously oversimplified. There is actually more going on.) The subnet 10.0.1.0/255.255.255.0 (or 10.0.1.0/24 for short) has 10.0.1.255 as the broadcast address. Setting it is not required, because it can (and will!) be calculated. The docs also confirm this. Its not related to bridging at all. The firewall is already not effective because the bridging firewall is disabled by default. Also, the resulting br-lan interface is in the LAN zone. You dont even need any firewall at all, because the Android phone is already acting as a NAT router. And the mobile ISP is most likely also doing Carrier-grade NAT. You also seem to have a slight misconception of how bridging works. Its implemented in software. The bridge appears as a network interface to the host OS. Its somewhat like this:",1
"So theoretically, you could build up a script doing a copy of an older version, taking a snapshot and subsequently copying the newer version, but you would lose the original snapshot time stamps. You should consult the documentation (and/or EMC support) for a supported way to migrate over a volume together with its snapshots. This is not the case. I do not know much about the EMC Clariion, Symmetrix or VNX product lines (I do not work with EMC products), but most likely, the files served through SMB are not stored on an NTFS file system. Even in a Windows implementation, shadow copies are not ""file metadata"" but a group of copy-on-write references for occupied file system blocks. You should instead do a file system level migration, this will preserve your shadow copies if the shadow storage is located on the same logical volume, or on the same NAS volume. File-level migration will not transfer shadow copies. So, if you can get a block-level access to your LUN on the storage level, you can directly transfer the LUN to a new NAS device, and your shadow copies should safely migrate. The Explorer is accessing the snapshot history which the NAS is offering via the Shadow Copy interface. Robocopy can't do that as it does not implement calls to the Shadow Copy APIs. What you can do is use the ""previous versions"" path to get access to a different file share snapshot and use robocopy on that.",2
"The easiest way to diagnose this is with a debugging instance of the SSH server. On the server, run: I would then instead use SSH overs rsync to make sure only a rsync command would be launched. This provides basically the same service as scp but your key then can be tagged with a force command which may be safer. I used inotify to implement the method  John Gardeniers suggested. It was easy and it now works fine.  I just wrote a C++ application that does the scp when a file in the directory is modified as can be done from the PHP code.  Works fine. To John's security concerns: if you decide to continue using the www-data account, you can limit its key to only certain actions (such as transferring certain files) using a ""forced command"" option in authorized_keys. Anyway, you should make sure your connection is restricted to this specific command. I am not sure SCP provide ways to restrict command (such as forced command with SSH). Having read the link you gave: make sure you use authorized_keys, not authorized_keys2. The latter has been deprecated for years and did not work at all in some versions of OpenSSH.",3
"A fourth approach may be to alter which disk the BIOS boots from.  However, this approach has traditionally been much less compatible between different systems, so that approach is more of a theoretical possibility than something that is actually used.  (Although, many users have used this approach interactively, often by pressing F12 or something to pull up a boot menu; sometimes they just enter BIOS setup and change the default disk that will be used on subsequent boots.) Then, the traditional MBR contains some ""code"" (instructions).  When the system is booted, this code is run.  The code will check the MBR to see how big each partition is, and see which partition is flagged.  It will then continue to booting process by running some additional code which is located in the partition that is flagged. Basically, the process is going to involve changing the flag bits of a partition, or changing the boot code of the MBR, or changing the behavior of the code that is on the partition that the MBR uses to continue booting.  So that's three different approaches, and each of those approaches are actually used by some of the different options that are available. I know this may not be quite as automated as what is desired, although the question that was asked did not specify non-interactivity.  So, the simple answer is that yes, there is such a command.  It is ""fdisk"". The traditional MBR contains some bits that specify whether a partition is flagged.  The standard specifies that only one partition is flagged, although that isn't necessarily enforced by the boot code.  When a user specifies that a specific partition should be flagged, a traditional fdisk program will typically check for another flagged partition and, if one is found, unflag it.  There might be alternative words used to describe a ""flagged"" partition, like a ""selected"" partition. Virtually every OS offers a program to do this; OpenBSD's is fdisk(8), similar named programs are in Windows 9x and DOS, and many other operating systems. This can be highly desirable for OSs or systems which take a long time to shut down and reboot -- you can set it and start the reboot process, then walk away, grab a cup of coffee, and come back to the system booted the way you want it -- no waiting for the Magic Moment to select the next OS. So, that's the standard that is widely supported.  Unfortunately, this standard method involves going into fdisk and interacting with it manually.  So that's not just a single program that you can run.  It is, however, the most widely supported standard. AFH's answer seems to show how this can be done from Linux with grub.  The Windows Boot Manager from the XP/2003 days would use these sort of sector images (which would be referenced by a ""Boot.ini"" file), so getting out of Windows could be done by altering the Boot.ini text file.  (That process might have changed in Vista/2008, so check into the new boot manager before trying that on newer systems.)  A precise way to implement any of these solutions will depend on which operating system you're trying to stop using.  (For example: which version of Microsoft Windows.) These details might not exactly match how UEFI handles GPT systems, but it does describe MBR-based systems. The official (old) standard is to use what is on the Master Boot Record.  The ""fdisk"" command can be used to change the details of the MBR. It's funny how, with all of the fancy replacements people have made,  the advantages of the most widely supported official process can be overlooked so often. The most common method is probably to create a 512-byte image of the entire MBR when the system boots in one operating system, and then save a 512-byte image of the entire MBR when the system boots with a different operating system.  Then, use whatever software you need to, to directly write those 512 bytes to the first 512 bytes of the disk.  (Something like ""dd"" would do the trick.)  The biggest challenge with that might just be having permission by the operating system. Now, there are some other variations, and so automation is probably possible if you're seeking such a solution.  You may wish to check your operating systems for a command named ""boot"" or something like that; I know the old OS/2 operating system did contain a ""BOOT"" command that could modify which operating system was started.  The precise command(s) you would need to run depends on what boot code you actually use.  There's multiple ""boot manager"" programs available, and this functionality is now built into more operating systems.  That's the good news.  The bad news is that the process isn't quite as standardized, so I can't just provide you with one specific process that integrates well with everybody's different variations.  Basically, you need to pick a solution and use it.  The primary concern is likely to find a process that can be done from multiple operating systems, and which can be used to write to the sections of the disk that are handled while the system boots up.  Those sections are typically not part of a filesystem, and anti-virus features may try to limit a person's ability to write to those critical sections of the disk.",1
"then check if there is anything open after port 6000 (for example 6011). If, for any reason the DISPLAY is set wrong (maybe to your colleague machine?) then try to do: Also, when you do ""ssh -Y"", on the server the X server should listen to a TCP port 6000+n where n is the display number assigned to your trusted connection. You can check by doing: ssh -v will tell you if the server declines to allow X forwarding; you can also look at the sshd_config to see if it disallows X forwarding. David pretty much nailed one half of the equation -- the ssh mechanism automatically takes care of setting up the xauth authentication and defining your display, so don't override that manually by setting the DISPLAY variable yourself. When you modify the DISPLAY you tell the program to connect directly to your laptop (without encryption). In order that to work you should allow it on your laptop (do something like ""xhost +myclient.mydomain.com). The other issue is that some ssh daemons on the destination side of things are configured to deny X forwarding even if you ask politely. Probably you will get something like ""localhost:11.0"". This means ssh will automatically forward any connection going to localhost display 11 to your X11 server on the laptop. So, just start your program.",2
"The problem I'm facing is I want to simulate the ocean currents and trade winds on the world map so when the player is sailing on the map, the speed of the fleet will be affected by the current and the wind. It does not have to be a really accurate simulation because this is not a high-detailed simulation game but it should resonably simulate the ocean currents and the wind.  I'm creating an RPG/simulation game which is similar to the Uncharted Waters Series. The game allows the player to control a fleet to explore and navigate the world on the world map. My Game stores the world map as a tiled map into a 2158 * 1080 2D array.   I'm thinking about dividing the world into several regions and each region can be subdivided into smaller regions like a quad tree. For look up I can do a contains(x,y) and find out which region the point is in and then for sub-regions to the same thing until it arrives a leave of the tree which stores the strengh and direction of the current. What I would do is copy Minecraft's habit of adding metadata. Basically, metadata is a method of adding additional data to your tiles (or blocks, continuing the Minecraft explanation) which confers non-essential information (essential information being things like tile type and location). What this would mean is that in the XML file where I presume you are storing your tile data, you would also store the direction (1-8) of your wind and current, and its strength, or possibly aggregate both for a lower memory footprint. You would then load this information at runtime and store it in a 2D array akin to your tile map. As a C++ struct this translates to: Is that a reasonable solution or there are better ways to do this? What are the gotchas for implementing it? Notice that (at least if in C++), your struct or equivalent should have the strengths first, then the directions - this is due to memory alignment issues (data is most efficiently read at boundaries divisible by its size).",2
"Not a recommended way to fix it naturally! if you can find another way to recover the data then go with that as you might make it worse, but if you get desperate, take the drive out & hit it against something and give it a try again.  (if all else fails it might just make you feel a bit better!) That seems to be not an uncommon problem with WD drives. About half of my drives I had from WD exhibited that problem sooner or later. As long as you get it running again (might take hours) you probably can get your data off of it again, depending how long it survives before going into that click-click routine again. Before you try the freezer thing (and yes, it sometimes works. I successfully used this trick once myself before), try changing the drive's power source. Sometimes an underpowered drive trows up the click symptoms as well. I once had a HDD fail in a similar way (clicking noise) that I recovered by giving the drive a good whack on the side, as if something had got stuck.   But for all practical purposes the drive is dead. The only thing you could hope for would be that you still have warranty on it. If possible, use a different power supply. You do not have to swap out the current PSU, just attach the failing drive to a second PSU. If that is not an option, try using a different connector from your current PSU that might be on a different 12V rail.",3
"It's not a particularly neat/elegant solution, but it may just get the job done. I did something similar years ago when mucking about with ld.so and not wanting to mess up the entire system :-) Things are going to be further complicated by the fact you need a modern version of GCC.  I don't have a decent solution for you, you're going to need to play around with running things with the alternate version of glibc in order to get your newer gcc to build properly. I've only had to do this on CentOS 5, so you may be able to get away with higher software versions then I mention. Ahh, welcome to the fun of super outdated libraries.  I've run into similar problems, and our solution was to compile a second version of GLIBC and explicitly use that when starting software. You could install a newer glibc into ~/lib and install gcc in ~/bin and linking against the newer glibc. I'd advise to use that only for your application that runs on the supercomputer and to link everything (including libc) statically to avoid making a bigger mess than you'll have to make for this.",2
"It's still not entirely clear what you really want, but based on the clarification in the comments you're better off setting restrictions on when users can log into computers using something like logon hours. It sounds like you're more interested in preventing logins to the box after a certain point (like when the smart card is removed) rather than getting security benefits from the smartcard itself. Given that the logged in user will already have admin rights on the box, a smartcard isn't going to give any additional security assurances. I would like to set up a domain-joined computer for passwordless login with some low-privileged domain account that is specific to that computer. I've seen some blog posts that store the username and password in a registry key, but I also thought that a virtual smart card with no PIN would work just as well. Is it valid to provision a smart card with no PIN?  Since TPMs are almost always used as two-factor authentication, it is pretty unusual to even want to use a smart card without a PIN, but in this situation I am fine with 1 factor authentication. You might also consider automatically imaging this box on some schedule (nightly, weekly, etc) to wipe out whatever malware or badness is likely to wind up on it.",2
"but then bob as the owner would be able to chmod and give himself excessive permissions or to take them away from the ubuntu group, or to mangle with ""everyone else's"" permissions. Any answer that advises you to make bob the owner must consider this flaw. You need to set the user to be the owner chown -R <username> <foldername> (-R operates recursively) then set the permissions to be read only for just the user chmod -R 400 <foldername>. Root will be able to read and write the files regardless of the permissions set. Now the problem you're going to have is that root won't be able to execute any files unless at least one execute bit is set so you're going to have to let the owning user also execute the files with chmod -R 500 <foldername>.  You may want to set the directory ownership and permissions in a similar, yet somewhat different manner. You may want to do these changes recursively. Note that chown -R affects directories and files, that's why you need an approach that can tell them apart. See this answer. The approach works with chmod and chown and other tools. Adjust it to your needs. This way ubuntu has all the permissions and bob (as the sole member of bob group) can only read the file. I'm making a ""wargame"" similar to the ones on overthewire.org. I have the root user, Ubuntu, that should be able to read, write, and execute files in a folder. I have one user per folder that should have read-only access to the files in that folder, and everybody else should have no access to the folder. What is the chmod command to do this? Thanks! I don't know whether you refer to the root user or to some regular user ubuntu which you consider a root user for the purpose of your game. This doesn't change much in my answer though. In Ubuntu each regular user has their own group by default. Such group by default contains only its corresponding user. This makes the following trick possible:",3
"A VLAN - "" virtual LAN"" is related, but probably not what you are asking about as this typically involves ""physical separation"". ( It a bit simplistic, but you can think of VLANS as groupings of related ports which usually can't talk directly to each other. A single port can be part of multiple groups though.)  From the router OS's perspective, each VLAN tag corresponds to a separate 'interface'. If the main port is named lan or eth0, then you'll have virtual interfaces like lan.2 or eth0.5, each with its own IP subnet and address configuration. When a single Router is connected to 3 or more networks, can a Router know 2 of those different networks through a single Ethernet port/interface or does each network has to be physically separated.  (Not all routers support it but) you can have a single interface/port with multiple addresses (in the same or different subnets/networks) associated with it and a router can route between this networks. These are sometimes called ""secondary interfaces"". Yes, 802.1Q VLANs are used for this purpose  they tell the router to tag each Ethernet frame with its ""VLAN ID"" depending on which network it should belong to. A compatible Ethernet switch can be used to split/combine this tagged (multi-VLAN) port into individual untagged (single-network) Ethernet ports. I have no idea what VLAN are, but if the above is not possible, than I'm going to guess that's what its for...",3
"CMS does not allow you to schedule agent jobs like MSX does, but you are able to run a script through SSMS on servers grouped in the same folder in CMS.  For example, if you highligh a subfolder in CMS and hit New Query it will connect to all the instances registered in that folder and run the script on each one.  Below is an example script you could run; CMS will return the instance name and the user running the script as columns by default.  You can also use CMS for grouping Policy Based Management evaluations and policies. CMS is an extension on the registered servers concept and allows you register servers into the MSDB database of a single instance through the SSMS GUI.  Once the servers are registered in the CMS you can logically group them by business unit, version, etc.  Once you have them registered any DBA/user with permission to the instance can connect to the CMS and see the same grouping. The MSX/TSX is indeed related to the SQL Agent, allowing you to schedule jobs on a master server and have those jobs pushed out to slave servers.",1
"I'm looking for some type of remote linux monitoring software that you can view using a web interface. And I'm not just looking for the basic load information. I'm also looking for process information, similar to the info that you get from TOP. Like I'd just like to be able to pop open this webpage to view whats going on with the server at a moments notice. We use a combination of self-hosted solutions like Cacti and Nagios together with managed solutions like ServerDensity. +1 for Cacti as well, but since I don't see it mentioned, I'd say take a look at kSar. It has the benefit of not requiring anything but sar (very low impact) and ssh running.  ServerDensity has the great benefit of being incredible easy to setup and it has snapshots, which allows you to go back to a moment in time and see how your server was doing.  None of the previous responses really answer your question.  It sounds like you just want a simple webserver app that runs top once and displays the output.  None of the aforementioned monitoring products meet your requirements. There are performance monitoring tools like munin, cacti, zenoss (prf mon is just 1 feature) that will do what your looking for and much more. Although from your wording I am unclear whether you are looking for something like that or something more ""realtime"" with no historical information per se. In which case I got nothin for ya, sorry! For example, perhaps just a basic PHP page that is on the server that uses basic AJAX to display and refresh results from the TOP command in the page. Personally, I don't know of such a thing.  Probably because a) it's silly and b) it's probably written in-house all the time but not considered worthy of publishing for others to use.  You could whip something like this up in php in a few minutes.",5
"Consider using GlusterFS, it has an ability to combine several subvolumes into one volume. This feature is called ""unify translator"". Absolutely.   On the flip-side... this is a bit of a dangerous thing to do... as any failure on any of those 5 servers can mean total data-loss/corruption.  (the 5 servers behave as a disk.  5 disks spanned into 1 volume can have very bad outcome if one of those disks fails or becomes non-responsive) Say I've got.. 5 servers, each server has a partition with 50GB's of storage. Is it possible for me to combine these 5 partitions and then mount them as one on a 6th server?  some CFS strategies are block-based (the ""volumes"" are stored on each server as a block-device"" and some are file-system level only.  (the ""volumes"" only pay attention to ""files"" and pass the data to the underlying file-system to keep track of the blocks) This is the area of ""Clustered File Systems""  and there's a bunch of different ways to implement it... all with their own pros & cons. iSCSI can do this at the block level while being FS agnostic, but has the same drawback as a DFS or CFS, as @TheCompWiz stated. You can do a wikipedia search for ""Clustered File Systems"" and ""Distributed File Systems"" for a list of various flavors... and get some information about how to implement them.",4
"As an example, I have a saved game right now in Dead Space where my health is incredibly low, and not 1 second after the save, a big boss monster attacks.  I've tried loading and fighting it numerous times, and basically have given up on ever doing so.  My only solution is to go back to a VERY old save, and frankly I'm not sure if I want to do that.  It seems reasonable to assume that Mass Effect 2 is a state machine, and therefore, the internal state of the program at any moment can be captured and reloaded later.  This is basically a solved problem - games have been designed this way since the Half-Life era.  It also seems reasonable to assume that BioWare knew what they were doing when they made the decision not to follow this model - it's a tried and true system; BioWare wouldn't have done it the way they did without some good reason.   Finally saving in other times apart from combat is by design. Again, if the fight you're in is a particularly tense firefight, the developers would want you to experience that tension every time (or something to that effect). The first and probably the most important reason is to prevent really, really annoying the player. E.g. You're in a middle of a tense firefight, you save the game, leave it for a few weeks, load it back up and you're under fire from all angles without any room to breathe. Another reason is that saving the states of the enemies at that particular time could potentially be problematic. Saving the game state is fairly simple since everything is fairly static (objects in the game world, position of the player, etc). You could save the enemies, but depending on how their AI was implemented it could cause some serialization issues. If you save in combat, you run the risk of saving just before something is going to kill you.  Reloading that saved game just puts you right back into the game moments before that event, and it can sometimes be impossible to recover. So basically, you never want to have the player save the game just before something is going to give them a serious setback.  And being in combat greatly increases the odds of this. It gives the players a reason to really think about their tactics. With quick save you could just try it the same way over and over (slowly progressing forward) until you have ""beaten"" it. Without they have to rethink on how to approach the problem. (be it to be less aggressive, to make better use of the abilities or whatever) So I've been playing Mass Effect 2 (PC) and one of the things I've noticed is that you can only save your game when you're not engaged in combat.  As soon as the first enemy shows up on your radar, the save button is disabled.  Once combat is over, save functionality reappears. I don't think you should use arguments about being able to save right before you die as a reason why you shouldn't be able to save anywhere or in-combat. As the developer, you can implement auto-saves that kick in around areas where combat is about to break out. That way the player can always go back to before the combat started. You can also let the player have multiple saves, then they can go back to any point that they want if they're using that feature.",5
"Arrange your code logically. First you declare lab3. Then you declare word. Then you declare a lot of other stuff. And finally, you make sure lab3 opened correctly. Doing all the ifstream stuff in one place makes it so much easier to read/fix later on. E.g. Misc. std::ifstream's close() gets called by the destructor when it goes out of scope. No need to do it manually here. Your output says you have 4 sentences. You have 4 lines. Several more sentences. There's probably not much to do about it here, but whenever I see a whole bunch of variable definitions grouped together, my first thought is to move them closer to where they're actually used. But again they're already about as close as they can get here. You never use word. About using namespace std;. It's best to just do, e.g., std::ifstream etc each time. There's a lot of stuff here, and even in this example it would have made it easier for me to see that std::ispunc was built-in, and not something you wrote for this. For reading the file, personally I would use std::getline, then split each line into a vector of ""words."" First of all, this would it easier to check whether to count something as a word (I think yours breaks on contractions.) And then you can just loop over each character of each word of each line for the small stuff. Variable names: If your name is two words put together, you should use either snake_case or camelCase consistently. E.g. countletters -> countLetters. I'd find numLetters even more readable. Some people would even say you really ought to make it numberOfLetters. lab3 also ought to be more descriptive.",1
"When it gets in this state, the errors when we get when we try to unicast to 169.254.20.10 or broadcast to 169.254.255.255 are either 'Host is down' or 'No route to host'. This is confirmed by using an app such as IT Tools to ping the device. We are using the iPad to make an ad-hoc wireless connection to our proprietary device. DHCP is not available, so we are using addresses in the Auto-IP range. The user selects the device from the list of wireless networks on the iPad and about twenty seconds later, DHCP fails and an Auto-IP address is assigned. The device has a fixed IP address of 169.254.20.10.  The problem we are having is that from time to time, the iPad loses it's ability to connect with the device. This issue appears to happen much quicker with the iPad 4th generation and iPad Mini. Usually a combination of switching to another wireless network on the iPad and then back to the ad-hoc network and/or turning Wi-Fi off then back on again fixes the issue, but sometimes the iPad needs to be restarted.",1
"Jul  4 11:50:46 Virtualization kernel: [ 6239.229383] e1000e 0000:07:00.0 eth2: 10/100 speed: disabling TSO Beginning at 9:22, every fifteen seconds or so, syslog spits this out (""Virtualization"" is the server's hostname): The cables and connections are all well tested and weren't touched between 8:05 and the time I came in. I'm scratching my head. I recently configured a server with two identical Intel NICs inside a DMZ. This morning at around 8:05, I successfully SSH'd in through the internal interface (via my LAN-based PC) and had access to the outside world as well. When I got into work at 9:30 or so, while I still had external access (eth1), I wasn't able to SSH in, nor could I ping out to the LAN (eth2). Restarting the interfaces or rebooting had no impact.  Jul  4 11:50:46 Virtualization kernel: [ 6239.229261] e1000e: eth2 NIC Link is Up 100 Mbps Full Duplex, Flow Control: Rx/Tx Jul  4 11:50:46 Virtualization kernel: [ 6239.229380] e1000e 0000:07:00.0 eth2: Link Speed was downgraded by SmartSpeed",1
"Finally got a call with Microsoft over this issue, and they found a solution for us.  We were having this same issue - on Win7, we had the monitor set to turn off after 5 minutes, but screen saver after 15 (which is what would lock the screen).  In Win10, it was locking after the screen turned off, at 5 minutes. This only occurred on domain-joined machines.   On a related note, it is possible to disable the Windows 10 login screen so that you don't have to enter a password after booting your PC, but that subject would warrant a separate tutorial, and it comes with one big downside: If you disable the password prompt and login screen on Windows 10, you'll lose the ability to sync settings and data across all of your Windows devices. Hopefully this process becomes a bit more stable in future builds, and we'll cover it at that time. I got the same issue in windows 10 go to cortana and type secpol.msc then go to Local Policies folder, then Security Options, then find Interative logon: Machine inactivity limit click twice to open it and change the number in seconds for example 57600 will give 11 hours or you can set it to the highest 599940 after this  go to cmd and type gpupdate after is done re-start your pc and enjoy no logout from your account :)  In Advanced slideshow settings - there is a setting: 'When my PC is inactive, show lock screen instead of turning off the screen' that was set to ON, change it to OFF. I have taken it from here: https://windows.gadgethacks.com/how-to/disable-lock-screen-windows-10-0162965/ You say that you ""use the blank screensaver for a couple minutes, and then just have the monitor turn off a couple minutes after that"". If the new account solves the problem, you wont need to do a Reset, Restore, Refresh or reinstall of Windows. Computer/Administrative Templates/System/Logon/""Allow users to select when a password is required when resuming from connected standby"" There is an obscure policy that affects this.  To get your Win10 machine to behave like the Win7 machine did (stay unlocked after the screen turns off), Enable the following policy:",5
"The voltage divider circuit you have should work fine.  However, it offers very limited over voltage protection.  Upon a high voltage condition, if the Zener opens, then your board will likely be damaged. I'd just use a resistor divider with say a 50k and 25k resistor, no capacitor, no zener.  The resistors would naturally limit the current.  Even if the wall wart was over 5V the current should be small enough to be handled by the GPIO protection circuitry. I'm just wondering since there isn't any load on the 3.3v end of the voltage divider, do I need some sort of current limiting resistor in line with the 3.3v to protect the GPIO input? From the recommendations, I've come up with a simple voltage divider circuit, with a capacitor in parallel with one of the resistors and a zener in parallel with the entire circuit. The 5v source will be from a 5v wall transformer rated at 1 amp. The capacitor will just increase the likelihood of damaging the GPIO (although the low impedance will probably protect it).  I'm working on detecting whether or not an external 5v supply is on or off and found a few helpful related questions, specifically this one: If you are concerned about the possibility of exceeding the input voltage (which the capacitor almost guarantees) use a diode clamp to 3.3V OR just use 2 equal value resistors, which will give 2.5V - well above the threshold of the GPIO. (I am an electrical engineer ;-)",4
"So I'm trying to play a sound only once from a raspberry pi using pythons pygame library. The raspi is hooked up via hdmi and sound is configured correctly. My original sound file was created and exported from Adobe Audition. I tried to export from audacity with the same results. I've even tried different formats. I also downloaded someone else's example with audio files and I get the same results. I've tried pre_init() with various settings, nothing here effected the non-working examples below. The indirect approach is generally useful if you don't need total control over the playback but just a simple interface. Sometimes I use it to play a song for a certain duration and stop by having a timer callback terminate the process note the trailing & will launch the process in the background and let the boot process complete when exit 0 executes One another indirect way is use python to launch ogg123 or mpg123 using the subprocess module Popen like so Assuming your objective is simply to play a single file once there are much simpler ways than writing a python script Ok, I'm not sure if this is the proper way to fix this, but I found that the script ended before the audio has a chance to play. So to fix that I added: If your objective is to play the sound as part of special effects (say as a part of a game) your python based approach is more appropriate.",2
"No one in my group can explain it besides a guess about packet loss. Is there a common cause or set of causes for result like the above? If not, run ""EXPLAIN"" to see what the execution plan is. This will give you hints on optimization or indexing. Do you have the query cache enabled? I found this (closed) bug and one reason given for the long behavior was an improperly configured query cache.  You may find there's unnecessary complexity in the query that can be pushed to the application or just factored out. I'd say this really depends on the query itself, while you're only scanning 739 rows in your table and returning all 739, it depends on a few things. One how much RAM mysql had to work with for that thread/query. Also it depends on the query itself. Imagine a query with 20 inner joins and another 20 where clauses your result set would be limited but the amount of work to pair down those result sets would definitely be significant even if the result set is relatively small. my idea is to diagnose the entire database with some deeper-monitoring tool - I'm using nms express (free). http://bit.ly/nwwdKS",5
"If God is not a functional programmer and would only include one PFDS in the Book, it would have to be the zipper. Result is pretty simple, but super helpful to use in practical scenarios, and algorithm design . It can help to reduce computation complexity by load balance in algorithm based on ""the power of random two choices"" From the world of purely functional data structures and algorithms, Grard Huet's zipper comes to my mind. I think that Hensel Lifting is pretty nifty too, and it has many applications in algorithmic number theory and algebra. Floyd's Cycle Finding Algorithm is one of the most beautiful things I've seen.  Especially the part where he finds where the cycle begins. Normally, PFDS do not expose local structure, due to the absence of explicit pointers. If you want to access a certain node in a tree, you're out of luck.  I would add universal hashing (or more generally pairwise independent hash functions) of Carter and Wegman. While not really an algorithm in itself, it is the enabling technology in a lot of fantastic randomized algorithms. To name a few: However, with an extremely simple insight (just ""turn the tree inside-out"" and remember the path structure from your location), accessing specific locations in PFDS is made possible, all in a purely-functional manner. This, basically, is the essence of the zipper.",5
"There are a couple of minor and fairly unlikely possibilities as well, including daft firewalls at your site rewriting DNS traffic on the fly as it passes through them.  But since you've not provided proper data, there's little more to narrow down and rule-out the possibilities that you can obtain from random passers-by out on Internet. If multiple tries all fail, there is something broken. Chances are you're seeing cached answers with the ""normal"" request; expect the breakage to surface as soon as the TTL expires. Note that the only caching that occurrs here is local, on your resolving proxy DNS server.  The content DNS servers that you are querying don't cache.  (Or, more strictly speaking, if they do cache they cache the back-end databases that they are working from, in ways that have little to nothing to do with resource record TTLs, and that aren't publicly visible through the DNS protocol.) To eliminate any chance of faulty queries, could you try dig +trace example.com? It will follow the chain for you. If that succeeds, (it will only try one of the authorities at each level), you have at least one working trail. In bind you should have the following three options specified or the default will be applied, which has changed with bind versions. Just like to point out one additional possibility, since I just ran into this: someone set the TTL of the DNS record to zero. This actually resolved fine when querying the name server directly, but gave a SERVFAIL when asked trough the local (ubuntu) resolver. This sounds like an ACL issue on the name servers. The best practice is to separate resolvers and authoritative servers. The domain sounds to have 2x authoritative servers and three caching resolvers that have restrictive acls preventing your query for the domain.  Your ""however"" case works due to the request being a query and not asking for recursion. Don't obscure your DNS data when asking for help with DNS problems.  It's pointless and silly, and this is a classic example of how it has served to obscure the actual problem that you have.",4
"Notch actually made a Mario clone in Java and has the source code available for perusal. Notably he has a Sprite class that gets extended by classes named things like Fireball, Enemy, and BulletBill. His Mario class is also a subclass of Sprite but whether that class is called Mario or Player probably won't help or hurt in the whole scheme of building this. Passing the Graphics context to the classes that represent visible constructs so that they can draw themselves will work out nicely and is indeed what Notch has done. However, you will most likely want to control when this happens instead of letting Swing make that choice for you. This would mean calling the equivalent of paintComponent yourself which would then make it unnecessary for these things to be JComponents. As for the use of TimerTask to update a the state of the game; when the game hits the performance wall you won't be simply dropping to a lower framerate, the game logic itself will slowdown. This will be noticeable even when the drop in framerate on its own would normally not be. What's worse is that the TimerTask schedules will stack up during this time so the player will experience a period of slowdown followed by a fast-forwarding of both the game logic and framerate. When a game drops frames we expect to miss out on things, not for the passage of time in the game world to be altered. It is common to separate the logic and graphics updates into multiple threads for precisely this reason. The thread that controls the rendering could have its core logic look something like the following:",1
"This is enough for most use cases I've seen. If you want to do something with the textual output itself you'll need to go a bit further as Powershell will store the stderr-originating lines as a RemoteException instead of a string. A brief example of how you might deal with something that needs to log output to the console: You may be running into Double Hop problem explained here: http://blogs.msdn.com/b/clustering/archive/2009/06/25/9803001.aspx Unfortunately (at least in my opinon), git outputs a lot of information to stderr even when there isn't any error. For example git checkout -b somenewbranch will output things like modified file lists to stdout but the Switched to new branch somenewbranch message (which I think is the most relevant one) to stderr. I see this 'error' as well when using Powershell ISE and Git locally. It isn't actually an error; rather, Powershell ISE is incorrectly interpreting Git's console output. If you try the console-based Powershell, you should notice that there is no error information displayed. It seems that this is just a peculiarity of Powershell ISE (and possibly other Powershell hosts that don't handle console application output well).",3
"The problem with the model that I've created is that the child controls are added to the Game's components when they're constructed, so they start updating and drawing and thus show on the screen before the dialog is fully constructed. The case I'm trying to debug is where the labels for a dialog box will appear in the top left, and then move to the center of the screen a split second later once the Dialog is constructed. Dialogs are popped up in game by instantiating an XNADialog object, unlike MessageBox.Show() which uses a static method. The constructor of XNADialog, in turn, instantiates a couple controls that are 'children' of the parent dialog control. Children are tracked by the dialog, and removed from the Game.Components list automatically when the Dialog is closed. I'm working on a control library for XNA to use in a couple game clients I've written. The way I've implemented Dialogs is to have them derive from the base XNAControl class. What is the best way to keep the child controls from being updated and drawn prior to their parent control's construction? My only other ideas are: XNAControl objects follow the Game Component model and are all derived from DrawableGameComponent. They are added to the Game.Components list as soon as the base class XNAControl constructor is called.",1
"The rest of the design, the over normalization,  doesn't strike me as necessary for a temporal database,  but I could be wrong about that. The mention of EVA and 6NF gave it away, I just need to search for ""entry value model 6nf"" and Anchor Modelling was amongst the results. Note that the end datetime of the first row exactly matches the start datetime of the new row. And finally in March the account is deleted. After the deletion the table holds When implemented manually every query that touches such temporal tables must include the ""from"" and / or ""to"" columns in the predicate. I don't know how that is called, but unless you have a database that uses a column store, that design is horrible: That's 6NF. I've never ever seen a DB implemented to this level of normalization. I shudder to think what the queries would look like. The write amplification must be quite huge, too, given that every table will repeat the PK and index it. In today's world I'd much rather use column-oriented storage, let the DBMS handle the decomposition, and enjoy whatever compression it can apply. The NULL shows this row is applicable indefinitely. On 2nd Feb that user changed email address. The table then holds One aspect of the design you outline reminds me of temporal databases.  It's the aspect that every fact is tagged with a time interval during which that fact is valid.   That's temporal tables. Some DBMS provide syntax to declare this in the DDL. It's not difficult to implement manually. Each table to be tracked is given timestamps to show when the value(s) in that row were considered the current value. To use your 6NF email example, the table would be The second (i.e. later) rows is closed. The user is logically deleted from the system although the rows are retained. In this way audit and ""time travel"" queries are possible.",4
"A DC jack will run you between $3 and $10 on EBay, so the parts aren't that expensive.  It's taking the laptop apart, and de-soldering the old jack off the motherboard that is tricky.  My ex-boss had ruined many motherboards by forcing the old DC jack off the board, removing the tin collars that the jack's pins were set into, so that the soldering job on the new one no longer made contact on both sides of the motherboard.  So, it's best left to a repair shop if you aren't really handy with a soldering iron. When I've had this happen in the past, it's always turned out to be the cable that is the issue, not the internal connector.  So, the first step I'd take would be to find a compatible adapter and try it to see if that resolves the issue.  If so, buy a new adapter, and job done. As a computer repair technician who has repaired dozens (if not hundreds by now) of this exact issue over the years, I can tell you that you CANNOT assume it is either the port or the cable without direct examination.  It could be either, and it would be irresponsible to say definitively which is it. The port on the laptop can easily tell you if it is broken without taking it apart.  If the laptop DC jack has a pin in the center, and that pin is wobbly (aka, touch it with tweezers or a pencil eraser, etc) chances are, it's not making a solid contact any more.  The plastic housing of the DC jack might be broken from the cable taking one too many hits, and that would be harder to see... but if there's no visible damage on the end of the cable, and the center pin on the DC jack isn't wobbly, then the damage is most likely in the plastic housing of the jack itself. I have a laptop I purchased about 2 years ago which has an issue concerning the port where you plug in the AC adapter. Through normal use, it suddenly noticed this issue where you cannot get the laptop to charge unless if the cable is angled just the right way. My only guess is that somehow the connector inside the laptop somehow shifted. The laptop is no longer under any sort of warranty. I have no clue how to go about fixing the problem and I feel like I won't get much money selling the thing as is. Its a $1700 laptop which is currently collecting dust. I'm looking for some advice. What would you do in my situation? That said, you can take a look and see for yourself.  If it is the plug on the end of the cable, the damage will most likely be visible as the cord would be separating, or the braided ground shielding will be showing, etc.  If the barrel end of the cord itself is damaged in some way, separating from the plastic or crushed, then this too would be plainly evident.  It is possible that the AC/DC adapter brick portion of the cord is damaged, but for that, you'd need multimeter. 1) Sending off to the manufacturer or taking it to a computer repair shop that can diagnose and resolve the issue for you for some cost.   Repair?  If it's the cord or the end of the cord, replacing the cord is the easiest repair.  You could purchase a new end at Radio Shack and cut the old end off, but that does take a basic level of skill... it is not easier than just purchasing a new DC adapter.  If the issue is the DC jack however, that does take a bit more skill... and I would not recommend it for a beginner. 2)  Find a manual with disassembly instructions (often downloadable from the manufacturer support site) and see about taking it apart to determine if you can find and repair the issue yourself.    An alternative, is to purchase a two contact connector like this, and step away completely from the pin-barrel design that has already proven to be shoddy.  Then, you simply solder in two wires in place of the DC jack (one to the positive connection where the center pin was connected, the other to one of the many ground connections), knot or fasten the cable inside the casing BEHIND the hole the DC jack used to go out of, and connect one of those two contact connectors to the wires.  Then, remove the end from your DC adapter, and connect the mate for that two contact connector.  Why would you do this?",3
"I found if I have other PDF files open while trying to insert a PDF into a Word document I get this message.  If I close all opened PDF files running in my background I am able to insert a PDF with no problem.  Not sure why but that was my solution for my laptop.  I have a desktop and I am able to run Adobe Acrobat and install PDF file into word with out having to close anything.  Could be in my settings in Acrobat.  Hope this helps   The ""Correct"" answer suggested here (first search result in google) didn't work for me and gave same error  .   To insert the PDF into a word document you simply need to open Word select insert object, in the Object windows select the create from file tab and browse to the file you want to insert, select it now you need to tick on one of check boxes below depending on how you want the file to appear, if you want it to appear as an icon you both check boxes must be selected otherwise select just link to file. While digging for escape routes facing the same issue, I figured this error could occur from a crashing adobe/acrobat program running in the background in Windows system.",4
"There are two strongly connected but still a bit controversial things in the industry that makes this kind of question being asked all the time since 2010. I would say DevOps is encompasses managing source control, managing the build pipeline such as CI/CD and managing tests(unit tests, integration test...), managing the infrastructure including managing the cloud service being used(if any). With the tools that are being offered to teams, there is added complexity. The increase complexity required more time for a team to setup correctly. So what would happen was that one or two team members would gain experience in setting up all the necessary tools. These team members would need to spend more time with the operations side of development and these team members became the DevOps.  So you are absolutely right  DevOps should be done by everyone in a team, just because it is about team itself, not some person who makes changes, and everyone and everything else remain old-fashioned. People realized, that being badged as DevOps engineer, not simply being a sysadmin or developer  makes them more unique and valuable on the market by that time. That is how market works overall - you make offering and advertise what demanding person is looking for, so he prefers you as a supplier, not the other guy. The role of DevOps is still being formed so the responsibilities can be different between organizations. Also, this was the developers responsibility not too long ago. All developers should have some knowledge in the operations side of development, but it has grown to the point that it its too much to ask all developers to have sufficient knowledge current operations and the development work they need to know (how all components are tied together, any frameworks that are needed...) It is way harder to make same kind of measurement for collaboration between team members, communication improvements, whatever. And higher management usually looking for kind of measurable results, rather then ephemeral ones. Popularity of the methodology arisen - as it was intended to solve problems that majority of industry players had more or less. So some people started to see non-existent silver bullet here. Some magical potion, which simply could solve issues they have. So we got high demand here. And as a result  supply started to grow as well. And obviously businesses were looking for someone who could implement DevOps for them. How do you usually initially identify someone who could work with some tool/methodology/whatever? I bet - the answer is - by looking for appropriate badge in their CV:) First one is a nature of DevOps methodology itself. It was born all around idea of better collaboration between different parties of software delivery process. This involves a lot of non-technical aspects, like shared responsibility, better communication, knowledge sharing. Another improvement that was aimed - was product delivery itself, starting from designing software and up to deployment and operating live systems. So it brought us technical improvements as well. There was nothing new, actually, as all those things you see in majority of DevOps job openings like CI/CD, configuration management, log management, monitoring, whatever  were existing for decades, before even term DevOps was used for the first time.  And somewhere here we got that focus shifted from whole methodology to technical side of it. I have no exact answer, why this happened, probably because majority of market players missed the key point of DevOps ideas, and understood only those which could be measured easily.",2
"SSD's have different method for secure erase.  I will say that it seems to be very cumbersome to do, because you usually need a certain type of SATA controller that can do IDE emulation, and the procedure can be complicated.  Some manufacturers provide tools to secure erase their own SSD's, but you can also do it with hdparm on Linux:  https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase.  But you'll notice in those instructions that you have to make sure the drive is not ""frozen"" before you can proceed.  This is one of the more difficult steps because it requires finding a motherboard & SATA controller that will let you ""unfreeze"" the drive while the system is booted up, which usually involves unplugging it from it's SATA cable, then plugging it back in. Assuming that what you are seeking to prevent is the next customer reading the disk to see the old customer's data, then writing all zeros would actually still work. Writing zeros to sector 'n' means that when sector 'n' is read, it will return all zeros. Now the fact is, the underlying actual data may still be on the flash chips, but since you can't do a normal read to get to it, it's not a problem for your situation. So it's worth reading articles such as this. If someone has physical access to the disc then retrieving information is easier. Have you considered encrypting the data on the SSD and then all you need to do is securely forget about the private key which should be an easier problem. I can see SSD being a big win on vps's because of the much better random access performance.  Although one answer is already accepted, I think the command blkdiscard /dev/sdX is still worth mentioning here. there's no way to make 11011011 become 00110011 (notice that it would be necessary to turn one 0 to 1, and it's not possible to do that in SSDs). So, another memory cell will be used. It IS a problem if someone can physically get hold of the disk and take it apart (because then they could directly read the flash chips), but if the only access they have is the SATA bus, then a write of all zeros to the whole disk will do just fine. After, when you want to save something different, the previous content and the new one are compared. To do what you want: first, erase (delete) the files. The memory cells to that files will be marked as free. Then do a TRIM: all those memory cells will become 1's, without any sign of data.  If the previous one can become the new one by writing down some 0s, ok. If it's not possible to do that, another memory cell is used. Anyway, my recommendation is to do your research & pick an SSD that comes with a secure erase utility that can be used on a system convenient to you. According to Arch Wiki: SSD, the blkdiscard command will discard all blocks and all data will be lost. It's recommended to use before ""you want to sell your SSD"". I am not familiar with how TRIM works so I don't know whether there is a guarantee that the data will be erased. But I think it's better than doing nothing. When you TRIM a drive, you are reseting all the unused memory cells to 1. So, they'll be clear to be used again. And the saved data is preserved. You definitely do not want to use traditional methods of erasing SSD's, such as using dd to zero out data, or other methods that write random data to the disk.  Those methods are better suited for platter based disks.  It is effective in erasing the SSD, but it will also unnecessarily use up a lot of the SSD's limited write operations, thus decreasing the SSD's expected life.  That would get expensive quickly.  It can also decrease the SSD's performance over time.",5
"There are applications in the market that allow you to add entries to your hosts table in Android. They would probably need root though... Not sure on that point. Additionally, some routers can also allow you to define a name for the static IPs and provide DNS resolution for those names within the network. But that would depend on the capabilities of your router. Paid features include domain and subdomain reservation - but this and the sign-up features don't appear necessary for your purposes. But when I try this url (http://gm20152.local:3000) from my other devices - which are connected to the same wifi network as the macbook - The browser cant find the server. I guess this is because the android phone that I am trying this on has no way of mapping this name (GM20152.local) to an ip address (my macbook ) . Whats the best way to accomplish this ? There are applications and services that streamline the process of exposing a localhost port (or a configured local virtual host) to the internet over HTTP or HTTPS. Additional features that require sign up include different types of network tunneling, multiple tunnels, password protection of your tunnel(s), and custom subdomains. If you prefer to manage the tunneling service yourself, you can even run your own ngrok server (Docs: https://github.com/inconshreveable/ngrok/blob/master/docs/SELFHOSTING.md) but again, not necessary for your purposes. I have a web application that runs on my macbook on port 3000. I want to test this on a couple of phones and looking up the ip of my macbook eachtime i want to test this is turning to be cumbersome. I wanted a way to do access the web application from these phones by using my macbook's hostname .  One example is ngrok (https://ngrok.com/features) though there are others like localtunnel, pagekite, or forward. Depending on the router you are using, you could assign a static IP address for your Macbook by identifying your mac address and then use a hosts entry in the mobile phones. The free features and basic download and usage of ngrok (See: ngrok.com/usage) should allow you to create a tunnel that exposes your local web server via a randomly generated ngrok domain. You will also be able to inspect traffic requests.",3
"So to your first question I have to say 'category error'. It's a little like misunderstanding what a car is and asking if you can pull it with a team of horses; sure you could but it isn't even slightly what you'd do in practice. A Pi is not a microcontroller - the ARM cpu it uses is a full microprocessor (a quad-core on the Pi2) with floating point hardware, the ability to use a lot of ram, blah,blah,blah. Just because it's so cheap you shouldn't think of it as some sort of PC adjunct or Arduino-like.  If your needs are not much bigger than lighting up an LED when a button is pushed then you'd really not need a Pi. It would be like using a Tesla as a golf cart. If you want a fairly powerful SBC that can do that and go on to do a whole lot more, maybe a Pi is the answer. A big practical factor is that a Pi is so cheap you may as well get one anyway.; you're going to have fun with it one way or another. You can run full linux (the default is a Debian derivative) and develop using any gcc stuff (plus many other languages) or a minimal RTOS, or even do bare metal if you really want. The forums at raspberry.org cover all of them. You can even run RISC OS which is the only OS worth caring about (yeah, yeah, I know). I shudder to write it but you can even run the Windows 10 IoT kernel. Yuck, now I have to wash my hands.",1
"First, here is a picture of two pages of the same manual, which seems contradictory to me.  One page says only a single connector is required, while the next page says both are required.  The entire manual for the card can be found here: http://www.nvidia.com/content/PDF/kepler/Tesla-K20-Active-BD-06499-001-v02.pdf In case anyone needs it, the owner's manual of the R720xd can be found here: ftp://ftp.dell.com/Manuals/all-products/esuprt_ser_stor_net/esuprt_poweredge/poweredge-r720xd_Owner%27s%20Manual_en-us.pdf The relevant page is page 68, which clearly indicates that the 8-pin female port on the riser card is for a GPU. Neither the R720xd NOR the GPU came with the necessary cables.  And given what appears to be a contradiction in the GPU manual (above), I'm not even sure at this point what we actually need.  I have searched high and low online for things like 2x6 pin PCI-E to 8 pin male-to-male and so on, and for the life of me cannot find what we need. I am trying to put an NVIDIA Tesla K20C into a Dell PowerEdge R720xd.  I'm having a bit of trouble understanding the power requirements of the card.",1
"It really depends - (Assuming Windows) Open up task manager, If your Memory usage is high, increasing the memory could give you the ""umph"" that you need. If however, your memory is in the 45% or lower it is unlikely that adding any more would be noticeable to speed. Really, the answer is ""What is your current bottleneck?""  If you're playing certain video games (and that's what you really want to speed up) the best upgrade might be a new video card.  If you do lots of database stuff, the RAM might be a better choice.  If you're doing enormous amounts of pure math, say 3D rendering ... you'd have to see what's being overused, I guess, because that's both RAM and CPU intensive.  In any case, the answer depends on what you need, which we don't know from the way you phrased your question. Then, what OS are you running?  Certain OS (XP) may benefit from more RAM whereas others like Linux won't really gain that much.  The RAM type if old may be expensive also. I am not to familiar with AMD CPU names, but if it is more than 3-4 years old, you can probably find the highest one that is compatible with your motherboard cheap on eBay which may be worth considering (remember, you will also need thermal paste and/or (as usually included) a new Heatsink and fan). You can get a ""cheap"" Dell for $400. If you look to upgrade, you might find yourself needing a new motherboard, a new CPU, new RAM, a new GPU and possibly even a new PSU to power everything. If your computer is rather old, CPU upgrade might be possible, but there's the chance that other components in your computer will not be able to keep up with the performance of the new CPU. No point in worrying about RAM if your board can't support more.  Second, what kind of CPU's can your motherboard support?  Again, no point in worrying about it if your CPU options are maxed out. If you want to get a much newer CPU, it probably would not even fit in the socket or be compatible requiring new memory anyway which will mean you will need to do both. Next, what kind of CPU socket types can your motherboard support?  If your board doesn't support more advanced CPU's, then carefully consider what you gain by upgrading at all.  If you can only go from 1.8 to 2.2, then the increase isn't that significant. Upgrade both at once, that machine is so old you may well have to change motherboards and RAM technology entirely to upgrade the CPU, in which case you will have wasted money on RAM.",5
"You're making this more complicated than this needs to be. Use the MDT database, that way it's all pre populated before the tech even runs the UDT enviroment. It'll take time now, but pay off in the long run. So, I have written my own OU tree chooser, and I would like that to run before the UDI wizard pops up.  This OU tree app allows a technician to select an OU to build the machine in, and then injects that OU into the UDI_Wizard_Config.xml file, so that the UDI wizard and the task sequence knows to put the machine in the chosen OU.  My questions are: We have a very decentralized environment, and the UDI Wizard's OU selection page just isn't good enough to fit our needs, since I have to manually input every OU that a technician could choose when they build a machine.  I have developed a .NET app that will query Active Directory and inject the proper XML to populate that drop down box with every OU in our forest, but the width of the dropdown box is too small, and our long AD paths won't fit.",2
"It's probably best to prepare all the header lines in a separate file (e.g. Notepad) and then quickly copy/paste them into the Telnet session. However, Twitter doesn't support Basic anymore. The only available method is OAuth, which is far too complex to use manually. There are tools such as Twurl for accessing Twitter APIs. I'm trying to use telnet to access http://stream.twitter.com/1/statuses/sample.json but  when I connect to it I get a 401 Unauthorized error. Being able to specify username and password would allow me to get in. When doing this in a browser, I get a popup asking for it, is there a way to do the same through telnet? If you want to use Telnet as an HTTP client, you have to speak fluent HTTP. In particular, if the website requires basic authentication, you'll have to send the authentication credentials in the header with your request. You can do Basic authentication with just a Base64 encoder program. (Of course, curl -u foo:bar http://stream.twitter.com/1/statuses/sample.json would be far easier, but your choice.)",3
"What are protocols? I'd imagine they're some sort of code.... Can you create your own protocol? How do you get a specific port to run a specific protocol? What language do you use to create a protocol? How do you define or invent a new protocol? I hear people talk about ports and protocols (in relation to computer networking), and they often provide analogies for them (for example: ""a port is much like a shipping port, it sends and receives data like a shipping port sends and receives goods from other ports"") and things like that. See https://journal.paul.querna.org/articles/2012/02/22/designing-network-protocols/ for a blog post by someone who has recently developed a new networking protocol and what sort of things he had to do along the way. Note that this all refers to port numbers, from a TCP/IP perspective, the actual data being moved across these ports could be anything.  It doesn't care or have any awareness of applications, so if you had web traffic on port 25 and email on port 80, it would be none-the-wiser. Ports are, for the most part, implemented at the transport layer (layer 4 - Yes, the numbering is correct.) Only one process may bind to a specific IP address and port combination using the same transport protocol. Common application failures, sometimes called port conflicts, occur when multiple programs attempt to bind to the same port numbers on the same IP address using the same protocol.  TCP has been defined as having a total of 65535 ports, which any program can use to do whatever they want with (although many OSes limit the use of ports under 1024, giving them an ad hoc special status). Although there are some limited lists, there is no real standard for defining who gets what port and what port runs what program. A port is, thus, more or less a random number that different implementations of a program decide to agree to communicate on. Of course, the designers of such programs try to avoid ports that other popular programs have chosen already. This lets the web service know that these are separate requests, but also, the return traffic from the webserver - the web pages - are sent back to the respective source ports, which enables the browser to know which request the server is responding to. In regard to being able to increase the number of available ports, you cannot assign a port above 65535 due to the math that allows networking to work (binary) - so the answer to this question is no, you cannot increase the total number of available ports above 65535. Granted that it's in no way required to run using TCP. Some protocols run on the bare internet layer, or even the bare link layer, mainly for purposes of efficiency, or because these protocols were invented before even TCP or IP existed. Of course, when doing so, you trade away the simplicity and the extensive bug-checking of OS networking libraries. A port number is a 16-bit unsigned integer, which means the range of ports available for use is from 1 to 65535 (port number 0 is reserved and can't be used). A process associates its input or output channels via Internet sockets, a type of file descriptor, with a transport protocol, a port number and an IP address. This process is known as binding, and enables sending and receiving data via the network.  Futher to Hello71s answer, it might help to visualise a port by thinking about the structure of an address in a packet.  A packet being a unit of data passed around a network.  TCP is an example of a transport layer protocol that uses ports, and is commonly used over IP.   So IP has two addressing components - the source IP and the destination IP.   TCP adds to this by using a source port and a destination port.  It is the ports that enables the recieving machine to differentiate traffic destined for the same IP address - ie, if you have a server that recieves both web requests and email on a single IP address, then you need to determine which application should recieve the data - the email service or the web service.  So they may look like this if a single user was to carry out a web request and an email request to the same server: Although this question has already been marked as answered, I wanted to address some of the additional questions asked in the OP. The operating system's networking software has the task of transmitting outgoing data from all application ports onto the network, and forwarding arriving network packets to a process by matching the packet's IP address and port number.  Within computer science, a communications protocol is a system of digital rules for message exchange within or between computers. When messages are exchanged through a computer network, the rules system is called a network protocol.  A protocol is basically an agreed upon set of instructions/commands/calls that two networked devices can both communicate over.   Think if we didn't have agreed upon protocols and web servers just randomly sent data to web browsers that the browser did not know what to do with?   Luckily we have HTTP and which every web browser created has built into the software so it can communicate with any web server that also speaks the same language (HTTP).     The source port is ""ephemeral"" - in that is it made up at the time the packet is sent.  However, it still serves a useful purpose.  It enables both ends of the connection to keep track of separate conversations.  Consider if our user sent two simultanous web requests: Yes you can create your own protocols.  Protocols are written in a wide variety of languages.  I am not a software developer, but I am pretty sure that as long as whatever language you are using has libraries that allow you to write software that can communicate over TCP/IP (there are other protocol suites, but TCP/IP is the most widely used) you can use that language to write a protocol.   The programming language 'C' seems to be the most commonly used to write protocols.   This is due to the fact that many of the first network protocols were developed on UNIX in the 1970's and C happens to be the language that UNIX itself is written in.     The above paragraph is key to understanding why ports/protocols are used in networking.   If we did not have a way to specify the protocol, which transmits data over an agreed apon port number - you would not be able to do more than 1 thing at a time (check your email and use the web) because your computer would have no way to differentiate between data for your email client and data for the website you are browsing.    Hm. I would think that the best place to start would be to look at the IP Suite, aka the TCP/IP model. (Ignoring the other layers of the OSI model for purposes of simplicity.) It is up to the sending and receiving application to ensure the data is the right structure, and this is where application protocols come in.  HTTP is an example of an application protocol that web browsers use to communicate with web servers.  It is a well defined protocol that ensure that the browser will send requests to any web server and that webserver will understand and respond sensibly.  But what it doesn't include in its definition is anything about how packets get from A to B - that is the responsibility of the preceding layers - the transport, internet and link layers. The web service owns port 80 and the email service owns port 25 - they ""listen"" on their respective ports, which enables the traffic to end up in the right place. Are these ports physical objects? Are they something built into part of my computer? How many ports are there? Can I increase or decrease the number of ports? Are they even something physical? Or written in code? Where is this code? The operating system? What truly is a port? This differs quite a bit between OS's.   For example, to change the port number Remote Desktop Protocol runs on in Windows, you need to edit the registry.   On Linux, many of the network services can be configured directly from a .conf file for a particular network service.    The majority of TCP and/or UDP stacks implemented in OSes use a basic system of assigning ports to programs and simply raising events in these programs as packets come in over the network. However, any implementation could theoretically work; there may even by hypothetical alternative stacks pondered in academia. I understand what this all means, but only at a very artificial level. Basically, I know what a port and I understand what protocols are, but what are they really?",4
"I executed the a.out thus generated on the Rasperry Pi board and it displays the correct output. Next I did Now, I am unable to find the file libcofi_rpi.so on my host Ubuntu 12.04 machine. So I am wondering how did the hello-world program get cross compiled in the first place. If I rename the libc.so* cross compiler arm libraries on the host Ubuntu 12.04 machine, the cross compilation fails. So I know, the cross compiler is definitely looking for these libraries. How then did the cross compilation succeed without the libcofi_rpi.so library on the host Ubuntu 12.04 machine? libcofi-rpi.so is a hack to speed up certain memory operations. It's loaded through a linker configuration file rather than being loaded by the binary you are running. I successfully cross-compiled a simple hello-world.c program for Raspberry Pi Board on my Ubuntu 12.04 host machine by following the instructions given in this libcofi_rpi.so is a shared lib, and linked at runtime. As long, as you do not execute the program, it is not necessary to have it available, it is enough to have the contained methods and fields defined in an included header, which is surely a part of the crosscompilation suite. ldd output is a bit confusing in this situation. It doesn't distinguish between libs loaded by the binary and libs loaded through linker configuration.",3
"The majority of the biggest computers on the planet are large clusters of many individual nodes, each of which is often ver much a standard server. Without knowing the nature of the video ""transformation"", what software you are using, or the level of scripting/programming at your disposal, it will be impossible to comment further. See http://www.top500.org/, specifically the breakdown by architecture: http://www.top500.org/stats/list/33/archtype. In your specific case, you have many files to transform.  if each file can be transformed independently of the others, it's almost easy: distribute them among the machines and put them all to transform their own files. Given that you describe the problem as ""a large number of video files to transform"", yes, this would work on a cluster. It's possible, but there'd be a fair amount of work involved in building a cluster. It's difficult to say more without more information Can I have someone sit at each computer and do part of the problem, and will that speed things up?  In other words, can the problem be split into chunks that are basically independent? IOW, what you have to do is to create some program that runs on each of your computers, and performs part of the problem. Adobe After effects has a sort of clustering mode they call ""network render"" but it isn't an ideal solution. (last time I checked it involved rendering your video out to an imagine sequence, which would then presumably need to be recombined into a video for your use) The answer to your question depends largely on the software you are using to ""transform"" these video files, and what that entails. The master computer runs a script which looks in each of the shared folders.  If they have nothing in them then put one file to be transformed.  If they have a file named done_yourfilenamehere then move it to the done folder.  Loop until nothing is left in the master folder.   Basically all the clients should be kept as busy as possible, with each client taking what ever time they needed to transform each file. what you're ""creating"" is not ""a faster computer"", but a cluster.  which is nothing more than a group of computers used together. every so often, and, if something is in the shared folder, renames it to work_yourfilenamehere and runs your transform.  When it's done it renames it to done_yourfilenamehere.  If not, just wait for 60 secs or so.",5
"I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn't seem to be the issue. What you can also use is stochastic gradient descent to solve the optimization problem. Sklearn features SGDRegressor. You have to use loss='epsilon_insensitive' to have similar results to linear SVM. See the documentation. I would only use gradient descent as a last resort though because it implies much tweaking of the hyperparameters in order to avoid getting stuck in local minima. Use LinearSVR if you can. If you do not want to use kernels, and a linear SVM suffices, there is LinearSVR which is much faster because it uses an optimization approach ala linear regressions. You'll have to normalize your data though, in case you're not doing so already, because it applies regularization to the intercept coefficient, which is not probably what you want. It means if your data average is far from zero, it will not be able to solve it satisfactorily. Did you include scaling in your pre-processing step? I had this issue when running my SVM. My dataset is ~780,000 samples (row) with 20 features (col). My training set is ~235k samples. It turns out that I just forgot to scale my data! If this is the case, try adding this bit to your code: Try normalising the data to [-1,1]. I faced a similar problem and upon normalisation everything worked fine. You can normalise data easily using : I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized. I do not have anything to add that has not been said here. I just want to post a link the sklearn page about SVC which clarifies what is going on:",4
"WiFi Direct only works on a few very limited phone models (Galaxy S III etc). And Ad Hoc mode support is even more limited and seems to only work on a rooted phone.  Option - 2 Make use of yowsup module and give Raspberry a whatsapp interface. Make the script to run for ever (as long as Raspberry is alive.). Now based on your request message on your whatsapp call the endpoints which you have already written. any idea how to achieve my requirements. if connecting raspberry pi with android by USB to microUSB cable will solve the purpose , please let me know how.  Option - 1 Make use of Kivy Python to create an android/iOS app. You can make use of any language you like  But it's not correct that ""its not possible to have wireless network or carry wireless router where ever the pi go.""  You can do that by making Pi itself an AP. Here is a tuturial of how to implement it. i want to connect my android mobile with raspberry pi so that i can send some data from pi to mobile through UDP sockets and vice versa. This is because i want to use android screen as remote control , some rPI data display and live video view of pi camera output from android web browser. i am interested to use wifi connection but without using any access point/wireless router in between i.e i want to create a peer to peer wifi adhoc network. in home with access network this will work , but its not possible to have wireless network or carry wireless router where ever the pi go. for prototyping purpose i tried creating Adhoc wifi network in windows vista but my mobile didnt detect the wifi possibly because the frequency of the wifi network cant be recognised by android. i tried wifi direct feature of my galaxy Note2 (android jelly bean) but i seems to work with only samsung specific phones. it didnt work with nexus 4 neither with windows vista.",3
"With those extra pieces of information you will be able to evaluate more objectively which query is better, and not relying exclusively on EXPLAIN, which only provides limited pre-execution information. The other thing that should bring your attention is the Using temporary; Using filesort. Filtering is not the only thing where you should focus, as these extra pieces of information are telling you that a large sorting has to be done using a temporary table (that may or may not end up on disk, but at least has to be materialized). That is another indicator of potential bad performance, that in some cases can be avoided with the right indexes. I will not tell you which is the right query to use (partially, because I do not know all the variables: indexes, tables structure, etc., and in most cases it will depend on the particular hardware/resources available), but I will tell you the tools to decide: In particular, on your first query, you are hitting a well know MySQL bug? limitation?  in which an IN subquery is identified as a DEPENDENT SUBQUERY, even if it really isn't, forcing the outmost query to be executed without an index (full table scan) in order to test all possible values of the first table. That is usually an indicator that it is a bad query. It seems not to bee too bad in this case, as the table is small, but it is usually an indication of bad performance.",1
"If you cannot find the option then Office 2007 does support the ability to create protected PDF documents.  While the PDF standard is open, certain features, are only supported by certain editors. Creating protected PDF documents is one of the features, that only certain editors support, which means programs like Foxit will not respect the fact the document is protected. Since Office 2007 does not have the option you would need to purchase PDF authoring software that does support it.  However, I will again point out that, programs like Foxit will not respect the fact the document is protected. I am currently using Microsoft Office 2007 version. It has all the updates released so far. There is an option to save documents as pdf but I cannot find an option to protect them from copying its content or editing. In other words to make them read-only. I attempted to use Foxit Reader but it does not seem to have this option neither. There is a way to protect documents with MS Word own protection but that's not exactly what I am looking for. Does it exist a way for this which doesn't involve the necesity to purchase an additional software for that matter?",2
"I worked for a guy that ran Windows NT 4.0 for over 2 years without a reboot.  Personally, I let Windows Update accumulate 'enough' patches, and then I go ahead and install.  Probably go about 2 months. Personally I'm not a fan of rebooting servers (except my terminal servers) unless there's a need for it. The various servers I am in charge of there are 3 that I don't reboot on a quasi-schedule DCs 1&2 and our Exchange server. Everything else can pretty much go up or down after 5pm but before 10pm.  We had an internal web server (Windows 2003 & IIS) online for 600 days before we rebooted it. Of course, 600 days without a reboot also pretty much means 600 days without any patching. And that's not such a smart idea. Even if a server has been running for a year, would it just add an unneccesary toll on the components? Back in the bad old days of NT 3.5 and Novell it was usually a wise idea to reboot regularly, but in this day and age it's not so important. I do it mostly because of windows' patches and cleaning out the crusty stuff. Our Citrix ICA server, has the most set schedule of reboots. I was just visiting a server at the datacenter and notied that it has been up(without a reboot) since last October (11 months).  It is a VMHost running Windows Server 2003.",5
"5) Your repeater will have first to receive the data, and then send it again on the same channel, halving available bandwidth. 2) Your repeater/router uses the signal received on both antennas to decode the WLAN signal (details depend on the router and protocol). If you asymmetrically boost one antenna, reception may actually get worse, not better. 4) Besides signal strength, there are a lot of other factors that influence reception. One very important one is the presence of other Wifi senders on the same channel (note that depending on the protocol, multiple channels are used). In addition, there's the hidden station problem, which becomes worse with increased range, and use of repeaters. 6) So if you can, the best solution is to use LAN. Either attach the IP camera directly to LAN, or place an access point using a different non-overlapping channel range close to the IP camera (e.g., if the camera is outside), and connect it up via LAN to your main router. 1) By exchanging the antenna, you may exceed the legal maximum transmission power in your country (an additional 6 dBi = four times the wattage). Fines and chances of getting caught depend on your country.",1
"Since I don't see it yet, I might as well throw in Microsoft Office Picture Manager. If you have MS Office then most likely, you already have this installed. I'm not even sure if it requires MS office to be installed in the first place. You may want to try Batch Picture Resize, which is very easy to use, and free. I used it many times in the past. You can either choose your option by the menu, or renaming the file to set the resolution. If you want to resize only one file, of course, Paint can do it. Just take a look in the menu of Paint. If you are using Windows 7 (question does not specify) then Windows Paint is adequate for this task.  If you are using Windows XP then avoid Windows Paint completely as it is not very good with JPG files. I usually use one of the image resizing shell extentions - microsoft has a powertoy for windows xp that does it ( download link )and there's options for newer versions of windows as well. Almost no UI cruft, and almost instant in my experience. A free, open source and specially written for Windows is http://imageresizer.codeplex.com/ It can do batch resize, too. If you can't locate it easily, I believe you can simply open up the Run... prompt (Start > Run OR Windows Key + R) and type in OIS and press OK Microsoft's own Image Resizer PowerToy adds simple right click options to Windows Explorer (link doesn't work in Chrome, open this one in IE)",5
"I found it quite convenient to installed different version in separate locations and just symlink to the version you want to use, like: this will slightly standardize your setups and ease deployment on many servers. also you'll not need gcc on production machines [ which many will consider as security benefit ]. If you're going to be installing this on a single machine, then doing it from source every time is problem the best way. If you are going to be installing this on several machines and you want to make sure it's consistent, it's probably worth learning how to make Debian packages. You could probably use the packaging in Ubuntu as a base. There isn't a great way. The reason that effective package management was created was to solve this very problem. Upgrading and uninstalling source-compiled things is hard.  Of course you still have to re-apply any configuration changes you've done to the previous version, but for that you can use some versioning system (RCS/SVN/GIT) or configuration management tool like Bcfg2. i'm afraid this is the only way. if you have more servers to maintain - consider having separate test environment where you compile and possibly package result of your compilation.  If this is a one-off case, then re-compiling from source is probably your best bet. If it's on an array of machines, it's definitely time to move to the supported package management.",4
"We did not need that service, so our solution was to remove the Work Folders feature from ""Turn Windows Features on and Off"" > ""File and Storage Services"" > File and iSCSI Services"" > ""Work Folders"". After that, we were no longer prompted for Digest authentication. I am new to Windows Server 2012 and I don't have much of a clue where to go from here. Any pointers in the right direction will be most helpful.  It is a fault by Windows Server 2012 R2. You have to uninstall the workfolder feature. It took me several nights. I recently got TWO IIS servers running perfectly smooth. I was able to access these from the outside with no problem, however as of a few days ago, out of no where.. when I try to access IIS Server A from the outside, I get a login prompt for DIGEST.  I had a similar problem. I did a grep search for the word ""digest"" in any ""*.config"" file on the server. I finally figured out that my C:\Windows\System32\SyncShareSvc.config had digest turned on. This is related to the Windows Sync Share service and the Work Folders feature. Look in your IIS W3SVC logs and trace back to the first time it did that and look to see what changed on the server.  Odds are something changed something.  Don't rule out that someone else changed it without your permission.. Digest is not installed under IIS in the Add Roles and Features"" tool. The other server still works just fine. I have not done any updates since I booted them so I don't know why this is happening now :( I had a similar problem. I did a grep search for the word ""digest"" in any ""*.config"" file on the server. I finally figured out that my C:\Windows\System32\SyncShareSvc.config had digest turned on. This is related to the Windows Sync Share service and the  Work Folders feature.",5
"<input class=""btn btn-primary log-in"" id=""log-in"" type=""submit"" value=""Log in""> Can you let me know if there is anything I can do to make my code more scalable and more toward production standard? First time posting in Code Review! I have started learning front-end development to complement my UI design skillset and am starting with CSS Grid and bootstrap.                                                 <input type=""email"" class=""form-control"" id=""inputEmail3"" placeholder=""Email"">                                                 <input type=""password"" class=""form-control password"" id=""inputPassword3"" placeholder=""Password"">     <link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"" integrity=""sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"" crossorigin=""anonymous"">                                 <h3 class=""welcome"">Welcome back <span class=""cust-name"">Alan!</span></h3>     Have a look at the snippets. I think the 2 work quite well, using CSS Grid for layout and the Bootstrap library for things like buttons, forms and navs etc.",1
"I'd like add something about rapidly-exploring randomized trees, or RRTs. The basic idea has good discussion all over the internet, but it's probably safe to start with the links off the Wikipedia page and with Kuffner and LaValle's original papers on the topic. Ittai Abraham, Shiri Chechik, Cyril Gavoille: Fully dynamic approximate distance oracles for planar graphs via forbidden-set distance labels. STOC 2012: 1199-1218 Jittat Fakcharoenphol, Satish Rao: Planar graphs, negative weight edges, shortest paths, and near linear time. J. Comput. Syst. Sci. 72(5): 868-889 (2006) The main idea is to use an incremental algorithm, that is able to take advantage of the previous calculations when the initial calculated route gets blocked. This is often investigated in the context of robots, navigation and planning. Koenig & Likkachev, Fast replanning for Navigation in Unknown Terrain, IEEE Transactions on Robotics, Vol. 21, No. 3, June 2005 introduces D* Lite. It seems safe to say that D* is outdated in a sense that D* Lite is always as fast as D*. In addition, D* is complex, hard to understand, analyze and extend. Figure 9 gives the pseudocode for D* Lite, and Table 1 shows experimental results with D* Lite compared to BFS, Backward A*, Forward A*, DynamicSWSF-P and D*. I do not know the newer algorithms you list (Anytime D*, Field D*, LEARCH). Very recently I saw a robot that used D* Lite for planning in an environment with random walkers in it. In this sense, I don't think D* Lite is outdated by no means. For your practical problem, I guess there's no harm in trying the usual engineering way: take some approach, and if it doesn't fit your needs, try something else (more complex).  Data structures called distance oracles handle such problems. However, most research results are for static graphs only. Philip N. Klein, Sairam Subramanian: A Fully Dynamic Approximation Scheme for Shortest Paths in Planar Graphs. Algorithmica 22(3): 235-249 (1998) The most important feature of RRTs is that they can deal with real-valued spaces of extremely high dimension without choking. They can handle dynamics, are not optimal but are probabilistically complete (guaranteed to succeed if possible as computation time goes to infinity), and are capable of handling moving targets. There are some extensions that let them work in non-static spaces, the best of which looks to be the Multipartite RRT work that I've linked below. If the graphs are grids (and thus planar), some dynamic data structures exist (unclear whether the constants are small enough for the application in question):",3
"I figure out this myself accidentally and hope it help others. The problem arise from the ROM itself. So no matter what you do to connect the phone it will disconnect itself and making impossible for you to flash.  Need help with this issue. After installing this driver and ready to flash phone the connection of usb keep disconnecting thus preventing me from flashing. I'm using latest windows 10 anniversary update. Help please! I search everywhere including forum and all for days but no avail. Lots of suggestion but no solution. So its really easy solution now I'm enjoying my Lenovo S920 with original firmware just like newly bought android. Remember this is not a complete guide and only as reference. What you need to do is simple but dangerous. Instead of check ""Download Only"" option in SP Flash Tools Check ""Download Only + Flash"". What you doing here is you brick the phone in order to flash it. Sound crazy right! But this option will wipe out all error in rom so you will be able to flash it later. Now the phone is brick :(, when you put the battery back the phone wont even start to boot. Just blank screen. You also cannot charge the phone. Remove the battery and try connect it again this time check ""Download Only"" in SP Flash Tools. This will install back fresh ROM and also flash the phone. Wait the flashing phone. Disconnect and voil!  Congratulations!!! You just flash your phone :). Pat at your own back cause this is also a method to revive the dead/brick android phone.",1
"You should be using Interval Partitioning (11g+) to automagically create your partitions as you need them. We have same type of table with billions records in our Project. So we do followings actions in our tables for have performance and fastest access to data : AFAIK, the creation of the index needs to be done only once, and then daily, the index on the new partition only needs to be created. Besides that, indexes on partitions older than 5 days will never be used anymore, so there's nothing to do there. We need to do selects on this table, over a period of the last five days, and including the accountid. In order to keep things a bit performant, I suggested to partition the table on day level (done), and create a local index (prefixed or non-prefixed) on accountid. Now, one of the teammembers says it will not be possible, since they need to create an index on only the last five partitions of the table, and that would require too much effort. We have a big transactional table, partitioned on transaction date (day). Each day, several million records are added.",3
"The USB 2.0 interface can be a limit due to signalling and command overhead as well as spacing between packets. I have a fast SSD connected by USB 2.0.  The drive is much faster than the interface (by more a factor of 10). I'm getting only 30MB/sec between my computer and a USB drive, despite the fact that USB 2.0 supports 480Mb/sec (or 60MB/sec) transfers. (Therefore, I'm only getting half the rated speed) Is there something present in the USB standard which should result in such half apparent speeds? Read Speed maxes out around 33 MB/s and Write Speed at 17.5 MB/s.  Write Speeds are almost 50% slower due to a verify-read after the write and the fact that the USB signal is half-duplex as another answer mentions. USB 2.0 supports 480 Mb/s signaling speed. On the Wikipedia page, it says effective throughput is up to 35 MB/s. There is a disparity because bits aren't usually transmitted between devices in the same way that they are represented internally. A number of factors needs to be accounted for when transmitting data between devices, like electromagnetic interference.  This post is a bit old, so not sure if this is still relevant or helpful, but USB 2 speeds normally max out at 280Mbps (35MBps) due to bus access.",4
"However, try as I might, the AWS interface switches the order when I click ""SAVE"" so that I always end up with I have a single VPC in Amazon Web Services with the subnet 172.31.0.0/16. I have created an EC2 instance in this subnet and given it a public Elastic IP. There is an Internet Gateway on this VPC. So, my route table looks like this: Note: The external service will allow me to add a single IP address that it will allow access. If I only had a single EC2 instance, I could simply give them the EC2 instance's Elastic IP address. But, I want to add several more EC2 instances set up the same way. Hence, the NAT gateway. Also, I cannot simply dispense with the Internet Gateway and use only the NAT gateway as I need services on the EC2 instance to be accessible by the outside world. In order to get around some IP access issues on an external service I do not control, I added a NAT gateway to this VPC so that all traffic to the single external address A.B.C.D would route through the NAT gateway. That is, I want the route table to look like this: This route table seems silly: the NAT gateway would never get used and my traffic to A.B.C.D still appears to be coming from the EC2 instance's Elastic IP.",1
"Without listing the make and model of your router, no answer can be 100% accurate, as they might be using some non-standard device. However, that tech support person sounds like he has no idea what he is talking about or is lying for some reason.  The entire purpose of a router is to transmit data from one location to another efficiently.  If a device is off, a router will not send data to it, or expect to receive from it.  Even if the device is on, unless there is an active connection going to it, no data will be transmitted to/from the device.  A router can have any number of devices connected to it and the speed will not be reduced, unless there are multiple active connections running. They say bandwidth Is split to all connected devices whether or not they are actually using data or not. Is there any truth to this? If so would adding a router between the connected devices and the Hughesnet router reduce the draw down? There would only be one device connected not many.  I have been told by Hughesnet tech support that having multiple devices connected to their router adversely affects internet throughput to each device even if the devices are not turn on. (For example an audio receiver shows up as connected to the router).",2
"In short, it takes a single cer and applies it to the local machine. If the user is not admin, it prompts them to restart the script as admin. I intend to scale this out in a foreach fashion so that when a cer is added to a particular network directory, the script will notice on launch and attempt to install the new cert. If user is not admin, it prompts ONLY when it needs to install a new cert. The easiest way to pull a thumbprint from a .cer file would be to create an certificate object in powershell and import the cert in that. I could just install every cert in the directory every time my script is ran, but I fear unnecessary overhead, and the show stopper is that it would require admin rights each time it is ran, rather than on first launch or detection of new certs. There is a little more logic in the middle, but overall that is what I seek to accomplish. My primary problem is pulling the thumbprint from the remote .cer files, or determining if they are installed locally in a different fashion (to my knowledge the thumbprint is the most logical way, but I am totally open to suggestions). The primary problem is that I am unable to gather the thumbprint from the .cer file, and Google suggestions are fruitless thus far. If I can figure out how to pull the thumbprint from the remote .cer, I am fairly certain I could figure out the foreach logic necessary from that point.",2
"I have setup a VM with Windows Server 2008 x64 R2 (Server Core). This obviously leaves out the GUI but only gives me a command prompt. http://www.petri.co.il/installing-remote-server-administration-tools-rsat-windows-vista.htm - This link provides details on an update for Vista to do configuration of the server from Vista (my host OS) but I am not on any of the supported versions. MS documentation for Server 2008 Core has instructions on configuring AD, DNS, DHCP, etc via the command line.  This can be done via local or RDP session. If you are on Vista Home then you are probably going to run into other problems because Home can't join a domain, but RDP can be used from a Home computer.  This isn't a GUI but will allow remote connection to the cmd line. Which is supposed to list the services, the CMD says this is not recognized as an internal or external command. Also, I have the original Core Configurator but as it was pulled from the web I don't think redistributing it would be advisable - but it rocks for setting up the basics including firewall rules and of course dcpromo without the need to prepare an unattend file manually.",3
"If you are set as to what the server will do and don't plan on changing it then you can run the Security Configuration Wizard (assuming sp1 already installed or higher).  This is a great tool (carefully used) to lock down the server to just the services needed (in addition to other security measures if necessary like IPSEC). If you don't have a wireless network card on your server (and I bet you don't have one), that's a service you can safely disable. Generally the defaults are optimized well enough as far as cpu and memory utilization goes, performance gains are reached by optimizing disk configuration depending on what the role of this server will be. For even more info on locking down a W2k3 server see here: http://www.cites.illinois.edu/security/by_os/win2k3srv.html Windows Server 2003 is pretty much secured and tightened on its own; one thing that puzzles me a lot is the Wireless Auto Configuration Service, enabled and starting automatically... wireless servers are quite unusual, I think.",3
"So, what happens is that when I'm just using the computer (blogging, photo editing, playing a game, doesn't seem to matter what I'm doing) it will shut down, as if the power just went out. I still hear things working in the case, either fans or the PSU, after the shutdown. When I press the power button on the front of my case, nothing happens. If I hold it down for 5+ seconds, nothing happens. I have to flip the power switch in back off and on, then press the front button. Sometimes my USB keyboard won't be recognized during boot and I have to unplug it and plug it back in. Now, when I installed Windows 7, there were some issues. I was installing to what was originally Drive F, which then became Drive C with the new install. Some boot files ended up on Drive D (Formerly C) and had to be copied over or I had weird boot errors. If there are any missing files that might cause what's happening, please let me know. But I ran SFC and it brought up no problems. First, purchase fresh thermal grease (there are many brands, do a bit of research to find the kind you want or need.)  Thermal grease is cheap, and a tiny tube will last a long time.  It's cheap insurance. I recently encountered a similar problem with a brand new Dell 8100. After days of trying to sort it I stumbled across an obscure post on a forum suggesting that I disable the hybrid sleep setting on my power plan. Done it and the machine has been stable since. I'd suggest you try that: (If the metal surface of the heat sink is damaged or the mounting connectors or hardware are damaged, strongly consider buying a new CPU cooler.  Be sure to buy one rated to dissipate the amount of power your CPU draws - an 80W CPU chip needs a cooler rated to dissipate at least 80W.) Clean any dust and dirt from the heat sink fins and from the heat sink fan.  Compressed air is good, a old toothbrush works, pipe cleaners, anything to get it clean. I'm running Windows 7 64bit and having some very strange errors. I have new RAM and a new Cooler Master PSU; I've ruled out the RAM already since it did the same thing with another new stick. I don't have a different PSU to test, sadly. Apply a very thin and even coat of thermal grease to the CPU chip.  There are really good online videos on how to properly apply thermal grease.  A good coat is so thin as to be almost see-through. Once it's back on, and secured so there is no wobble at all, be sure to reconnect the fan's power supply wire (they're often connected to a pin header marked CPU_FAN on the motherboard.) The surface of the CPU is very flat silicon and won't need attention, but make sure nothing is sticking up above the top of it.  The surface of the heat sink also needs to be flat, but because it's soft aluminum or copper it might be gouged or dinged through rough handling.  Make sure it's undamaged.  Be careful if you think you can use sandpaper or other abrasives to correct some damage, as it's very difficult to get the surface as flat as it needs to be without proper knowledge and tools. Some machines will automatically vary the power plan so you may wish to disable to this on each of your machine power plans. Hope this helps! Look at the existing grease that is on the CPU and bottom of the fan's heat sink.  If it's crusty and hard, or gobbed on any thicker than a sheet of tracing paper, that's also a strong clue that you're on the right track.  The adhesive stick-on grease pads that fan makers used to use are horrible conductors of heat, and if you find one that might be the problem. When this has happened to me and to family members, (and it's happened a few times) it's shut itself down due to overheating of the CPU.  Improper use of thermal grease between the CPU and the CPU fan would be one guess; a misaligned CPU fan (where it's not securely fastened evenly to the CPU) would by my other.  Either way, the fix is the same.  Replace the thermal grease.   Clean off the exposed surface of the CPU chip and of the bottom of the fan's heat sink.  I use dry paper towels to first mechanically remove as much old grease as possible, and then clean both surfaces with a paper towel lightly dampened with 70% isopropyl alcohol.  Clean them to a bright finish, don't allow any old thermal paste to remain, and don't dump the liquid alcohol directly on the CPU.  (If you still get the motherboard damp anyway, a blow dryer set to a cool setting for a few minutes will evaporate what's left.  Be careful not to start a fire, alcohol is extremely flammable and many things on the motherboard will burn!) Control Panel > Type 'Power Options' > Choose ""Change Plan Settings"" for whichever plan you are currently using > Change Advanced Power Settings > Sleep > Allow Hybrid Sleep - set this to OFF. The only error that seems to show up in event logs is a kernel-power one letting me know that the computer shut down unexpectedly, which I already knew.  Next, take the CPU fan off the CPU, and as you do, take note of how the fan is mounted and how the fan mounting hardware works.  Also check to see if the fan is wobbly before you unscrew anything, that's a strong clue you're on the right track. Remount the heat sink and fan.  When mounting the heat sink, be sure each of the four corners is equally tightened by the connectors.  A stripped screw, broken plastic expander, damaged socket, bent piece of metal, anything like that can prevent even pressure from being equally applied to each of the corners.  If the pressure is uneven, there will be less contact between the CPU and heat sink on that side, and that gap will cause the CPU to overheat.  A proper mounting will feel solid.",3
"There is an amazing technique available for finding out impact of a different features on the model, it is called Permutation Importance. Read about PCA(""Principal Component Analysis"") and while implementing must standardize data on similar scale. Do so for each categorical variable.  Sometimes it will be better to assign, say, only 3 major responses plus ""other"". Then do one-hot-encoding, (=categorical to numerical using dummy variables) then do simple KMeans clustering and interpret the resulting clusters yourself for plausibility. I'm not an expert at clustering methods at all. Is there a way to known way of estimating the importance of a feature (or combination of features) in deciding which cluster a user falls into?  I have entirely categorical data (survey results from users), so I've used k-modes clustering to better understand my users.  Apart from this, if you want to learn how to manually analyse K-Means clustering algorithm please read this paper. If you have only free-text  responses in your survey, or lots of NAs, you have to do even more preprocessing first. Compute which feature value has the highest probability of predicting a particular cluster. It's a straightforward application of Bayes' formula.",5
"However, my database size is 1.3 TB. Thus, a mysqldump on Slave A takes several hours to complete, and even longer to load into my new empty Slave B before beginning replication. In the best case scenario, it takes ~36 hours from starting the dump to when replication can start. Master and Slave A are both in an MSP environment, while Slave B will be an RDS instance. I can connect to Slave A in the MSP just fine. Master binlog files are being copied over to the instance running Slave A. All binlog files are being kept around for a long time.  Is my assumption correct, that using --flush-logs reduces the likelihood of duplication errors arising after replication has begun? I would like to use --flush-logs, as described in the third code snippet here, to reduce the chances of having to use CALL mysql.rds_skip_repl_error; repeatedly after I begin replication. This is especially important because there is currently no way to accomplish the equivalent of slave-skip-errors = 1062 in RDS, and also because I'm unsure what the effect is on data consistency. If it is correct, what would the effect be of using mysqldump --single-transaction --flush-logs on Slave A? Would it behave as a tunnel and actually flush the logs on Master, or would it only affect the logs present on Slave A?",1
"It depends on your Window Firewall settings and on the limitations imposed by your ISP. If you allow RDP over you Internet IP at home, and the ISP doesn't block that specific port, than you can RDP into your computer from wherever you are in the World. However, note that RDP is not encrypted and Windows passwords are relatively easily hacked, so it's not really recommended you do that. Should I use a VPN network setup and make a RDP connection then? Or is it possible to use RDP by typing in my ip-address without VPN? It's free, it's secure (2 levels of PW needed, certificate-based security), it doesn't require any special software to be installed on your work PC (browser based, and can run in Java/HTML mode in extreme cases), it will be faster than RDP, and best of all it ignores firewalls.  The host service on your home PC will maintain an active connection to their server, which is how you can tunnel in.  Even if you are behind 4 levels of routers, there is no forwarding ports to fool with. So you can use RDP without setting up VPN-related stuff and it will be as safe as any other SSL protected protocol (HTTPS, VPN, etc). I STRONGLY recommend using a VPN. You have no idea how the RDP traffic may or may not be encrypted, and so your credentials may be intercepted, as well as your keystrokes later. Any access to your own resource should be using either a full VPN, or an SSH-based connection.",5
"Try OpenVPN.  I've used it going from OS X to Linux, and they say the server works great on FreeBSD. You specify the Mode_CFG parameters in a separate section of the racoon.conf file. The setup is fairly straight forward on FreeBSD, if you understand the basics of IPSEC. OpenVPN is also a possibility, although I've never used it on Mac OS X and so can't comment on ease of set-up. A quick web search reveals a few GUI clients, so that might be worth investigating. To perform client configuration (mode_cfg) I use racoon as the ISAKMP daemon on FreeBSD.  This is part of the IPSEC-TOOLS package.  It is available in the FreeBSD ports tree as  And you should be able to make it all work...  I have found FreeBSD to pretty stable as a VPN RAS server, and it is quite quick, compared to the smaller Cisco PIX firewalls (501/506/e) Having set up IPSec in a simple tunnel mode between two hosts once, I swore I would try and stay away forever. In terms of alternatives to IPSec/IPSecuritas, you might want to investigate setting up a PPTP VPN, as Mac OS X (and other major operating systems) contain a PPTP client built-in. There are two good options for PPTP servers on FreeBSD: I mostly use either IPsecuritas and pfSense. Or work via SSH Portforwarding. Look for the ""-L"" option. This is a fairly straight forward config on FreeBSD, if I understand you correctly.  You want to create an IPSec (presuming tunnel mode) connection between your Apple and a FreeBSD machine, and because you have a dynamic IP for the Apple, you need to be able to authenticate the tunnel some how.  I would presume you are using a PreShared key? I've used OpenVPN and OpenVPN-AS extensively on FreeBSD gateways and a variety of clients. AS is a commercial option, but the client is better with more comprehensive support than that of the standard OpenVPN client. Setting it up really is a doddle with the instructions.",5
"It's also worth pointing out that AWS is very much oriented towards separating which can be managed efficiently with large economies of scale from what is unique to your application and delegating what they like to call ""undifferentiated heavy lifting"" to managed AWS services. For example it's common on AWS to use either SQS or a Redis based MQ on ElastiCache, which both support a HA architecture out of the box (EC via Multi-AZ). Likewise you could delegate your LB to ELB, freeing you to focus on optimising your application and possibly DB servers (if RDS doesn't suit your needs, I'm not sure what the Postgres situation is) servers. Ideally your MQ should be located on a separate instance(s) so it can be scaled independently of either your application servers or load balancer. This is also makes maintenance easer (you can swap out your load balancer instance without having to migrate your existing queue for example) and it's better for security to have them isolated (your LB needs public access whereas your MQ doesn't).",1
"What about data? According to this page, the TSA can take a copy of all your data for any reason without suspicion of wrongdoing when crossing the US border. The data may be shared with other agencies. Mobile 4-post racks, yes.  Expensive, but if you do a lot of travel, it might be worth it.  And you can certainly mount a 1, 2, 3, or 4u box.  If you're willing to pay the price for this, be sure to include a UPS unit in it, there are several to be had that are rackmount-ready.  Do this because you don't know the power situation ahead of time. Unless you cook up a homebrew setup of some kind (dolly cart, etc.) you're probably going to have a hard time.  I certainly wouldn't put a server though luggage, that's a sure-fire way to end up with a box of broken contents.  Maybe look into packing a powerful, high-end PC class board into a portable LAN party case that doesn't have blazing dragons and etched tattoos on it? If you are shipping servers around, are you carrying commercially sensitive data that could harm your business if it got into the hands of your competitors? I suspect that it may be cheaper to buy a ATA-approved shipping container and send the computer via UPS/FEDEX.  I've sent computers overseas this way before.  I think we got our shipping container from Time Motion Tools -- or one of the other tool vendors.  Try Google with ""computer shipping containers."" Pack a regular bag if you can, and unzip it for them to look around.  If it looks suspicious, then it is. Try to act like a terrorist.  Then they'll know you're not really a terrorist, because no real terrorist would actually act like a terrorist.  Real terrorists would act like they are not terrorists, and the TSA knows this, so if you are trying to act like you are not a terrorist they will naturally assume you are one. :) Does it look like a bomb?  Is it clearly accessable (openable)?  Can it hide a bomb?  I've seen CPAP machines receive swabs for explosive chemical tests because the TSA can't open it up to look in it. NOTE: After every trip and before you power anything on, open up the servers and ensure that nothing has wiggled loose.  I don't say this from experience but from paranoia.  There may be some big bumps on the ride.",5
"I'm not sure it's ""the best of both worlds"", but the latest beta of Sublime Text 2 has a very impressive ""vintage"" mode that covers a lot of useful Vim features (motions, text-objects) and maps other things (/ or ? for example) to its native features. I find its overall design a bit irritating so I don't see myself switching to it anytime soon but, again, the vintage mode is impressive. You can use VisualStudio Code and it has several plugins that add many of Vim's functionalities alongside with many goodies. into your vimrc then it will be gone. on the bonus if your using windows lower than 10 your gvim resized better ( than default windows cmd ) and has better color scheme support and looks good. Vim is intended to be used without GUI. But if you are concerned about the menu bar and tool bar in gvim that looks out of place. You can disable it so, i doubt there will be any gui-version of vim that matches your expectations of what a decent ""graphical"" editor should look like or should behave. vim is designed for using the keyboard efficiently, not for using the mouse. thus, i find your complaint about a bad ""gui"" a bit awkward. if you do not like vim or if you do not want to learn how things are done in vim, thats just fine. but then you have to use another editor because vim just does not match your requirements. Perhaps something like Oni is what you're looking for. I've not personally used it, but it's based on neovim, so you shouldn't get a half baked feature set.",5
"This example reinforces Daniel's point (which I agree with) that it would be surprising to find a problem that is W-hard by vertex cover, without having some additional input given besides the graph. The catch with our reduction is that we need to produce an edge-weighted graph, i.e. we need the edges to have different lengths. We can replace weighted edges with paths, but then this only proves W-hardness when the problem is parameterized by feedback vertex set. Indeed, if all edges have uniform weights then the problem is FPT parameterized by vertex cover.  $(k,r)$-center is another (arguably natural) problem that is $W[1]$-hard parameterized by vertex cover. (See a recent preprint by Katsikarelis, me, and Paschos here - sorry about the self-promotion!). The problem here is to select $k$ vertices (centers) so that all other vertices are at distance at most $r$ from the closest center. This generalizes $k$-Dominating Set (which corresponds to $r=1$). More strongly, the problem is $W[1]$-hard parameterized by vertex cover and $k$.",1
"With the above you are giving the user the option to define these later or now with no penalty either way. Just keep the following simple philosophy of Python in mind: I've written a Time class that records the time of day and performs simple timing operations (add 2 times, convert a time object to an integer and back again,etc.) following the prompts in How to Think Like a Computer Scientist: Learning with Python.  In your add_time method, I would suggest constructing a new Time object using the values of the self object and then returning that with the incrementation. And, in general, import statements occur at the top of the Python module, unless it is a really special case. Overall, everything looks pretty good. I hope this was somewhat helpful, if you have any questions be sure to comment. Is including a method to name instances bad form, as I suspect? Would it be better write a functional-style version of add_time without importing deepcopy? I don't really see a point in naming instances. Do you have a rationale for that? Also, if you choose to name them, I think that returning said name after setting is an unexpected behavior and gives the name function more responsibility than it needs. It is perfectly fine to implement an __init__ method in this case. I think the only thing you should note is, by the way it's defined, the Time class forces the programmer to give values for hours, minutes and seconds to define a Time object. So, with that constraint in mind, it's really up to you as to whether this is an advantage or disadvantage. Do you want to force the programmer (most likely yourself) to enter these values here? Or should you allow him to first construct an object and then define them later? This is your decision; I don't think Pythoneers will try to sway you one way or another. Alternatives are (1)  as you've already implied: removal or (2) giving these variables default values. The Time Class example in the text I'm following does not use an init method, but passes straight into creating a time object and assigning attributes. Is there any advantage to including an init function, as I have done? Removing the init method seems to make it easier to adopt a terser and more functional style.",2
"I think the answer lies in the MAC address that would get stamped on these internet bound packets.  If it's the MAC of the router, I gather the switch will send the data directly to the router.  However, if it's the MAC of the game server, the switch will retransmit, and only then will the router will figure things out. From what I read, these broadcasts could put a burden on all of the devices plugged into the switch if they happen too often, but I'm wondering if this will be a problem in my future setup. If you are reading about switches and their MAC ""stamping"", you should also read how Address Resolution Protocol works in detail. When your computer needs to communicate with gameserver, it begins with the IP address. It asks ANY device on your network WHO HAS THIS IP? (possibly only once). The router should answer, because gaming server's IP is from outside of your network. If i remember correctly, answer should be ""hey, i don't know if this IP is out there, but i will try to send it"". Then your computer stamps packets for gameserver with router's MAC. And will save this answer for future use. For example, let's say I'm playing a game on one of the devices plugged into the switch and it needs to communicate with the internet.  Will these packets be retransmitted? Your setup is nothing uncommon. In fact, I have the same setup with Linksys gigabit switch and linksys gigabit router / wireless AP. Go get the switch. Your network is not / will not be the scale, at which you should be worried about it. I currently have a single wireless router near my TV, with a bunch of devices plugged into it.  In my current setup, these devices are all somewhere near the router. I'm about to move and in my new home, I'd like to hide the non-HTPC related devices away as there won't be as much space around the TV.  Through superuser and serverfault, I learned a lot about network switches.  My plan is to put these other devices on a switch with a single ethernet cable running to the existing router.  The router would, in turn be plugged into my cable modem and I'd be in business. One of the things I learned about these switches is that they're able to send data from one device directly to another by comparing the destination MAC address against a list of observed MACs corresponding to devices plugged into each port.  So if a packet is destined to the MAC address of the device plugged into port 2, the switch will send the data directly to port 2.  But, where a packet is destined for an unknown MAC address, the switch will broadcast it to all ports and leave it to the devices to either drop the packets or accept them.",2
"As a home user you can learn from my advice and learn better ways to do things or you can choose to be blinded by the ""smell"" of cats, your choice... but know that if you openly use this practice you will be called on this practice all the time and you will silently have to admit they are right and you are stubborn because it's true. :-) Now I can sit here and debate things like code smell or readability all day with anyone, but at the end of the day it is a matter of write or wrong and any time you use resources on a system and gain nothing for it... it is wrong. If you will never ever try to instruct others with your examples or share your code, then by all means please use it at your own leisure. For all who say cat is acceptable to use because it ""smells"" better or ""is more readable"" I would only say this:  As far as efficiency goes, if you are executing a pipeline from the command line, it takes less time for the machine to execute cat somefile | than it does for you to think about whether it might be more efficient to use < somefile. It just doesn't matter. I will also add this comment, as a long time Linux user and Admin/Engineer... (and there are many of us) it makes our eyes bleed to see this. Why? Because it uses resources on systems that we control resources tightly on. The cat command and the pipe itself use extra memory and file handles that are completely useless. You have tied up resources that my system needs free and you have gained NOTHING that can explain the usage of these resources. This is a huge no no. I think the position being taken by some of those commenting on something being a UUOC is that if one really understands Unix and shell syntax, one would not use cat in that context. It's seen as like using poor grammar: I can write a sentence using poor grammar and still get my point across, but I also demonstrate my poor understanding of the language and by extension, my poor education. So saying that something is a UUOC is another way of saying someone doesn't understand what they're doing.",2
"Your computer is connected to a proxy server ( Local IP: 10.10.80.12). This proxy server has two IP addresses(Local & External). This proxy server is connected to Internet. It's external IP is 212.25.28.65 which is given by the ISP. Your computer's IP address is 10.10.80.111.  Your computer has one IP address 212.25.28.65 which is given by the ISP. Your friend's computer IP address is 121.15.27.65 which is given by the ISP.   Router1 is connected to Router2. It means LAN1 (Network 1) is connected to LAN2 (Network 2) via Router1 and Router2.  Your friend's computer is connected to another proxy server ( Local IP: 10.10.80.13). This proxy server has two IP addresses(Local & External). This proxy server is connected to Internet. It's external IP is 121.15.27.65 which is given by the ISP. Your friend's computer IP address is 10.10.80.111.  In this situation if you type 'what is my IP address' in google you will be seeing your IP is (212.25.28.65). Because this is the external IP.  But within a single network, IP address is unique. It means, if the IP address (192.168.0.100) is assigned to you no other person can have the same IP address (192.168.0.100). Because IP address is unique within a single network.  (You) 10.10.80.111 <--> 10.10.80.12 (212.25.28.65) <--> Internet <--> 121.15.27.65 <--> 10.10.80.13 <--> 10.10.80.111 (Your Friend) I would suggest you to read functions of 7 layers in OSI model. Also read the devices that are involved in data transmission.   Let's assume your computer is within a LAN (let's say LAN1), and your computer's IP address is 192.168.0.100. Your friend is in another LAN  (Let's say LAN2) with the same IP 192.168.0.100. Now you want to chat with your friend. How will it happen ?",1
"The ""rules"" vary depending on your requirements. And there are special cases, like, for example ZFS: ""At 90% capacity, ZFS switches from performance- to space-based optimization, which has massive performance implications."". Yes, this is a design aspect of ZFS...not something derived via observation or anecdotal evidence. Obviously, this is less of an issue if your ZFS storage pool consists solely of SSDs. However, even with spinning disks, you can happily reach 99% or 100% when you deal with static storage and you don't need top-notch performance - for example, your personal all-time favorite movie collection, which never changes and where safety is priority 1. If this is a media or long-term storage drive, then 5% to 10% free should be sufficient, and 10% would be preferable if it is a spinning disk. You don't need as much free space because this drive will rarely require data to be moved, so performance isn't nearly as much of a factor. The free space is useful mainly to allow bad sectors to be discarded and replaced, and to allow files to be more contiguous.  For SSDs there should be some space left because rewrite rate then increases and negatively affects write performance of the disk. The 80% full is safe value probably for all SSD disks, some latest models may work fine even with 90-95% occupied capacity. I wouldn't push any drive past 95% capacity for more than a day unless there is a very good, clear reason. Next, btrfs - an extreme case: when free space gets too low (a few MByte), you can hit the point of no return. No, deleting files is not an option, as you can't. There is simply not enough space to delete files. btrfs is a COW (copy-on-write) file system, and you can reach a point where you cannot modify metadata any more. At this point, you can still add additional storage to your file system (a USB thumb drive might work), then delete files from the expanded file system, then shrink the file system and remove the additional storage again). Again, this is some aspect caused by the design of the file system. One thing to consider with mechanical drives is that the throughput of the outer edge is higher than the inside. This is because there are more sectors per revolution for the larger circumference of the outside. People who can give you ""real (serious) data"" are probably the ones which deal with ""real (serious) storage"". Twisty's (excellent) answer mentiones hybrid arrays (consisting of massive amounts of cheap slow spinning, lots of fast spinning disks, many SSDs...) which are operated in an enterprise environment where the major limiting factor is the speed at which the admin is able to order upgrades. Going from 16T to 35T can take 6 months...so you end up with seriously backed reports which suggest to set your alarm to 50%. There are many, many factors that contribute to the result in very setup-specific amounts. So, there's no hard-and-fast number, this can only be measured as a function of those parameters. (That's probably why other users report no specific research on this specific subject made - too many variables to compile anything conclusive.) If this is the main drive on the computer and files may be moved, then 20% free space should prevent significant slowdowns. This will allow enough open space throughout the drive for data to be moved and copied as needed. A spinning drive will work best when the free locations are nearer to the original data, while in an SSD the physical location doesn't affect daily performance. So, the spinning drive should have more free space for purely performance reasons. On the SSD, the reduced free space will reduce drive longevity, but it won't reduce performance. SSD's try to store temporary data and random download files in least-used locations so they can balance the cell usage across the entire drive; otherwise a portion of the drive will age much faster than the rest of it. Some newer SSDs may intelligently re-arrange files so the empty space moves, but older drives may not do this. As the drive reaches capacity, perfomance will decrease because only slower inner sectors will be available. It depends on the intended usage of the drive, but in general 20% to 15% free space is a good answer for spinning disks, and 10% or more is good for SSDs.",5
"The envelope can contain either another printed copy of the whole list of one-time passwords, or the original (non-skey) password, which still works. (Of course for maximum security, if the envelope is ever opened you want to change whatever it contained [either the sequence of OTPs or the original password].) Background: Two admins share an account. Another one should be given an ""emergency envelope"" that is sealed and contains information for that account. Breaking the seal is allowed only in case the other admins are unavailable. Things like Vasco Vacman provide RADIUS authentication, which you could use to log into linux using regular PAM. They require a physical token, and are pricey though, so probably don't suit your current situation.  I used to use OPIE. It asks for the passwords by number, so you know which one on the list it wants. take a look at mobile-otp - it's 'cheap' soft-token you can run on java-capable mobile phone. i used it successfully only with web-apps, but as i see they have pam module as well. Every time someone logs in, it sends them a visible ""challenge"" which includes the index number of the password it wants. Enough information is given to figure out which line of a printed list of one-time-passwords is being requested  ...without any sort of coordination with any other user sharing the account. (Another way to look at this is all necessary coordination is provided by the computer itself, with no need for the multiple users to ever talk to each other.) Similarly there's enough information (both the index number and the seed) to give to a program to regenerate the requested password (provided of course you know the secret).  S/Key seems to be the ""usual"" solution but it requires strictly following the order of passwords on a list. This is undesirable for shared accounts as all parties would need to synchronize use of the list. Surely it is no good idea to log into a remote system from an untrusted computer. But sometimes it is plain necessary. Exposing an unencrypted SSH keyfile is no option of course. Entering a regular password is none either. So skey seems to easily fulfill both requirements. Tell me again why you're searching for something different?",5
"Resolution is converting the name to an IP address.  The client absolutely needs to do this.  The only way to avoid this would be to create URLs with the IP address, like so:  http://127.0.0.2/ but that's generally a bad idea and, in many cases, will not work. Obviously, you have to register and set DNS records both for ""mydomain.com"" and ""my-domain-which-has-a-very-long-name.com"". The only way around it is if your code or something else physically grabs the page itself and then sends it to the client as its own. Qualification is expanding http://example/ to http://www.example.com/.  This also technically needs to be done by the client.  However, if all you're trying to avoid is typing the long name, you could use server side includes to expand something shorter into the full URL.  Ugly hack, but doable. By putting an entry in my /etc/hosts file, or DNS magic, or something similar. I'm running nginx. Any way I can accomplish that? Instead of having the client resolve URLS, I want to let my webserver do that. So ideally, I could turn this: Like others said, you cannot alter DNS resolution. It comes way before your webserver got involved at all. If you just want to have shorter URL for the same domain, you can use Apache and mod_rewrite.  e.g., i redirect automatically all mY ""example.com"" to ""www.example.com"". You can easily accept all requests for ""mydomain.com"" and expand that into ""www.my-domain-which-has-a-very-long-name.com"". doing this is a good idea anyway, as it makes it easy to move the site to a different domain name.  highly recommended, almost essential, if you want to have a three-stage deployment like ""dev.long-domain-name.com"" -> ""testing.long-domain-name.com"" -> ""www.long-domain-name.com"" Probably the most logical workaround, if you absolutely can't stand the thought of typing the long URLs, would be to write a Makefile that runs your source HTML through sed to convert the short versions to long versions. if that www.long-domain-name.com URL is in a page being served from www.long-domain-name.com, then you don't even need to specify the domain.  all browsers will fetch an unqualified link from the same site that it came from.",5
"You are probably experiencing this on a server that is only infrequently accessed, and with a lastlog (/var/log/wtmp) which should be rotated less frequently. This will most likely be the action of logrotate If you look in /etc/logrotate.conf you'll see something like Now when you use the last command, you will see a longer list of logins, over a longer time span, and you will feel better ! Which says rotate the /var/log/wtmp file monthly and keep one previous copy. If you want to keep more then increase the rotate count This issue involves some odd psychology involving ""paradigms"", belief systems and expectations based on conditioning through repetition.  I have a proxy server which is not accessed much, and every time I execute ""last"", the very short list always causes me to feel suspicious.  The feeling that something is wrong is merely the psychology of the paradigm that ""you are accustomed (thousands of times) to seeing a much longer list of logins on your other servers"", therefore you expect a long list !  A paradigm will cause a person to make a mistake over and over, all the while thinking that they did it correctly.",2
"No. You dont own anyxthing - noone does. IP ddresses are not owned, they are assigned like a rental car. That said, a C network is NEVER assigned to someone - RIPE's smallest block that one can get assigned is 4096 IP Addresses. You got a C network from your provider for use with them. YOu kill your provider - the C network gets back to the provider, much as you return a car from a rental agency when you end the rental. That pretty much is it - no way. You can route whatever you want, but unless the ISP supports it this is like trying to order a mercedes by calling a pizza service - it will get you nowhere. What you route through YOUR router is totally irrelevant unless the ISP forwards the packets fourther. Technically, I don't see that it would be a problem. I suggest talking to your service manager about your needs for routing to your own /24 over whatever infrastructure your new ISP will hook you up to. Routing an IP network needs to be done through BGP protocol. So either your ISP let you announce BGP routes yourself, or the ISP does it for you. There is a way to do this.  You have to have a very cooperative ISP (in addition to Comcast) and, at least in my case, a colocated system there.  The company (Linkline) added my /24 to their BGP table and pointed it at my colocated system, which knows how to route packets for (part of) that network over a VPN link to my system that sits behind a Verizon FIOS connection.  I've been using this setup for over three years now and it works, although I certainly don't recommend it for high-volume or low-latency applications.  I use it for incoming email and ssh, primarily, and a couple of other things.",4
"Now, if you set the virtual terminal password, then it probably just set a password on the line vty block, this should let you in with Telnet without needing a username.  Once you get in to the switch with telnet you can run sh run | inc username.  This will tell you what the username is set as, and probably the password if you don't have the password-encryption service on. I set the enable secret, the enable password, and the virtual terminal password. At no point does it ask for a username. If I exit or let it time out, then I need to get back in, at which point it asks for a username/password. What is this username/password? Is the password one of the three mentioned above that it asks me to create during setup? Is it some default value? Is the username something like ""cisco"" or ""admin""? In setting up a new network switch (Catalyst 2960-S), I did the initial configuration dialog (via the console port), in which it asks for host-name, secret, password, virtual terminal password, and other things. On the Cisco 2960-S, the default credentials are cisco/cisco.  These get deleted after the first time you login to the switch. There shouldn't be any access control on the console, unless the express setup puts it in there. Cisco IOS has no default username and/or password.  The admin (or whatever setup wizard used) has to configure authentication to have (and check) user and password. Without a password set on the vty -- or AAA configuration, the only allowed access is via the console. And the console will require no authentication. (that's how you got to the CLI setup in the first place.) The ""getting started guide"" suggests that the initial web-interface default password is cisco and that it ignores the username field. I tried that.",3
"Heres my application: I want to set up a non-SuperUsers computer to turn port forwarding on and off.  There are a variety of applications (e.g. certain games or VNC) that require this user to have port forwarding set up, but due to the security concerns, I dont want to leave it on all the time (nor is there a predetermined schedule).  It occurred to me that I should be able to record the specific interactions I have with the webGUI interface to this persons router to turn port forwarding on and off, and then set up a shortcut to an application that runs the script of those steps.  Basically, this would equivalent to a macro recorder, but for http. I tried locating such an application through Google and I did find one possibility, but thought I might locate more/better options here.  Is there a tool I can use in Windows to record and then programmatically (i.e. under a script or batch file) replay a specific interchange between a browser (either Firefox or IE) and a website? Im doing this on a Windows 7 machine that uses Firefox as the primary browser, but would be interested in solutions under XP or Vista.  The particular router in question is a Netgear (I can edit this to supply specific model), though if the technique Im suggesting is feasible, the specifics of the router and its webGUI shouldnt matter.",1
"If your mail is stored on Exchange, which is often the case in a corporate environment, you will be limited by how much space you have on your mailbox, and any restrictions put on the server by it's administrators. In this scenario it would be best to contact them and ask them if they have any limitations. https://support.microsoft.com/en-nz/help/2768656/outlook-performance-issues-when-there-are-too-many-items-or-folders-in If it's local to your machine, in a PST file, then the only limitation is the amount of disk space you have to store data. The number of folders are not restricted. Be careful however, a lot of folders can become hard to manage, and can become an issue when using a mobile device or web browser to access your account if it is on a corporate network. For outlook 2007 and later Microsoft states that there is a limit of 500 folders in cached mode or when using a pst file or shared mailbox. The problem comes in ""searching""   Outlook is currently hard code limited to 64 subfolders for indexing/searching after that weird stuff starts happening.",3
"If you're using an external monitor, this combination only displays the taskbar on the windows' monitor, whereas Windows+T shows all taskbars, and doesn't switch the windows. With Windows+Comma (Not the comma on the numpad), you can peek to the desktop (which will also show you the taskbar) - but only temporary. 1) Assuming that the taskbar is at the bottom, find an open area of the desktop and swipe down.  You ""might"" need to tap the desktop first just to give it focus but not always. You can press Alt+Shift+Esc  to switch between windows (which also brings the taskbar to the foreground). Touch a free space on the desktop and drag quickly past the end of the screen where your taskbar is located (hidden) then quickly drag back to the desktop and the taskbar will appear. 2) If you are in an application, then tap near the bottom of the screen and pull down and back up.  Sometimes just pulling down is enough.  Depends on the application.  In Wordpad a simple swipe down was enough.  In Chrome I have to swipe down and back up.  If Chrome is fullscreen (F11) then nothing works.",3
"I think your ""virtual machine"" is actually an OpenVZ container (which you can verify this by running virt-what). you are almost there, It does not matter if is a virtual machine or a physical machine, those settings are always changeable. Two methods above are the most commom. There's another one, and it may work for you, it's by using sudo, almost like you were doing: I have elasticsearch and SugarCRM7 running on CentOS 6.5. Every day I face the same problem: java outOfMemory error. That happens because of small vm.max_map_count value, 65530 only when 262144 is recommended. (/etc/rc.local file is executed after all startup linux services, it may not work if elasticsearch starts before it as a service, but this method can be useful on another setup if you need in future, or you can use like this by putting them inside your elasticsearch init script, because init script run as root, so it's the same syntax above to use inside init scripts) Hope I have helped someway, at least by giving the 3 different options to deal with the problem, since it's almost a year old your question ;) First, need to be root: (sudo does work in some distros, but does not on some other distros like you tried, this first method is universal and works on any Linux, macOS, or any Unix-based. Hope you have access to root password.  keep going with the others, save /etc/sysctl.conf, reboot your server to apply changes, or execute: sysctl -p to apply the changes without reboot. They will be permanent across reboots. Those parameters will be set on every reboot, AFTER all init services have started, just before the login prompt shows.  And it's already applied and functional. By changing values of any pseudo-file under /proc the settings becomes active instantly. But they don't persist after a reboot. You can play with values and measure performance changes at elasticsearh or any other application or system metrics. Go tunning your system, writing the values on some paper, keep the best values. On any mistake, reboot and they will all be back to original values, and start again until all wished values are optimal. There's a lot of disk and memory tunnable parameters under /proc. And they make a huge difference and performance gain if you to tune them well (and have time for it). You are on the right way. So how can I change this variable's value? Restarting elasticsearch every night is not a good idea... Or at least may be someone knows why this error happens? 2) /proc on unix is not a real filesystem, it's a in-memory kernel file system, but it appears to be like a normal disk file system. You can call it 'fake filesystem' or 'special filesystem', you cannot edit those fake-files with vi or any other editor, because they are not files, they just look like files. I stuck with the same problem years ago. This is a well known issue with elasticsearch (issue #4978). It's not just Elasticsearch. Java apps are well known to perform poorly on various OpenVZ providers, mainly because the hosts are often poorly tuned and there's nothing you can do about it. One commenter on that issue echoed what would be my recommendation exactly: You can also copy them now and paste them for instant changes. The parameters above are valid, tuned and running on my apache cassandra server.",3
"This pipes a single ctrl-C followed by input from the controlling terminal, while -tt forces a psuedo-terminal to be allocated. All told this gives you a (somewhat malformed) shell on the remote machine while bypassing as much of .bashrc as is possible. Run ""man bash"" or ""man (your shell)"" for options to disable the start up files. You only need to use an abhorrent shell for the time it takes to fix the problem. From the suggestions and responses given above, I'd say it's not the .bashrc or .bash_profile files. Also ssh manpage says that if you specify a command to be executed then your profile files won't be read.  However, if you are in the ""no such luck"" situation, I offer this solution, based on user60069's and Dennis W's solutions: Mashing ctrl-C works as long as you can get a ctrl-C in before the .bashrc exits. Unfortunately, this can be difficult to do if exit is early in the .bashrc. Kudos to user60069, it worked for me, but I use the shell-specific startup file .bashrc, so logging in with /bin/sh worked for me. If you system is setup normally, .bash_profile won't be run for a non-interactive shell (such as running a command). I'd suggest try executing a different login shell (ksh? csh? sh?) from the absolute path; also, beware that it might be a totally different problem (quota? execute and read permission on your home directory?), so a side approach would be better. Can you ask another user to do a ls -la $YOUR_HOME_DIR and mail you the result?  All ssh commands run your login shell. ssh $COMMAND runs $SHELL -c $COMMAND, scp runs $SHELL -c /path/to/sftp-server, plain ssh just runs your shell.",5
"For my OpenSSL database file the second column corresponds to the expiry and my invalid entry that caused this error was '20160505000000Z'. This particular entry had 2 extra digits compared to all other entries. The 2 extra digits were the '20' of the '2016'. I manually changed this to '160505000000Z' and then this error went away.  Not sure how it allowed that to be written in the first place, but that is what fixed mine. Hope this helps somebody. To fix this just go manually modify your database file and make the expiration date look as it should. 'entry 22' refers to line 22 of the database file for the CA - in your case it sounds like this file is 'index.txt'. My error was 'entry 477' which refered to line 477 of my CA's database file. I know this is a really old question, but just ran into this issue myself and took a long time to solve - hoping this might help somebody in the future. Also, seeing how this is still one of the first results google provides for a search on this error, it seems relevant to post here.",1
"Importantly though, this might only be the entry point (firewall) to another network. My (large) employes uses a proxy server in California. When I connect to the internet while at work, you might think I was in CA but that is not so. Whenever you query a whois on an IP Address, you always will see which company is responsible for handing out that IP Address. This is either the ISP or webhosting. There are 2 versions of IP Addresses. IPv4 and IPv6 addresses. Both work differently from the other, so I'll be talking about IPv4 (xxx.xxx.xxx.xxx) addresses. Regarding geo-location Often the fact that an ISP 'owns' or has access to a block of IP addresses means that it is quite well established who gets assigned an IP from this range. Hence it can be simple to identify an approximate location. However this comes down to how that ISP hands out IP addresses. Some will not pay any attention to geographic locations and therefore the precision of geographic location from the IP address can be very low. Incidentally a ""trace route"" can be used to find the intermediate points (routers) that get the signal from one place to another. These intermediate points often have names that tell you something about the routers' position - without needing to look things up. Whenever you get an internet connection, you always have to agree with the terms, whicn state that no abusive activities are to take place. If complaints get to the ISP, they can suspend or even terminate the internet connection. But because someone's ISP can always be found by the IP Address, and the whois information always lists an email address to send abuse complaints to, it is always possible to find and punish people who are abusive on the internet in one way or the other. If someone uses a major ISP in a country, the WHOIS information will tell anyone which country that person lives in. Some sites however seem to have a database that seem to be able to go up to city/street level about pinpointing where an IP Address originates from. Although these sites are not always accurate, there are ways to find someone's location. Lets assume this is not the case. If a user disconnects from the internet, or their DHCP lease expires, the ISP may give out a new IP Address. Usualy an ISP will only do so for disconnected users where it needs to assign an old used IP to a new user because it ran out. If that first person then connects to the internet again, and his/her IP was handed out, they'll get a new IP Address. An IP address is often uniquely tied to you by your ISP. This means that if your ISP co-operates with whomever has the IP address, they can determine your identity and therefore what services you have been using on the Internet. The best information about the location of an IP address comes from the switch / router closest to your house. Your ISP is responsible for that ""last mile"" of your connection - and will know exactly what router (billing address) a particular IP address is associated with. However you can never guarantee that the IP address you're connected to is actually tied to the identity you think it is. It is almost always possible that someone else has access to the IP, either because there's an insecure wireless access point there or because it's an open proxy/VPN service. An IP Address is a network identification number that allows one address on the internet to communicate to another address. Given that one place can send data all over the internet to your location and back, it is logical to understand that if software can do it, so can other software. An IP address is often a volatile thing - you may have one now, and a different one in five minutes. Further, it may only get to your ""front door"". If you have a wireless router at home, you may have several computers behind this wall. The IP address doesn't tell anyone which computer is connected. On the internet, A corporation called IANA registers ranges of IP Addresses valid for different countries. Per country, each ISP is given a small set of IP Addresses that they are allowed to hand out. Because there are more people than available IP Addresses, IP Addresses are not assigned to a person or domain unless an additional fee is paid for it.  Because the whois information also lists the name + address details, it can be figured out upto a reasonable detailed GEO location to where the IP is residing.",3
"It's either the case that the ports are blocked by a firewall, DNS resolution isn't working or is giving out the wrong address, or the service you're looking to access isn't running. I have setup an mx record (about 36 hours ago), e.g. mail.mydomain.org, and also one for mydomain.org. I don't know if I need both. So I'm guessing these ports aren't open, and I haven't configured the server to listen on IMAP or SMTP properly.  Check that, if your mail client is trying to use SSL to connect, your IMAP and SMTP daemons are actually configured to accept SSL connections.  If you don't want to do that, turn SSL off in the client settings. Check that the DNS names you're specifying in the client for the SMTP and IMAP servers are correct, and that you can open a connection (using telnet or something) to that host on the appropriate ports (eg. 25 and 143).  If you want, try doing an IMAP or SMTP session yourself in the telnet client and see if it gives out the right responses. I have installed Dovecot, which I understand handles imap requests for Postfix, but i'm such a newb that I'm struggling to work out if my server is using sendmail or postfix. The problem is: When I try to add an IMAP account on a mail client, it tells me it the server is not responding. The same when I get to adding the SMTP. I am setting up my first sendmail daemon on centos, using virtual domains and emails. So far, I have got it working and emails sent from remote servers will make it to the inbox. The MX record does not indicate much of anything to the client.  Mail clients look up the A or AAAA record for the DNS name provided as the IMAP or SMTP (or POP3) server.  MX records are for providing an indication of which servers are mail exchangers for your domain (eg. which server to send mails to when the recipient is at your domain).",2
"With it (probably also with command line version) you can select an area of the screen to capture, very easily. (tutorial here) (you can do the same by commandline, specifying offset and width/height parameters. A bit more tedious, though) On the file size issue. You can lower the quality slider in the GTK gui. (there's a command line parameter for this), mark ""encode on the fly"", and unmark zero compression. And do a small test to find the lower quality you can use. FPS, frames per second captured, also vary the file size, but it can end up in a ""jumpy"" motion... Another option is just record it all, then use these tools for splitting the file. n.2 is the most important feature I need. I will consider as correct answer the suggestion of an alternative (even on Windows platform) as long as it could satisfy both my requirements. BTW, the software actually saves temp files, I think in chunks of 500mb, calling them img.out or something like that. With a recent ""repair"" function you can recover crashed session chunks. Might be this be a way to somehow re-create the chunks...An starting point, at least. I need to use recordmydesktop to record a six days conference. Every day there is a 3 hour seminar that I need to record.",2
"Based on the use of the embedded controller, you should be able to just remove one of the drives.  I would also check the controller's firmware settings in order to disable the second drive if the setting is available at all.  We have several Proliant ML 110s here with RAID 1 pairs and they have continued to run even when one drive in the pair failed. HTH. How would I go around removing one of the drives, removing the RAID 1 while still keeping the Data on the drive, and being able to boot my linux distro from it? Yes you can certainly remove the RAID 1 drive, but don't forget to change the settings to normal for the proper drive functioning. I have a a Proliant ML 110 G5 server with an Embeded SATA controller running RAID 1 on two 160 GB drives. I had an issue with my CentOS Installation and after running e2fsck (via Ubuntu Pendrive) y booted into a system that was not recognizing my RAID 1 Array showing me both drives with different partition tables, not mounting two of the 6 Partitions on my HDD, and using only one of the HDD.",3
"Now wether that's unsafe or not, i don't know, but i'd simply leave only up to the max in to be safe. In general, it will depend on the BIOS and hardware. It won't be able to handle the big modules but if there are still smaller modules available, the system might decide to just start up with the memory of just the smaller modules. But in general, the BIOS won't be able to use it thus your system would have no free RAM to use. Assuming that you install the correct type that this motherboard accepts, anything above 2GB won't be addressed and will be ignored. This really depends on your MotherBoard.  I have seen some cases where the system functions as normal but will only recognize the 2gb, and I have seen some that will not start up and throw an error. I've done it on an older computer, and the computer booted just fine - however the BIOS / start up scrolling list only reported what the maximum for the motherboard was, so it was presumably ignored. Been there, done this. The BIOS complained about the RAM and refused to start up. I had to downgrade again. But this was an old Pentium 133 from Dell and about 4 PC's in the past.",4
"3) In the < head > tag, it highlights all indents with white for some reason. I don't want it to do that. As you can see, I want to make small changes. I tried opening htmldjango.vim and editing it but I have no idea where it defines the colour of the tags. htmldjango.vim says this 2) It colours all opening tags (< and >) yellow and all ending tags (< / and >) red. I want them both to just be blue.  1) It highlights all variables (whatever starts with {{ or {% and ends with }} or %}) white and gives it a red font. I want all variables to just he yellow.  4) it underlines all the link names. (example: < a href = 'link' > linkName < / a>) I don't want this either. I'm guessing this is where the tags are defined. But how do I change what the tags are coloured and what not? I even tried opening html.vim and reading through that and it seems that even in html.vim it doesn't define the actual colours (I couldn't find the name of any colour or hex code of any colour in neither html.vim or htmldjango.vim). Where is the file which defines the colours of the tags / variables in htmldjango.vim?",1
"Caveat: Technically, multiple home support was introduced in 8.1.5-- if you're using earlier versions, the answer changes a bit.  You can still generally do it, just not as simple. That said, running two instances (other than dev/test) on one server isn't generally recommended. If they are small, you'd probably get better performance as separate schemas in one instance, and if they are big then go for separate servers. Also consider virtualization as an option. I recently tried installing Oracle 11g on a server already running 10g. Each would run independently with no problem, but 10g refused to run simultaneously with 11g. Can this be done? I don't need a solution so much as confirmation that it is indeed possible. It is certainly possible.  You do need to ensure that the different versions are installed in different Oracle Homes, but other than that, you can have as many versions of Oracle as you'd like. You can always have multiple oracle accounts; the oracle account does not need to be named oracle.  For example, you can have an oracle10 and an oracle11 user.  This is more straightforward than a single 'oracle' user and multiple ORACLE_HOMEs. Yes, you can have different versions of Oracle installed and running on the same server. I'd probably go with different listeners on different ports. And you wouldn't want two instances with the same name running. There's plenty of other areas you'd want to be sure they don't bump into each other (mostly disk locations).",4
"I am fixing a computer for a friend, which has a Windows XP Home Edition installation that will not boot. I think there's corrupted system files; I was able to boot from a Windows XP Pro disc, tried fixing the MBR and everything else I could. I've determined that the best course of action is to re-install Windows. I have her genuine product key, but I only have a Windows XP Professional disc, not a Windows XP Home disc. I don't think Microsoft offers any OS downloads on their website and I can't see any way to fix this computer without actually having an XP Home disc. The computer is a Dell Inspiron 6000, is there a way I could get it from Dell? I know some Dell computers come with recovery partitions (as this one did) but when those are gone... well, they aren't much good. 2) Call the folks at your local high school or community college.  Chances are, if they have Dell systems, they have CDs to spare.  Or, they may make a copy for you.  Any Dell OEM XP cd will work for you. http://support.dell.com/support/topics/global.aspx/support/dellcare/en/backupcd_form?c=us&cs=19&l=en&s=dhs 1) Go on Dell's support website.  They have live-chat available with their tech support people.  They can tell if you they can send you a new CD.  It may cost a few bucks, but it would be worth it.",3
"I think before you go pointing fingers, you make sure that this is in fact the problem. It might very well be, but before you invest a lot of time an effort in the number solutions below, try profiling your code. You should be looking for large CPU usage areas. If improving that improves performance, then awesome, mission accomplished. Some devices ship without FPUs, so the performance with floats are very poor. As for actual methods of improving performance that take a while to implement, you have a few options: Of course I understand dynamic mesh will always be slower to render than static mesh, but I wonder if the method I use is good or if there is a faster / better way to do this .  Here is an easy sample as the index buffer does not change so I only update the vertex position, normal and texture coordinates.  You might try different graphics rendering modes to make sure that you are in fact limited by vertex transfer rates, i.e. using the same number of vertices in a static mesh is dramatically faster. If it isn't, then your model(s) are too detailed and filling a buffer faster won't fix anything -- your GPU simply can't handle the geometry + any other stuff you have enabled. You can turn down/off many of the effects like fog/lighting (lighting is especially expensive if your GPU does not have TnL and your CPU does not have a FPU).",2
"It definitely sounds like the drive is defective.  If ita new, I would suggest returning/replacing it to the store you purchased it from. If it is out of warranty and non-returnable, you could open the enclosure up, remove the hard drive, and hook it directly up to your computer.  This would allow you to try and format it directly and see if the problem is the with the drive itself, or with the USB to SATA electronics.  However, from your description, the drive itself sounds defective.   I have a WD hard drive and it was extremely slow even though the drivers were fully updated , it first showed up as a local disk then I had to click it then after 10 minutes of loading the hard drive finally shows up and every action I do on the hard drive takes 10-20 minutes to respond. I finally formatted it using disk manager but then it became RAW and now I cant format it again or use cmd to fix it.  If you are unable to exchange it at the place of purchase, you could call Western Digital for support.  They also might replace it if it is under warranty.",2
"It's got all (most?) of the bells and whistles others' do, plus a few extras (like 'grid' storage between clients and such).  It's pretty straight-forward to setup and run, and for our purposes it was/is way cheaper then I would have thought it would be. :) Do you have a solution in place for clients that are inside your corporate network? If so, can that solution be extended using a VPN solution? There's an open source solution that looks promising. (never used it but I have heard good things) http://backuppc.sourceforge.net/info.html Personally, I'd use rsync as a preference. Setup the outside server with rsync (there by default on Linux, Deltacopy on Windows. You can then install the Deltacopy client onto any Windows machines you wish to backup, and while the first backup will take a long time, subsequent ones will be very quick. We use Vembu's StoreGrid to do our in-house backups, as well as provide off-site (to our private servers) for our clients. If you aren't happy on Linux, or don't have the option to setup an rsync server; then FTP is probably your best option. I've used Cobian in the past (free, used to be open source) - it supports full/incremental/differential backups, and have options to keep multiple timestamped copies. If thee server is untrusted, it can also encrypt the files for you.",4
"I am unable to answer this question for Mac OS X because I do not run that operating system, so at least one other answer (or edit this answer!) will be needed. Note that many organizations' proxies can only handle HTTP and HTTPS connections, and are not designed to handle (or outright reject) other protocols by performing stateful packet inspection and rejecting all the other protocols. They might also limit which ports you can use. This Squid method should work for all outbound HTTP/HTTPS traffic that obeys the routing rules of the IP stack, which, all application traffic should. It doesn't require any special hardware, but it does require administrative/root access to your own system. In the general case, you can ""force"" all traffic to be routed through a proxy on localhost which can then route through the HTTP(S) or SOCKS proxy of the organization by using a configuration such as Squid. This should work on all operating systems but the configuration would be slightly different. This is especially applicable to Ubuntu, but I have no idea how well Squid runs on Mac OS X. To set up Squid, you can follow this tutorial, or follow these general steps (outlined briefly here; should be enough if you know the tools):",1
"It's a hdd, so does that mean it's not a flash drive? (Is there a disk that's spinning?) Surprisingly, I could not find an answer to that question. I ask because I am worried that since I won't be removing/unmounting it or turning it off all that much, I might be harming it (I assume a flash drive is not harmed by keeping it plugged in indefinitely). That said, I've had one computer running here since 2008, 24/7/365 - never sleeping, never spinning down the drives. I plan to connect a relatively old external 2.5' backup drive (Seagate STBU1000200 1 TB) to my router for simple NAS (movie, photo sharing on the home network). If you are using it for media sharing on the network it may be in use fairly often, so it's probably best to just leave it connected and powered on. If you only use it infrequently you can either unplug it when not in use. The software provided for the drive allows you to set this timeout, but it will probably not work with your router. Depending on your router and its firmware you may be able to control whether drive goes to sleep and after how long. Most machines will spin down the drives after a period of inactivity, unless you tell them specifically not to. The STBU1000200 is a hard drive, not flash. Flash storage is getting cheaper but anything as big as 1 TB is going to be a lot more expensive than a hard drive. Hard disks are designed to remain powered on for long periods. Of course it will eventually wear out, but having it spin down too often will actually wear it out faster.",3
"(There is a bug in your implementation: stuck if both i and j index an element equal to the pivot value (num).) (Getting late: the following code is work in progress; posting this to save the above, mainly (not quite trusting SE's autosave)(Never used C# - give me a break on documentation comments, const-correctness, commendable use of static or some such.)) Seems to me like you're on the right track, but the significant problem with your implementation is the fact that you're using recursion. Instead (to make it iterative), you should have one function that partitions your array, a small data structure to use as a pointer to walk across the array, and a function that performs the sorting. I started on my own solution, but found this one to be more descriptive. There are also plenty of other examples of other sorting methods linked to it that I hope will help. Though it is character-building to transform a recursive algorithm into an iterative form (and vice versa) I would not worry too much about iterative vs recursive. Of the Implementation issues mentioned in the wikipedia article, two reduce the likelihood and severity of worst case behaviour: For readability, you can and should separate the concerns of picking a pivot index, partition, and sort. This is quicksort, using Hoare's partition scheme with a twist (use two reads and two writes to resolve one inversion wrt. pivot instead of a swap (/""exchange"" - making it the counterexample to labelling quicksort ""partition-exchange sort"": a direct exchange is not essential (partition is)), conventionally taken to be equivalent to three reads&writes, each. With today's memory hierarchies, don't expect it to be any faster because of this).",3
"Again, for simple turn based games ease of development can easily take priority over format efficiency, so unless you want to develop an efficient binary format - don't. BlazeDs sounds like an excellent tool for your needs, but rewriting a Java game server from scratch is not that big of an endeavour, using for instance Netty and Protobuf :)  I'd definetely go with a custom solution : even though you might lose some time in the short term, it'll definetely scale better if you need it to, plus the experience you gain will be massively reusable for your next games. To sum up - I recommend using an existing solution, if possible. It is likely to save you a lot of time. As for which pre-existing solution, that's up to you. Take a look at the following netty game server. It supports a binary protocol. Note: Written by me! Has TCP and UDP support, uses jetlang for extremly fast in vm messaging. This is not a good reason for choosing to implement this functionality on your own. It's just something that's comforting to know, if you decide to follow this path. Having used both SmartFox and ElectroServer quite extensively, I always recommend ElectroServer. It does all the same stuff as SmartFox, but is just a bit more solid, and includes binary support. Both for hosting and technical reasons. If your game is not fast paced then that should be a good place to get started and up and running, and have the scope to scale. You have to weigh how much time you save by developing a custom solution and deploying it fast, versus using an existing solution and possibly doing a longer deployment. Odds are that the development time will offset what little benefits faster/easier deployment gives.",5
"The best two books here are both from Itzik Ben Gan - Inside SQL Server T-SQL Querying and Inside SQL Server T-SQL Programming. Two excellent suggestions from @MikeWalsh. I would add Grant Fritchey's SQL Server 2008 Query Performance Tuning Distilled along with the excellent free ebooks from Redgate. SQL Server Execution Plans is an excellent reference to start with. From a different angle, I strongly believe knowledge of internals are important for all aspects of working with SQL Server. To this end, both Microsoft SQL Server 2008 Internals and Professional SQL Server 2008 Internals and Troubleshooting are essential reading. However, when you set the refactoring standards, don't apply them manually, but use a tool to refactor the code Here's a good book for code refactoring: Joe Celko - SQL Programming Style. I like to compare it to MS Manual of Style for Technical Publications. The latter one you use to write your articles, blogs and documentation; the first one to write your code If you have problems with code readability (either yours or team's), this book will help you set your own standards. Here's from its review: Read the Querying one first. Covers all the important information on how queries are processed, thinking in terms of sets, etc. Covers all aspects of querying. The second book goes into programming constructs when working with T-SQL. Amazing books.",3
"Also, you declare an inner version of Test in the if statement: int16_t Test[960 / 2];.  But this version only exists in the body of the if.  Once you leave the if, the original Test[960] is active again. What's your convention for variable names?  Some of them are capitalized, some aren't.  I don't care what convention you use (some folks would care, a lot), but you should be consistent. Maybe this code should be split into 3 different functions, one for MixMono, one for left, and one for ""other"". Is there a relationship between bufferSize and the size of Test (which is 960 or 960/2)?  Why are you looping over elements of micBuffer, instead of looping over elements of Test?  Have you tried writing it with the if-else if-else  on the outside, with a (different) loop inside each block? You set the booleans to true, then test them in the if.  The if condition will always be true, so what's the point? Where's the declaration of micBuffer?  If s is supposed to be a 16-bit integer, why isn't it int16_t?  (The parentheses around the RHS expression seem useless.) Does this code actually work?  I haven't figured out what it's supposed to do yet, so I can't be sure.",1
"I have an Apache web application which is serving ~2k Requests Per Minute. The current infrastructure includes two servers, one acting as Apache + database, and the second one just database.  I'm trying to evaluate the added value from moving the Apache to its own server and/or maybe even a couple of small instances and a load balancer. New relic has plugins for all aspects so you can monitor load across each element of the infrastructure. You mentioned a small instance - if you are in AWS - you can use their elastic load balancer  in front of your Apache instances - it works extremely well.  They also offer MySQL DBs as a service too. There are also many options available to scale MySQL - we find read only slaves very effective for our applications. This format also benefits from providing the ability for a Apache instance to be taken down for updates and allowing the site to remain live (you must consider how you manage data changes). It is a pretty standard n+1 architecture you are describing.  It will certainly provide you with many scaling options going forward. How can I perform such evaluation? I'm using New Relic for server monitoring and seeing most (~70%) of the app server time spent is in PHP.",2
"ISPs run authoritative servers to host the domains that they manage on behalf of their customers.  The name servers run by the TLDs and ccTLDs are also authoritative servers, as are the root name servers. In addition to pQd or if you've got a longer commute, listen to the dns-specific episodes of SecurityNow. On first glance episodes 163, 157 and 155, though there have been some additional episodes (after these) with listener Q&A. The sources you find with Google are either very very technical and so hard to get a proper feel for the whole system or so dumbed down they are not very useful. Anyone have a good source that explains it as a whole but without be having to become an expert in the process of reading it? This is the server that holds the definitive (authoritative) information about a domain name.  Authoritative answers from such a server have the AA bit set. The answers from authoritative servers always contain the actual configured TTL from the zone file.  i.e., if the TTL is set to 86400s, that's the value that'll be in the responses. That server may or may not know the answer, but think ""recursion"" here.  This process will repeat as necessary, all the way back to the ""root"" server (called '.' or ""dot"").  Eventually the answer comes back down the chain, with each DNS in the chain pretending that it knew the answer all along! A forwarding (or proxy) server doesn't (usually) cache.  They're used to proxy packets between one network and another, typically sitting between a stub resolver and the recursive resolvers. A recursive resolver only receives requests from stub resolvers.  If the answer is in its cache it will return it immediately.  If the answer is not in the cache it will iteratively ask the relevant authoritative servers for the answer, and then return it to the stub resolver. There's a ""hosts"" file on your computer.  You can put entries in there; for instance, you can put names of things on your LAN that translate to their IP address.  The file can be empty, too. I want to get an understanding of how the DNS system works. How domain names actually get resolved and how admins setup a domain name so it is resolved correctly to a target machine.  A stub resolver isn't a server in the normal sense, it's typically a library, and calls to gethostbyname() and related functions just invoke the code in that library. ISPs run recursive resolvers for your PCs talk to, albeit most consumers actually rely on the DNS proxy in their home gateway, which forwards the query to the ISPs resolvers. That said, as has been mentioned previously, DNS & BIND really is the bible when it comes to DNS; if you're going to be administering your own DNS servers, it's probably worth having your own copy. Despite it's name, DNS for Rocket Scientists might be what you're looking for? The DNS overview section covers key concepts like domains, zones, record types and delegation. If you're then looking to implement DNS with BIND, it goes on to have more detail about BIND itself. Similarly answers received from the recursive server show the decreasing value, not the original value from the zone.  Per the example above, if a record was received with a TTL of 7200s exactly an hour ago, the answer from the recursive server will say 3600s. The stub can't do anything except talk to a recursive resolver, relying on that recursive resolver to obtain all of its answers for it.  All such upstream requests have the RD  (Recursion Desired) bit set. So failing all the easy answers, this DNS service does the next easy thing.  I looks to see who your ISP said to ask (when you connected you got this info), and so it asks that server. Caching is a fundamental feature of recursive servers.  The TTL received from the authoritative server continues to tick down, and when it reaches zero the entry is purged from the cache. First of all, there's a (usually small) DNS server right there in your computer.  And maybe it already looked that address up recently.  So if it's in a cache, you're done.  If not, we dig a little.",5
"Here is a detailed article that gets to the heart of the matter - it doesn;t matter how powerful and detailed your access control and security systems are... if its too complicated to set them correctly, you will end up with security holes. In this case its complexity of the systems - the larger the 'surface', the more chance there is of a security bug. Nobody would dispute that writing buffer overflows on Windows is substantially harder than on linux. As well, The ACL system in Windows is vastly superior to the *nix system in numerous respects (Its still possible to use setpgid() to break outside of chroot()/jail() and transfer the psuedo-root tokens to effective UID 0).  I used to see this with our domain groups - its too easy to give someone access to a secured resource if they are in the wrong group if you have too many groups. The register describes this better. This one sounds relatively good to my novice eyes... a bit old and slightly biased, but not so much. In short, If you're lazy, You're better off with Windows. If you're dilligent, You're often better off with *Nix (From a security perspective) If process division attacks are a concern for you, Not to be that Crotchety Unix Admin, but Windows has suffered far, far, worse Linux, BSD, Solaris, and AIX have the virtue of having user-made patches which implement very impressive security features. I'd name the PaX/GrSEC projects, Which, regardless of security shortcomings in the past few years, Have set the standard for implementing Address Space Layout Randomization, Likewise for StackGuard, W^X and the numerous other utilitiees designed to prevent Heap and Format string attacks from being successful. Strictly from an access point of view, There are many extensions to the admittedly outdated current system. I'd like to know are there any more detailed articles or paper comparing security mechanisms and designs in Windows and Linux?  In my opinion, although it seems that Windows users are more subject to viruses and vulnerabilities, I believe it is mainly due to most Windows users are less experienced computer users and Windows platform attracts more attackers since it has more users. I'm reading MIT 6.893 lectures on which it says protection in Unix is a mess, no underlying principle, and it also points out that Windows has better alternatives, which can pass privileges from one process to another over IPC.",4
"(unless the keyboard is USB or similar, of course, in which case the connector and circuitry around it are designed with hot-plugging in mind). One thing though: I hope you are not pulling the keyboard out of the machine while it is running. They don't like that. You risk shorting the keyboard controller. I suspect that is why the external keyboard+mouse ports stopped working on a college's laptop at work many years ago. Personally, I'd boot the OS, modify /etc/default/kbd, shut it down, pull the keyboard and cold boot. If simply ""don't remove the keyboard"" is not an option then you used to be able to get little dongles for keyboard ports that the BIOS would see as a keyboard being connected. They were useful for machines that didn't have the option of disabling the on-boot keyboard check. I don't know if you can still get them anywhere but you could probably make one easily enough (IIRC they were nothing more fancy internally than a resistor of the right value between two of the pins. Anyway, check out /etc/default/kbd in a text editor. There are various switches in there you can use to disable keyboard checks on reboot. If you just want to pull the keyboard while SunOS is running, without rebooting, you can 'kbd -a disable' (iirc, read man page to be sure) and pull the keyboard. The default keyboard abort sequence on a Solaris system is sent with the L1-A or STOP-A keys pressed together on an attached Sun keyboard, or the BREAK signal on the serial console. Inserting or unplugging a keyboard on a running Sun system will also send an abort sequence, effectively dropping you to the Open Boot Prompt (""ok"" prompt). To disable the keyboard abort sequence for future sessions (i.e. after a system reboot), change the following in /etc/default/keyboard.",3
"No similar statement holds if i is congruent to 1 or 5 modulo 6.  So as long as you have checked for divisibility y 2 and 3 and you count up instead of down, you know that no new information can be revealed by is that are not congruent to 1 or 5 modulo 6. where the increments continue alternating between +2 and +4.  Unroll your loop (once) to do both of these per pass. For such small numbers, you are overwhelmingly more likely to find small factors by counting up from 2 in fewer steps than finding the largest factor counting down (which you will find before the largest prime factor, in general).  Consider that every factor greater than the square root has a cofactor less than the square root, so the density of factors is much lower among the integers greater than the square root.  You really should start at two and count up.  Every time you find a factor, since the number has no smaller factors, you know the factor you found was prime.  (This makes your IsPrime() superfluous.)  It's traditional to divide this prime out of your number and continue testing the same (and then larger) trial factors from the reduced number.  (You test the same prime again because one of its powers may divide the original number.) Since you're now counting up, you can skip about one third of the candidate factors you would check in your current code.  Other that 2 and 3 all prime numbers are congruent to 1 and 5 modulo 6.  Consequently, you would test the list  Of one wants to unroll a little more and skip about 20% of the remaining candidate factors, one tests 2, 3, and 5, and then only",1
"Ignore Lucas Kauffmans post! For one, a 32bit processor can't run 64bit OS any way, but given the option 64bit OS will be able to map more memory and process more per cycle, giving better performance.  I currently have 104 servers all running Ubuntu 12.04 Server 64-bit without issue. They consist of web servers (mostly), MySQL database servers (combination of stand-alone, master/slave, and master/master), mail, DNS, development, and various other needs. Haven't had any problems at all. I will be building a new server for a small network of Windows XP and Windows 7 machines. The server will provide the following services: According to me, Ubuntu server 12.04.1 64bit  will be smart choice, more stable, having latest security updates. You should use 32bit in case no hardware supported 64bit. Our current server, is running Ubuntu Server 10.04.2 LTS. It is showing its age, that's why it will be replaced by the new one. But the overall stability and performance over the last 5 years was very good, so I want to stick with Ubuntu. There is no reason to prefer the 32-bit version. Canonical even recommends the 64-bit version when downloading Ubuntu Server. Although I've been running 64bit versions of 12.04 since it's release date I've not had samba installed on it - the pair of them (have 2) run web services - so DNS, web server, mail relays etc and all been stable. Only problem we had with them was when the UPS died .. oops .. learnt a valuable lesson that day .. redundant PSU's I would not recommend downloading 12.10. The non-LTS versions often include more unstable software. For instance, in the first month after 12.10 came out, PHP-FPM would segfault at boot. The LTS is usually the way to go for a server, as it has been thoroughly tested. 32-bit should only be considered if you are running older hardware and the CPU isn't 64-bit capable. Once you get above 3.5GB of RAM a 32-bit kernel will need the PAE extensions in order to access the remainder of the memory, which just adds overhead. If your hardware is 5 years or newer you shouldn't question putting a 64-bit OS on it. Also, I would like to continue using a browser based frontend for administration (currently we use Webmin and I like it)",5
Additionally you can make use of the pst import functionality in Office 365. You can upload all PSTs to Office 365 and then import them to the newly created mailboxes. This way your users can start using their new mailboxes while in the background all their emails are imported. 3) reopening Outlook let the user to use the POP profile (you can set it as default) until you enter in production with exchange online 2) in exch-profile add a new data file (.pst) and select the file of the POP profile (you will have exchange mailbox and local-pop mailbox) When you will be in production with exchange online set as default the exchange profile (doing so the user will not open the wrong profile) and let the user move from local mailbox to exchange all the messages he wants/needs without downtime More details can be found here: https://technet.microsoft.com/en-us/library/ms.o365.cc.ingestionhelp.aspx My suggestion will not cover the user migration process because it may vary from hoster to hoster (reading your question I suppose you are evaluating BPOS).,2
"What sucks even more about this is that certain ""power user"" (i.e. - someone who has just about enough knowledge for it to be a dangerous thing) websites provide direct downloads of individual DLLs, so people grab these rather than running the proper installer from MS.  Cue random and mysterious crashes. So, in summary, for 9 you have 2 options.  Either use use the latest version (and tell people to update their DX install - even if they think it's already up to date), or backtrack all the way to 2004 or thereabouts when D3DX was statically linked and none of this was a problem. From sometime in 2004 (IIRC) onwards the D3DX stuff moved from being statically linked to dynamically linked.  In order to handle this, the player needs a version of D3D that is up to date.  In a fit of ingenuity, MS didn't include these up to date versions with either Windows Vista or 7 - I've personally been in a position where people have said ""I've D3D11, I'm up to date"" but yet don't have the up to date D3DX DLLs for 9.  That sucks (and is actually quite difficult to explain to someone). Agreed, the latest release of the SDK is generally the best one to use, but there are other considerations.",1
"I'd suggest taking what I refer to as a development detour ... something where you take off 3-6 months to dig deep into an adjacent domain, which when you become competent, really supports your current thrust of work. for this, I'd highly suggest taking the time to learn 3D Mesh Modeling, specifically Blender. The concepts you'll learn will greatly support your gaming efforts from the primary objects you are looking to create to the effects you seek to create in supporting your game. Blender animations can also be exported as transparent PNG's which can be combined to form sprite sheets (very useful) fairly easily. I think it'd be worth your time. I would recommend you the book ""The Animators Survival Kit"", it's not directly targeted at 2D animation using computer software but more about general animation techniques (and quite entertaining to read). As you go trying to do higher resolution 2d animation, the thing gets more complex. A fast advice is to draw, in a, for example, 13 frames walk cycle, the frames 1, 7, 13. Then draw ibetweens of those, like 4, 10. Then the remaining ones. (is a bad example, tho) Heres a good tutorial to get you started on one particular method using Sprite Sheets. Sprite Sheets are just that, a sheet of sprite that you run in code similar to a flip book animation in real life. But you use code to do it. It can be difficult to get started, but I feel is the most rewarding and grants the most control. Climb aboard a brief Google of the name Sculptris. You should, in your journey, discover a three-dimensional modeling program of the same name. In my own two-dimensional-sprite-modeling endeavours, I too came to the conclusion that ProfVersaggi has. Unfortunately, I dissolved countless frustrated hours letting my brain slither over the program know as Blender. I found that I share a need with many others for a more rapid return on my ""labors"". Sculptris gave me this. Also, it's free as of this posting. A caveat; this program is still in development, and does experience crashing. That said, the creators of Sculptris were kind enough to implement a crash recovery feature, making the crashes little more than a mild nuisance. Open wide. Drink deep. Swallow no frustration. Digest only knowledge. Make us proud. To summarize, Sculptris is an easy way to create a 3d models for your sprites. Then, you can even press the ""prtsc"" or ""print screen"" button to copy an image of whatever is displayed on your screen to be pasted in a 2d animating software like Graphicsgale or a paint program like Paint Shop Pro. I simply cannot make it any easier for you short of doing it for you lol Here is the first thing I found using flash which is also a good tool. I personally have not used flash perhaps another poster can chime in with their experience. 2D animation is a whole art and profession. And actually a very hard one to master. Not every animator out there does good animation, indeed. But for basic animation as you said... It depends. It's mostly simple to achieve an ok level of sprite animation for mobiles using like 4 frames for a walk cycle 24x24 pixels sprite. You can do that indeed just with Gimp and using layers transparency to have the previous or next frame in overlay as a guide.(""onion skinning"") .It has other features to help in this. http://animation.about.com/od/2danimationtutorials/2D_Computer_Animation_Tutorials_StepbyStep_Lessons_in_Flash.htm",5
"That being said, this is rather unlikely. Less unlikely are brownouts due to insufficient power, which could result in data corruption or other errors. A voltage spike from the hub could damage or destroy any USB device connected directly to it. This is true for both self-powered and bus-powered devices, because the spike may occur not only in the power connection but also in the data connection itself. Another issue could be a data corruption in transfers that can damage the datas on the hard drive. But not the hard drive itself. I wouldn't worry too much because data storages have CRC check. If something goes wrong...copy gets interrupted with I/O error. I use the hub to connect two external hard drives to my laptop at all times. One of the drives is portable and the other one isn't. I recently read about somebody's USB hub crashing and frying the devices that were connected to it. Is it possible if this thing malfunctions for it to supply too much power or something? Can it damage my hard drives or laptop and if so, is there a different type of USB hub that won't do that? It's POSSIBLE, but I'd say it's pretty unlikely depending on the brand as you said is a cheap one it may occur due to USB does carry a small amount of power, and so I suppose if you had a bad hub that was sending spikes of power down the connector cable, it could cause problems. I have a self-powered 4-port USB hub. It's about as cheap as can be. I got it 5-6 years ago at the register of an office supplies store. I think it was $1 and it has no brand name. Modern drives and I/o controllers are pretty smart these days but prevention and not saving money is always best strategy.",5
"Sounds like you could probably just enable Internet Connection Sharing on your XP or Vista client ... the situation isn't much different from using one PC to share an internet connection.  Check out MSDN's info on NAT with ICS for more info.   I have never done it, but I have seen people setup a ""gateway"" windows box that has the checkpoint client installed, and then have that windows ""Gateway"" setup to do port forwarding of all their specifically desired ports (22, 21, 3389) to that remote network.  Looks like this site has a nice tutorial on it My org has a Check Point firewall.  I can only connect to it using Windows, despite making attempts with Openswan.  Is there a way I can use the Windows box to VPN into my org and then use the Windows box to NAT other local workstations into the remote network?  It seems like I might be able to but I'd like to know that for sure from a networking expert before I run down another rabbit hole.  :-)  Somewhat more complex is setting up XP routing (which internally might actually be the same thing as ICS).  I did this a few years ago when we were using OpenVPN internally, and followed these instructions.",3
"If there is another app that modified, like I have 'Edit with GIMP' here, new shell item appears. If there is something there, just check the command. With your accidental 'edit' of the image file, it may have caused the virus binded/masked to it to run. I was trying to open a .png picture today and clicked edit instead of open, which in my opinion wouldn't affect anything. However, most likely coincidentally, as soon as I clicked edit, a large number of cmd prompts opened and closed quickly. But since you accidentally clicked ""edit"", it was opened in MS Paint, which might use a different library that does not perform buffer overflow checks, therefore crashes and the crash causes the virus to become active. The virus manages to run a command via the command line. Some viruses can be set to mask or bind into images. That way when you open the image file, it opens up the virus too. The way most of these things work is, the virus is stored into a .rar file and then placed inside the actual image. But most other binders work too. It is possible to set up a debugger to launch automatically when a program is started. This is done via the Registry Key ImageFileExecutionOptions. Depending on what type of debugger it is, it may open a console window. When the debugger decides to detach, the console window may close. Browsing the file extension key in HKEY_CLASSES_ROOT will say the key for the shell menus. Now find that shell key in the registry editor. It might be that some virus/application modified the registry key that executes which command when clicked on edit. Check yourself first in the registry. Fix: no fix. Run an antivirus program. If the virus is very new, it may not be detected. Don't use the PNG, but maybe keep it if you want to contact an anti-virus vendor. Does anyone have any idea what happened and what can I do to fix it or keep it from happening again? the PNG file itself is prepared in a special way and contains a virus that opens a command prompt in order to perform a special operation (whatever the virus wants it to be). I noticed that the file location was on the headers of the cmd prompts. This is what it said, from what I gathered before it stopped: C:\<user>\AppData\Google. Depending on the program which is used to open the PNG file, the virus may become active or not. E.g. it may not become active in Windows Image Viewer, because that program uses a newer graphics library which checks for buffer overflows.  It might be that the program that is opening that file from the context menu might be faulty and it generates the errors. If that program is having errors, just delete that entire key, and the edit should ask you to select the program to launch. You should run an anti-virus scan your entire system and the image file, as soon as possible. And delete the image file so you won't accidently edit it/run it again.",4
"So classes seem great right? There is one major disadvantage: it is hard to create such a system, and much harder to create it right. This will require lots of thinking and creativity, and will never be perfect. Additionally, you will have to prevent new players from using very difficult cars, usually by a car unlocking system. And I am sure there are other disadvantages and advantages I missed or didn't think of, that will probably be pointed out in the comments of this awnser, but with dedication, you can overcome or mitigate these problems. This will also have a side effect helpful to your situation: a ""skilled"" player may only be vary skilled in one or a few classes. If you make certain classes have an advantage against certain other classes, insomuch as to cause a 50-50 winning-losing chance for that combination, a you get closer to your goal. The skilled player either chooses their car or picks a different one he's not as skilled in. Either way the good player would be uncheatingly challenged, and the worse player has a real chance. Your game is symmetrically balanced, which leads to the better player winning. A solution is to create classes of different cars. Have classes that don't take a lot of skill, and doesn't really punish the player. And create skill-based classes that are difficult to utilize optimally and/or punishes harder for mistakes of skill.",1
"I am not a swift programmer so i will not be able to give you exact advice on how to improve the quality of your code but rather the algorithm you used to solve this challenge and nitpicks i have with your solution. I've tried several variations and appart for a minor improvement on the offset calculation (to allow negatives to rotate to the right), I couldn't find anything meaningful to add: Lastly, Swift's array class has methods to make this much more elegant. You should be able to implement this without a loop & without any vars. (Read up on the functions dropFirst and dropLast.) First, whenever you see a phrase like ""given x, perform y"" you should think of the 'x' as inputs into a function that returns the result of 'y'. Second, the problem is underspecified. What if the distance to shift left is negative? Your solution will crash in the middle of the algorithm. It would be better to use the precondition function to make it explicit that this algorithm won't work with a negative count. 1 for loop up until the ""break"" where the array has overflown, and a second for loop for the rest of the untouched array. This will shorten your code considerably and make it run a whole lot faster because no extra variables are needed and you arent doing array manipilation, its just a way to iterate over an array So you should have something like func rotate(array array: [Int], left distance: Int) -> [Int] somewhere in your code. Getting the data from the user and displaying it should be separate from the function that does the work.",3
"The 3rd party agents will often have ""brick level"" backups and restores which allow you to restore parts of a database (tables, etc.) rather than only the entire database.  They also do not require you to stage the output (doubling your disk requirement) by backing up the database directly. Although SQL 2008 has it built-in, SQL 2005 and below benefit from many third-party backup compression capabilities. I am currently looking for some new backup software and have noticed that a number of the products out there charge a lot for the SQL server agents i.e. around 500. Could anyone tell me why these addins cost so much when SQL server has built-in backup. Would you advise against using the MS SQL server backup functionality and then backing up the output, if so why? Some third party products, such as Quest's LiteSpeed and RedGate's SQLBackup, will also provide you additional data level compression and encryption options unavailable through SQL Server's native backup mechanism.  This is much favorable to options at the NTFS level. I can't say that I would advise against using MS SQL backup functionality because it works well and is obviously free (included in SQL Server), but if you're looking for more bells and whistles, or looking for a more integrated recovery solution, then the third-party tools are the way to go.",4
"But if you really want to lower your CPU temperature, then I suggest you buy a new heatsink. Stock heatsinks aren't very good at cooling your CPU to ""your satisfaction""   If you don't know where to start for your heatsink, try here http://www.tomshardware.com/reviews/cpu-cooler-heatsink-roundup,2788.html Yes, it could be the thermal paste, or even not quite a flush mount. I would suggest picking up some new thermal paste from either an online resource like Newegg, or from a local retailer like Fry's or Microcenter and follow the instructions provided with the product closely. Additionally, make certain the heat sink is flush, and mounted firmly with all 4 pegs. If even one is loose, the gap created could cause the problem you are having. Thermal paste is VERY important for CPUs and GPUs...  it's not typically 24-degrees C different... but a must-have none-the-less.  It's more likely that you didn't seat the heatsink/fan combo correctly.  (sitting at angle... not all 4 posts are seated on motherboard... etc...)  As a tip... remove the heatsink/fan combo by turning the black pegs 1/4 turn then pulling them up.  they should pop up fairly simply.  (don't force them... )  After removing the heatsink/fan... apply a dab (1/2 pea-sized-ish) of thermal paste... (don't go for cheap white paste... get some good stuff like artic-silver) and spread it around.  Don't put gobs of the stuff on there... as you don't want it to go places beyond the CPU... you just want enough to cover the cpu and not ooze over the edges.  To re-attach the heatsink/fan... make sure all 4 posts are in their upright position... turn them back the 1/4 turn to the locked position... line up the white pastic bits with the 4 holes on the motherboard... and push down until they lock in place.  Make sure the peg goes through the hole... and the plunger locks in place.  It should be locked rigidly in place for each post.  (no wiggle room) First rule of thumb. Always clean and replace thermal grease when you reattach heatsink. If your heatsink comes with thermal pad, besure to remove them before apply thermal grease. For instruction on how to apply thermal grease, please see http://www.tomshardware.com/reviews/cooling-air-pressure-heatsink,3058-9.html",3
"If I logout and then in again I get don't get that login screen, which is perfect. But if i close IE, open it again and go to portal.office.com again the login prompt comes up. The SSO have never really worked properly, so i'm going to contact them and about it. But before I do so, I just wanted to make sure that it isn't working as intended. So my question is: Is what i explained above correct behaviour for a properly set up ADFS SSO, or is there something amiss in the configuration? Isn't the point of ADFS for it to automatically log me in using my AD credentials? In my world that login-screen should never come up. In skype for business all i have to do is type my username+upn and i get logged in for instance. When i go to portal.office.com and type in my username+upn and click in the password field I get redirected to another login screen (see image below, its in Swedish, but you should get the jist). After doing some more research i found this TechNet article that got me onto the right track. I did not know that the ADFS Endpoint needed to be in the trusted local-intranet site list. After adding this the ADFS works flawlessly. Will make a GPO for this!",1
"I think you're doing premature generalization.  Don't get stuck too much on future-use questions right now.  Pick an answer and run with it, learn from it.  Flip a coin if you don't have a preference. And for 'framework vs library', I would base it off of how tired you get of writing a game loop over and over again and stuff. ;)   Start by building a game.  When you build your second game you'll have a better idea what parts are re-usable and can be turned into robust library functions.  When you build your third game you'll have a better idea what structure is re-usable and can be turned into a robust framework. Mike Acton and insomniac games (www.insomniacgames.com) have written a lot of topics and discussions regarding game development, in particular data-driven. Look them up and see what kind of information that is too complex and which you find interesting and understandable. They're great developers with a ton of experience. If you're not sure which way to go - just do whatever your game requires. Try to write small, basic libraries for stuff that you feel might be usable in the future. Look into data-driven software development. I suggest looking into component-based entity systems. Don't base the code around things that are unique such as a player. A player is just another object, just like anything else. An entity has components, perhaps a list of components.  Writing small stand-alone components often increases readability and debugging possibilities of your code.  Don't worry about the future quite yet.  Worry about your current game, now.  Otherwise you are not going to get anywhere because you are fretting on the details.   By writing small, contained libraries that interact with each other, you can go ahead and determine which ones to re-use for your next game. Personally I have things like a ""container"" library for data-storage types. I have a math-library for such things. There are several things like these that you can sort out and write as modules. Camera, effects, input, entity, movement, physics, rendering, resource-handling, threading, the list goes on.  You should concentrate on building the game first and then later, if the game was successful enough, you can extract it out into something reusable for your next game.  This will keep you from being locked in to something else and give you full control over your project.",4
"Does the server NIC that's bound to RRAS have DNS servers configured in the TCP/IP properties? If not then that's the problem. When using a static ip address pool for VPN clients the RRAS server assigns the same DNS servers to the clients that are configured in the TCP/IP properties of the server NIC that's bound to RRAS. You probably want to configure the RRAS server NIC to use your internal DNS servers, that way VPN clients will use them and be able to resolve internal DNS names. If you use static IP addresses for RRAS clients, it's basically a same approach as if you'd use static IP addresses for the computers in your LAN. I searches for a solution but the ones I found are about RRAS setup with a remote DHCP server.  But in my case it's the RRAS server that hands out the ip addresses (option for redirection for WINS, DNS is ON and set to the LAN NIC). I have a RRAS server setup on a Windows 2003 machine with two NIC's. The VPN works like a charm, I can ping all the other computers on the network. But it fails when I try to access resources with hostnames. When the client has connected to your local network through VPN, the client behaves more or less the same as your other LAN clients. You should configure the VPN client Network Interface Card (NIC) DNS servers / WINS server just like you'd configure the computers on your LAN. If the VPN client will have the internal DNS server configured, then the VPN client will be able to resolve internal DNS names through this internal DNS. If the client will be configured with the external DNS, the DNS queries from VPN client will traverse through VPN, through your LAN network, through your LAN gateway to your ISP DNS server. FQDN is a Fully Qualified Domain Name - i.e. microsoft.com. <-- note the dot at the end. I.e.: 'cat.animal.com.' is FQDN, 'cat.animal' is not a FQDN, nor is 'cat'. From what I've encountered so far, Windows Server 2003 configuration files sometimes skip the trailing dot in FQDN.",3
"You need to be familiar with the concept of short-time Fourier transform (STFT). Basically STFT tells you what frequency components exist in your signal at each timestamp. The result of STFT (its squared magnitude, to be precise) is called the spectrogram, which is what people usually visualize. An example of spectrogram from the link above: The most commonly used speech feature (as input for neural networks) is the Mel-Frequency Cepstral Coefficients, or MFCC, which carry the similar semantic meaning as the spectrogram. Other commonly used features include PLP, LPCC, etc which you can google for more details. But directly feeding the result of FT or STFT into a neural network is not the best practice. You may refer to matplotlib.pyplot.specgram or scipy.signal.stft regarding how to plot a spectrogram in Python. The vanilla version of Fourier Transform (fft) is not the best feature extractor for audio or speech signals. This is primarily due to that FT is a global transformation, meaning that you lose all information along the time axis after the transformation.",1
"take a patch cord cut it into, next remove about 2 inches of the outer shielding to expose the twisted pairs, cut off the blue/white pair, orange/white pairs, and the white wire from the brown/white pair. Certain toners or network testers mainly high end fluke brand ones will tone through a live patched cable..  Most other toners are not very practical on ""live"" lines.  But are great on lines that arent patched into anything at all (fresh install) Data switches and computers ""eat"" the tone when applied to the twisted pairs.  1&2, 3&6, 7&8, 4&5.  when toning out the cable use split pairs.  For example apply the tone generator to pins 2 and 7.  THIS WORKS GREAT! place 1 of the clips from your tone generator to the green/white pair and the other clip to the brown wire this will allow you to short out the active port on the switch so you can tone out the cabling without harming the port on the switch. Cut a patch cable in half and only connect one toner lead to one wire. When the cable is plugged into a switch you must only tone a single wire not a pair My experience with tone generators is that you're going to be touching the pins on the jacks on the patch panel, or, if you've got a really sensitive probe, the terminations on the back of the patch panel. I've never used a tone generator / probe combination that was able to pull tone thru the insulation of a UTP cable. this will leave you with the green/white pair and the solid brown wire, next remove the shielding from all 3 wires twist the green/ white pair exposed wires together.  I don't know that I'd plug a tone generator into the far end of a cable that's patched into a switch, either. You probably won't blow anything up, but I wouldn't try it.",5
"http://ask.debian.net/questions/what-is-the-best-way-to-create-non-interactive-installers-that-include-a-custom-set-of-packages Ubuntu has a couple of option including kickstart used in Redhat. Another very interesting option is Preseeding I tried making a deb package, however it doesn't allow me to update ubuntu while running a deb package. I recommend Puppet or Capistrano, which are two of the most well-known for this specific task.  I know alot of developers that love Capistrano, but I'd look into both.  Puppet is pretty easy to set-up.  Shell scripts work, however, using an automatic system like these two option will allow you to set up multiple environments and automatically deploy certain applications to certain environments giving you complete control of post-install installs. I need a really simple way to automate software installation after a fresh install of ubuntu. I currently use a shell script to install the software and packages which clocks in at 800+mb which includes updating ubuntu. However, after copying the files, due to permission issues and the need to open a terminal to execute the script, it is quite user unfriendly. I need a solution kinda like a installation wizard or somesort that will allow me to execute in a single click.",4
"Our company's current customer-facing site & intranet site is currently built on Classic ASP running on IIS 6.  The current site only contains about 10 customer-facing pages, and an internal site that manages HR records, scheduling, etc. Find someone willing to offer you colo in different racks (on different power feeds), on two access switches to two core routers to two very different uplinks. Your concern about load balancing across ISP's makes me think your more worried about redundency than load balancing because you will need to worry about keeping the data involved sync'd between the different data centers, which is a bigger worry than how to load balance across them. Once the users' traffic arrives at a particular datacenter from there you want a loadbalancer component.  There are tons of options here that can be broadly categorized as appliance-vs-software and layer4-vs-layer7.  Judging from the details you did give I'm gonna bet your needs are pretty simple and your budget pretty small so lets just skip straight to nginx for this part.   Within nginx you can configure it to serve your static content and to loadbalance your dynamic content across as many backend servers as you like running your python application. Also, are there services that provide a way to direct the traffic in the cluster so that we do not have to host it ourselves?  We need to be able to distribute our traffic between the 2 data centers, but if the WAN link goes down at either, it can't affect the ability for the cluster to direct traffic to the cluster that is still available. Recent versions of CherryPy (3.0.4 and 3.2, for example) include a fix to the WSGI server to make this more robust. Previous versions would accept new connections and hang on to them even if the incoming request queue wass full (that is, if all worker threads were busy). Now, you can set server.accepted_queue_timeout to 0 if you'd rather have them be rejected right away. Closing those connections immediately allows ZXTM to try to pass the connection on to another node right away. CherryPy's ""server"" shouldn't be used at all in production, it's great for development but you really should use Nginx and a Fastcgi setup in front of your CherryPy app.  This will give you better control over the load of the server and how many instances of the CherryPy app you need to run to manage that load. There are three main ways to do multi-datacenter: BGP/""anycast"", GSLB/DNS, or using an origin-failover mechanism within a CDN.  none are simple, easy, or cheap. Management has decided that we will use this rewrite project to finally develop our online retail store.  They want us to use a geographically separate facility (also on a completely different ISP) to provide a failover in case the WAN link at our primary facility goes down. Depending on how much budget pain you can handle is what will drive the decision to either use a hardware load balancer or a software solution.  If you are looking for redundancy you can have the server setup mirrored at another site and then change your DNS records to point to that site in case of failure.  Anything else will require hardware solutions that involve the alphabet soup of acronyms like cagenut mentioned above. Are you sure this is what you need? Ask your management a figure on acceptable downtime, they may very well lower the bar to realistic levels. We have looked at the various Python web development frameworks, and CherryPy seems like a good fit for what we need, which is a minimal environment for serving Python-generated content.  However, I am having trouble finding information on using CherryPy with load-balancing and failover technologies. We have chosen Python for the rewrite as we want to be able to move to a Linux-based platform, and we currently use Python in other in-house development projects. It would appear that we will be forced to host CherryPy on Apache in order to leverage a failover/load-balancing cluster that supports sticky sessions.  Is this correct, or this there a way to do it using CherryPy's internal server or a different HTTP server altogether? Two completely independent sites puts you in the big boys category, with matching pricing on the solutions. There are quite a few sub-questions in your question that really should be their own entries - but let's tackle them anyway...",5
"what is a setting ""SMTP only""? There is no any reference about it on official site postfix.conf.5 and www.postfix.org/STANDARD_CONFIGURATION_README.html I need to have email setup just to send out registration confirmations and new password requests. No one will have a mailbox on this server. Postfix Null clients means there are no local clients that it delivers email for.  So you can still send email over it, but email is never going to traverse its way back to you via that server.   If you want to use your server only for sending service information, you cat use Postfix on a null_client An SMTP only server can also be a null client server.  Since SMTP is only for outgoing email a user could never pick the email up via POP/IMAP etc.  But this machine could still be responsible for the email, so it is in my mind not always a null client server.  For example email could be passed from here to an exchange server.  So in that use case, the SMTP Only server is not just a null client server.",3
"And on the topic of disk failures, the issue RAID6 has is the same as RAID5 - parity bit calculations are complex, and take a lot of time, resources and I/O to do, meaning that you're actually at a bigger risk of array failure during a rebuild than you'd think, and I've actually seen the nightmare scenario of a RAID6 array (with a hot-spare, even) failing during rebuild - not terribly shocking considering all the I/O a rebuild takes, but terribly devastating if it happens to you. And you'll need true backups with some kind of retention for the same reason.  Not having any backup means that data corruption could take out the whole thing as well, because if you're replicating the one system to the other, you're likely to replicate any corruption that occurs as well. If you decide to go with ZFS, then go whole hog and set up NexentaStor. That's probably about the safest you'll get with your data in its intended, uh, home. You don't need the best possible RAID  simple RAID5 would suffice. RAID is a high-availability solution, not a data protection solution. What you really need is a very good backup solution  one that will backup automatically, often, keep several old backups and backup periodically offsite (this is not automatical  it can be a for example a set of disks in a deposit box in a bank). If your data is truly that important that losing it means bathing with the electrical appliances, then there's no way around needing a second server on which to mirror the devices, and having true backups.  Anything you do on just one server has at least one single point of failure.  (At a minimum, the server itself - for the most basic example, what if there's a fire?  Or a the PSU goes out and takes everything else with it?  Etc.) As a final note, you don't want to kill yourself by electrocution.  It's quite excruciating, so you'd probably be better served by a different method of suicide. And since no one's said it here yet, RAID6 and big SATA disks is not a good combination.  It does provide more redundancy than a RAID5 setup, but not by enough. On such large disks, you're almost certain to have a read error on one of your parity stripes, which really means that on one of the disks, somewhere (but you won't know where), you're effectively running in RAID5.  Which is all well and good until a disk fails.  Then you're running on failed array RAID, which is too big a risk with critical data. If you're going to go the RAID 6 route, then do yourself a favor: Visit eBay and pick up a couple of PERC 6/i cards (one as a spare) and two SFF-8484 to 4xSFF-8482 cables to hook up the drives. You'll thank me later. Given some of the trade offs you listed ditching DASD (Direct Attached Storage Device) might be a good idea.  You may find looking at something like GlusterFS (Red Hat Storage) or CEPH more useful.  Gluster for instance replicates data over a cluster of file servers, where that cluster can lose one or more disks, and potentially one more more nodes and maintain data integrity. Overall your list lends itself to two or more solutions.  You may need to use fast DASD or SAN for a RDBMS, in which case you will want to spend time and money investing in mirroring and so forth.  Other use cases may be very well suited by something like Gluster which can allow for easier scaling and can abstract you from needing to know the intricacies of each RAID type. And think very carefully about a backup strategy; neither RAID nor ZFS are backup. Backup of terabytes of data is not easy or cheap; it's not usually something you can do over the Internet, and terabytes of ""cloud"" storage would set you back a lot more than the drives anyway. Anyways, just before using RAID6 take drills for loosing 1, and 2 disks. Go even further  take out 3 or 4, then make it working back. With LSR it's quite possible. And piece of advice  use its bitmaps, it would save hell-a-lot of the time for you. The simpler is the better. LVM on LinuxSoftRAID-6 looks like quite reasonable. Another important thing left to consider is FS on top. In Linux world up to this moment only Btrfs has built-in data verification mechanisms. But I doubt its developers would share your attitude regarding bath-n-toaster in case. ;) I'd recommend running it in 2xRAID10, for what it's worth - you get performance benefits, as well as some measure of redundancy.  Rebuilding a failed disk in a mirror RAID is a lot faster, and therefore safer, than you'll find in any parity-based system.  If it matters, I have a 12 disk RAID6 array at home (12x2TB near-line SAS), so I definitely do like RAID6, but it's just not safe enough for critical data.",5
"By the way most makers won't tell you that the range is open-air, nor the fiddle factors. It makes for bad PR. Wireless networking / WIFI works by reflecting electromagnetic waves across surfaces (up to a point), therefore the aerials may be at the bottom of the router and you may be creating an effect similar to a Faraday cage by placing it on the ground where by no signals are ""getting out"". The distance that toroidal pattern reaches is proportional to the inverse square law, the power output, the radio frequency, and a handful of other factors. But as the angle to the horizontal plane increases, so does the attenuation. In electromagnetics, the ground plane is very important - it determines the total performance of the antenna system. Sometimes a ground plane with well defined characteristics is essential (eg aircraft landing systems use huge big metal rods on the ground and pinned in, to ensure a good ground plane). This depends entirely on the router. Modern routers with multiple antennas can even shape their coverage dynamically. And there's another reason why the wireless LAN frequencies are free to use: they have lousy performance. At 2.4 GHz the path loss (which gets higher with frequency) is terrible, there are lots of interferers (microwave ovens, Zigbee, Bluetooth, DECT phones, wireless doorbells, baby monitors, headsets, the list goes on and on). So poor performance from Wifi is normal at all times, and when you put the equipment close to the ground / floor it should be expected to get worse. These systems don't rely on reflection (as stated by one of the other answers), reflections are really bad in RF systems, they causes peaks and troughs (nulls), and in the null you can see received signal power drop by a factor of several hundred (in RF speak, > 20 dB nulls are common). Just moving equipment a couple of inches can change performance. If you see this, you had reflections and suffered from a multi-path null. In the case of something like wireless LAN, the nature of the floor material has a big impact. For example, there are huge differences in the electro-magnetic properties of a timber floor over a cavity vs a reinforced concrete floor. The concrete is full of steel which can form a form of Faraday cage (ie not much will go through it). However if you are on the same level, and not trying to get the signal THROUGH the floor, then its far more likely that you are just seeing absorption in the floor. When it is a foot up, it gets much better. Does this make any physical sense? What it the typical geometric shape of the wifi coverage? Is it a fixed radius with the wireless router at the center or does it plume out? A great many modern WiFi systems have multiple antennas. This is used, usually during reception, to pick the one with the highest signal (and discard the others, you CAN'T combine them at RF). When transmitting, normally one will be used (transmitting on all causes a phased array to be formed making a directional signal - not usually desired.) Such an arrangement makes the router less susceptible to the vagaries of multipath reflections. Without photographs of your room and a detailed survey, there is no way to know for sure, however based on the fact that you have observed this behaviour, I would say that obviously yes it is possible! A rule of thumb for in-building operation is to take the open-air range quoted by the maker and divide by a factor of between 3 and 10. What the factor is for your building depends on building materials and building contents. Think of the floor as a low impedance path (lower than the air), and so the propagating wave is much happier to zap around in the floor than in the air. In this case instead of the floor being part of a ground plane that forms an integral part of the radiating properties of the antenna, its just a big sponge. Systems like wireless LAN always have range quoted in open air (usually with the equipment mounted 1 metre off the ground), because this is the only means of getting a repeatable range measurement. When my router is on the floor directly, I get terrible reception (even none) in my room which is 40 feet away. In general, a simple vertical antenna has a roughly toroidal coverage area. That is, its signal tends to stay within a few degrees of the plane perpendicular to the antenna. It depends on how the antenna is mounted or grounded, and on where it's situated relative to Earth ground, metal objects, and obstructions, all of which will attenuate the signal. If the router has a movable antenna, it could help to try different positions, or if not, to try different (rotational) positions of the router itself. All of this can influence the coverage.",5
"If you are still nearing 100% utilization, talk to your programmer and see if he can improve things by making more efficient DB queries. If this isn't an option, then throw money into some faster disk hardware (move to RAID0 or SSD disks). Investigate what else is running at this time (DB dumps running? backups? slocate/mlocate?) that may be causing I/O bottlenecks. I'm going to assume this is a SATA disk system so 181 TPS is about the limit. You can try iostat -x 1 to get some extended statistics (example below). Note the %util column. This will tell you how much the read/write load the disk is under. I would guess this to be nearing 100% and causing the database problems.  Another metric is the svctm. This tells you how long it's taking the disk to complete any given I/O operation. The higher the number, the worse the situation. Disk utilization can also be caused by the system having too little available RAM. As soon as memory becomes tight, disk swapping starts and everything comes to a crawl.",1
"Just Move to a cell which is in range (the one not broken of the Range), click on Format Painter and then paste on whole Column. Again it will show Where it has broken, you just need to do a Format Painter to the Cell which has broken the range. Now, this too may seem a bit lengthy, you can just build a simple macro  for this. You extend the area by selecting the last row/column and ""copy-drag"" on the small square at the bottom left of the ""cursor"". This way the ""Apply to""-range should remain intact. (Note that this will not extend any formulas on rows or columns like insert does.) This works for me when I have conditional formatting that's applied to columns, and I usually set the formatting for the whole column, eg. $F:$F. It should still work if you're formatting for a partial range, just make sure that when you're done adding/removing and resorting that all the data you want formatted is still within your original range parameters. I sometimes also define a name for the area I want to format. It won't be used in the rule, but you can clean the rule up by removing all rules but one and use the area name in the ""Applies to"" section. I've experienced a very similar issue. I've made a few macros to add rows, and copy down formulas, and then adjust columns and row sizes to format how the sheet looks. I've found this issue occurs in one of two occasions. 2) When there are merged cells inside the ""applies to"", and any of the rows or columns get adjusted. It appears during the merged cell issue, that excel has to unmerge everything, recalculate its conditional application, adjust all the cells (add or delete rows or what not) and then remerge them back. Its invisible to us but that seems to be how it is applied. I've found that rules are very easy to break, but here's something you can try that don't seem to break any rules. If you paste without format (Paste > Paste Special > Unicode Text or similar) the special formatting will not be copied, and as such the ""Applies to""-range should not change.",4
"The only other attribute that I can think of that might matter for a reset is userAccountControl - it'll be used if, for instance, you check the ""Password never expires"" box. First you should use the delegation wizard rather than manually setting passwords.  This ensures that the proper permissions have been set.  In this case these are the correct permissions.  Second make sure the user is a member of the correct group you have delegated rights on.  Lastly ensure that you are trying this against a DC that has had the changes replicated to.  You can force replication to the DC that the user has authenticated against (which will be the one that the snapin connects to by default) or connect the snapin to the DC that you have made the changes on. 3) The user account that you are trying to reset might be protected from permission inheritance. This happens if they are a member of certain built-in AD groups. Inspect the user account in question and make sure that the admincount attribute is 0. If it is 1, then it means that it is or was a member of a protected group, such as Account Operators or Backup Operators. Active Directory does not allow permissions to inherit to these accounts.  If that doesn't work, enable security logging for AD changes, then watch for which attribute is failing in the audit log - but before doing that, start with the basics; make sure the user making the change has allows (and no denys) for the right items in the effective permissions tab. 1) The delegation is not inheriting correctly down the OU structure. Inspect the permissions of an actual user account object or sub-OU in AD and make sure that the group that you are delegating to is listed correctly. 2) You did not delegate the correct permissions. For simple operations like Reset Password, there are pre-defined ACEs in the delegation wizard. Run the delegation wizard on the appropriate OU and check the ""Reset user password"" option. This will simplify your direct interaction with the AD permissions to rule this out.",3
"If you right click on the application executable, click properties then on the compatibility tab select ""Run as Administrator"" then click apply, you should no longer get the warning box. Basically you just download the program and enter in the .exe location and a name for the file and the icon will be placed on your desktop.  The service will run in the background to allow easy access to any of your account restricted links. I was getting UAC prompts for CPU-Z and Malwarebytes, even though I had configured them to run as administrator, and in own memory space etc. I had to go to administrative tools, local security policy, security options, user account control: behaviour of the the elevation prompt for administrators in admin approval mode. In that you get a few options for behaviour, selecting the elevate without prompting got rid of annoying warnings and system hasn't been compromised to date. You have to run, as administrator, the correct ""Compatibility Administrator"" program.  There are two of them, one for 32bit applications and one for 64bit applications. I had that problem with one app. At startup popup a window to permit it to run, I used many answers but the solution was to click on Unblock in Properties. To disable UAC prompts for one application only you have to use the Microsoft Application Compatibility Toolkit:",5
"I have just successfully dealt with this very same issue with this particular motherboard. In my case this situation was caused by a faulty RAM stick. ou will need to determine which hardware part is causing the problem in your case but before we get there, let me first explain what's going on. So, what you need to do as a first step is plug in a speaker (see last paragraph for tips), as Ash suggested. Then power on your computer again and wait for the beeps. Refer to your motherboard's manual to find out what the beep codes mean and identify the problem. I was unable to find them in my manual and the only relevant thing I found on AsRock's website was this page from their FAQ section. Supposing what is written there is true for this motherboard you could still hear no beeps which would indicate a faulty CPU (ouch) or RAM. After you find out what the problematic part is you should contact the shop you bought it from and invoke your warranty if possible. Many computer cases provide a speaker for your motherboard in their front panel. If so there should be a cable along with the other cables coming from the front panel (LEDs, power, reset, USB ports, etc), which should be labeled as ""SPEAKER"" (don't confuse it with the ""HD AUDIO"" cable!). Moreover, if you have an old case or motherboard you can look for a speaker there, unplug it and then plug it in your current motherboard! The reason you get no video output is because the POST (power-on self-test) fails, which usually means some hardware is faulty, and so it doesn't even load the BIOS/UEFI and gets stuck there. When POST fails you should normally expect to hear those beeps which represent some code to let you know which part is faulty. However the AsRock H97 Pro4 motherboard doesn't come with an integrated speaker and so it is unable to sound the beeps!",1
"It's very difficult to say if a web application will handle X number of clients on a particular system. It's heavily depends on many factors including how the application was written, frameworks used etc. I have a link sharing site (think reddit). It is hosted on godaddy deluxe shared hosting now. I am planning to migrate it to aws. I have seen an aws webinar on scaling till 10 million users. But basically it just says beyond an extent you can't scale in a vertical manner so choose to scale in a horizontal manner from the beginning itself. Start with say t2.micro, as the user base grows simply upgrade the ec2 instance. The same goes for the RDS instances. Please keep in mind that i have no one to assist me and this is a solo effort. I can find someone to assist on the aws side if it hits users. Being able to scale horizontally will provide a much better route for handling growth in the future. What happens if the site struggles on the larger instances, or you need to scale past 100,000? You may also find that just moving to bigger instances will likely not return a linear increase in performance. I know that 1 EC2 or RDS makes it a single point of failure. If any of these break i plan to terminate that instance and start an identical one from an image/backup. I am okay if the site is offline for 1 hour every 30 days. Store the images, and anything else that you don't specifically need application servers to handle somewhere else. As you're already using an AWS stack, S3 is the obvious place for images and other static data. Don't waste application server time on static resources. I don't need to support 10 million users. Assume that i need to support 100,000 users daily. At most the site will have say 1000 concurrent users. The best option would be to find something you can use to benchmark/simulate usage of the application. As you already seem to have the web application running you could easily set up any size ec2 instance for a short amount of time, run some tests, then shut it down.",2
"Finally, int main(int argc, char *const *argv) isn't a standard signature for main().  In a portable program argv should be an array of mutable pointers to characters. The help message doesn't indicate what's meant by ""special"" characters.  I'd call them something like ""punctuation"" instead. Traditionally the command lines are built mentioning the options first and then the remaining arguments. GNU allows to mix them freely. Your program even requires the options to go behind the arguments. There is no reason for this restriction, so you should follow the style of other programs and use optind, too. Your use of strncat fits well into the GNU universe, where obscure string functions with a bad API are used all over the place. Everyone else would have simply used strcat or even memcpy. There's really no need to allocate memory to hold chars.  For this small string (whose life is within the function scope), it's going to be much more efficient to have a local (stack) variable large enough to hold the largest combination of character groups: At this point, we know a non-zero length is required, but we output nothing and return a success status.  That could be misleading, as the calling program might assume the success status means we have given it a strong password. We should either exit(EXIT_FAILURE), or (more usefully) default to including all character types.",2
"Perhaps taking a look at the assumptions around the fog of war.  In my experiences with RTS games, the fog of war exists immediately at the start of the game.  But perhaps instead, put it to the players to create the fog of war. There's a reason no-one made space games like this. It's so difficult to control. Even just catching up to a ship and matching speed with them is difficult. So the player would simply click the 'match speed with target at optimum firing range and arc' button and let the helmsman take it from there. The helmsman a.i. would attempt the maneuver, and would report back if they failed ""sir, the enemy ship is retreating and they're too fast for us to catch up. Should we abandon the chase? y/n"" or ""they're too manueverable. I can try to get us a firing arc but we'll be outside of our optimum range y/n."" And, by the way, concentration is flat throughout the game. It's just that you focus less and less on planning and more and more on using your assets, but the focus doesn't increase, it just shifts. There would also be default a.i. routines to allow them to react to surprises without input from the player. If they spot an enemy, the a.i. would decide to fight or flee, accelerate to a speed that allows evasive manuevers etc. One question might be to ponder whether you actually want to make an RTS at all.  The ""real time"" part of the game encourages tactics to appear beneath the strategy, and tactics are always time critical.  You might want to make a turn based game instead. I'd also point out that Poker meets most of your criteria, especially when played face-to-face.  You can't gain an advantage from betting faster than your opponent, so you have all the time in the world to study their face and try to figure out what they have without revealing what you have. Considering that reason, would there be another way of reaching what you want without breaking the game ? I'm going to attempt to address the lack of information due to the fog of war, and ways to mitigate that. Or perhaps you want to make a hybrid.  Perhaps you can make a slow strategic game in which armies enter ""battles"" instead of actually fighting each other.  Then, on their own time, players join those battles and fight in them RTS style.  This would feel similar to how RPGs often have an overworld which then enters a ""fighting mode"" when you run into a monster. Another assumption in your question seems to be that players will have no idea what the other player is doing. However, in games like Starcraft, some units exist with the intention of being able to scout for information. Or there are some other means of revealing information like scanning from a Terran Command Center. The balance in RTS come from that basic relationship, which itself has been exploited for millenias in various pre-electrical strategic games. However, it may be quite hard to apply that to RTS : it would imply that players are already met with various and complicated moves available right at the begining of the game, and that this complexity doesn't change through the game. Strategic games are about setting and using assets on a field in order to reach your objective against one or more opponents with their own objectives that are (often) mutually exclusive with yours. There could be a way to regenerate these units automatically (a clone bay, in space) and to auto-repair the buildings. And there could be an automatic upgrade program which upgrades the units/building as time go by.  At the begining of the game, you have almost nothing, so the variety of your moves is at its minimum of the entire game. Meanwhile, since the game just started and you haven't built anything yet, the variety of moves you can plan is at its maximum. Your strategies aren't yet limited by the options you would have sacrified by prefering others and not knowing what your opponent did - since he hasn't done anything either - don't guide you toward a path in particular. Rather than making micromanaging easier, I intended to discourage the player from micromanaging by making it very hard. Like, impossible. To answer your question, you could perfectly design a game which flatens the player's need for concentration simply by flatening the moves the player has : If the complexity of the player's opportunities is constant through the game, it's concentration will reach its maximum right after the begining of the game, which will also be its average. That's how FPS work, by the way, and MOBA in a lesser extent. I can't recall or find it at the moment, but I believe one of the Red Alert games had a structure that created a black out on the other players map. This was a counter to players being able to launch something like a GPS satellite and have full map vision. I started making a game a while ago (never finished it because I am lazy/terrible at programming), which was a deliberate attempt to make an a.i. that was much better at the tactics part of the game than a human would be, in virtually all situations. Reshaping the battlefield to your own purpose so that you get the edge and win is the basis of strategic games. Remove that mechanic and you are stuck with something that looks like a RTS but isn't, like dummy food. But if you start with the assumption of full map vision, and make it part of the early game to decide whether to build obstructions to that vision or build buildings and units that would be visible to the opponent. You could then cut down on the lack of knowledge in the early game, and provide some more meaningful options as well. If you are interested in that aspect of games, look at Game Theory, that's the name of the mathematical field that study the behaviour of rational agents being able to interact with a system in various ways, which apply to actual games as well as more abstract situations like negociations, conflicts or parking your car. Like reactives and products concentrations in chemical reactions, board games have two composants that evolve as time goes : RTS are like board games like chess or go, except that your opponent's moves are usually hidden from your sight. It was a space naval battle game. While I built the ability for the player to directly control the engines, guns etc, I also had (or at least started to develop) a.i. routines to micromanage those things for you. The idea was the player was the fleet admiral. They shouldn't be firing the guns, they should be issuing orders down the chain of command. That RTS's increasing need for concentration is unnecessary, or even that the begining of a game is devoid of it is your assumption. The problem is that with so much going on, many inexperienced and even experienced players may forget or miss information this way. So another approach might be to make that aspect of the game more approachable for novice players. Perhaps a easier to use scanning ability, or some form of automated spying. You could remove the parts where the user has to create their base and their army. So when the player starts the game, the base and the army they have is all that they'll get for the rest of their play session.  Secondly, I used actual Newtonian physics. Engines provide linear acceleration and rotation based on force x distance from center of the ship. There were no speed limits. Bullets were fired off with a muzzle velocity relative to the velocity of the ship they were fired from. Firing guns forwards propels your ship backwards. Getting hit by a bullet on the front right of your ship both moves and rotates your ship.  What makes a non-twitchy RTS tricky is the limited interface that we have to our computer.  It's very good at making well timed decisive actions.  It's not so good at providing nuanced feedback.  If you have a non-twitchy game, it's going to have to have nuances or it won't keep the attention of anyone.  Nobody wants to play a murky game of tic-tac-toe at a slow pace. Fair warning: if you don't find this sort of game play in the RTS world, that means there's a good chance that your market of gamers don't find value in this sort of game.  Always be careful with the ""if you build it they will come"" argument. Either remove, or disincentivise the ability or need to micromanage tactics, whilst allowing and incentivising the player to focus on strategy. The fact that your units stand under a bomber's path in TA unless you tell them not to is a flaw in the game. They should dodge even if you don't tell them to. As time goes, you will make moves, these moves will give you opportunities. The more concrete means of interacting with the game you have, the less you will have room for planning, since each choice you made closed some of those that you didn't choose. Take the example of your first base : Once you've built it, you won't be able to build your first base again. You will certainly build others, but hopefully you will take the positions of your assets into account while deciding where to expand. That's, I believe, why you can't think of a single game that works like it, or even how you would do it yourself.",5
"I right-click on the file and select Open With->Choose Default program. In the ""Open With"" dialog, I select the browse button. I choose Program Files (x86)\Microsoft Office\Office11\msaccess.exe and press ""Open"". I have seen cases where some dialogs don't prompt you for security elevation and do not give you a message that the change could not be made.  Try doing it through control panel I end up back at the ""Open With"" dialog, but nothing has changed. Access is not listed as a program to use to open the file with. The original program (Adobe Acrobat) that is associated with this program is still selected. Or just type ""file open"" into the start menu search and choose ""Make a file type always open with a certain program"" This does not make any sense to me (as when i hit ""copy path"" in the shell menu of windows explorer, the path contains Program Files. And when I add the Media Player by browsing in the ""open with"" dialogue, its also added with Program Files), but it worked as a solution, where simply deleting the reg key did not. The simplest way to fix this is to browse to that key in Regedit, right-click it and select Permissions, click the Advanced button, check ""Replace all child object permissions"", and click OK. I fixed the problem for my wmplayer.exe (that was missing and un-addable in the open with dialogue) by changing the I recently had a similar problem with all of my media file associations, and it seemed due to an obnoxious media player called DAPlayer. Even under Control Panel/Programs/Default Programs/Set Default Programs, I was unable to change the default program. Apparently, it set the UserChoice keys under HKCU\Software\Microsoft\Windows\CurrentVersion\Explorer\FileExts for all of its file associations to read-only access, even for administrators. Even after uninstalling the program, all of my associations remained stuck on ""Unknown Application"". But be careful with bulk registry operations like this: improper use could render your system inoperable! I take no responsibility for your actions.",4
"It's a bit tricky to setup that kind of environment but once you are done and all is tested and works, your deployment will be flawless and literally ""at one click"".  I did not mention 4th type of environment which is Development. Every developer has his own copy of everything (it is actually Integration environment) so can he can run/check deployment and unit tests on local workstation before he commits code to the central repository.  First of all you will need to do reverse engineering and covert your production database into SSDT project. If you have many databases, you need to convert each one separately. After that you create your CI projects using your chosen build software.   Your typical deployment project comprises many steps, so you can deploy your website(s) and database(s) altogether and the best thing out there is project templates for centralized setup.  Maybe it's a good time to consider Continuous Integration (CI) and Agile methodology. You will need SSDT (SQL Server Data Tool) and a build software -Team Foundation Server, Bamboo, Teamcity etc. We use TeamCity, it allows you to have up to 20 projects for free. And of course, you will ned source control software, like Git, SVN or TFS.  There was excellent tool Adept SQL Diff, lightning fast, but it's not supported anymore and does not work on Windows 2008 onwards. So Redgate is the choice. You dont need 3 physical servers to have all above. You can either use virtual machines, SQL sever instances or just name your databases differently for each environments using project parameters.",1
"I've seen errors similar to this one, but it wasn't related to Outlook SP2. It was related to removing the last vestiges of the Exchange 2003 routing groups after we fully completed the migration to Exchange 2007. What was happening is that Outlook was storing the names of the mailboxes internally using an AD syntax to describe where to find the mailbox. Exchange 2007 uses a different location for that. This caused havoc in things like Delegates, replying to old emails, and the frequently-mailed-contacts list.  There is also the ugly and unpopular ""recreate the account in a new profile"" diagnostic step that sometimes makes Exchange problems go away. However, the status bar indicates Online with Microsoft Exchange, and I can both send and receive emails from my primary account.  What could be going wrong? I have had my Outlook 2007 open additional mailboxes via the advanced account settings.  After updating to Office SP2, the list of emails in the additional account still displays, but I get a message for each mailbox item: That's a system-level fault, and it may not be your problem. But your symptoms seem close to what I've seen. Although going into Outlook's MS Exchange settings(connect to Exchange using HTTP) was  good and well, didn't do anything for me. I performed a ""dnsflush"" command. I don't know if that would solve you issue but it worked for me. The error on adding additional mailboxes was fixed by the hotfix http://support.microsoft.com/kb/968858/, however the ""Unknown Error"" when opening messages remains. Also, if I try to re-add the mailbox in the Advanced tab of my account settings, I get an error message What was happening was that each user that was migrated from Exch2003 had an attribute called ""LegacyExchangeDN"" that pointed to the old AD location. Users created fresh in Exch2007 didn't have it, and didn't need it. After opening an SR with Microsoft to figure out what the heck, the fix was to add a new X500 address to each mailbox to the value of the old LegacyExchangeDN. This allowed cached Outlook entries (and there are a LOT of them) to find the mailbox. The error you recieved is usually a problem with locating the mailbox in the global address list. When you add the accounts into Outlook, try doing so with their UPN name, the name@domain.com, and using the pre 2000 login name.  I've had a similar error, only that i had been adding a second, secondary mailbox on the already existing users mailbox. This after migrating from Exchange 2003 to 2010, on users machine, Windows 7 - Outlook 2007.",5
"The websites' source files are in the htdocs directory, so this is the ""root"" directory of these websites. From PHP-files (FastCGI) on our server the scripts are able to read my system's root directory (c:\), but even list the content of c:\windows. What is more troublesome that I can even create directories in the root like this: mkdir('c:\asdasd');, and it works without any problems; I can even read the other websites' directories (so actually maybe I could map the whole system's directory tree from a PHP script). I think that's a huge security problem. Noone should be able to read or write c:\, or other hosted website's content. I just want to enable read/write permissions to the first parent directory of the website, like for example c:\net\web\php\website1 (so e.g. ""c:\net\web\php\website1\other"" could be written, but c:\net\web\php\ should not be). Run the programs concerned under a very restricted account - one that does not have write or execute permissions outside your web content location. I know there's the open_basedir directive in PHP, but I think setting this directive doesn't solve the problem as I could also use any other server languages to access the same mentioned directories.",2
"When communicating with the MX for example.com all three receivers will still be in the To header, but there will only be a single envelope receiver. There are other scenarios where it makes a difference such as when using bcc and when forwarding email. As an analogue to the above imagine you printed out three copies of a letter with three recipients written on the paper. You would then put those three pieces of paper into three separate envelopes and write just one address on each envelope. As a slightly contrived analogue imagine that you are exchanging letters with some entity. Unknown to that entity you create a photocopy of each of those letters which you put in an envelope addressed to your lawyer. The most common scenario where you will see a difference is during delivery of an email with multiple recipients. Your mail server will now establish two separate SMTP connections with each of the receiving servers to send the email further. When communicating with the MX for example.net all three receivers will still be in the To header, but there will only be two envelope receivers. When your mail client is sending the email to your mail server all three addresses will be repeated on both envelope and headers. Next your mail server will look up the MX records for example.com and example.net to continue delivery.",1
"But, clients from EU have speed problems. This is due to the fact that the EU EC2 instances connect to the US-based RDS instance. As far as I know Amazon has not yet enabled RDS multi-region replication. Do you have any suggestions on how to properly speed up the whole setup while using the single RDS instance?  https://aws.amazon.com/about-aws/whats-new/2013/09/05/amazon-rds-new-data-migration-capabilities-mysql/ We would like to have a presence in the EU (Ireland) region as well. This means at least a new EC2 instance there (identical to the others, serving the same application).  I have copied the desired AMI, setup the new instance, setup a same ELB configuration (required for SSL termination) and configured latency-based routing in Route53. And it works as suggested. One possible solution to improve latency could be to use Amazon ElastiCache (which is basically Memcached under the covers).  Even if you move out of RDS and try to replicate your data between regions, either asynch or synch, you will get latency issues that will give bad performance to your users.  The easiest way is to set up a dedicated RDS server in EU and share nothing between these instances.  Recently, AWS made a move towards the direction I had previously asked about in my question by announcing cross-region RDS read replicas. However, this is only a small step towards a true multi-region setup. I'm trying to scale our web application (PHP, MySQL, memcache) in a multi-region scheme. Currently we are using a setup with two EC2 instances behind an ELB and an RDS instance, all of them in US-EAST (Virginia) region.  RDS is great for single-region deployments because of the low latency, but it becomes a different story when you start expanding to the different regions. If you want to keep the RDS instance, you could set up your own MySQL server in the EU region and do replication. This way, the speed will be far more acceptable. You would have to create an ElastiCache Node in each region (US-EST and EU) and have your application logic (EC2) use the cache node whenever possible. If you go this route you will have to re-architect your application 1) know what to cache and when and 2) to grab as much as possible from the local ElastiCache Node.  Running in a multi-region environment is much more complex and usually giving back performance due to the inherent latency between US and EU.  Also, any ideas in general on how to scale things up? Ideally we would like to continue using the RDS technology for various reasons. Nevertheless, I am open to suggestions (I guess the next idea would be to host our own MySQL servers). You should think carefully why you need to have the same data in both US and EU. After all these are different users.",5
"OS X Lion by default sets the locale in env variable LANG in Terminal. This was set to nl_NL.UTF-8. Sshd on the CentOS system imports this environment variable in its shell when you ssh into your CentOS box. Don't know if you run OS X on your desktop, but bottom line is: sshd on CentOS imports the locale settings from the ssh client workstation. At least Fedora 16 defines language in GRUB command line. See /boot/grub/grub.conf and /etc/grub.conf.  Apart from the fact that this greatly bothers me, I am having a pretty hard time actually changing it back. There does not seem to be a setlocale function, and system-config-language tells me I am using an English locale, even though my environment says otherwise. Both my ~/.bashrc as ~/.bash_profile contain no locale settings. Additionally, /etc/bashrc does not contain any locale references either. I just stumbled on this exact same problem, and found out why this is happens in my case and thought i'd share. I recently got a new iMac at work that runs OS X Lion (previous workstation was OS X Snow Leopard) I solved it by disabling 'Set locale environment variables on startup' in Terminal preferences (settings->advanced)",3
"/proc/sys/vm/drop_caches does not have a use in system operations. Don't use it, only makes things slower.  One guess at how much RAM would be enough is from the kernel, Committed_AS. Enough to not page out. Which can be a problematic metric, but it is one of the few that directly estimates physical RAM usage.  From your meminfo output, about 23 GB, or 150% of MemTotal. Half again what you have might be fine for some workloads, but I would be uncomfortable on a system with no swap and suspected memory capacity issues.  Since I've found discussion of buffers/cache on that site, not Unix of SE, I post my question here. I've read In Linux, what is the difference between ""buffers"" and ""cache"" reported by the free command? and Meaning of the buffers/cache line in the output of free, where it is written: Even ignoring Cached as if it were zero, this system already has relatively high memory utilization. AnonPages + Shmem is 14 GB and changes, alone only 1 GB less than MemTotal. And indeed, add in other stuff and MemAvailable is 0.5 GB. Currently free reports 8Gb of buffers/cache, however, system when approaching zero free memory becomes unresponsive for long time and sync; echo 3 > /proc/sys/vm/drop_caches does not change much. Why? I post output of free -m and also more detailed output of cat /proc/meminfo: Some totals add up to more than 100% because the virtual memory system is being both lazy and clever. But only so much it can do. Shared memory (presumably a database) and apps anonymous memory adding to nearly as much as the host risks miserable performance. Well, it does have some uses, but they tend to be corner cases. Testing storage with a cold cache, unusual virtual memory workloads.  See previously on Server Fault: Why drop caches in Linux?",2
"Here, you have 10 time-steps for the sequence and the final softmax output is giving a prediction for the next value of the categorical variable at every timestep (Assuming 1-hot representation of the categorical feature).  The Y value for each time step for training is simply the 1-hot representation of the categorical value at t+1.  The LSTM layer output gives (batch_size, 10, 100).  So, you now have a 100 dimensional representation of every time-step.  You pass this to the TimeDistributed Dense and then to softmax.  So, at the very end, at the softmax output layer, you have a Y vector of size (batch_size, seq_len, 1) that contains the true output for all 10 time-steps, for every sample in the batch.  You then would use this the same exact way as any other NN model using mini-batches to calculate loss and backprop errors to network weights.   It is my understanding that there are no differences for loss function optimization in RNN's as compared to any other neural network models.  The loss is calculated at the very end of your network output.",1
"This formula only works halfway. The condition in the IF formula is working. However, the ""value/action if true"" is the part I have issue with. After finding the correct productID where the ""X.test"" is greater then 3 I want to fetch the comment on that row. I'm currently using VLOOKUP again, but that will only find the comment associated with the first productID it finds, not if the period in the same row is greater then 3. I have tried using match, row and index. However the combination between them might not been right. The solution might just be right in front of my eyes without realizing it.  After reading multiple posts on this forum and others without having any success or finding similar problems I've decided to try my luck with my own post. I will try to explain the problem to the best of my ability without pictures from excel. Hope you'll understand. I added a IFERROR in case VLOOKUP doesnt find the ID. As stated by @Alex M, you where missing the number of returned chars.   What I want to do is fetch the comment from Sheet1 based on the ProductID from Sheet2. However, I only want the comment if the number in the period column is greater then 3. For example ""2.test"" is not ok, but ""4.test"" is fine.  My current DAX formula (located in the Comment column in Sheet2) is described below. Worth mentioning, since it's not clear from my illustration of the sheets, is that B2 is the cell containing 123 in the ProductID column in Sheet2.",2
"If the chassis, backplane, etc. in the RAID enclosure are fairly simple, they may be resilient enough to ensure 99.99% uptime (say, 4 hour repair time and 1 in 5 chance of a failure in a year) or something like that.  That's likely better than your network or power availability, even with a couple network paths to your site and a reasonable UPS infrastructure. Probably the best definition of ""highly available"" is that you can do some math to get an estimate of your uptime. (99.8%, 99.999%, whatever.  usually measured over a month or a year)  And it's got to be a measurement of the availability of services to your customers; any measurement of ""the server itself was up, it's not my fault the network was down"" doesn't count. It's probably more useful to figure out your uptime requirements and then work backwards to address the different components. Also, to me, HA covers availability in a single location. Business Continuance/DR addresses the multi-site part. So you can have a combination of both, HA in each location with DR being the most expensive. Once again, RTO/RPO requirements affect the design and decision process. Figure out how these and similar questions apply to your situation and you will have a good working definition for your site. As with everything there are different degrees of high availability. Highly available direct attached storage of the type you describe has the ability to withstand most single points of failure but if something kills the array then obviously it's going to fail.  On the redundant power front high end systems have independent standby power systems as part of the standard setup and for the truly paranoid you feed them from independent AC grids with independent backup generator options. Battery backed cache\cache destaging is used to ensure consistency even if everything you've done to keep it running still doesn't prevent a failure.  ""High Availability"" means a lot of different things to different people  When I was writing software for carrier-class telecom systems, we had several redundancy requirements: In addition you may (should) be considering replication to a separate array in a different server room\datacenter\city. Replication is tricky as failover to a replica is always a complex process but even at its most basic it will be far quicker than rebuild\restore.  It's possible that the RAID enclosure is really electronically two separate units with a separate path to each drive (dual-connector drives are common enough now), some kind of heartbeat between them and both systems connected to both controllers.  If the non-redundant parts are simply a metal box and very simple wiring, that would qualify as ""highly available"" by most standards, since metal boxes don't generally fail and simple wiring is unlikely to fail unless it's made poorly. In my opinion, the above isn't quite HA as the disk-array in question sounds like it only supports two hosts at most. ""True"" HA would involve a disk array on a storage area network of some kind, be it Fibre Channel or iSCSI.  This has been used as ""Highly Available"" in the industry for quite some time, and is one step on the storage marketing tier. The next step up is, ""Business Continuity"", when they try to sell you the block-level replication technology between two of their Disk Arrays. Some also have an up-sell that allows the host OS to seamlessly fail between the two. At the array level the features that are added to improve availability are things like redundant power supplies, redundant controllers (with cache\state mirroring), redundant fans and redundant IO interfaces. All of which should ideally be hot swappable so you can withstand a failure\carry out proactive maintenance without shutting it down.  Parts in the enclosure could and will fail at some point. But depending on the array, there can be redundant parts inside the array. So that leaves the enclosure itself which IMO is not that big of a deal. If you have a second array, typical DC layouts tend to cluster storage arrays together so if one say catches fire, it's probably going to affect others. It's worth noting that the single biggest problem tends to be human mistakes - designing storage so that it is capable of withstanding human error is pretty hard. SAN's get around this to some degree by recommending dual (or more) fully independent fabrics so that physical and configuration errors are isolated but at the array level that level of resilience is not something I've seen much of. This almost certainly means that you don't have any component requiring maintenance that brings down services when it gets a firmware update or something like that.  Once you get past about 3 nines, it's likely your servers will have better availability than the network, your power, etc...  (seems like there's vast swaths of the internet having problems for a few hours every few years, so you probably can't realistically get past 4 nines if the customers get to you over the internet) There's a difference IMO between ""Highly Available"" and ""No single point of failure"". Plus you have to consider the scope of HA, you could have something that is HA for storage but not for the app (single server. Silly, I know but I've seen it happen).",5
"The question can be restated as ""should a less trusted network be allowed access to a more trusted network, given that the only protocol is syslog""?  You need to weigh the cost / benefit of placing your syslog server on the inside. I'm personally a proponent of having the syslog server residing in the internal network - it's actually a rather high value asset.  Is there any way to transfer theese logs safely without exposing a path to my admin network form the DMZ? I assume the syslog server must be on the admin vlan asit contains vaulable and sensitive information I do not want on my DMZ.  Or you could have two separate DMZ subnets via your hardware firewall. DMZ1 -> DMZ2 <- Admin where DMZ2 contains your syslog server.  For example you could have your syslog server physically reside in the DMZ. From there iptables can allow syslog from any host both DMZ and your admin network and then restrict SSH and web access if required from only your admin segment.  I have a firewalled router that connects to two virtual Lans. The first lan is an administration network and the second a DMZ. My virtual machines in the DMZ need to send syslog messages to a syslog vm in the admin vlan. In order to do this i must enable a rule from the DMZ to my admin network for syslog. This implies that if a DMZ server were compromised it could be used to attack the syslog server on the admin lan. I personally think that a breach of a syslog server is pretty low in likelihood but there are a lot of ways of doing this with different combinations of hardware and software firewalls.  The question then becomes the likelihood of an attack breaching specifically via the syslog daemon. If you think that your syslog server can be breached you want to isolate it from your admin network.",2
"Example: Many years ago, I was in an organisation which suffered from users picking the same passwords across many systems and these systems accepted telnet-access from anywhere. This was exploited by attackers on more than one occasion. It probably took about 3 months or so but the results were very good. Once the senior managers go the hint, often through reminders from their own staff, things became a lot more official and my work (in that respect) was done. Microsoft have a toolkit you can download which has some ideas in it.  Sophos released some material recently which also has good ideas.  As do Symantec (as you mentioned) and most leading IT orgs, since it is a way they can slip in some marketing. Your best option is to manipulate your way to success. Whatever you're trying to get your users to do, make your secure way the easiest/fastest/cheapest way. Users skip security advice because it can save them 2 seconds, so reward good behavior whichever way you can. We in the security field need to come to terms with the fact that people have better things to do with their time than follow our silly rules, most of which they don't understand and any consequence for the user is so delayed (hours, weeks, months) that the majority will never learn. It's pure psychology and we seriously need to take a clue from what the last 60 years of marketing/spin/manipulation has taught us about the human brain. Killing telnet and going to ssh with key authentication solved the security problem and removed the need for users to type in username and passwords on every remote connection. Having to not type their password for every new connection made it OK that they had to unlock their ssh key with a passphrase every morning. There is only so much you can do with training, especially when there are no (perceived) consequences for not following the rules. I've found the most successful topics for awareness are those that have immediate and clear benefits.  Changing passwords regularly doesn't have obvious benefit for most users.  Same with avoiding clicking online ads.  But if these can be worded in ways that appeal to your audience then you're more likely to succeed.  For example if you have parents, they will be sensitive to computer security advice that can protect their kids (oh and incidentally teach them good work practices too). Regarding IT staff, security awareness seems to have less impact.  Clear procedures and policy, good management guidance, and a culture of security are more successful, in my experience. It's hard to put together one that works.  It helps to have engaging content, and some rewards (eg run a simple quiz and give away something of interest).  It helps to have influential leaders in your organisation actively promoting participation in the awareness campaign. When I started work in a previous company with about 60 staff the Post-its weren't even hidden. They were stuck to the monitors, because it saved that extra step of having to lift up the keyboard or phone to read it. Attempts at a wholesale education process were a complete waste of time. Once I realised that I had one on one talks with the worst offenders. Where possible I identified those who loved to chat and gossip and focused my attention on them and let them (unintentionally) help me spread the word through their gossiping.",3
"As cron itself cannot do what you require it's a matter of creating your own routine. The use of sleep, as suggested by others, will not give the correct results, as it does not take into account the time required to run the command. However, it's a simple matter of reading the system clock, in whatever language suits you, and firing off the command every 15 seconds based on the time, rather than a time delay. EDIT: It appears that while the CRON expression requires 6 or 7, the cron exec that runs it wants 5... I'll look into this! Once you have your routine don't fire it off via cron every minute. Just run it as a background task continuously. It seems that Cron doesn't support a seconds interval. What is the easiest way to run a cli script (php) every 15 seconds? Is there a cron tool that works specifically with seconds (then I could use Cron to call it every minute)? I think you have to write a shell script which calls your command and then sleeps 15 seconds -- I don't believe most cron implementations allow for more than one minute granularity.",4
"Disclaimer: As long as we are only talking about 2 small ID columns, 1 of which is the cluster key, the impact will be pretty much zero (0).   Thinking Ahead: But do now think about the future and how you want to be designing more complex indexes to support a more complex system.  For What it is Worth:  I have seem code that tried to include pretty much everything in the index.  (I am sure that you are not planning to do that.)  But the big fat index that was generated was, let us say, not optimal. If you create an index on SchoolID and ID is the clustered index, then your index on SchoolID will include the clustered index as well.  Depending on the distribution of your SchoolID it could be that the SchoolID index could be better used alone in some contexts.   This could be when you are searching some data set where the SchoolID is the important join to some  data in another table.  This other table will likely have its own clustered index. You have received good advice for keeping the index as narrow as possible, both for space use and for controlling the amount of I/O.  But you have maneuvering room in your decisions. If you assume that you will eventually be creating a larger database, with many tables, and many indexes, you should begin thinking about these issues now, so that you are developing a strategy that will stand up well for the future.",1
"I believe VirtualGL might help you with this - it allows you to run the graphically-intensive program on device and view the results on another. I doubt you'd be able to run just a graphics card by itself via ethernet. But you could connect to another PC using some kind of remote desktop application (VNC, etc). However, there may be compatibility issues due to the operating system's inability to set up resources for the PCI connection to the GPU, so browse around and make sure it's compatible before you buy anything. Consult guides such as PCWorld's eGPU tutorial to see if you have everything you need to use a desktop GPU with your laptop. It's 2018 and eGPUs are a widely available thing now, it's still missing in these answers so I want to leave it as an answer in case someone isn't aware of those great things: Connecting a GPU by Ethernet is like connecting your PS3 controller to the PS/2 keyboard port: sounds like it's trying to fix a problem but the solution turns out to be completely outlandish and impractical. (Granted, if you can pull it off, you'll gain more popularity as that one crazy guy.) As already pointed out, a solution involving ethernet has several flaws, the biggest being that your laptop's port most likely doesn't support more than 1Gbit/s. That said, I advise you not to try to do this because the interface simply doesn't exist. You'd have to go through a lot of hoops to get your card to accept low-level commands through high-level interfaces and then bring back a video output. The latency would be absolutely horrible, unless you are willing to pay thousands and thousands of dollars for an enterprise-grade solution that lets you do this for one reason or another. However, don't fret. You may still be able to connect your GPU to your laptop. For instance, if you have an ExpressCard slot, you can use an ExpressCard to PCIe adapter (along with a power supply unit) to have a seamless experience. These devices fit a GPU and a power supply (most of them have them built-in, the above one has a 400W one built-in to power the GPU). It's using Thunderbolt 3 which has a bandwidth of up to 40Gbit/s, offers 18V @ 550mA, can drive multiple 4K displays, and much more fancy things. There has been rumblings about being able to connect an external GPU via Thunderbolt or USB 3.1, and AMD says they want to make a standard for it: This would be your best solution. It's simple, there's cheaper ones too, and it guarantees you a perfect system integration and performance (you can even run a GTX 1080 inside these things). Newer laptops may be able to do this with thunderbolt, but that's a whole different protocol, with the necessary clever hardware available.  Such a device would essentially be a video card, plugged into a PC, rendering what you want to play on the video card, compressing it, and sending it over as video.  Chances are very slim that your laptop has a 10 gigabit ethernet adaptor - its uncommon on high end desktops. You'd also need some clever hardware to translate between PCI-e to ethernet, and the latency, oh the latency. You have a device which is typically put as close to the processor as possible with 16 dedicated pci-e lanes right into it, and you'd be adding a ton of latency.",5
"In the context of image classification, what does it mean to be stable to deformation? Say I were trying to classify digits, what would the difference be between an operation that is stable vs unstable to deformation? It is said that pooling layers insert certain stability to these deformations, this is analysed by Ruderman on his paper Learned Deformation Stability in Convolutional Neural Networks and in Pooling is neither necessary nor sufficient for In computer vision algorithms for non-constrained environments it may be desirable for a predictive model to be ""stable"", i.e. robust, ideally invariant, to arbitrary transformations that are common in that problem. E.g. a digit classifier would benefit from been stable to common paper deformations such as those caused by wrapping and unwrapping a long letter. Images are susceptible to deformations, i.e. afine or arbitrary deformations, such as ""melting"" effects. Some features, such as SIFT, BRISK and HoG try to deal with the most common deformations in image (Scale and Rotation). Most of convolution-dependent methods are already invariant to shifting as that is a feature of convolutional filtering itself.",2
"Are they saying that they want to give you an additional port on their switch, and then you buy a switch, and connect two ISP ports on your switch, and 1 port to your sonic firewall?  Depending on what actually happened, that wouldn't actually really add a lot of redundancy to the system - I'm not sure what problem they're trying to solve. We were in contact with the ISP for well over 2 months and were consistently requesting further information regarding adding two switches to our rack to handle HSRP. If you do not feel like buying and installing another switch, you can simply create a VLAN with 3 ports on your existing (extreme) switch. No need for shipping and mounting: you can do it remotely and simply have them repatch the cables. By doing this they shift the redundancy (and thereby the single point of failure) a bit further towards you. You get two ethernet cables to the outside, automatically picking one that is not down. Makes sense. As expected, after some due diligence on their part, our ISP confirmed that they had a switch between their router and our firewall that provided HSRP and that at the time of the outage was misconfigured. If they want to provide you a redundant link, that would mean two Ethernets going to you. Anything they provided before that would be a single point of failure. If you need to run those two links to a single firewall that only has one 'outside' port, you'd need a switch to hook it all up. The single point of failure will now become this new switch and your firewall. Duplicating those would allow you to be even better redundant.",4
"Now S can be anything. It could be an X coordinate, it could be a Y coordinate, it could be the distance between the objects, it could be an angle. I'll call the path of the target over time the function S(t) where S is the position and t is the current time and the approximation to the path is A(f) and f is the date in the future one is approximating. One way to do dynamic path finding is have the entity predict where the target is going and go there. Calculate where B will be in 'T' time (if B continues at the same speed / angle) and go there instead. If the terrain is reasonably open and the target isn't too far from the pursuer, then you could use the intercept steering behavior. Essentially, you take the position and velocity of the target to calculate a position out in front of the target that isn't too far, and not too close, and you steer the pursuer towards that point (calculated each at regular intervals).  I am working on a 2D RTS like game, basic A* works perfectly for moving a unit from point A to point B. Also there's probable better methods of predicting the future path of an object so I'd look around a bit. I know this can be rephrased in terms of change in time which is probably more convenient for a game. To do better than that is very difficult. Unless you actually have knowledge about the unwavering path of the target, you're heading into the land of quite hard AI because you'll have AI's second guessing or simulating the target behaviour in order to guess where they will be and path towards that. This kind of AI is a real-time AB-game from game theory, an area that's not standard in any 3D game AI toolkit. This is not the perfect way as the distance changes but much simpler than making a perfect solution and much better than just trying to get to 'B'. But now I facing the continuous path-finding problem, like A attack a moving object B, call A* at each frame once Object B's position changed seems inefficient. One method I have seen work was to assume the target continues moving in the same direction and change the goal position as you run through your path finding algorithm. This does mean you have to hold two metrics in your A* nodes (cost and time as opposed to just cost).",5
"Side note: This is actually a veiled peak at RAII, or the concept that resource life times should be tied to object life times (over simplified, but yeah). It's probably a bit early at this point to go reading into that too much, but once you're deeper into C++ (perhaps when you find yourself becoming fairly comfortable with the object model), make sure you look it up at some point. This has already been said before by someone else, but just to reiterate, don't use globals. Just don't. At least not until you come across your first situation where you truly cannot do anything else (and it's possible to go years in between those situations). To be a bit dramatic, globals go against pretty much every software design principle there is. That's a lie. If close() fails, it sets the failbit but otherwise does nothing. It's completely possible that flushing out some cached stuff failed and the the full content of the file didn't make it to disk. Your program can very easily eliminate globals by using local variables in main and passing parameters to functions as needed. I know in an example program this simple and small it doesn't seem like it really makes a difference (it might even seem more cumbersome), but it really is a habit you should get out of. In large or complex systems, globals can quickly turn into a a testability killer and become Petri dishes for weird, hard to find bugs. You have basically no error checking. File opening, stream token extraction, and file closing can all fail.  strings has no reason to be global, and it should be called line, especially since it's a single string, not a collection of them. Or, better yet, there's no point to read the stream line by line just to output it line by line again. You can just copy the stream to std::cout with std::cout << inputFile.rdbuf();. (Note that this does have very slightly different semantics since it's a binary copy instead of a character stream.) Functions should be responsible for one discrete responsibility, but this is broken in a few places. For example, read(): 1) prompts for user input, 2) opens a file, 3) copies the file to cout, 4) closes the file, 5) terminates the program. inputFile should not be global. It's not necessary for it to global since every function that uses it immediately closes it. Even better, since you're not actually explicitly checking if the file closes successfully (and really, it'd be fairly overkill to do that unless you're making a very robust program), if you use a local variable, it will close itself automatically. That's a lot of things for one function to be doing. Main should handle the exiting (since it's responsibility is to oversee the flow of the program execution), and instead of opening and closing a file, read should be passed an already open file. (Note: immutable globals [usually called constants] don't have nearly as big of set of problems [though they still have their own set of problems]. If you actually heed my end-of-the-world-warning about globals, just know that you can chill out a bit towards constants as long as you're sure to be mindful of if/when they become determinant to the quality of the application.) This is wrong. It will actually 'read' an extra line erroneously since eof() doesn't return true until an attempt has been made to read past the end of the stream. The idiomatic way to do this would be:",1
"I'd triple check that your CTS is being properly set and handled (specifically, that both ends know they're supposed to use hardware flow control).  It's astonishingly rare for anyone to get this right, and even rarer for some sensor to have enough buffer to be able to store all its results whilst the machine is rebooting, so even if the flow control is working right, it's quite possible the sensor is discarding some data (either oldest or newest, depending on its design) because your machine is taking too long to reboot and reassert CTS. The solution is to simply tell Windows not to do this, this way you don't lose your data since it's not being received. We can do this by modifying boot.ini and ensuring that we're using the /fastdetect switch. However, this will already be on in most cases, as it's enabled by default in Windows setup. I found the solution here, the article also includes informatation about NT and 9x (there are different solutions). There is also a command line tool available to disable scanning on selected com ports in the registry, if you want to avoid manually editing the boot.ini file. I have an application that is monitoring a serial port. Data is sent from the other side only when the port is opened but I found out that I lose some data when I reboot my computer. Why is that? This is caused by Windows opening the port at boot time for a very short time (under half a second). During this process, all of the serial ports are opened in order to detect input devices such the mouse.",4
"If you do not recognize or trust the mail.jbsgroup.ie hostname that appears to be sending mail on your client's behalf, then you can use SPF records to add a great layer of protection. SPF records will allow you to specify which servers are allowed to send mail from mullanlighting.com, and can also tell email recipients' servers to ignore all messages that are sent from anywhere else. For example, you might configure SPF records to identify Google's mail servers as the only authorized servers for your email domain.  A client's email is handled by G Suite. When I check senderscore.org a strange sending IP and hostname (mail.jbsgroup.ie) appears amongst other Google-related hostnames. This particular domain belongs to another organisation in the locality. Can someone explain to me why is it appearing here and should I be worried? The important thing to note is that your MX records ARE properly configured for G Suite. MX Toolbox is a great site to verify this on https://mxtoolbox.com/SuperTool.aspx?action=mx%3amullanlighting.com&run=toolpage. Are you sending emails from web form? If yes, you might not be using SMTP authentication which is causing hostname and IP to be taken from the local mail server from you sent the email. You should use one your email account for SMTP authentication. The following are the settings you need to use in your script.",3
"Alternatively, you could use something other than LAG/LACP, such as round robin. But that has very serious drawbacks and likely will be worse than using a single link. The way to get more throughput with LAG/LACP is by using SMB multichannel, HOWEVER it is not considered stable even under the latest SAMBA release, so can it be done? Yes, via SSH to your Synology, modify the samba config file and restart the service or reboot and also verify with powershell on your machine that multichannel is enabled and in use during a file transfer. NOTE: Realize that you are potentially putting your data at risk IF you choose to do this because it's unsupported by Synology since this feature is not considered ""stable"" yet by samba. Best way around this is get a model with a 10Gbe card but your switch also doesn't have SFP+ ports, just SFP which is also 1Gbe so you would need another switch as well or just directly connect to your machine with another 10Gbe NIC. All the packets going in one direction for a single connection have the same MACs, VLANs, Ethertypes, source modules, port ID, and so on. So there's no way to distribute them over multiple physical links. Thus LAG/LACP limits them to the speed of the fastest link.",2
"you will need to use ""ifconfig -a"" to list all available network interfaces - at first you run just an ""ifconfig"" (without the -a flag), memorize network interfaces that it shows you and then run it again with the -a flag, and find that extra interface. In my case by running just a ""ifconfig"" I got two interfaces - enp0s3, lo and when running with -a flag I got an extra interface - enp0s8. After we got a name of that extra interface name you edit /etc/network/interfaces file and append these two lines there: Both of the guest os have different ips. I am able to ping host from both guests, guest to guest and guests to host. 0-EDIT: to clarify, my setup is Ubuntu 16.04 running Oracle Virtual Box 5.0.24 Host & Windows7 Guest. After that change you reboot the machine and now when running ifconfig should get a host-only adapters IP that you can use to connect from you host machine. I went to network. Turn on network discovery on the guest machine (Windows Server 2012 R2). I think it will work for other guest OS. I set the network adapter from NAT to Host-Only and it automatically addressed a new ip to my guest machine. It was 10.0.x.xx and now 192.168..xx.xxx. Then i pinged it from the host and it responded. Hope it helpes someone! If Mick's settings don't work for you,this may. I followed Mick's settings, but was unable to ping back to the VM Guest in the end. I made one change, and that was to enable the firewall and set to ""WORK"", instead of disabled. I edited the firewall via  ""Allow Programs to communicate through windows firewall"", and selected ""File and Print Sharing"" under Home/Work (Private) profile. I was immediately able to ping my VM Guest from another pc in the LAN. I'm pasting Mick's settings below for clarity and single post help. Thanks Mick!! I searched 3 days for a solution why I couldn't ping between my windows server 2016 guests on my Virtual Box internal network ... it appears that the firewall is blocking the traffic!",5
"I've configured a virtual machine for use as a web development platform to distribute to web developers in my company.  Currently, I have the install script for each site creating a new virtual host on a new port so we can use root level links on the sites correctly.  Each site has a link that looks like: http://192.168.1.100:8080/, http://192.168.1.100:8081/, etc.  These addresses are difficult to remember for me and I certainly don't want to ask others keep ports mapped to sites in their heads.  I would really like to create a new local network scoped name for each virtual host and broadcast its presence with something like NetBIOS or Bonjour or both so the host machine won't have to be configured to use the network name. In short, I want to dynamically create new local network names to point to various Apache virtual hosts on a virtual machine that will be platform ambiguous so the host operating system can be Windows, OS X, or Linux.  All configuration should be done on the virtual machine so that the host will require zero configuration.",1
"So, make sure that you have a account exists with 'mysql_user'@'10.4.0.211' and then try to connect from other IP than 10.4.0.211 From the documentation, I understand it like, the server accepts connections only from the IP - 10.4.0.211. None of the above  has the bind address in them, but how come I am able to connect to the service. Please clarify. The server listens and accepts connections only on (not ""from"") the stated address.  Where you can connect from is determined by the entries in the `mysql`.`user` table, which are created by GRANT statements, and, of course, whether you have network connectivity to the address where MySQL is listening. Otherwise, you can report this issue to http://bugs.mysql.com/ mention specific MySQL version and OS details where your server is running If you intend to bind the server to a specific address, be sure that the mysql.user grant table contains an account with administrative privileges that you can use connect to that address. Otherwise, you will not be able to shut down the server. For example, if you bind to 0.0.0.0, you can connect to the server using all existing accounts. But if you bind to 127.0.0.1, the server accepts connections only on that address. In this case, first make sure that the 'root'@'127.0.0.1' account is present in the mysql.user table so that you can still connect to the server to shut it down.",3
"Additionally, you might get partition elimination but your query patterns would need to fit a very specific and repeatable pattern within your system - the partitioning key and clustering key and any unique keys become interconnected and very important. If this balance isn't treated acknowledged and designed around, you end up with performance nightmares. As far as performance is concerned, you can change lock escalation to AUTO (default is TABLE) like so: So, at what point should a table be partitioned? That depends on your query workload, the profile of your data, but most importantly, it depends on which of the management features of partitioning you absolutely must leverage. Partitioning isn't for query performance, it's primarily for data management and administration. As noted in the linked article, the primary benefit of partitioning is that you can quickly move data by using partition switching. For example, you can archive ""cooler"" data to slower storage and keep your ""hot"" data on fast storage. At regularly-scheduled intervals, you can quickly archive data by rolling it to archive partition(s) without having to go through the process of waiting for an ETL to perform the transfer. As noted in one of the early comments to your question, though, this will take some careful thought and planning before implementing it. Also, depending on the SQL Server edition that you use (Enterprise), you can leverage data compression to compress individual partitions.  I'm going to take a different approach and note that partitioning (in SQL Server) is primarily a data management feature with query performance being a possible secondary outcome, depending on how you manage it.1  With the advent of SQL Server 2014, you can also take advantage of incremental statistics which is very handy if you proactively monitor and update/create statitsics on large tables.",1
"On the other hand, if you want to land your data somewhere other than AWS - you can use any backup tools. Some of the mentioned above also have archive mode. This option allows you to utilise less API calls for seeding a lot of files. I see that it's also a requirements for you.  I agree with the previous answer about many backup software tools that allow to seed the data and then automatically synchronise it with your cloud account. If you have good speed, you need a bit of patience to upload all the data. It will take time in any case, even if you encrypt it as the amount is big enough. As for the encryption, any third-party backup tool can do it for you e.g. Acronis, Cloudberry, Veeam etc. offer such an option. Some of them give even a client-oriented encryption for a person to feel safer and more secure. The whole scenario depends on your bandwidth. Depending on your ISP capabilities you can evaluate how much time will you spend for this task. Nowadays you should evaluate how much time and money will you spend by uploading such a big amounts of data somewhere offsite or to the cloud. I tried CloudBerry Cross-Platform Cloud Backup which provides a simple GUI to manage backup and restores and cloud storage account comes bundled with the software. It also provides enterprise backup features like job scheduling, CLI, compression and encryption. A third-pary tool is an option in your case to do with things quicker. In most cases you don't pay for them and they work with all modern cloud storages. For instance, AWS offers the service called AWS Import/Export Snowball. With this service it's easy to ship the Terabytes of data to your bucket without paying extra money to your internet service provider. I am wondering if anyone here had any suggestions about way to upload an entire external hard drive to a cloud service. I currently have an external hard drive with 4 TB's that I would like to upload to something like Google Drive. Before doing so, I'd like to encrypt it using something like 7zip before uploading it. I've naively tried to hookup another 5 TB drive as the destination for the encrypted file, but it is taking much much longer than I thought. Are there simpler ways one may do this? Thanks! At the enterprise level, Veeam, Microsoft DPM and other EMC products can be an option but it totally depends upon the backup requirements.",4
"But when you're connected to a full-tunnel VPN, those requests are not visible to the local network anymore  they're tunnelled inside the encrypted VPN connection. Indeed, for many people the whole point of using a VPN is to prevent interception by local networks. Of course, most networks actually block all non-HTTP traffic until you log in, so the VPN-tunnelled requests end up going nowhere. Quite possibly those 15 minutes are what it takes for your VPN software to give up and close the tunnel entirely. An OS could work around this by having a dedicated captive-portal detection service which tests each interface individually, bypassing the system's routing table (e.g. NetworkManager on Linux does this)  and then opening the login webpage in such a way that it bypasses the VPN as well. (Of course, bypassing the VPN is often undesired and many ""privacy-oriented"" VPN apps try all sorts of things to prevent it anyway.) Wi-Fi networks with browser-based login pages (aka ""captive portals"") work by intercepting plain HTTP requests, as well as plain DNS queries, and making the response redirect to the local network's login webpage instead of the real server you wanted. Other than that, there's no real workaround as long as all of your web browsing goes through the VPN.",1
"I suggest starting two terminals, one for monitoring and another for sending a request. Terminal1 will show responses from all existing DHCP servers including MAC address. This example was run on Ubuntu: There are also a number of utilies avaliable which proport to do this one is DHCP explorer another is DHCP probe which you mentioned in your original post. https://social.technet.microsoft.com/wiki/contents/articles/25660.how-to-prevent-rogue-dhcp-servers-on-your-network.aspx  On a Mac, run ipconfig getpacket en0 (or en1). See http://www.macosxhints.com/article.php?story=20060124152826491.  20:a6:80:f9:12:2f > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 332: 192.168.1.1.67 > 255.255.255.255.68: BOOTP/DHCP, Reply, length 290 See http://en.wikipedia.org/wiki/Rogue_DHCP for a list of tools (many of which were listed in other responses).  https://web.archive.org/web/20141022013752/http://blogs.technet.com/b/teamdhcp/archive/2009/07/03/rogue-dhcp-server-detection.aspx  Another way would be to use Wireshark packet capturer/analyser to look at your network traffic and find DHCP connections, there is a lab worksheet on how do do this avaliable from here. 00:23:cd:c3:83:8a > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 590: 192.168.1.253.67 > 255.255.255.255.68: BOOTP/DHCP, Reply, length 548 That monitoring terminal is needed just to see all responses (nmap is able to show only the first response). You can get the IP address of the server by running ipconfig /all on a windows machine, and then you can get the MAC address by looking for that IP address using arp -a.  dhcpdump, which takes input form tcpdump and shows only DHCP related packets. Helped me find rootkited Windows, posing as fake DHCP in our LAN. There are several ways, if your running a small network the simplest way is to turn off / disable / un-plug your dhcp server and then run",5
"(Love the three minute pause requirement between answering questions.  Next I'll get the robot captcha.  Sometimes it just isn't worth trying to be helpful.) (I could not find a way to add a comment but) Also, I would like to add that not having a robots.txt is also a problem in the sense that you will not be able to provide a Sitemap for it. Remember that Sitemap's are only located by either them being specified in the Robots.txt file or through direct submission to search engines, but of course the latter means you have to do it one-by-one, rather than just simply having all quickly find it. Robots.txt is a strictly voluntary convention amongst search engines; they're free to ignore it, or implement it in any way they choose.  That said, barring the occasional spider looking for email addresses or the like, they pretty much all respect it.  Its format and logic are very, very simple, and the default rule is allow (since you can only disallow).  A site without a robots.txt will be fully-indexed. robots.txt is completely optional. If you have one, standards-compliant crawlers will respect it, if you have none, everything not disallowed in HTML-META elements (Wikipedia) is crawlable. I haven't had robots.txt on dozens of domains I've had registered, some as far back as 1994, and have never had a problem with them getting placed in google/yahoo, etc.",4
"My biggest issue was coming mostly from the GUI world of Windows and OS X... and thinking I could do what I needed from a GUI in Linux.  In this first I try failed miserably.  My second try was setting up everything from CLI, and that was actually WAY better.  Right now, I still don't think I can do everything I want from the GUI.   Same with understanding drives and partitions. In windows, a hard drive is a hard drive, where in linux, is it a scsi device? IDE? I ran Ubuntu Linux for a couple years before recently switching to Windows 7 on my primary laptop.  I found the learning curve to be fairly time consuming but also rewarding as I love to learn new technologies.  I had to run XP in a virtual machine due to my addiction to Outlook 2007 as well as some Win-only gui mgmt tools.  Working between the two OSes was a bit tedious, especially when I had links or attachments in Outlook that I wanted to open on the host machine.  On the other end, it was nice to have both systems running concurrently for testing of anything that came up. In the end I decided to switch back to Windows with a Linux server available for testing. 1- Working from the command line and config files - many Windows folks struggle with it, because it is so rarely necessary in basic Windows installations. Setting display settings. xorg.conf, window decorators and changing settings in cde, kde, gnome were all different from Windows, and confusing since each distro is different. Remote displays, display redirection, etc. Windows abstracts away a lot of things, and those abstractions are taken for granted by Windows folks.  This starts with the alphabet soup of *nix flavors and distributions.  There are lots of decisions in *Nix that Windows has made itself. In Windows, I can see all the devices and files associated with it's driver. I can easily diagnose problems. In Linux you have to know what you're looking for in /dev or forget it. As a Windows administrator, what were the issues you encountered with trying to learn ANY linux distro? Condescending people, some of whom were not even born when I first got exposed to *nix, patronising me with talk of ""this linux thingy"" is one of the biggest problems I've found. Kinda puts me off the whole thing really. Well that and the whole ""read the source code and figure it out, wintard"" attitude I've encountered in the past.  In Windows, I can pretty easily figure out what's starting up when the machine comes up. It took a while to find that equivalent in Linux. 2- file system permissions, chmod, the numbers, etc.  Linux permissions often just don't compute initially to Windows folks.",5
"I'm going to assume your using python and scikit-learn mostly because it has a method for providing model metrics. The most common methods are Platt scaling and isotonic regression.  There is a third and more recent method, beta calibration, and there are a few more exotic ones around.  The three ones I've named all fit to a new dataset a univariate function with inputs your model's scores and outputs the actual observed labels.  Platt scaling fits a sigmoid function, beta calibration fits a parametric model that is more general than sigmoid, and isotonic fits a nonparametric, arbitrary non-decreasing function.  XGBoost's outputs are biased away from 0 and 1, so the sigmoid is generally ill-suited, so in this case go with beta or isotonic (or find something else to your liking).  Isotonic, being more well-known, has more open-source implementations. And the classification report goes into more depth about how good the model is predicting each class more info here crossvalidated SE explanation. You included that probability-calibration tag, which is prescient: there are a few techniques, all called ""probability calibration,"" which adjust the scores output by a model to better fit observed probabilities.  After this, the scores should be close to representing real probabilities, and should therefore be directly comparable.",2
"If you need to read 1,000 rows, don't get them one at a time, each with their own query. Use a WHERE clause with a list of values that you're looking for, or a range comparison (like greater than a particular ID, and less than another ID.) You are almost certainly 'doing it wrong' in one way or the other. If your application truly needs to operate that way, you probably should be using some other sort of datastore. Maybe something like Hadoop or a key-value store. Alternately, you may be doing something that is inherently a batch operation and should be done on the server side rather than remotely. Your description sounds like load-test simulation software designed to find the limits of connection pooling. If you need to write 1,000 rows, try working in a batch instead. Writing all 1,000 rows in a single statement can result in less blocking - whereas 1,000 statements trying to write to the same table, simultaneously, can be a blocking nightmare. (Especially when that table has multiple indexes.)",2
"The primary question I have is one of usage. By the name sparse_array, it sounds like you want to expose an interface that is similar to an array. Perhaps it needs dynamic sizing, or perhaps not, but in particular I expect to be able to iterate over its elements with code like this: Today, I tried to create a really simple sparse_array container adapter. I did not provide all the STL-like functions, only the elementary ones as a proof of concept. I also trimmed the class from its copy constructor and all the unecessary things for this review actually. Here is the code: It needs to be clear what the output from that would be. Would it include the default values in most indices? Or is this usage prohibited (perhaps due to that unclarity), as currently there are no iterator related methods? Maybe this is just part of what you intentionally excluded from the review, but it leaves so much unanswered, including things that probably come back to your proxy class, such as how *std::begin(sp) = 3 will work. Do you think there are obvious design flaws in this code? I bet there are some in the proxy mechanism, and it would be great if you could hilight some of them. The one definite gap I see has to do with operator&. Will you support code that looks like normal array-style addressing? Right now the following code will certainly not work correctly; should it fail to compile, or should it modify sp[5]? What I tried to do is to have a sparse array whose number of elements is strictly equal to the number of elements that are different from default_value, hence the elements that are deleted when default_value is assigned to them. Here comes a small example:",2
"It looks like Pingdom are using a really old version of .SE's DNSCheck tool. We removed the test that's failing for you a couple of years ago, for the reason that it's highly unreliable. It gives false negatives or false positives as much as half the time, which makes it worse than useless. It's also not really fixable, since it tries to answer the question ""Will mail to this address reach a recipient?"", which is both outside the scope of DNS and very ill-defined. So, in short, you can safely ignore this failure. 2) Failed to deliver email for SOA RNAME of domain.co.uk (server.domain.co.uk) using server@domain.co.uk. It looks as though the two warnings are related: the second field in your SOA field is server.atomicpenguinclothing.co.uk., meaning that the DNS contact is server@atomicpenguinclothing.co.uk. Does mail to that email address go through? I've set up a new domain on my new server and when testing the DNS I receive 2 warnings that I'm concerned about: Does anybody have any thoughts on how I can resolve these errors. I have a SOA record in place as well as rDNS (as far as I can see in my DNS), but I can't work out why these errors are displayed. The email address is definitely set up and working too. Or try testing your domain with either .SE's DNSCheck or the newer tool we (together with AFNIC) wrote to replace it, Zonemaster. Both tools are Open Source, written in Perl and available from Github. Zonemaster is also on CPAN. Both tools have command-line versions if you prefer that to web-based services. The field server.atomicpenguinclothing.co.uk. is supposed to be an email address which can be used to contact the DNS administrator, with the first dot . getting replaced by an @. Is that a valid email address?  Because it appears that SMTP port (tcp 25) is not responsive on the MX record atomicpenguinclothing.co.uk.",4
"Using a good managed DNS-based solution would be easier though, if the cost for a reliable enough provider is within your budget, but this might be a viable alternative. UltraDNS and Akamai can provide this service, however they are very expensive. I settled on using DNSMadeEasy which have DNS servers on both coasts, as well as europe. It is in the Debian repositories (http://packages.debian.org/lenny-backports/geoip-database) which implies it is in fact properly ""free"" in many regards (though you might want to check the to make sure the license(s) are compatible with your project's codebase and direction). How accurate do you need it to be? If you can live with a little coding of your own, more-or-less accurate location resolution, and giving a redirect response to browsers that  hit the ""wrong"" server first time around, then you could use the freely available ""CeoLiteCountry"" database to handle it yourself. I've got it on a list of ThingsToConsider for one of my planned projects. We would like to use a DNS service to direct users to the nearest server, but can't find any. Does anyone have an recommendations for solutions. A cheap, hosted solution would be preferred as we would rather not run our own DNS servers. We've got a website visited by users mainly in the USA and UK. The site will be run on servers in both these locations.",3
"With the deployment of a large number of systems, in particular a large number of servers where uptime and availability is of prime important, doing a major version upgrade every six months is a lot of work, particularly is you have servers co-located, or a widely dispersed (e.g. nationally across the US, Canada, and/or India) network. So one of the goals becomes maintaining support over a longer period of time, while being proactive with security updates, and trying to strike a balance in regards to supporting new devices (e.g. new CPU model, new motherboard chipset) that newer systems may be based upon. A lot of companies have taken to life-cycling enterprise hardware ~3-4 years, which matches the longest regular manufacturer & OEM's warranty term. Of course, some try to combine new hardware purchase with new OS upgrade if possible to reduce effort. Thus GNU/Linux distributions with longer support periods tend to win in usage preferences. While many GNU/Linux based servers do use CentOS, many still use other options, professionally I use Debian for workstations and servers. Other suitable options would be Ubuntu Server (LTS), ArchLinux, or Slackware.  The major rationale behind using CentOS or Red Hat Enterprise Linux (versus Fedora Core), or Ubuntu Server LTS or Debian stable (versus Ubuntu desktop latest) has to do with release schedules, and support (updates) lifecycles.  One reason that my employer didn't go with CentOS, was it wasn't around then, and White Box Linux which was available appeared based on a small group of contributors. At the time Ubuntu didn't have a server edition as far as I recall, and hadn't launched their Long-Term-Support releases, so we went with the long time stable, albeit slow release process of Debian. I believe one reason that CentOS is commonly found in the server environment, is that is work-alike to Red Hat Enterprise Linux, which many 3rd-party training courses base their Linux training upon, so system administrators are familiar with RHEL and CentOS specific configuration file layout and included configuration GUI front-ends.  I hope that answers some alternatives distributions that you will also find on servers, as well as some of the rationale behind the decision process in general.",1
"They tested it that way before they shipped it.  Why spend any more time to disconnect the cable?  There are another thousand to test today. What is the reasoning behind attaching a cable in the first place and why the VGA cable in particular? People are stupid. There isn't really anyway to get around that. Some people wouldn't know where to plug that same cable in on the computer if it wasn't color coded. I wonder if it has something to do with the fact the older monitors came with a built-in VGA cable (no connectors). When opening a new monitor I am amazed that the VGA cable is always attached. Even with new high end monitors that include Display Port and DVI cables the old VGA cable is always attached while the others are just left in the box. This is a little pessimistic, but I would imagine ""user friendliness"" is part of it, so that the most commonly used connection is the easiest for the layman to use. And not everybody makes a connection between these connectors and a monitor, unfortunately; everybody's gotten used to VGA, because it was (and still is) used on older monitors, quite a lot of modern LCD TVs and a lot of projectors.",5
"run debchange --nmu and type in a description of the change you made. This will auto-incremement the version number so that the system will not try to ""upgrade"" your custom package back to the standard one. Well, it comes down to recompilation. If you don't want a full debian approved package creation environment (the right way)I've tended to use checkinstall for quick and dirty package creation (the easy way) - its not the debian recommended way, but its a lot simpler than the other way. configure then make, and substitute makeinstall with checkinstall. Then install your new package with sudo dpkg -i Note: People have found issues with the aptitude hold command, so imho you should prefer the dpkg command to hold the package. Alternatively, you could hold the package via the Synaptic package manager GUI interface (Package > Lock Version). My preference is to use dpkg because it has worked well for me. The answer to #3 and #4 is that you should put the package in hold status once you've installed your version. This tells the Debian package management system to ""hold"" any future changes to this package, in effect lets you manage the package manually. If you download sources from official site, than there should be package itself available via apt. So you cat just download ready package, modify it and pack again:",4
"I had created 3 linux instances on the Amazon EC2 cloud without a passphrase for the private key. I had stored the private key in a file id_rsa in one of my instances(the master) to allow it to ssh into the slaves. I am in the process of configuring hadoop in my nodes. When you ssh to one of these instances from the master what happens?  scp should behave just like ssh in regards to authentication.  If you can't ssh successfully then debug at the ssh level (ssh -v is your friend).  ssh -v will provide information on the steps it's taking to authenticate.  It may not be using your private key, the debug output should show what key it's trying to use.  Also ensure you have permissions setup correctly for this file. However, after configuring hadoop in the master node, when i try to push the configuration into the slave using scp, I am prompted to enter the passphrase for id_rsa file. When I leave it empty and hit Enter I get the Permission denied (publickey) error and then Lost connection.",2
"I've done this before, here's some PHP code that I used.  The ftp1.optonline.net link probably isn't helpful to you, you basically just need to find a large file on a http/ftp server. YOu could probably cook something up from ab and cron, installed on one or more machines, just doing performance tests on your web server. I fear that our ISP suppling our web server's connection is not providing a good service. What software do people use to monitor the spped of a web server's internet connection? (or any internet connection for that matter). I am thinking something that will periodically monitor it to allow a picture to be built up over time. I suggest to create a simple html page (to rule out database issues etc) and then use a website monitoring service like AlertFox. Once you have those, what you do with them is up to you.  I'd suggest RRDTool, though it's interface can be a bit confusing. you can use http://www.speedtestfile.com/ and cron or Task scheduler to run this file download every few minutes. This wouldn't necessarily tell you if it's your ISP ""at fault"", but would at least show if there's a variation throughout the day. If everything else stays the same, the only performance parementer is your ISP's internet connection speed. Do you have another machine you can use?  Basically, use FTP and a command file (see 'man ftp') to upload/download a semi-large file.  You can then use grep to grab your upload/download speeds.",5
"A simple rule is, if your GameObject is not really static and needs to be moved or be altered in any other way, do not mark it as static. Make sure you are not trying to prematurely optimise your project, make something that works, and when that's done, only then start optimising on the most performance-intensive parts. When you set a GameObject as static, Unity starts optimising behind the scenes and assuming stuff about your objects. If for example you mark multiple objects as ""batching static"" you are basically telling Unity that these objects will never move. This means there is a chance on your final game all these objects will be ""merged together"" as one mesh, and be rendered like that, to make performance faster. Although I can't know in detail, I image that when you are disabling/enabling static objects in a scene, you interfere with these optimisations, as it's not possible to disable part of a mesh. This is just one example, as far as I know the optimisation behaviour in Unity is not defined, and can be different from platform to platform as different things can be faster or slower on specific devices.",1
"Ideally, before anyone can really give you a definitive answer on the problem you need to have a telnet or something and get a server response telling you why you cannot connect in the first place. The message you posted is a failure to connect which doesn't necessarily mean they refused your email, it seems more likely a dns issue or some other connection issue. Some things to check/think about are: Where is the NDR coming from, presumably your IIS SMTP server, although I don't work with IIS SMTP very often so I'm not sure if the IIS SMTP server is capable of generating NDR's. This was the issue we had specifically.  Our local DNS server was resolving the domain to a 127.x.x.x IP address.  That was identified immediately as the problem and we tracked it down to an issue with MalwareBytes not allowing the DNS server to resolve the domain.  We had to disable MWB, clear the DNS cache and then query the domain again to get the correct IP.   In any event make sure to enable logging in the properties of the IIS SMTP server and then check the logs after you receive an NDR. If there are no entries in the log file that correspond to the email that generated the NDR then I would suspect a problem with the IIS server's dns client being able to resolve the MX record for the recipient's domain. If there are entries in the log file that correspond to the email that generated the NDR, then the SMTP status codes in the log file should clue you in to what's happening. I may be a day late and dollar short but I just recently ran into this and alas, your question came up.  My turned out to be bad DNS resolution as a result of MalwareBytes.     If it does not connect or just gives a black screen, press ENTER.  If it dumps you back out to  the command prompt it's not connecting or the server is refusing it. Do the domain names you are sending from have spf records and if so, is the ip you are sending from included in the spf record? Who are you hosting your application with? I've had problems with shared hosts having all sites hosted on their servers blacklisted into the spam hole. Are reverse DNS records setup properly for the IP you are sending from for the domains you are sending as? Step 2:   Telnet to the mail exchanger domain on port 25.  You should see the following.  Try to type EHLO domain.com if it connects and you should see some more things pop up.   I ran packet capturing on our firewall to see if I could see the traffic going out of the e-mail server.  It was not so I knew the packets were stopping within the network.  I also did not see any blocking messages in the firewall logs specifically blocking the traffic.   Ideally, before anyone can really give you a definitive answer on the problem you need to have a telnet or something and get a server response telling you why you cannot connect in the first place. Just because your application isn't sending spam doesn't mean that another app on the shared host isnt. Since it's a shared host, all smtp traffic comes from the same source, which can end up being blocked. I could see the e-mails hanging out in the IIS SMTP queue and eventually it would fail.  Here is some things I did and ultimately figured out the cause.      Can you check your application server to make sure it is properly resolving the dns and MX records for the domain you are trying to email. Step 1: Go to MXToolbox and plug their domain name in for the MX Lookup Tool.  This will give you the correct IP address you'll verify in step 2.   The message you posted is a failure to connect which doesn't necessarily mean they refused your email, it seems more likely a dns issue or some other connection issue. Some things to check/think about are:",4
"If you have physical access to the server, you can always boot from a rescue disc, or even a liveCD and mount the old filesystem to copy the important stuff off. Reboot your computer to get to the grub boot screen, press e to edit one of your Grub boot configurations and edit the line starting with kernel. Append init=/bin/sh and boot the modified configuration by pressing b. When you arrived in your shell (without logging in this time) remount your file-system so it's in read/write mode: mount -o remount,rw /. Now you can start changing passwords with passwd now :)... this will drop you in at a shell prompt, you can then change your password via passwd command or bring up network manually ( ifup eth0 ) and move the files off the machine from single user mode. Booting in single user mode won't work for Debian and Ubuntu because you still have to enter the root password for maintenance mode. at boot time press esc to get the grub menu to appear ( just bang on esc key as it begins boot )  and then edit the menu line to a ' single' to the end of it. Or you can choose Rescue mode, this will also let you do recovery as well.",3
"A nice app is Windows Tile Color Changer if you just want to change the background color of your installed apps.Thanks for reading! Save the changes and rename the file to appname.VisualElementsManifest.xml e.g googledrive.VisualElementsManifest.xml. If you like you can change the icons/images of any application you have installed.Just right click the app-open file location-open file location until you meet the original .exe file,like the example before,except that you don't have to copy and paste the .exe file. To change the foreground value write dark inside the quotes or if you prefer a custom image 1024x1024  (prefer square images and be careful not to exceed 200 kb,or the image might not show at all). The final step is to rename the shortcut and pin it to the start menu and voila,you have a fully customized app tile. In that folder copy and paste chrome.exe file and rename it to you app name (e.g Gooogle Drive to have googledrive.exe). Edit the code so it matches your standards.In the example,the background color (you can find the corresponding hex values on the web) is black and the foreground color is white. Select properties and in the Target replace chrome.exe in the path with googledrive.exe in this case. How to create custom apps from websites with Google Chrome (or any .exe file) and pin them with a custom icon or image in the start menu (Windows 10) It will probably be something like C:\Users\username\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Chrome Apps. You can  access it via the apps shortcut in bookmarks bar.Set it to open as window and create a shortcut in start menu. Put a copy of your image or icon in the same folder and rename it to appname.png (e.g googledrive.png).",1
"This means that you do not have to specify the port (http://www.example.com:80, https://www.example.com:443) when using a web browser. One way of boosting performance would be by having a load balancing device/service listening to TCP80/443 which would then redirect the request to servers on different ports and/or ip (Local Balancing) or even different remote sites (Global Balancing).  But this is another topic altogether In addition to this unfriendliness, there is no performance benefit: a port is just one part of the (dst ip:port, src ip:port) 4-tuple which uniquely identifies a TCP connection. If two connections share a dst ip:port, that doesn't mean they share some system resource - they can reside in different threads, or different processes. The server doesn't waste resources by handling connections in one or more ports. Server resources are allocated to handle connections, and the port number is just a way to connect a specific program to a specific connection. Now, if you have logically different services which both happen to use HTTP, there is no problem with running them on different ports. It just makes the URI a little uglier. For example: the HTTP server knows that he'll listen to connections that come in the port 80. And the server knows that anytime he receives some request on port 80, he'll handle it to the http server. After that, the http server will handle the communication and then will consume resources. Why isn't it sensible to dedicate more than one TCP/IP port to http? Although admittedly naive, isn't it somehow intuitive to think that server performance could somehow be increased? Having a default is useful precisely because you don't have to type it into your web browser with the URI. Also, most Proxies and Firewalls will not let connections to those ports unless specifically configured to do so (Without configuration, Outgoing proxies won't be listening to non-default ports, hence will not forward the request to the webservers, while Firewalls would simply block non-TCP80/443 connections attempts) Port 80 is a well-known port, which means it is well-known as the location you'll normally find HTTP servers. You can find it documented in the HTTP/1.1 RFC. If you want to have a webserver listening on any other ports, the users have to manually add the port to the URL, or it has to be encoded in any link to that particular port. If you run an HTTP server (or in fact any service) on a non-standard port, you force the client to remember which arbitrary 16-bit number you chose and type it in.",4
"Judging from the results of a search for ""break microsoft word password"", it is not so secure. If you really need to keep it secret, then encrypt the file. Another option is to use an email service that offers PKI secured e-mail. You and your recipient would both need accounts. You could use 7z to compress and encrypt the file.  It uses AES-256 which is the same encryption standard used by the US government. One should prefer methods that were thoroughly examined by mathematicians and computer scientists. It was one of the arguments against accepting Office Open XML document format as an international standard. If you don't trust Word, you could create an encrypted file container using TrueCrypt. There's a good tutorial which explains how to do this. On older versions of Word, it was like putting a cheap bicycle lock on the handle bars. It was easy to crack, and didn't really make it difficult to read the file. You could open the file in a hex editor, and read most, if not all, of the data fairly easily.",5
"We recently migrated to a Windows 2003 R2 Enterprise x64 Server (SP2) as a print server.  For the most part, we found that it wasn't too hard to get both a x64 and a x86 driver for the printers we were using.  Shortly after switching, we noticed that certain printers were taking far, far longer to spool their jobs.  In particular, we noticed that our HP LaserJet 8100 was taking approximately 10-20 seconds to spool a job compared to its previous behavior of spooling almost faster than you could click. Is there a solution to improving the Universal Print Driver performance or am I stuck going back to a x86 print server? Unfortunately, the 32-bit specific drivers cannot be added to the x64 printer share because of the difference in print name.  Apparently you're only able to add 32-bit drivers if they are named exactly the same thing (i.e. they must both be Universal Printer drivers).  This has created quite a dilemma.  The performance is so poor with the universal print drivers, it makes multi-print jobs take many times longer than they did before.  Doing a stack of prints for our Engineering team literally takes hours where before it took a half hour. I was migrating from a Windows 2003 print server to 2008 R2, within the first few minutes of testing we had major print delays. It was taking at least 30 seconds to spool each page, I tried several suggestions from this site and others with no avail. I decided to try different configurations myself, after some trial and error I found the fix for me was to change the print processor under the advanced tab to hpcpp115 RAW. Hope this saves some headaches. At first we suspected it might have something to do with the x64 version of Windows managing the x86 client print requests.  However, the behavior only seemed to manifest on certain printers.  We eventually narrowed it down to the HP Universal Print Driver.  Any printer using that driver was extremely slow spooling.  HP doesn't offer a printer specific W2K3 64-bit driver for our LaserJet 8100, only the universal driver is available (as of 2/25/09).  They do offer an 8100 specific driver for 32-bit systems in addition to the Universal driver. It seems our options are limited.  If we return to an x86 Windows Install to support the 8100 specific drivers, we lose the ability to support x64 systems.  It would be a waste of money and resources to create both 32-bit and 64-bit print servers.  It would be a lot nicer to eliminate the Universal print drivers or find a way to improve their performance.",2
"I think it the solution must be something to do with routing but I'm afraid that is where my knowledge is lacking and I'm coming a bit unstuck with it all. Just turn NAT off and the firewall (or open needed ports) on the remote APs, add routes to the remote subnets pointing to the outside interface address of the Router/AP at the remote locations. Each device has plenty of modes and options (gateway, bridge, repeater, etc.) but we cannot seem to get them all working the way we would like. Should we be creating separate subnets at each location or should we extend the original network IP range to each site? The first device in the chain connects to our network via cable and is assigned an IP address on what it calls it's WAN side. It's LAN side then has a different IP address so I think that each wireless device is creating it's own subnet (they seem to be in 'gateway' mode). We have 2 satellite locations which are linked to our main network with ""Wireless AP/Outdoor CPE/Network Bridge/Repeater/WIFI Signal Booster & Amplifier"" devices. There are many options to fix this.  A simple solutions would be to set the devices comprising the links into bridge mode, and have a separate VLAN for each link at your main site. The main site would then perform DHCP for the remote sites, and you would have routes to the remote sites. We can access the internet from each location but we cannot seem to ping/remote to any of the devices connected to each of the locations. Bridging will work but will come with a lot more configuration and opens up more problems from in operation. DHCP over WAN will cause local resources at the remote locations to become unavailable if the main site or the connection goes down. Also DHCP over a wireless WAN link can lead to problems if that link is ever saturated or if it get's affected by weather conditions. Also you'll have to think about where your gateway is going to be. Also those links will now have all layer 2 traffic on it when it wouldn't need to be. I don't know the bandwidth of them or current utilization but this could issues that are easily avoided.",3
"In the past, I've dealt with this kind of problem in a number of ways, the best of which will depend on your environment.  QoS is an option, though it's kind of a blunter instrument than you may want, as noted above. Alternately, as suggested by Andres, you can install bandwidth-limiting programs on that PC, and depending on the OS and environment you may be able to apply bandwidth limiting settings natively.  If it's a specific user causing this disruption, consider using (or creating) acceptable network use policies to deal with it that way. Finally, as it's a wireless router (and I assume the client at issue is connected wirelessly), have you considered trying to improve the performance over wireless?  Are you having channel interference or wireless congestion, can you maybe improve the situation by adjusting the antennas or moving the router, or eliminating obstructions, etc?  Looks like a consumer-grade wireless router, which would severely limit your options, in addition to the number of clients and amount of traffic it can handle without problems. Similar solutions might include client-specific settings at the firewall (if you have one), or fun with routing tables... such as redirecting all traffic between that client and YouTube to nowhere, or just all traffic from YouTube in general to nowhere (again, in the example that YouTube downloads are throttling your connection). If, for example, this one computer is causing problems by downloading a bunch of YouTube videos, using QoS to throttle it at the router would also throttle regular web traffic, which might not be desirable. You might also consider setting up a 2nd bandwidth-limited VLAN, if your router supports that - I've had to to do this in situations where a large number of users' personal web activity on the corporate network were generating too much traffic for our poor pipes.   Problem is that one of the computers on the network consumes a lot of bandwidth - due to constant downloads - severely limiting the network speed on other devices on the network. A quick check of your router's manual reveals that it supports QoS (QoS Remarking based on IPP/ToS, DSCP and 802.1p). There are several devices accessing the internet thru a wireless network. I cannot change settings directly on the client-computers, but can have full access to the router.  Quality of Service is, in this context, essentially a mechanism for prioritizing different types of network traffic based on their destination port, source port, protocol type, MAC address, LAN IP address, connected ethernet port, etc. As the other two answers so far mentioned QoS, I'll point out that this will only be effective if the problem client is, say downloading over a protocol the rest aren't, or you want to limit ALL downloading on a certain protocol, or prioritize other types of traffic. You may also (at the router level) be able to apply MAC-address filtering so only authorized devices can get on the network (or so that specific devices cannot). Whether this is an option depends a lot on the router as well, and I'm not familiar with that particular brand. It's very dependant on the router, don't know for sure if you can limit on that router itself. A better option would be to configure QoS on the router but it's router dependant too. In your router's administration panel, look for QoS settings and see if you can create a traffic shaping pattern that works for you.",4
"My password manager was giving me a message that it could not find default browser so it finally clicked that this may be related to my Drive issue. I recently moved my back up and sync to another user profile on my Win 7 (I removed laptop from a domain I was no longer attached to).  I had a similar problem, where I tried to start back up and sync and received a blank window (no ability to login).  When I clicked ""If having trouble, use browser to login' the browser (Chrome) would not launch.  I do not have IE installed. I just ran into this issue on Win7-64 with 2FA enabled while attempting to install Drive on my desk for the first time. The 2FA request got sent to my phone (Nexus 5x) stating that another computer was trying to sign on to my account.  I answered YES it's ok, the phone beeped and the notification went away.  The Sign In window on the desk never acknowledged that auth, apparently.  Fixed the issue by selecting ""Try another way to sign in"" then ""call me"".  An automated call gives you 6(?) digits to enter into the desk Sign In page, which then worked as expected.",2
"As Draco18s mentions, when you have sensitive update orders, you can control the order in which scripts Start / Update under Edit => Project => Script Execution Order. Though generally it's better practice to have the object fully initialized and ready to use in Awake if you can, to avoid hard-to-diagnose update order bugs sneaking in later. All you're guaranteed is that it's had Awake (and, if spawned in the enabled state, OnEnable) called, because this happens before the AddComponent / Instantiate methods return for dynamically-spawned instances, or before any Start methods for objects loaded with the scene. The key word here is ""just"" - Start is called when Unity is about to run a FixedUpdate or Update on the script that hasn't yet been Start()ed. Since EnemyManager.SpawnEnemy() isn't called from inside EnemyManager's Update, there's no particular reason to expect EnemyManager's Start() to run first. MyGameManager's Start has as much right to run earlier, if that's what the engine decides to do (and it doesn't have to make the same decision during testing and in a built executable).",1
"... and deal with raw files. rsync -aAXv /path/to/mounted/mmcblk1 /path/to/destination/folder. This may or may not work for you depending if your filesystem is mounted or not. ... and accept larger space useage. Instead do something like dd if=/dev/mmcblk1 of=NanoPi.img bs=64K conv=noerror,sync status=progress. Smaller block size gives you a little bit more security in case of read errors. If dd fails to read at least one sector - rest of the block will be not processed. Block size is the max amount of bytes you may loose for one bad sector. conv=noerror ensures cloning continues in case of error. conv=sync shifts pointer in destination file forward in case of read errors so data remains aligned and thus valid. I am using a Nano Pi device which has Debian OS in it. The OS image is existing in its internal memory (8 GB). The total size of the used memory is somewhat 4GB. I want to take a backup of this OS into an image file. You can not do this easily using dd because block device is just a dump of bytes. It may contain data of long deleted files in unallocated space. dd is not aware of that because it is filesystem's job. You have few options: The total image file size goes up to 8GB. How can I make the image of only the used by memory and not all?",2
"For what it's worth, I just checked the PDF you linked to with Safari 4.0.4 on Mac OS X 10.6.2 and while there is some Engrish, the PDF it renders flawlessly without any onscreen ""garbage"". Perhaps you're having Unicode issues (more common on Windows than Mac OS)? The download of file 1 failed for me, file 2 I could open with xpdf, a fast and open-source pdf-viewer. I guess it can't handle forms, but for pure text and grafic I prefer it for its fast startup time. Some documents can be protected from being converted to text by fooling the Adobe Reader. For example letters can be drawn in several overlapping shapes in such way that visually they would still look the same, while text recognition software would fail to recognize text. Your document is an example of such protection. Unfortunately it cannot be helped. PDF documents do not actually contain any letters, but they contain shapes of letters. In other words instead of reading a letter and drawing it on the screen Adobe Reader as any other PDF reading application would simply draw the vector graphics encoded in the file. One way would be to print the document into an image and let text recognition software recognize it. Higher resolution for the image will improve the quality. This method however is not really handy. However, some PDF readers come with software that allows to analyze the shape and recover the text by using text recognition. It works same as if you scanned a paper of printed text and used software like ABBYY FineReader to convert it back to text, but due to infinitely high quality of vector drawings results are typically much better than for scanned documents. Simplest way to get around this is to open the file in a recent version of Google Chrome with built-in PDF reading plugin.  Then you can use Chrome's search feature to find text, and copy-paste works correctly.",4
"Note: After step 4, my VM forced me to login to the workstation again. Strangely, my Cisco session was not disconnected. There may be some Cisco configuration that may disconnect in this circumstance and cripple this workaround. I have considered connecting via RDP to the VM instead of using the VPN Client, but one of my VMs is primarily used to connect via VPN to another network, and when I tried to RDP into that machine and then connect to the VPN (using Cisco AnyConnect), I got an error saying I could not connect to the VPN through an RDP session). You can paste text into the virtual machine (Clipboard -> Type Clipboard Text). It should work with Ctrl+V but it regularly doesn't for some reason.  For your specific case of wanting to RDP, then connect through VPN, you might try adding another NIC to the VM and using one for VPN and one for RDP. I use Hyper-V on a development machine for multiple reasons and one of my biggest annoyances is that I can't cut and paste to/from the virtual machines. What is annoying is that the Hyper-V manager and SCVMM say they use RDP to connect to the VMs so it should work. Although so should the mouse integration before installing the integration services but that doesn't either. I've been through all the options and can't find anything that seems like it would allow this.  Does anyone have any good solutions? Easiest solution is to close the VM window and login with Remote Desktop.  Clipboard sync works correctly thru RDP.",5
"@DaveRix starts out good by saying ""use RAM for caches, not RAM disk"".  I very much agree.  But then he goes on to advocate dangerously high values for max_heap_size and tmp_table_size.  Those settings limit the amount of RAM allocated multiple time per connection.  That is, a complex query might allocate N*64MB.  This risks running out of RAM.  I suggest no more than 1% of RAM for them. Temp is used to store temporary on-disk tables for queries which go over the in-memory tmp tables limit. When MySQL needs to insert to a table and there is not enough space, the query stops - You get disk-full warning and you can either free some space or kill the offending query. Once tmpdir has room, @Cosi's query should run.  But perhaps not efficiently.  The next thing to do is look at the query, its EXPLAIN SELECT ..., the schema of the table(s), and especially the indexes.  Some common mistakes: I recently set up a ramdisk tmpfs on our mysql server and the performance improvement has been quite noticeable.  I've allocated 24 out of 32 GB's of ram for the ramdisk.  My only concern is what happens when there's a terrible query that ends up requiring more space than the allocated tmpfs?  I've read in a few places that it depends on the type of query but wanted to confirm that in some instances such a query can effectively shut down mysql. Grrr.  Neither of the answers so far quite says the right things, so I feel obligated to give yet another Answer. MySQL works much better when it has more memory allocated to things such as buffers and the innodb cache. It's better to have plenty of space available for the tmp on disk, as if you do end up filling it, MySQL will stop as described by @jkavalik's answer. Personally, I'd be allocating more RAM to MySQL itself rather than the RAM disk, and then putting the tmp store on physical media. Also make sure your max_heap_size and tmp_table_size variables are set to as high as they can be (64Mb), so that the majority of tmp tables are handled within memory, and only those larger than 64Mb end up on disk.",4
"So I have enabled event logging, and all the needed events could be observed with Event Viewer. What is the best option to query that log and generate user-friendly reports for the files that actually was accessed (i.e. copied from the server)? If you search Google you will find a lot more products which provide this, including open source and other commercial offerings. In order to get useful reporting based on the events you are now seeing, you will need to either write some sort of script which will parse the event log (Powershell, Python, Perl, ... come to mind), find a script that somebody else has already created, or install a third-party software product. The last option should be the easiest, but obviously has a cost associated with it. I have had good experience with EventSentry, which will automatically parse file access tracking events and provide reporting through a web interface (Compliance-File Access). This link gives you an example of what a report could look like, reports can be automatically emailed as well. I have a Windows 2008 R2 server with several file shares. Manager wants to have some kind of report with a daily total number of file reads for each user, like",2
"At the same time these signal are missing in Mini DP pure connector. http://pinouts.ru/Video/mini_displayport.shtml So, additional circuit is required inside the adapter to convert signal levels from DP to VGA. Like this one: https://en.wikipedia.org/wiki/DisplayPort#/media/File:DP_to_DVI_converter_unmounted.jpg VGA is analog port using basically 3 signals. When signal is converted between HDMI, DVI and VGA the same 4 contacts are used (see picture): Where @Hardoman description is completely accurate, the shorter answer is simply that you can't magically convert analog signalling to digital.  VGA adapters typically only carry the existing analog signals to a display that supports analog inputs. (on the same cable that could do digital)  Display-port (and Mini-DP) are digital-only.  There is no Analog support.  So, you'll never find a passive VGA/Mini-DP adapter.  There are some active converters that will essentially take the analog signal, and process the data to produce a digital signal, but these generally are expensive, and only work for a limited set of resolutions/sync rates/etc... The chip on the board converts the voltage levels generated by the dual-mode DisplayPort device to be compatible with a DVI monitor. Another is required to convert signals to TTL VGA levels.",2
"So these Windows send back packets through the originating router. This is probably not a ""good"" behaviour, but for my need it is very nice. If you can setup your ADSL modem/router to disable its internal port whenever the internet connection goes down, then you have another option.  Just set a secondary gateway in the IP configuration of the hosts.  They'll try to use the primary, but with that interface down (the lights on the port on the switch for the ADSL router turn off whenever the ADSL connection goes down) the host will be unable to contact the gateway and will go to the next one (the LTE modem/router). The other option, again depending on your hardware, is if you have separated modems and routers (instead of all-in-one units).  If the modems can be separated from the router, then you can plug both modems into one router; this will allow for you to do route tracking and fail-over in the router (making the solution 100% on network again).  But this tends to require that you have an ""above-average"" router available. Since the gateways (inside port) and the host(s) are all on the same subnet, then the use of NATs like @KamilJ pointed out is very easy.  And it has the benefit of being 100% in the network. The answer to How to tell Windows 2012 to send packets back to the originating router,  la Windows 2003? is very simple: you can't. This all depends on your routers.  The ability to do this is uncommon in consumer@home routers, but is more common in business class (and the norm in enterprise grade) routers. Logically if your windows 2012 default router is the ADSL one it doesn't matter from where the original request came from, the response will always go to the default router in all cases. @KamilJ is correct in pointing out that this is best served at the network layer.  What you're looking for is a redundancy with fail-over of the default gateway to an active ISP. If you want the traffic response go back from where it came you can always source NAT the original traffic with the internal IP address of the router (this option is not always available). The drawback is that you'll lost the source info The Windows's network stack is simplier than Linux's one, and rare needs as the one described here are currently not addressed. The default gateway is 192.168.0.1 (ADSL) so the backup router is never used for outgoing connections. This case is hard because your traffic will be routed to the ADSL provider and lost there. Adding another default route will not solve the problem This is an easy case: just add another default route in the ADSL router going through the 4G router. When the Wan interface is down the first default route is disabled and the traffic will be redirected to the 4G With Windows 2012, the packets are sent back only through the default gateway, wich is the logical thing to do. With an old Windows 2003 and an old Windows XP, the incoming connections are ok from whatever router: when I connect through ADSL or 4G I can interact with these old Windows computers. I have one ADSL connection and one 4G backup connection, each with its own modem/router on the same 192.168.0.0/24 subnet: 192.168.0.1 (ADSL) and 192.168.0.2 (4G)",4
"You could try something like this, which I wrote to roll my nginx logs. I looked for some details on the tux userspace process but couldn't find any way to get it to reopen it's logs, so you may have to resort to stopping and starting the process. If it only corrupts one entry I can live with it - my fear is that the whole log file could be lost. rotatelogs can handle time or size based rotations for you, if you don't send the logs to a central logging server I'd use that, if you do send to a central logging server I'd send it to syslog, just let syslog create another socket which tux will write to (preferrably /var/log/tux), and syslog should handle the rest for you... I am considering Tux as the web server for a new CPAN mirror I'm building.  I've got it running and it's very fast but there is one big catch: how am I supposed to rotate the log file? Before I suggest something ""TUX has never been an integrated part of the official Linux kernel"". Are you sure you want to maintain kernel patches on your own? Personally I'd try to create a pipe which tux writes to and then use rotatelogs (from apache) to handle that stuff. Maybe even logger so that it uses syslog directly. I'm a bit curious here: What do you expect of incoming requests per second since you need a HTTP engine running inside your kernel? Isnt tinyhttpd fast enough? The logrotate application can do that for me but since the log file is binary I am concerned that this might lead to corruption at some point.",4
"I have the ability to write a quick script to clear these folders out every so often, but I would like to avoid that if possible. I have tried pointing it to a different location, such as C:\temp0\ and changing the registry key to reflect that, but it still has a limit on it. I have looked through Outlook 2010 settings and have not found anything. When I google or search on this forum I can only find band-aid fixes, which I'm trying to avoid. Recently I've had many users report to me with errors when opening attachments in Outlook 2010. The issue is because the temp folder assigned for Outlook attachments is getting too full and won't allow anymore to be saved to the temp folder. The easy solution is to open up the folder and delete everything inside of it, but I am finding that I have to do this constantly for people who receive up to 50 attachments per day.  Looks like I should have done a little more research. This is a known issue with Outlook and was resolved in Microsoft Office Service Pack 1. This explains why this was happening to a few users and not all of our users. Once I updated said client's PC with Windows Updates, the process tested fine.",1
"A disk imager should be able to make a full image of your card, even if the underlying OS can't read the partitions on that card. Win32DiskImager is popular on Windows. The image will be the size of the full card, whether or not you actually use all of the space, so a 32 GB card will yield a 32 GB image file. You do have to ensure that the target disk is larger than the source image.  Also you can write this image made from 32 GB to a 16 GB or 8 GB disk, it is not complaining about that the image is too large anymore. I am using a Win7 machine and a bunch of Pis. Some of them took weeks to configure. I would like to backup their SD cards in case they break. It should be backed up so that if it breaks I could simply buy a new card and reload the backup onto it and plug into the Pi. When I put the cards in my SD card reader on the Windows machine, it does not recognize it as containing files. It takes about 5-10 minutes to complete. After that boot the pi using this new sdcard. And enjoy the same OS which you have it on the old sdcard",4
"For just brute-force verifying your data, however, I think the best you can do is to manually mount the sparsebundle on your network drive, and use something as simple as 'diff -cr' to compare the bulk of the files from the 'Latest' version against your system drive (which is obviously going to throw up some that have changed since TimeMachine last ran.) Another way is to use this tool to install Mac from Time-machine to another virtual Mac on HD. I have not tried this yet. Not sure if it is possible to make a virtual box with OSX. The Time Machine command-line utility has an option to do this: ""tmutil compare"". Check ""man tmutil"" for all the various switches available. Doing a Time Machine backup to a network-based share is known to be more risky and fraught with problems. So, if you really, really, do not want to run the risk of hitting trouble with your upgrade and then having further trouble getting back to where you were then I would strongly recommend temporarily using a local disk and a tool like ShirtPocket's SuperDuper! or  Carbon Copy Cloner As per Mac OS X 10.6.4, you can initiate a verification of your backup by option-clicking the Time Machine icon in the menu bar.  I'm not sure what exactly is verified, but when I did it, Time Machine recommended that I start a new backup to improve reliability. You may try one application from Mac's utilities (Application->Utilities): Migration Assistant. With this tool, you may use one option to transfer information from your backup Time Machine to local computer.",4
"The SM Bus is going to be you chipset.  You want to get the most current chipset drivers, basically three of the 4 listed under the chipset section.  Skip UTIL_WIN_R304257.EXE Then under Network get NIC_DRVR_WIN_A01_R294111.EXE unless you sprang for the intel card.  Once you have those guys knocked out you should be clean.  But if anything else still lacks just go to Devman, then the properties of the device then to the details tab.  Then drop the selector to Hardware IDs and then just copy the stuff you see below into a Google and that will likely find the correct component so you then know what driver you still need.  The Broadcom USH COULD come into it but I think that will be knocked out along with the SMS Bus. The Dell Latitude E6320 comes with one of 3 wifi cards.  My machine came with an Intel N 6205.  I downloaded the Intel WiFi Link 6205 drivers from the Dell site and everything works fine now. The R308494.exe also worked for Windows 8 Pro 64-BIT system, you can find it here: http://ftp.dell.com/security/ it did prompted for an update, didn't run it yet. Install your chipset and Intel AMT drivers, restart the PC then tell me what is missing in Device Manager. Install chipset first, then AMT These are a pain the ass.  More than likely the Broadcom USH is NOT your issue.  The free fall sensor more than likely is.  I wouldn't even attempt to install the control point.  Worst piece of digital filth joke ware I've ever come across. Then go back to the APPLICATIONS section and get this one for the sensor ST-MICROELECTRONICS_FREE-FAL_A10_R309372.exe (5MB) In my case a file named Dell_ControlVault_A08_R308494.exe helped with 'Broadcom USH' and ST-MICROELECTRONICS_FREE-FAL_A10_R309372.exe (fall sensor) helped with 'Unknown device' (I had to install it even that the fall sensor was switched off in BIOS).",5
"Can anyone suggest a tool/method to simulate various scenarios when using WSUS?  Or am I asking the impossible?  I'm curious to know if I deploy X number of patches over slow links is there any way of determining when all of the machines on the other end of the wire will be updated?   Virtual machines in a test environment that allows you to implement rate-limiting (to simulate various link speeds) is the best solution. But, as Andrew noted, there are considerable costs associated with this. Not knowing anything about how your network is engineered makes this a difficult question to answer concisely. If you're concerned about bandwidth and remote WSUS servers isn't an option, then you can also tune your BITS settings in a GPO for those sites; the clients may get the patches slower but better than not at all. Another approach might be to simply break out your pencil and perform some back-of-the-napkin calculations. What's the link speed, how large is the set of patches, can they be multicast, how many machines on the remote end, and so on. Once you've considered these variables you might decide that it's simpler to deliver a single set of patches to a second WSUS server on the far end and move on with life. Do you have a bunch of mobile users, and you want to know when they would all be updated? But you don't know when they will connect and over what speed links? Apart from doing a dry run, which is very costly in time, there isn't an easy way to model this. Perhaps it would be better for management to insist that these machines connect for a certain period within which patches can be deployed?",4
"I agree with Ricky Beam that bonding cannot be accomplished with discimilar WAN technologies from different providers. However, you can do some load balancing with fail-over if your firewall is intelligent enough.  That said, there are ways to get the same net effect using a 3rd party service. Sort of a ""multilink VPN"", but I wouldn't recommend it. Having said all of that, it appears your greatest desire is to increase your individual download speeds and this will not address that directly as  Multi-WAN operates on a connection-based round robin basis.  Multipath TCP (RFC 6824) could be a solution to explore. In my understanding you can split a single session across different technologies, so it should be possible to divide up- and downlink and spread it across different links. That being said, it is an ongoing effort so actually using it requires to employ an experimental Linux kernel module (download and documentation here) and to build a Linux router that operates that way.  The term is bonding. And no, you, the end user, cannot combine the connections from two different ISPs into one larger pipe. You can load balance between the two, but the speed will be limited to the connection selected.",3
"Yes, they both measure the exactness of y and y_hat and yes they're usually correlated. Sometimes the loss function might not be accuracy but you're still interested in measuring the accuracy even though you're not optimizing it directly. Google's TensorFlow MNIST example minimizes/optimizes cross entropy loss but displays accuracy to the user when reporting results, and this is completely fine.  Sometimes you don't want to optimize accuracy directly. For example, if you have serious class imbalance, your model will maximize accuracy by simply always picking the most common class, but this would not be a useful model. In this case entropy / log-loss would be a better loss function to optimize.   Log loss has the nice property that it is a differentiable function. Accuracy might be more important and is definitely more interpretable but is not directly usable in the training of the network due to the backpropagation algorithm that requires the loss function to be differentiable. When your preferred loss is not directly optimizable (like the accuracy) you use a loss function that behaves similarly to proxy the true metric. In case of binary classification you would use a sigmoid at the end and a log loss to approximate accuracy. They are highly correlated. I'm a bit confused by the coexistence of Loss and Accuracy metrics in Neural Networks. Both are supposed to render the ""exactness"" of the comparison of y and y_hat, aren't they? So isn't the application of the two redundant in the training epochs? Moreover, why aren't they correlate?",3
"If you want static rather than dynamic data you can copy column c afterwards and paste special(values). none of these answers will work.  what WILL work is to take the destination sheet, filter what you want to insert, put a color fill into the selection, un-filter, then sort (not filter) by color.  you can paste your information on the sorted by color cells, then change the color back to the original color and re-sort by whatever method you had before. A better approach might be to use an IF formula something like =if(A2='filter criteria',B2,"""") in column C.  You have to do it the other way around. Copy/paste everything, filter out what you want to keep then delete the contents. You can also do this by copying as normal and using the paste special ""values"" function.  However, you must paste on to a completely separate excel session for this method to work.  You can then copy from the new session back to the original session. Yes, I agree this is stupid. You can do it using the 'fill handle' - select the cells you want to copy and drag the fill handle across to copy them into the next column. This only works for a contiguous selection, and only if you're copying into the adjacent column. But presumably you could always move your column temporarily. I can't see any other way to do it without recourse to VBA.",5
"Whenever the client makes a request to the server, it sends a frame of 5 bytes (not including the 40 extra bytes that come from IP and TCP).  I have a TCP server built with sockets in Python. The application I'm building is time-sensitive, so the integrity of the data is important, therefore we need TCP. The bandwidth is very low.  On the other side, the server either responds with a frame of 5 bytes (in most cases) or a frame of > 70 bytes (generally every second) And there's a client which requests data from the server every 50 ms. The client gets as response an OK message in case the server doesn't have the data or the actual required data.  Everything runs fine on the local network (no lag at all), but whenever I connect to the server from the public IP (I'm port-forwarding) it lags a lot. The lag can go up to 15 seconds (at that moment it times out), which is incredibly much. Most of the time the RTT stays at 200-210 ms. On WireShark I can see that there are lots of (spurious) retransmissions and dup ACK.",1
"Secondly an Atom based board would not cope well with all that even with light use, they are optimised for the sort of things a single user does to make up for their lack of raw power. I wouldn't trust it. I use old systems for testing, for one-off things that aren't important tasks, etc...you'd be talking about slow systems with parts that are either uber-cheap or old and thus could fail without a good backup in place, so I hope the data isn't too important. If you're talking about a server that is actually used for business, i.e., you have money riding on the data or will experience difficulty if it crashes or loses information, you seriously need to reconsider the amount of money you're budgeting for it or consider outsourcing...and it sounds like you need something reliable if you're going to use it as a PBX. If you are set on that price then perhaps you could get a 2nd hand slimline desktop.  This would also allow you to use a pci card. For something more, you can buy a Dell T20 which basically is a PC re-pourposed as a very entry level server (the basic config has no ECC RAM, for example). $200 is not safe.  If you want simple bottom end get the cheapest Dell you can find and add second drive to it to give you RAID1.  Please...reconsider the budget. Being budget conscious is one thing, but it will cost you more in aggravation and headache (and repair costs!) down the road if you go too far with saving money; if this is an important part of the business the budget should reflect that. Please please please, as someone who has seen others make the same mistake and pay for it later on... One solution for that, especially if it really is lightly loaded, is VMWare or some other virtual solution. I'm not too happy with VMWare now that they're bullying other vendors to cut functionality from their products out of fear of their own business model being threatened (thus creating bigger opportunity for Citrix, Xen, KVM and Microsoft's Hyper-V) but I mention them because we're currently running 7 servers of light activity on a single Poweredge 2950 from Dell without issue so far. Here's an article from a guy who made a rather beefy server for $600 to run ESXi (which is free) and then you can create many small servers for each purpose and thus test, upgrade, and swap out platforms as needed (and if you get a better server later, install your VM platform of choice and just copy the virtual machines over with minimal hassle to your software installation and still get better performance!). The only thing that may be an issue is the PCI card for your PBX. It's true that some old systems will run and run and run, but if I had money riding on keeping that system going, I would have money allocated for the system, a backup system of some sort for proper data recovery, and a backup computer for replacing the loaded system should it (when it will) fail. For that sort of system your price is unrealistic.  Miniturised components cost more.  You might be able to get a netbook but obviously that would not take a PCI card. You also might want to look at finding a way to separate your services a bit if for nothing else than to simplify management. You're saying you want email, web serving, PBX, and a file server, along with antivirus and backup on it? I've had cases where program interactions can cause issues (especially antivirus)...something to consider while planning. Also upgrading and if you decide to change your platform of choice for certain functions (like CRM products) can cause issues. Is it going to be an issue if your email platform needs to be changed or upgraded and it interferes with your web server, or if in the course of fixing email you take down all the other services? A $200 system isn't really feasible unless you get an old used system or parts being thrown out by a college or other institution; you can cobble parts together to fit that bill. Thanks for the tip, I hadn't thought about 2nd-hand slimline desktops. The problem is that I need new hosts to sell to customers. This will be the safest way if you want a server.  If you go with less then this, your data is not that important to you so just go buy an old something used thing.",5
"However when I go to a .php file on a browser (using my VM's ip-address/~username/phpfile.php) it does not display it as it should. Instead it offers to save to file/asks what program to open it with. Interestingly though that dialog box does recognise that it is a php file. I'm running a Ubuntu Server on a VM, to test out different web forum solutions. I have set up a ~/public_html/ to be accessible with the apache2 web server, and that works fine. This needs to be done in httpd.conf for the directory in question. It would look something like this: You need to configure apache so that it will start the php interpreter when a .php file is requested instead of just returning the file's contents. Prizes go to whoever can explain why? :D (And by prizes I just mean a well done, no actual prizes I'm afraid.) Turns out files this behaviour was only apparent on files in the ~/public_html/ directory. All php files in /var/www/ work fine. Check .htaccess for AddType row. I had a custom AddType which caused bad content-type. Firefox helped me debug the issue, but you can also use wget.",3
"Syslog-daemon will then use pattern matching and perform $foo if some match is found. For example, in syslog-ng you can set up a log file hook and filter it like this: You can make use of the already available Linux tools like tail, grep, and named pipes. First, create a named pipe (fifo) using: Just configure it to keep an eye on Apache log file, or alternatively configure Apache to send logs to syslog facility with CustomLog directive and logger. If using this technique, syslog-ng will spawn your script background waiting for new stuff to appear. Because of this you need to modify your scripts to wait for input from STDIN; here's a short Perl example: Everytime a new line is written to log, I want that entry to be checked to see if it matches what I am looking for and if so x happens. When I am doing this manually I used cat or tail -f.  I dont want to run the script every 30 seconds via cron and go through the whole log (or even the last 5 lines), figure out which of those lines are new since the last time the script ran and then so some things. Third, use tail to read new lines that are appended to apache log file and redirect the output to the named pipe. I am trying to write a script that will monitor one of the apache log files and take some specific action.  But how should I go about monitoring the log file? The -F option means to follow the file by name which should make it immune to logrotate. So, it will follow always the same file name. The -n0 means to not get any old line. The grep is useful to direct only the relevant lines. If fail2ban is not suited to your needs, you can try and modify flog.c which was written by Markus J. Ranum: Idea taken from the nagios check_log plugin.  See http://www.kilala.nl/Sysadmin/index.php?id=715 for more details. This script reads from the named pipe and prints the line to stdout until it gets the ""quit"" word. This is just an example that can be customized. Using this solution, you don't need any cron job. Just run the script and the tail command shown above.",5
"USB uses 7bit addressing so you will be able to address 127 devices.  Of course the bigger issue is power when using USB devices, if there isn't enough power they won't work. As long as the device number of the currently connected device is less than 127, the new ""next device number"" will just be the current device number plus one. At ""current device number"" 127, the ""next device number"" is reset to 1. So my question is what is the limit on the device number?  Can it keep increasing to 999 or even beyond or is there a lower limit, such as 256 or 128?  With this information I can code in a check on the device number and force a reboot if it gets close to the limit. Device allocation also checks whether the number proposed to be assigned is already in use, and if so, will revert to checking for successive device numbers until a free one is found. I am developing a remote monitoring device using a Raspberry Pi.  In order to provide me with alerts it has 3G dongle in it.  However, due to power supply instability the dongle will sometimes change device number.  So, for instance, when it boots the 3G dongle might show as (using lsusb): Linux device numbering on a USB bus maintains information on ""next device number to be assigned"". This is naturally updated each time a new device is connected. So, all in all, there is a device number limit, but upon reaching it, the numbering will gracefully roll over.",3
"You may also just be having an issue where WebDAV is working properly but the tests are failing, thus preventing WebDAV from working.  You can bypass the WebDAV tests in the admin site by adding check_for_working_webdav=false to the config.php file. As discussed in the OwnCloud forum, the OwnCloud sync client also does not necessarily work with self-signed certificates.  Try switching to HTTP temporarily and see if that works.  (However, others have gotten self-signed certififcates to work with no problem, and the error you are receiving does not say that this is the issue despite the fact that the symptoms are the same.) Alternatively, this could be because your server is not allowing the WebDAV HTTP Verbs through to PHP.  See the OwnCloud setup guide for instructions on making sure these settings are set up properly. The most likely reason for this error is that your server cannot communicate with itself.  This could be because your router does not have NAT reflection so your server cannot communicate with itself on its public IP.  Try adding a cloud.mcsoftworks.net pointing to 127.0.0.1 in your hosts file (or your internal DNS if you have) so that the server knows to contact itself instead of going out on the Internet. in the IIS 8.5 manager there is a tab called modules if you open that it displays tons of modules search for WebDavModule and delete it.",2
"You don't need to check for input lengths of 0 or 1; your initial values of ""start"" and ""end"" will catch that: end <= start in either case. You are returning a String so for symmetry it is better to accept a String as parameter and convert to a StringBuilder internally.  You can change that if you have already done the  conversion elsewhere in your code.  In that case consider returning a StringBuilder instead, so input and output are the same class.  That avoids possible future errors from an invisible change of class, hidden behind a method call. Your code assumes that there is at least one vowel in the supplied text.  Currently it will fail on an input of ""zzzzzzz"" as the indexes will fall off the ends of the array.  If your input guarantees at least one vowel, then that is not a problem.  If it does not, then you will need to fix it. Your ""isVowel()"" method does not recognise A, E, I O, U as vowels.  You know more about the expected input than I do, so this may not be an error.  You can always use toLowerCase() on the character before checking. The class name should probably be something like VowelReverser, and the methods should not be static. I'm not familiar with StringBuilder, but it seems that it would be more natural to pass around a String instead. @heijenoort's reworking of the while loop condition is the same as mine.  Better to avoid break in the middle of loops if you can, and this one is easy to avoid. Rather than juggling start, end and charAt() you should instead iterate over the characters in the string. This should also remove the need for the initial length check. I find it confusing that you call a StringBuilder variable ""string"", better to call it something like ""text"" or ""sbText"". Why do you pass the number of test cases on standard input? Why not just process standard input while scanner returns something?",2
"Some applications would no longer run. If the swap was hosted on the removed disk, some applications would crash, and possibly the whole system would, too. This of course refers to data at rest. Any data that was being written to the internal disk when the system crashes is almost surely at least partially lost... but I don't believe you expected anything different. In all likelihood, yes. There are conditions and setups that could result in otherwise, but they're unlikely to apply. If it was mounted, yes. At the very least the ""Disk Dirty"" flag would not be cleared by a clean unmount, so at the next boot Windows would require a disk check, too. If by ""affect"" you mean data loss or hardware damage, both are conceptually possible, in the same sense that you could die of a meteor strike as soon as the disk is unplugged. All three possibilities are equally remote. It also assumes that you're using the latest version of the ntfs-3g Linux module, or an equally mature and stable software that understands NTFS, MFT management, journaling and staggered R/W, and properly implements data safety procedures. Otherwise, a kernel crash can do literally anything to any disk physically connected, including replacing an unmounted disk's partition table with pictures of lolcats. Unless you hacked the kernel or whipped up a distribution yourself, chances are that while this uncomfortable situation is absolutely possible, yet you are not in it. I expect that in both cases you will be able to access both, in both cases being required to check the filesystem integrity first.",1
"If I just type something, it jumps to the Google Search Box. Also Backspace moves to the Google Search Box. The spacebar does something different (it is like pushing PgDn?). Aside from all the good answers already posted, you can always install a keyboard macro processor like AutoHotKey in Windows or AutoKey in Linux.  Once you do that, you can define hot keys that do anything you can do from the keyboard (and a lot more) regardless of what the applications or Window manager provide for you. Also since anything that isn't recognized as a URL is interpreted as a search on my browser (default search set to google.com), if you don't want to append to the text, but, rather, replace it entirely, then you might as well do CNTRL-L (COMMAND-L in OSX) and use the address/search bar combo in Firefox / Google Chrome. Could might try hitting ""ESC"" and then just start typing; the typing will probably be appearing appended to whatever string you've already typed in the search bar. This works for me on Firefox on OSX. Since the search box in a website is not identified in a standard way across all websites, there isn't a dedicated keyboard shortcut to jump straight to it. What you'll have to do is just keep hitting Tab (or Shift+Tab to go backwards) until you end up in the search box again. These keyboard shortcuts will cycle through all the website's links and input forms. Some search engines will let you begin typing immediately to do another search if you don't click anything else. Other than the method above, sometimes they'll code their site with some javascript that captures keyboard input. Experiment and see what the site provides.",4
"The US Air Force Research Lab's Encryption Wizard (http://spi.dod.mil/ewizard.htm) is free, DoD accredited, simple file encryptor.  It handles passwords, smartcards, and certificates.  Its Secure Delete can wipe the senstive file from a public computer. With EW, from zero you can encrypt and send a file w/in a minute and the recipent can decrypt in the same amount of time (assuming you use a certificate or call the person w/ the password.)   To comply with Massachusetts's new personal information protection law, my company needs to (among other things) ensure that anytime personal information is sent via email, it's encrypted. What is the easiest way to do this? Basically, I'm looking for something that will require the least amount of effort on the part of the recipient. If at all possible, I really want to avoid them having to download a program or go through any steps to generate a key pair, etc. So command-line GPG-type stuff is not an option. We use Exchange Server and Outlook 2007 as our email system. Working with similar legislations, I've generally setup PKI/SMTP/TLS between two or more organizations that frequently send/receive private/protected information; I just setup a smarthost at each organization matching the domains in question to route mail through either a  site-to-site VPN tunnel when applicable or used SMTP/TLS to encrypt the mail in transit with Exchange. Now that being said, it really isn't as painful as you think. We have done this with hundreds of non technical users. What we did was choose two products - the free GPG (which Kronick states have GUI front-ends) as well as the pay for PGP software. We wrote up some really good documentation that could be sent to our clients instructing them how to use the software that they chose as well as trained our Account Managers on basic troubleshooting and how to use the software.  There are better big, intra-enterprise solutions but we've come across nothing better that can works for pretty much everyone everywhere at anytime. Wouldn't it be easier to have them check a website with the data encrypted via SSL, with a button to print the data on their end? That way you're not transmitting anything and you're in control of the dissemination of the data. We've had to go through something similar with our clients for PCI. The best way would be to use some version of PGP/GPG.  Actually, the law says to encrypt the senstive data, not necessarily the message.  If the data is a file (and usaully it is), the easiest by far method is to just encrypt the file.   As an alternative we also bought some licenses of winzip so that we could use the built in AES encryption with a pass phrase. The commercial PGP software has the ability to create an encrypted file that is opened by passphrase only as well. Although honestly using PGP has worked out so well i think i only create these types of files 2 or 3 times a year. Other than having Java, there is nothing to install or configure on either computer - just run the .jar file.  Encryption Wizard runs on Mac, Windows, Linux, Sun, and other OS that run Oracle Java. Anything with email will likely be too difficult for your users; they'll involve key generation or downloading a keyring or other things users will find to be a hassle or confusing. Your support costs will rocket up, unless the users just give up in frustration. That has kept 95% of the issues that clients run into out of the IT queue. For the other 5% we made IT resources available to answer questions, as well as in the worst case get on a call to help the client out.  Is there a program that we can use to easily encrypt an email and then fax or call the recipient with a key? (Or maybe our email can include a link to our website containing our public key, that the recipient can download to decrypt the mail?) We won't have to send many of these encrypted emails, but the people who will be sending them will not be particularly technical, so I want it to be as easy as possible. Any recs for good programs would be great. Thanks. Does it just need to be encrypted in transit (SMTP/TLS), or in storage/at the endpoints too (PGP, etc.)?  Assuming your goal is an extremely easy to use and deploy solution that'll work across your diverse customer base....",5
"The RTT between New York and Tokyo is ~180ms and the TCP window is tuned to a theoretical throughput in the region of ~40Mbps, aka 1M tcp_xmit_hiwat/tcp_rcv_hiwat. tcp_max_buf and tcp_cwnd_max are also 1M.  CPU utilisation is not a problem at either end, SAs say boxes look good. Network congestion on the WAN link is also not a problem, networks say network looks good. In fact everyone says every individual piece looks fine, it's just still performing badly! Any thoughts on how to optimise for this situation? or things to investigate that might provide a hint as to what is going on? The former is the key to the problem, if I can work this out then the latter shouldn't occur. However the latter is odd, I was naively expecting it to quickly ramp to peak throughput and stay there until it had got through the backlog. I have an application that is distributing data from New York to Tokyo over TCP running Solaris 10. Mean throughput is < 1Mbps, peak throughput can reach 20-30Mbps for seconds at a time though typical spikes are more like 10Mbps. Individual message sizes are small (~300bytes) and consistency of latency is key. This means we are attempting to remove batching aka so nagles is off & application is configured to send asap rather than queue then send. The problem here is that we frequently but intermittently see mysterious ""pauses"" where the sender gets EWOULDBLOCK leading to a buildup in an internal queue and then subsequent discharge of data. There are 2 problems here",1
"1) they do have a linux client - the web-client - you know when you log into the .net client - well it says in that little text bit at the top something along the lines of 'stop using this, it's going away, use the web-client' - well, that. 4) Given you only have two disks might I suggest that switching to SSDs wouldn't be too expensive and you can forget all this tuning you're trying to do. 5) This may sound harsh but it's not meant that way - get some training, even the basic ICM course teaches these basics. 3) Rather than limit IOPS why not just set disk shares to be higher on the higher-priotity VM/s - that way you're not limiting the maximum performance of any given VM but just weighting their responses under contention. 2) As a rule of thumb I assume about 200 random IOPS per 15krpm disk - I know you may well get more but it's a reasonable assumption - of course you're using R1 so writes get a 2:1 penalty and reads may get a small boost but again I wouldn't bank on it. Oh and your definition isn't really right - there are lots of different types of IO benchmarking, IOPS is simply a measure - in basic terms there's sequential-read/write and their random versions too.",1
"I am wondering if PowerPoint for iPad supports audio in presentations, and if it does, how can I modify my presentation to play audio? I'm pretty sure the volume slider was up. http://office.microsoft.com/en-us/powerpoint-help/compare-powerpoint-for-ipad-with-powerpoint-for-mac-and-powerpoint-for-windows-HA103762595.aspx I downloaded PowerPoint for iPad and uploaded one of my presentations that has audio in it. However, when I ran the presentation, I was unable to hear the audio. Everything else worked just fine though. Several people seem to have issues with Audio not being played on their ipad, but if you plug something into the headphone jack like headphones or an external speaker, then the audio comes through on that. Some people on ipads with a mute switch also mention that turning off the mute switch may make a difference as powerpoint looks at the notification volume instead of playback volume like most apps do. I'm using an ipad without a mute button and still haven't figured out how to get the audio from a powerpoint to play on my ipad, but it does work via headphones.",3
"I have created a new test Domain (FQDN of which let call as ""test.net"") governed by a Domain-Controller (lets call this mc as 'test-DC'). Under this new domain (test) i have hosted a IIS server which is suppose to use windows authentication with kerberos delegation. I have enabled windows auth enabled while other disabled in IIS manager. Lets say this IIS machine name is 'test-iis' and active directory has it username for IIS server as 'user-iis'. Application pool in IIS server is running under service account as ""TEST\user-iis"". I am trying to setup SPN for HTTP service type.  I am using IIS 7 for setting up a website under windows authentication. I am seeing authentication issue which i am almost sure that it is related to kerberos issue and i am wrongly setting up SPN. The scenario which i am using is as below. [Edit] One more thing, i am able to ping from IIS server to 2 other machine in the same domain but i could not ping the IIS server with other two machines. can anyone help me understand why that is so? My second question is. I am setting this SPN value in Domain Controller thinking DC is the entity that will verify credentials and all. Is my thinking right??",1
"From my point of view, everything can fail and the cluster would still be operating normally. But what if the gateway of the storage network fails? Then I guess it would work as long as the switch has cached MAC ARP entries but when they expire the cluster would go down. Do you have any suggestions for me how to make the cluster fully redundant and optimize it? On the left you can see the OpenStack Compute nodes, everyone connected via two separate 10GE Links to each switch. Those switches are connected via 2x 10GE trunk to each of the dedicated Ceph cluster switches who are connected with each other, so Ceph nodes can communicate with each other for redundancy. Note, that even when I have used different colors this whole network topology is in one VLAN. I'm currently designing an OpenStack cluster. The part where I'm currently stuck is the Storage Architecture. I am only focusing on the storage component of OpenStack, therefore the Compute nodes are not connected to the network node or management network yet. I thought of building two redundant Ceph clusters in different racks with a different fuse and UPS. So far so good. The thing is, Openstack Cinder communicates via the network with Ceph. Even when I have redundant switches in place what if the router/gateway of the Ceph network goes down. So in order to achieve full redundancy I need two redundant gateways, am I right?",1
"I had this exact thing happen a couple of weeks ago.  It ended up being a SID issue, I simply temporarily joined it to a workgroup (took it off of the domain) and re-added it, and it worked.  The SID's can become corrupt, and this is quite possibly your issue. We've seen systems ""fall off"" the domain, usually due to time differences. If that system has its clock misset that can cause issues (think it's tied to the Kerberos aspects of Active Directory). We end up having to remove and re-add the system to the Active Directory again to fix the login issue if a clock change doesn't help. You might also want to consider adding a backup domain controller to the remote site in case down the road you have VPN issues that disconnect your networks, plus having a backup controller isn't a bad idea at a remote site for caching and recovery issues. Since your user can't log into the computer to see what the time is, you may be kind of stuck. I would advise installing something like a VNC server so you can log into it remotely as a local user and make adjustments as necessary (make sure the firewall software is set up to allow the remote connections). All new systems get random passwords set to the local admin account which we keep in a database for cases such as this. I would use a windows password reset disk and reset the local admin password and then rejoin the domain. If RDP is disabled, you can enable it, by talking the local user through editing the registry in safe mode, to modify this key.  If its value is 0 then RDP is enabled. You will be able to RDP into the computer using the local administrator username and password even though it is a domain member. Hopefully you have someone out at the remote location that can help you.  As far as the Admin account you may have to use something like trinity.   I have a computer in a remote location (6 hour drive) on a BOVPN back to our home office. The remote computer is running XP Pro, DC is on Sever 03 all system 100% up to date. The user was logged in and everything was working fine. The user restarted and now can not log on to the system. Gets the ""Domain controller can not be reached"" error. I can ping the system, and connect with the remote registry, but I get challenged for a password even though I am logged in as an Admin. When I give it the login information for the admin accoutn, it tells me ""Access is denied"". Any advice for what I could try before I drive 6 hours?",5
"When I scanned for hardware changed within Device manager (after removing it) Windows again detected the device but as unknown. Windows scanned, found a driver but then instead of detecting it as a network adapter I saw the System Devices list get updated instead. I have no idea which one it is. Attempting to install my downloaded drivers failed, so I told windows to update the driver software online, trying to bring it back to how it was a few moments earlier. I then went into device manager and removed the device and the drivers (perhaps I should have just removed the drivers in hindsight) with the intention of installing the new drivers I had downloaded. I've tried removing the card and then installing it again but I now can't get it recognised as a network card. I've just installed a PCI-E network card and when it was installed Windows (7 64bit) found a driver. It might just be something silly like unzipping the driver file you downloaded first, then just running the installation program.",2
"1.1/24 can talk to anything in 1.0/24 because the mask says they are ""local"" (""on the wire""). If 1.1/24 tried to talk to 2.1/23, it fails: ""2"" is outside the 1.0/24 range; and fwiw, ""1"" is outside 2.0/23's range. 3.1/22 would be able to send to 1.1/24 -- /22 covers 0-3 -- but 1.1/24 wouldn't be able to answer. As you state these are on the same switch, but you don't specify a VLAN assignment or otherwise, so I assume the broadcast domain is still the whole network. What you basically have is an overlay network, in which two IP networks share the same broadcast domain. In itself not a problem, but does cause additional traffic. Any host on IP network A that wants to communicate with a host on IP network B has to go through the router between the two (yes, this router must have multiple addresses on the same network interface, one for network A and one for network B). Even though they are in the same broadcast domain, the IP addressing prevents them from communicating directly. ""but the IP addresses could be said to follow the /24 paradigm."". That is the irrelevant statement. In Classless Inter-Domain Routing, as is common these days, the actual values in the IPv4 address are irrelevant for network address determination. Only the netmask is relevant to determine what part of the IP address is the network address and what part is the host address.  C cannot talk to B, since the C host's. 3 would be a broadcast address (not a valid host) in B's subnet masking.  are within the subnets of each, where the original post showed A with a /24 and B with a /30...while C had a /29. It might definitely work. But the  problem that arises when you are not configuring static IP's , then in that case /24 can assign 1.1.1.1, 1.1.1.2 as well and /23 can also assign the same - especially if you have same range in DNS/DHCP. so there could be potential IP conflicts that could arise",4
"This is just an example, it could be that 5% is fine, or even too much, to hold out for testing. It depends mainly on the size of your dataset, and in case of classification, for example, it matter if one class has very few examples, so that perhaps in your 5% for testing there may not be a single example of that class. K-fold cross-validation is a technique you use when you have too little training data. You use it to expand your training dataset, leaving only a tiny bit for testing (for example, 5%). Then you use k-fold cross-validation to select a different 5% for testing each time, and a different 95% of the full dataset for training each time.  That way, you can get a good idea of the performance of your model, even if you only kept 5% of the data apart for training. What you're trying to do is called dimensionality reduction. The go-to technique for that is PCA. Make sure you normalize your features first (subtract the mean and divide by the variance), and then drop features until you still have, for example, 95% of the variance of your data explained. If in the end you find that you cannot get good performance from your model, it could be that you have lost too much information. In that case, you may have to increase this to 99%, adding features back in. RSS is ok to reduce the number of training examples you have in your dataset, but it is only necessary if the order of examples is not random already. If it's random already, then you can just train on the first 10%, or whatever you need, and save yourself the trouble.",1
"The password for our windows server 2003 account is very long and complex, so I don't want to enter it each time by hand when I log in via RDP. However, copy/paste in the login screen seems to be disabled, so I have to. Can this be triggered by some policy? Or is there another way to enable this? You can not do this.  Sorry if this wasn't the answer you wanted.  You don't want your password in your pastebuffer anyways.  Huge security risk. I use 1Password to solve this problem. It has an Auto-Type feature that avoids using the copy/paste buffer and types it into the password field. It's been many years since I switched over from KeePass but I'm pretty sure I used it to solve the same RDP problem. It has the advantage of being free but I much prefer 1Password. I solved the problem by writing a small Autohotkey-Script that types the clipboard buffer after some seconds. You can create a RDP shortcut and keep the password saved in this- I would not advise that this is a good idea because of the security implications.",4
"To prevent an uninstall /or install program from accessing the web, disable your internet connection temporarily before uninstalling. In XP I click Start > Connect To > Show all connections. Under the 'LAN or High-Speed Internet' heading, I select Local Area Connection, right-click and select Disable or select 'Disable this network device' in the Network Tasks panel at the left of the 'Network Connections' window. The browser may still open when uninstalling software, which I agree is so irritating but it won't access the web. In my view it is also as irritating as when installing some software also often opens a browser and goes straight to some web page without the me being fore-warned of this. Hope that helps. Go to 'Tools' on the Menu Bar, Select 'Internet Options' (usually at bottom of list). Select 'Programs Tab'. Half way down that panel will be a section titled 'Default web browser'. Click the 'Make Default' soft button to set IE as default. You may also wish to select the Check Box to 'Tell me if Internet Explorer is not the default web browser.' Click 'OK' to effect change(s) and close the Internet Options panel or click 'Apply' to effect the change(s) without closing the Internet Options panel. Can I change this to a portable web browser?Or even prevent the uninstalled program to open up a web browser. You can change the default browser, but you can't guarantee the program doesn't have IE hard-coded as the browser to use.  What you can do is ""break"" internet explorer, so that it fails to start.  I wouldn't recommend this, though.  You'll find a lot programs make use of an embedded IE to show content within the program. Here's the answer I would use for an IE web browser assuming that's what browser you want (I'm using IE8.0.6). Other browsers generally provide a similar function to enable users to set THAT browser as the default and to also set a flag to notify you if that default setting has changed.",3
"This common subset is very limited.  Observe, for example, that this mode of message submission from standard input and command arguments is Sendmail's -bm mode, but not all sendmail programs actually even support a -bm option.  So to portably invoke this mode, you cannot use the option. You are missing the distinction between message contents (headers and body) and message envelope.  You're invoking sendmail with message contents, but you aren't supplying the envelope.  It's the envelope that controls delivery.  Without it, the message cannot go anywhere. There are also portability caveats when combining -t with recipients given as command arguments, for the details of which see Dave Sill's book.  So pick one form or the other.  Don't mix them. By default, sendmail expects the recipients to be specified on command line. Use -t to make it read the To:, Cc:, Bcc: headers. By putting Subject: This is the subject, I don't need to specify the subject in sendmail, but adding the To: myemail@hostname.com field gives me Recipient names must be specified You don't say which sendmail command that you are using.  Each MTS  Sendmail, qmail, Postfix, exim, and so forth  has its own.  The common subset of them all has just two ways of specifying envelope sender and envelope recipients:",3
"If a zombie has init as its parent, then init has stopped working properly. One of the roles of init is to clean up zombies. If it doesn't do it, noone else will. So the only solution is to reboot. If init is broken, then a reboot may fail, so I'd shut down important services, sync the filesystem then hit the power button instead. Let's keep the panic down, shall we?  A ""defunct"" or ""zombie"" process is not a process.  It is simply an entry in the process table, with a saved exit code.  Thus, a zombie holds no resources, takes no CPU cycles, and uses no memory, since it is not a process.  Don't get all weird and itchy trying to ""kill"" zombie processes.   Just like their namesakes, they can't be killed, since they're already dead.  But unlike the brain-eating kind, they harm absolutely no-one, and won't bite other processes. The only way you could remove the zombie/defunct process, would be to kill the parent. Since the parent is init (pid 1), that would also take down your system. Otherwise, I wouldn't worry too much. It's not running and it's not taking any resources and it's just there so the kernel can remember it.",4
"When you point at someone, there are three fingers pointing back at you, so I have some questions for you: Or if he's like me, perhaps he might want to try something that involves making something do things in the real world. I always liked messing around with my old Commodore, but what I really liked was building stuff. The two came together when I went to uni and specialised in real-time control. It totally rocks to work on some software, and then jump in the car where the engine controller is driving with that software. These days it's easier than ever to do this, with Arduino and Raspberry Pi. My advice is to ""get used to disappointment"", but be as helpful as you can, with whatever your son wants to do. Many people get to college (as another question here recently stated) with no idea what they want to learn about or do with their lives. This is incomprehensible to me (I was asking about computers when I was 7, long before the IMSAI hit the streets) and so I have no advice on how to generate interest, convey it, inculcate it, support it, or any other thing to do with it. It just is whatever it is, or it aint. For example, you can take a game that has shooting dynamics like an FPS or a scroll shooter (could be 2D to make things easier), and try to modify the shooting dynamics. Maybe have the shots fire 3 bullets at a time at different angles. Let him see you browse through the code, grepping around to find the relevant code. For a more complicated modification make the bullets travel slower, light up like lasers, and curve toward the enemies. That's where your son has to think about what he enjoys. Does he want to write games? Does he have ideas for some app? Does he want to try playing with AI? Does he like making up automated Word and Excel documents? Is he keen on music technology, perhaps? The other solution I found was to readjust expectations. Rather than imagining 'super cool awesome game that rakes in loads of money', I aimed more for what the programming language could do, within the resources I had. Text-based Rock/Paper/Scissors with a learning AI, for example. A screwdriver is one of the tools you'd use as a mechanic. Coding is one of the tools you'll use in software engineering. But coding is not engineering. Engineering is making something. I'm sure your epic coding sprees are not just ""hey, I'm writing C++""; they'll result in you saying ""I've made this, isn't it cool?"" for an end product you think really is cool. It is quite possible that your answers are: zero, zero, zero. That has been the case for me throughout my life, about basically every area which interests me. That bad news is that you cannot get anyone else to like the same things you do. I look in vain for anyone else who has even heard of 16 Bit Lolitas, let alone likes their music. (You could start.) I don't know anyone else who eats bags of Ghirardelli dark chocolate chips. When I was a child I had no friends who also scavenged parts from discarded TVs, or was even interested in electronics.  When trying to initially build things, the reality didn't match expectations (my expectations were: 2D game that's super-cool awesome, the reality was a console text 'game' where you input a number and got a generic response back). You say that your son is interested in something ""related to programming, technology, and/or game development"", but that is a massive area which includes many non-programming things. I used to own an acoustic and electric guitar, but try as i might, I had trouble making myself practice. What really interested me was the equipment: guitars, amplifiers, microphones, PA systems... But I couldn't ""form a band"" that did event staging. (I was involved in theater in High School though.) Besides making a simple game from scratch, like Kevin Workman suggested, you can also look for an open source game and try to modify it together. It'll allow him to see the reward much quicker. You can make incrementally more complex modifications. Let him watch you and imitate you. Explain everything you do, including the thought process as you're doing it. And conversely, if I'm slogging through some configuration scripts to do testing, that's pure work for me. No fun at all. It needs to happen, and it's still programming, but it's the bit where I just have to push through the boredom. I did a computer science degree but after that in order to learn industry specific skills in web application development I taught myself by creating simple games. What I did was actually trace art I loved (by sci-fi and fantasy artists) to simplify the digital component. This resource development is separate to programming but it made that part very easy and captured my imagination. Then I broke the graphics apart to make ""puppets"" I could animate and interact with then coded the collisions, movements etc. and a game loop using Flash ActionScript. Then I built a website locally so learned HTML etc. The total scope of the project was clearly defined, simple, and and felt and looked good when I showed people. It was fast to develop (outside of the learning component), rewarding and built my self-esteem. Many years later I'm a senior software engineering consultant. However motivation comes from within. I think its best to find a passion he might have and run with it. Computer programming is a broad area these days after all. These days I get a sense of enjoyment (maybe not 'fun' per se) from solving key issues and automation of particular tasks, which on a small scale seem trivial, but on a larger scale combined with other automation tasks are really awesome. When I was a member of a large and active Amateur Radio club recently, I did not know one other person whose interests really were similar to mine. The field is simply too large. I was an expert in magnetic loop antennas. We had an expert in contesting, one in Morse code, another in microwave equipment, and so on. All of us were essentially alone in our interests, and no matter how many times we introduced our fav topic, it fell flat, even in a group of like-minded people!  Some things are just not available. When I was a child, no one ever sought out and took me to the large, active Amateur Radio club right there in town. Parents and other adults didn't know or care about it, so I was not helped with that. Maybe you could find out what your son is really interested in and get him there? Taylor Swift's parents relocated the family to Nashville when it was obvious that she could sing and write music. I find there's two causes for programming not being fun: not be able to do a particular thing in a particular way, and a very 'manual' (low-level) programming language. Of course, this was frustrating and made the amount of effort seem not worth it. I was trying to program in C++. In reality, I probably should have been using a higher level language (JavaScript, Python, even Java). C++ does feel like work in comparison, and without knowing what language he's programming in, it might be worth introducing a language that does more of the legwork, if not already. Let me explain. Programming is a tool, not an end goal in itself. Suppose your son was interested in being a mechanic. You haven't come here saying ""my son isn't enjoying fixing cars"", you've come here saying ""I've spent all this time teaching him how to use a screwdriver, but he doesn't see the fun in using a screwdriver"".",5
"Anyway, please let me know when you've worn out a silicon chip by using it inside specifications. The bathtub curve rising section is many years away from the point it becomes obsolete and slow. The symptoms can range from blue screens of death to the occasional retransmit of a network frame or miscolored pixel. Basically everything has bit that could fail, and the function will determine the severity of the symptoms. In short: the end user can't determine it. You'll need in depth knowledge of the architecture and test methods used to test silicon. A complete test might not even be possible anymore once it has been packaged. (Eg: put on little board with heat-spreader) To make matters worse, this is also the same behavior when the power supply is unstable, the clocks are too fast, or its running too hot.  The symptoms are most often linked to bad memory, which is more likely to suffer from problems due to being on a separate board, build to a price and often lacks self correcting mechanisms. (ECC)",1
"If there is something wrong with your sound device, you may get a sound, but not the correct sound. I have seen systems with internal speakers ""beep"" instead of playing the correct noise. This generally means something is having problems. Another stupid thing ... do you rebooted your machine? I've seen things like this after suspends or sleeps. At that case, I would start with system restore and go back to a time when your sound worked. If that doesn't help, I would follow the advice by @Ganesh. One thing to check: the selected output device. For example, at Winamp check your output device to see if there are more than one and chage it. While the default sound profile is rather noisy, windows does not make ""periodic"" noises when you let it sit. Windows only makes noises when an event happens. Things like start up and shutdown are the most noticeable. There are a number of other common events that make noise. For example, when you open the basic volume control (left click on item in the system tray) and choose a volume level, the system makes a noise. This is so you have a way to tell how loud your speakers are. The default sound is best described as a ""ding"" noise. Also you can check the default sound device at the Control Panel -> Sound Options [don't remember the exact name as I have Win in another language].",2
"Most commonly, I've added a feature to the dataset that is actually a sort of proxy of the target variable. That is, I've made a silly mistake. Not sure if this is going to be a satisfactory answer... 1st thing that comes to mind whenever I get a 100% accuracy on test data is ""I must have done something wrong"". But sometimes it is not a silly mistake, like adding a feature. I don't really remember the source thus I cannot link to it. But I heard in a podcast about some guys that were trying to classify patients of cancer from some features (I mean not from images of cancer cells or anything like that), and they built a model that was pretty simple and surprisingly good (not 100% accuracy though). The point was they included some kind of id of the patient as a feature, and that id somehow contained information about the hospital that treated them. There were a few hospitals that treated the very bad cases, thus the model was learning that anybody going to those few hospitals was really sick, and not really learning about who was sick.",1
"The system works basically fine, but only I found the NDP function seems not working correctly. Firstly I noticed, with two such devices, I cannot ping each other by their IPv6 link local address (fe80::.../10). I then used tcpdump to monitor the tun traffics and found there is no Neighbor Solicitation/Advertisement message at all. Another strange thing I found is that, there is Route Advertisement sent out from one device of the network (the master PLC device which is so-call coordinator node), but not from any other device in the network.  I want to understand what's the correct behavior I should expect from such kind of network according to IPv6 NDP and how to make it happen. I am working on an old code that used to connect different IPv6 devices over a different kind of network (a powe line netwrok, PLC, which is quite similar to 802.15.4). To do that, it created Linux tun/tap interfaces on each device (Tun actually, not the Tap) with C code, which can receive egress packets from tun and deliver them to the PLC and on the other direction it can read ingress packets from PLC and inject into the tun interface.",1
"It depends greatly on the type of traffic that you're serving, but there are a number of ways to mitigate.  (I'm going to assume Web sites.)  A relatively simple and inexpensive way to solve this is by putting Varnish (or another http cache) in front of your web servers.  This will reduce the number of traffic hits that make it to your web and app servers greatly.  Also, using a product like HAProxy as a load-balancer can help somewhat by managing the distribution of your http traffic to your servers. Most mechanisms to identify and mitigate attacks like anonymous attacks are well known, and most Anti-DoS products and services can deal with them with high rates of success.  Traffic Scrubbing services from companies like Verisign, Prolexic and others are the most effective way to protect yourself unless you have the money to spend on a hardware solution like Arbor or Rio Rey.  - Does it include a behavioural learning and detection modules? or Does it use only rate-based thresholds?  There are DDOS-preventing measures available, but are going to be expensive.  I know that if you are using Rackspace for hosting, they have a product offering called Preventier (which I know to be expensive.) Anonymous usually use well known tools. There is no reason that a local SOC/NOC or service provider's SOC/NOC will not be able to block their attacks. The question is whether detection and blocking are accurate enough without false positives of blocking legitimate traffic as well. As the consequence of that is a successful DoS/DDoS...  If the limiting factor is the web server you can set up a linux computer before the web server to do filtering based on the source IP address. Allow only certain number of IPs to access the web server at one time and as soon as the transfer is over block the IP and give the slot to next requester. This way your server never exceeds it's capacity.  As you can see, there is no clear answer to the question, as it depends on many parameters, budget is only one of them. The quality of the service or product is a significant aspect as well -  - Does it include authentication options (for HTTP/DNS and other protocols)? again for reducing the chances of false negative.  Using Squid accelerator here would be great help. This cuts down the concurrent number of connections and processes and frees the web server resources faster in addition to caching static content. It could also be worth your time to leverage Akamai (or similar CDN) to host your content, which will also solve this problem, but typically has a high dollar-cost. - Can it generate 'real-time' signatures for accurate mitigation without affecting legitimate traffic? reducing the false-negative ratio? NOTE: I say inexpensive for Varnish and HAProxy because, while they are Free/Open-Source, it does have a cost in engineer-hours to implement and support.  Note that this is true of any solution, but these have a zero-dollar licensing cost. - What is the mitigation rate the service/product can offer, regardless of the legitimate traffic rates.  Security is a very specialised subject and shouldnt be dismissed. You need to be starting at your firewall and making sure every device / connection is secure. And also that users are educated about security and not to get Malware etc on their PCs (They could be used to DDOS someone else)  You cannot limit yourself to one group of attackers. Most groups including the Anonymous group would use a BotNet. This would come from a large range of IP's so you cannot just ban that range.  However, sometimes organizations and enterprises do not have a tuned or updated protection policies. Furthermore, I was amazed to discover that many of them do not have Anti-DoS protection at all, neither by product nor by service.  As in all things, a risk vs reward analysis must be performed, but you must keep in mind that beyond your service availability, you're also essentially paying for your brand's reputation. - Does it include an action escalation mechanism, a closed feedback option that can automatically use more aggressive mitigation actions based on the success of the current mitigation action taken?  The only way to minimise (NOT STOP as this is near imposable) is to keep on-top of your security. So updates are maintained, your firewall is checked for vulnerabilities. Well, it's very difficult. That's the whole point of ddos. You have one million PC computers sending request to your website at the same time. What is it the firewall should do? Most important bit would to keep the traffic out of your system. Dunno where you have your servers but if keep your servers at your office you should get a limiting firewall at your ISP's place. this would keep the traffic away from your limited incoming cable.",5
"It seems that if any other device within range of the modem is connected to the 5G network the network card on the PC, which is only able to connect to the 2.4Ghz, isn't able to connect at all.  If this turns out to be your issue, you can disable this secondary network manager so that your computer stops saving (potentially conflicting) network information in two places. Then suddenly, out of nowhere, my desktop flat-out refuses to connect to the network. I've tried just about everything Google and computer forums gave me and nothing seems to work. I once had a similar issue. The root cause for me was that my computer had two WiFi managers: a native Windows manager and an OEM (Dell) WiFi manager. Clearing out all saved networks from the Windows manager did nothing to fix the issue until I also removed all saved networks from the OEM manager. And absolutely none of it so far has helped in any way. I'm honestly at my wit's end. Without internet connection, I can't do my job. So any answers would be greatly appreciated. Now I'm not sure what exactly was the problem, but I get the suspicion that my roommate might have blacklisted me. It doesn't help his case that we're pissed at each other. thank you for the help! I tried to get into router settings but my roommate changed the password, so I ended up jumping the gun and factory resetting. Not only did it let me access the settings, it fixed my connection problem. Make sure all devices within range are connected to the same network type, if more than one is available. This may be random but I was having the same ""Can't connect to this network"" issue which happened only very occasionally in the US, Ireland and UK but sometimes multiple times per day in Belgium (Brussels). I run Windows 10 on a Lenovo 15"" Flex. I've restarted the router and my computer multiple times each, I've uninstalled my drivers, updated my drivers, forgot every network on the list, used command prompt, changed the properties of my network adapter, etc. For a while before this issue, my desktop has had frequent connectivity issues (slow connection, dropping offline and the like) that led me to constantly having to restart our router. According to my roommate, the wi-fi still works perfectly fine for him during these instances, so he's confused as to why I had that issue. Try searching for a program with ""WiFi"", ""wireless"", ""network"", or your computer OEM in the name to see if you might have a second network manager, too. I tried the various ""fixes"" but to no avail and resigned myself to simply rebooting. However, by pure chance I discovered that simply by closing the lid on the Laptop - which probably induces hibernation - an reopening after a couple of minutes I was able to reconnect without the reboot. I'm on mobile wireless, and the router offers two networks for devices to connect to; the 2.4GHz standard wireless data and 5G on 5Ghz.",4
"My initial investigation suggests that X11 forwarding is a bad idea, because opengl rendering will occur on the client machine  (or rather the X11 server--what a confusing naming convention!) and will suffer network bottlenecks when sending our massive textures.  We will never need to display the output, so it seems like X11 forwarding shouldn't be necessary, but Opengl needs the $DISPLAY to be set to something valid or our applications won't run.  I'm sure render farms exist that do this, but how is it accomplished?  I think this is probably a simple X11 configuration issue, but I'm too unfamiliar with it to know where to start. you can run a vfb- virtual frame buffer on the machine, its like a dummy X11. We used to run apps that HAD to open an Xwindow that we never looked at and just installed vfb and export $DISPLAY to that - kinda like screen on the cli My research lab recently added a server that has a beefy NVIDIA graphics card, which we would like to use to do scientific computations.  Since it isn't a workstation, we'll have to run our jobs remotely, over an ssh connection.  Most of our applications require doing opengl rendering to an offscreen buffer, then doing image analysis on the result in CUDA.   We're running Ubuntu server 10.04, with no gdm, gnome, etc installed.  However, xserver-xorg package is installed.",2
"I have a Cisco SPA942 phone, which is no longer supported by Cisco.  It's running the latest firmware available for this phone: 6.1.5(a).  Cisco replaced the SPA942 with the SPA504G, which is almost identical to the SPA942 hardware-wise.  I noticed that the SPA504G has a newer firmware available (7.4.6) that adds some features that I'd like to have (a call ignore soft button and G.722 HD support). Not quite sure if it'll brick your phone but I can almost guarantee you that you're probably breaking the license agreement for the 7.4.6 firmware. Don't put your company at risk, follow the licensing requirements. If you can't do that with your current hardware, then upgrade your hardware. I tried downloading and executing the 7.4.6 firmware upgrade tool, and going through the screens it didn't tell me I couldn't apply this SPA504G update to the SPA942.  I stopped short of clicking the Upgrade button on the final screen before the upgrade, as I wasn't sure if this upgrade would brick the phone.",2
"I went to the XLSTART folder to see if my personal workbook was indeed in the wrong place. It was there so I tried deleting and recreating it to no avail (just a big pain in my butt with the amount of macros I have).  All the shortcut keys and descriptions were maintained, but more importantly, I didn't get a second copy of Excel opening every time I opened a worksheet from Explorer. I had the exact same problem in Excel 2015 for quite a while, and when I hid the Personal.xlsb workbook, a blank page would open instead and I couldn't close that one without closing the file I actually wanted open. I went back to the XLSTART folder and saw there was also an add-in file, something I had to install for work. I deleted this file and restarted Excel - no extra window! PERSONAL.xlsb stays hidden too. Not that this is everyone's issue but it was mine I had to export each of my macro modules to another folder, then after removing PERSONAL.XLSB, record a new macro stored in my ""Personal Macro Workbook"" (which recreated PERSONAL.XLSB), then on the Developer toolbar, click ""Visual Basic"" and import the macro modules again. The solution is to find the real location of the Personal.xlsb file so you can delete it. To do that, open excel and switch to the Personal.xlsb file. Now select the ""File"" tab on the top left which will load an ""Info"" page. On the right side of the information on that page there is information such as ""Properties, Related Dates, Related People, and Related Documents."" Directly under Related Documents is a hyperlink that says ""Open File Location."" Click this hyperlink and excel will open the true location of the Personal.xlsb file regardless of where your particular installation has placed it. I had this issue as well. Thought it might be related to my PERSONAL.xlsb file like so many mentioned. However, it had always been successfully hidden up until recently.  But I finally found a solution! My problem was that I had a Personal.xlsb file in an alternate location, and for some reason a search of the C drive wasn't returning a match for that file name. My location was the same one that Amber mentioned above, but I wanted to share how I found it in case someone else has a different location. Now close Excel, delete the Personal.xlsb file, and reopen Excel. It should open only the file you want and a second window will no longer appear. From this point on, I expect other tips online about creating/deleting/showing/hiding the Personal.xlsb file will work as expected. But I have left it deleted and Excel is finally loading as I expect. In my case I simply couldn't delete macros workbook as it was full of macros. What helped me was to make personal.xlsb file visible, modify it slightly (I changed the columns width), re-save it and hide again. And voila - the additional blank window doesn't appear anymore! This can occur due to corruption in a personal macro workbook (%appData%\Microsoft\Excel\XLSTART\), and removing PERSONAL.XLSB from this folder caused the problem to go away.",4
"It seems unnecessarily cumbersome to have to explicitly initialize an empty Graph this way: g = Graph([]).  I think a better implementation would be something like Instead of nested comprehensions, just do [0] * v.  Also, if you don't care about a variable (such as x or y) use _ to identify it. Note that if someone adds edges to the list manually, or does other weird things, then the caching won't work. Similarly, I think the parameters for add_edge could use work. For one, it isn't a list of edges that you're passing it - it is a single edge, encoded as a list/tuple/collection of some kind.  I also find it a little annoying that I have to create a tuple/list/whatever to actually add the edge to the graph - I'd rather just pass in the two end-points.  I'd prefer something like this: Almost anytime you want to do something, you probably want to use someone else's code to do it.  In this case, whenever you're working with graphs in Python, you probably want to use NetworkX. You can split up iteration variables in a for loop - for src, dest in self.edge_list is cleaner than for e in self.edge_list: src, dest = e. Then, instead of searching through an entire list to find a given edge, you just have to perform a quick dictionary lookup for one of the endpoints, and then a theoretically smaller iteration over a (hopefully) shorter list.  I've also seen versions that use nested dictionaries very effectively. At this point, however, I have to ask why you're representing your graph as a list of edges - to me, the most intuitive way to think of a graph (and how I've implemented one in the past) is to use a dictionary - to use your example, I'd probably encode it like this Additionally, it seems like adj_mtx should just be called adjacency_matrix, and I would rather it be a property (potentially with caching) than a function.  Imagine something like this: Overall you could use more descriptive names in this function.  I'd probably write it something like this: You could clean this up a bit.  You don't need to initialize v where you do. It would also be easier if you kept the nodes in a set and added them whenever you added an edge. In general, prefer xrange in Python 2, although that makes compatibility trickier - I generally use a library like six to handle things like that, although if you don't need everything you can write your own file (good name is usually compatibility.py) that has only the changes you need.",1
"If you're distrusting whether wget is functioning right, there are multiple variations of wget, as well as other programs like curl that could be tried.  They'll probably act the same way, but if they don't, then you'll apparently have your answer. (Some small changes...  I converted one back-quote to an apostraphe, and tabs aren't showing up quite right...  my main point of showing some sample output is, it worked fine.) Anybody here can point me to the right direction?  I'm not a programmer or someone with great knowledge on that field but I'm knowledgeable enough to learn and understand.  TIA. I'm working from my office and we have a proxy server in order to get into the internet.  I'm not sure whether this affects my command using the wget. If it isn't working for you, my guess is that it's an issue with your Internet connectivity.  You might have a content blocker that prevents the HTTP connection.  Maybe an HTTP proxy needs to be used.  However, the wget command works fine.  So, from my test, I would say that the thing that you need to try fixing isn't your basic wget syntax.  If you do need to change the syntax, like referencing a web proxy, you may need to identify the settings another way (troubleshooting elsehow).  Can you visit the jpg file in any other web-grabbing software? I do have a command using powershell -Command and it works fine but could not run under task scheduler and so I opted to use the wget as most recommended this.  However, I could not seem to get it going when test trying it.",2
"This question includes code from the original description of the vulnerability and proof of concept file. It will in the worst case open a reverse shell that may grant privileges to other users and provide a shell interface to outside attackers. Do not execute this on unless (1) you know that your system is not accessible to third parties (firewall, no multi-user-systems) and (2) you know how to shut the shell down if it accidentally opens. The idea seems to be that while the text file will appear to onlu contain the string ""Nothing here."", it will open a reverse shell /bin/sh on port 9999 with netcat (nc) when opened with unpatched vim/neovim versions.  A vim/neovim vulnerability was discovered recently (and has been patched now in vim 8.1.1467). Besides instructions for patching, a proof of concept was included as a text file with the content It appears that this vulnerability has been there and remained undetected for quite a number of years. Patching, updating (not available on all systems yet) or disabling modelines will fix the problem. Of course, there are no guarantees that similar vulnerabilities might not continue to pop up in the future. This is why I think it useful to study this one. appears to be the actual command with the nohup nc 127.0.0.1 9999 -e /bin/sh starting the reverse shell).",1
"Your second router has a firewall and/or NAT enabled which prevents anything originated on the WAN side from reaching the LAN side. So, if you're not using one of the specified cabling pinouts, then your connection will be unreliable.  This may manifest in intermittent connectivity, one-way transmission problems, pattern sensitivity, etc. Yes, it's quite important to use the T568A (or B) wiring schemes for reliable ethernet performance.  Unshielded Twisted Pair cables are only able to reliably carry high-speed signals at full-duplex for distances up to 100 m when specific wires are selected:  For 100BASE-TX, The four wires that are not transmitting signals are just as important as the four that are:  They serve as shielding to prevent cross-talk, interference. echos, and fade.  For 1000BASE-T*, all four pairs are used, but it's still important to separate wires carrying certain signals from others that might be carrying signals that would create destructive or constructive interference on neighbouring wires. For short cables, at lower line rates, it's quite possible to 'get away' with non-standard wiring, but as distances approach the specification limits, and the fastest line rates, selecting the proper pairs to send, receive, and dampen signals becomes more critical.",2
"Whitespace is unconventional e.g. in ( (value_%4==0) && (value_%100!=0) ) ... I would have expected ((value_ % 4 == 0) && (value_ % 100 != 0)). Maybe your code editor/IDE has a ""format document"" command to auto-format such things. Maybe you should throw if a negative int is passed to a constructor, or use an unsigned int type (though you should perhaps allow negative years, but then again things like the Gregorian calendar change makes early dates meaningless). The comment precond: month must be normalized; value_ in [1..12] implies something tricky or wrong in the public API. Maybe months should always be normalized; if they can't be, maybe this trickery needs to be private and accessible to friend Date (or something like that). Maybe all the normalize methods should be private. Perhaps the Date constructor should implicitly invoke Date::normalize (because I don't like two-stage construction, where user code should remember to invoke normalize on a newly-constructed Date). ... maybe Month values could be stored internally as 0 .. 11, converted from 1 .. 12 in the constructor, and converted to 1 .. 12 in the stream output. Maybe that would be a good demonstration of encapsulation. Sometimes you pass by const reference e.g. Day& operator+=(const Day& other) and sometimes you pass by value e.g. Date(Year y, Month m, Day d). This statement return Day{0}; // invalid value_ should perhaps be a thrown exception. Are you able to construct test/user code which triggers that condition?",1
"You need to take the web server offline ASAP as you may be unwittingly infecting your visitors with something nasty. After a machine has been hacked into it is generally recommend that you completely rebuild it as you do not know what extra backdoors the hacker left behind to allow the attacker back in again once you clean the infection (unless you know for sure that the change was made entirely due to an unprivileged user account having an insecure password that was guessed, or a specific unpatched script that allowed the change in a way that would not allow deeper changes). Not one that I have seen specifically, but it tells you that your page has been altered to request content (probably an iframe that attempts to load a drive-by hack onto the client's machine, or a javascript file to create a pop-under that does similar). We regularly see customers with websites that have been defaced or otherwise modified, to the point that we just wrote a blog article on how to avoid getting your website infected with malware.  Basically, they usually get in by one of a few different methods: If you provide more specific details (OS, web server, what apps/scripts you have installed, ...) we might be able to provide more specific help.",2
"When the client tried to talk to other computers on my LAN, the packets from my client were reaching my LAN hosts (I didn't verify this, but I'm pretty sure they were), but the source address of these packets was the address from the OpenVPN network. The LAN hosts knew this wasn't on their LAN, and the only thing they knew to do in that case was to send them to the default gateway, which was my router. I doubt it did anything with them as sending a packet to a private IP range out onto the internet is pointless. I don't think you have to add a static route to all LAN hosts. You could just add a static route in your gateway on that LAN, pointing all openvpn-network addresses to the openvpn server ""lan-ip address"". I finally found out what the problem was. I am using OpenVPN's ""routing"" option which creates a new subnet for all OpenVPN connections. My client was getting assigned an IP address from this subnet, and so was my server, so they could talk to each other across this network. With IPv4 forwarding enabled on my server, I was also able to send packets out onto my LAN, and was obviously able to talk to the server via it's LAN ip address. The solution is to add a static route to all LAN hosts, or use OpenVPN's ""bridging"" option instead of ""routing"". I have not done this yet, but am sure this is the way to go. Had the same issue here (using this guide to setup: https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-14-04)",3
"I think the only way to avoid this is by having a default printer that is not remote.  Here is a simple JScript that will set your default printer to ""Microsoft XPS Document Writer"" (assuming you have it installed) which is always a local, non-hardware printer. There isn't an obvious (to me at least) rational reason for an application to interact with a printer when changing a font style, but if that is what is happening then at least a local printer won't be across an unconnected network. I've seen the exact behavior you describe when switching fonts, and I think it's because some printers have allowable fonts, and Word is checking to see if they'll actually render on the printer (though why this is necessary until you actually print is beyond me). One option would be something like this to automatically switch your printer based on IP address. It presumably could switch to none when you're at home. This will list all installed printer names in the Immediate window of the Visual Basic Editor (CTRL+G for the Immediate window if you're not very familiar with VBA) You could add a procedure to the ThisWorkbook/ThisDocument module of your personal.xls/normal.dot that would detect if you were at home and set the printer default appropriately. I've not tested this, but USERDOMAIN (index 30) should be suitable for this purpose.  The following code should give you a rough idea of what is required to do this. Note that the ports will likely be different on your PC, you can run the following code to check what VBA thinks your printers are called.",5
"You have two possible ways of share a disc between a Windows machine and a Mac and use it to backup both machines: The only filesystems that are readable on the major OSs and their variants are ISO9660 and FAT to my knowledge. With a 1,5TB harddisk you will have a problem I think. I remember some border on the FAT partition size and that was definitely below 1500GB Are they on the same network? You could share the drive on the Mac and have the PC backup over a local network connection (I have my Mac and PC directly connected together for such a purpose.) I can't think of any solution that supports both properly without some kind of bodge, and the idea of ""some kind of bodge"" being near ""backups"" makes me sad :-( As far as backup software for the PC, it depends if you want the software to do a full backup including the OS, or if you want to to just backup your documents and the like. (The latter would work great with Unison or rsync.) Best ideas are either backup to the Mac across the network for the Windows machine as Rizwan suggests, or failing that just buy a second drive - not what you wanted to hear but it isn't that expensive in the great scheme of things and certainly ""cheaper"" in terms of your sanity than a backup solution you can't quite trust. You could also possibly partition the drive into an NTFS partition (for your PC) and a HFS partition (for your Mac Time Machine.) I'd rather use some network attached storage to accomplish what you want. Cheap disks with network functionality are available at your local retailer.",4
"Most hardware firewalls are linux boxes.  It doesn't matter which is used - what matters is who implemented the linux build, and how good they are at it.  A dedicated hardware box probably has more time and effort spent on making the build secure, and it's probably flashed into the device so it's a bit harder to hack and change remotely, but beyond that there's no practical difference. Yes.  The ONLY downside is a little less performance on your machine.  The upside is that if someone manages to break into ANOTHER machine on the inside of the hardware firewall, then you still have some amount of protection from bad traffic inside the data center. I would recommend installing a software firewall like iptables on the servers whether or not you have a hardware firewall.  The more layers of protection you have, whether to provide more walls for an attacker or to provide more places that someone would have to do something silly to allow an attacker in, the better. If you are safeguarding a lot of money, or something of equal value, then yeah, I would have some layers of defense, and a 2nd firewall might provide at least a cod-piece token offering in this regard. I know, this is a bit off-topic, but I am going to tell you this: as for the security of your servers, do NOT be afraid of iptables. I don't know if ubuntu has some helper tool for dealing with iptables, but I would say, avoid it at all costs. Understanding how chain traversal works in iptables takes no time whatsoever, and closing down access to your server from certain addresses or setting up simple NAT takes as much as a few (extremely well defined) shell commands after that. Ideally the network is setup such that this cannot happen, but that's more expensive and difficult to maintain well, so you can't always depend on that. At the same time, I don't trust the colo's network either. Use the software firewall to control who can ssh in and expose a minimum profile to other hosts in the colo provider's network. As for the ""dedicated firewall box"" - for some reason people tend to think, that it should best be a ""hardware firewall"" (like the used-to-be-oh-so-popular PIX), which makes no sense to me, because it is not some ""magic box"", it is for all intents and purposes a PC, running some specialized software. If you get your hands on such a device (PIX, ASA or whatever equivalent of those), good for you, but you still need a professional to set it up, it doesn't ""just work"". Best thing to do will be to set up a dedicated linux (or bsd, its down to preference, really) box just for that. You will, of course, still need someone to help you with hardware and software - depending on the complexity of the firewall setup you are looking for. I am going to side with Adam Davis here and say, that the best solution would be the mixture of both - you should set up a box serving as a dedicated firewall and screen the connections  on the servers themselves, that is how we do it anyway. It takes next to none effort to restrict the exposure of the servers only to addresses, that you trust (takes a few entries in iptables).  If it's a one-or-the-other choice, then generally yes.  Let the professionals who focus on this work take care of this for you. That said, I would generally prefer to have both a software and a hardware firewall.  There's a real issue though, if it's a managed firewall, of the skill and responsiveness of the firewall provider.  If it takes them a week to open a port and you lose a customer because of it, was it worth it?  The vendor you are evaluating may be awesome, this is just a consideration I would have. If you have good experience with managing firewalls and network routing, then there's no reason to believe that a service would do better.   If you don't want to hassle with it, then let them deal with it. This is a function of the complexity of your firewall and routing.  The linux box can pretty much function as a dedicated low-end router, allowing you do to address rewrites, class-based queues, port rewrites, etc. In an internal, ""trusted"" (your version of ""trusted"" may vary) network, I would avoid using software firewalls, and instead utilize your network infrastructure to implement communications controls. Is a dedicated firewall, managed by professionals, a better solution than whatever I can manage with software? In your case, I'd recommend using the colocation facility's firewall to protect you from the outside world, as it is presumably managed by professional networking people who know what they are doing. Also, you save your machines from dealing with DoS attacks, and other internet nuisances. Not only is it overkill, but under certain circumstances, it can lead to other issues (packet loss, unidirectional pack transmits, etc.)",5
"Don't 'roam'.  Instead of seeking out the router with the highest signal strength, instruct you computer to connect specifically to your ESSID.  You may want to change the router to a different channel. I have used both Netgear and D-Link GigE wireless routers with the WiFi disabled - DHCP works fine.  It seems like you just need a wireless routing nearer where your wireless computer is sitting.  In that case it's pretty simple use a wireless router as an access point.  I've done this myself. Channel 8 still conflicts with channel 11. Change to channel 6, or go with an AP in the 5GHz range (802.11A, some 802.11N AP's). On that wireless router turn off DHCP and assign it an a static IP address in the range of your other router.  Then don't use the wireless router's WAN port at all, plug on of it's switches into your existing network.  The router portion won't be used, it'll just be bridge between wireless and your existing LAN. I do not believe you will run into any problems if you disable the Wireless on the router. If your router does not have enough ports the DHCP should work fine with an inexpensive GigE switch with additional machines on it. I do a lot of media work so I prefer reliability and performance of wired Gigabit Ethernet when transferring a TB or more over the network, and DHCP in home routers is very convenient for small networks.",4
"Yes it is possible with out uninstall/install. Just move the existing user databases by attach/detach method and you can modify the settings for both default data directory(which newly created databases be stored) and the default backups directory for the future. Here is how. 3 - if you have one datafile: create a new datafile on the right drive, and then select the original datafile from sys.sysfiles - and run something like DBCC SHRINKFILE(logical file name,emptyfile) - this will empty the data from the file, and put it in the new datafile. Then you delete the original datafile. I have an installation of MSSQL where I would like to move the data directory to another drive so that all the existing databases are located there and all new databases are created there, as well as the backups, logs, etc.  I know I can detach/attach the existing databases, but what about the rest of the settings (backup, new databases)?  Is this possible without an uninstall/reinstall?  Thank you. There is another way for datafiles for user databases that requires no outage, just a performance degradation - but it is for experienced DBAs. For larger files, you're also at the mercy of disc IO speed. Always do it in pre-prod first!",3
"Second question: it depends a bit on how your thin client is distributed. If it's an MSI file, then you can stick it into the AD GPO for your users and it will be deployed on all machines. details will depends on what version of Windows you're running (both AD and on the clients). The second part is much harder. First, check with the supplier of the software if they support a local repository. and then the client installs the entire software online. I want that all (client) systems in the network connect to the central system (server) to get the downloaded files and continue to install in their own system. The first part of your question, having a local update repository, is easy enough, and done with Windows Server Update Services. Search microsoft.com for details. If they don't, it's going to be hard to do. You can intercept the network calls to their servers, but faking their server may be difficult and might not even be legal. In the first point, Windows Server Update Services (WSUS) is probably what you're after, although this is one hell of a thing to set up. It will control all updates going out to the network. Or you can look at image based deployment tools like Microsoft System Centre where you build a first machine with everything on it, make a copy of the disk, and then use that to set up new machines. You could look at a network level cacheing solution, where the first person to install the software has to pull it over the internet but subsequent people get it from the cache. I want to setup a central system (server) which will directly update windows from the internet. The other (client) systems will connect to the central system (server) and",4
"The other possibility is that you are using a corporate PC and your company doesn't want you wasting bandwidth listening to music on their dime.  If so, get the Pandora app for your cell phone--I use the BlackBerry app at work all the time and it's great. Chances are something is redirecting pandora.com to 127.0.0.1, which is your local machine. Is this a work computer? Sometimes primitive content restriction systems handle blacklisted URLs this way. Someone had (I would bet a small amount) edited the %windows%\system32\drivers\etc\hosts file.  (Path will vary on 64-bit systems.)  Remove the line that refers to pandora.com. I encountered this via my IPhone 6 Plus. You can try to go to Setting & clear your History & Mobile Data. It's works for my case.  On our work homepage there is a link to our marketing studio. When we are on the wifi at our office we have no issues opening the marketing studio through our homepage. When we open the marketing studio while on a different wifi server, whether that be home, hotspot, etc it redirects us to the IIS7 page. IF you all could please help in any way it would be greatly appreciated.  You are probably running IIS7 on your machine. I would start by either diabling the world wide web service on you computer or uninstalling it. Then try to get to pandora.com and see what happens.",5
"I did menage to get the whole thing to run with ""SPI port forwarding"" program (http://download.cnet.com/SPI-Port-Forward/3000-2651_4-10764348.html?tag=downloadRatingModule;summaryRatings) To check if everything works as it should be I am using this site to confirm that my ports are accessible http://canyouseeme.org/ (I also tried some similar sites..).  I forwarded all ports on my router to my computer ( have also tried port triggering and some other options), I also added new rule to my Win7 firewall (firewall is turned down but just in case).  Im trying to run my custom game server on my local machine. I used no-ip.com to create a static internet location for my game to connect to. I downloaded their DUC program that should  access my data on that location and deliver it to my computer.  In order to make it permanent or configure it manually, you need to log in to your router to make the changes. How you do this depends on the router model (you didn't mention it). I keep getting ""Connection refused"" from canyouseeme.org... They say it means that there are firewall issues.  Most likely that program uses UPnP to request a port forwarding from your home router. This is a pretty standard protocol for port forwarding and most home routers support it.",2
"No. There is NO way to get the data back. You might have used RESTORE .. with REPLACE which REPLACES the data in the database with the data from the backup that you just restored. So there is no way. Only thing can help if you have anyone of these like snaphsot, ,mirroring, LOGshipping or replication enabled for that database. Perhaps the system sends email confirmations. If so, retrieve the emails (from sent items or from auditing on the email system) and hire some temps to retype the information. I do have to say though, I did have a client recover a QBooks database that way once, and I was really surprised that we got the data back.   it seems like your only option is a super long shot hail mary.  WITH REPLACE replaces the old DB but I'm not sure if it explicitly searches for those DB pages on disk and tries to overwrite the pages even if the DB is different.  If the data is only 'marked' for deletion from the OS then you can have a very expensive data recovery center look at it.  Sometimes, and I mean with a lot of money, they can look at the magnetic residue and rebuild something from there but the odds of coming out of there without any issues are probably slim.   Perhaps the system exports data and sends it somewhere else. If so, get the exported data back from where it went and get those temps typing.",4
"Left button is number 1, right button is number 3. Using xmodmap you can swap mouse buttons like this: However, I wasn't able to get this to work under Win7 last time I tried, so when my new work PC arrives I shall probably buy a Microsoft mouse to go with it. My Penguin Ambidextrous Vertical mouse works very well in remote sessions. Also, I only need a single mouse. If I want to switch from left to right or vice versa, I just click a toggle switch. The built in track point and track pad in my Lenovo x230 does not carry my settings over to a remote session. This drives me nuts when working in a remote session on the road.  Beware that I haven't tried it myself and it's expensive compared to a normal mouse. I wonder if modifying a normal mouse is worth the saving. Microsoft Intellipoint mouse drivers handle the swap buttons feature in a different way to the standard mouse drivers, and remote desktop works correctly: A left-click on the physically-connected mouse is sent to the remote machine as a right-click. And when connecting to the machine from elsewhere, Intellipoint does not swap buttons. One gotcha: You have to turn the standard ""swap buttons"" function off before installing Intellipoint. Otherwise when you remote desktop to that PC from elsewhere, the buttons get swapped. X-Mouse Button Control does its changes by intercepting clicks, so they carry downstream to any Remote Desktop session. But I don't think it has per-mouse settings. A hardware alternative may be a gaming mouse. I have found this SteelSeries mouse that claims to be ambidextrous and driverless. I guess you can swap left/right buttons by clicking another button. My work PC has two Dell mice, and is running XP. Fortunately the mice have different hardware IDs, so I've modified an .inf file in the Intellipoint driver's installer, and convinced it that one of the mice was made by Microsoft. However, the Intellipoint software does not swap the buttons over on a non-Intellipoint mouse. So, my home workstation has an Intellipoint mouse to the left of the keyboard, and the supplied Dell mouse to the right of the keyboard. Works a treat, for both left- and right-handed users. In an X Window System, you can use  xmodmap (utility for modifying keymaps and pointer button mappings in X).",5
"So, if you look in your /opt/google/chrome/you'll find a PepFlash directory, where inside there is a .so file (libpepflashplayer.so). You have to delete that folder and create a new folder called plugins where you put inside the libflashplayer.so downloadable from the link above. Do you get that message with some specific site or every site that has flash? Can you watch youtube videos or play games/video on newgrounds.com? When I clicked on the link to update the plugin it took me to a page that said: If you are using the Google Chrome browser, Adobe Flash Player is built-in but has been disabled.  I searched some and found another page from Google that says that I should be using libflashplayergc.so as the plug-in that is integrated with Chrome. I'm using Google Chrome on a Ubuntu 10.04 x64 system and I just finished doing an update which got me version 19.0.1084.46 of Chrome.  Now for the first time I see a message that says Adobe Flash Player was blocked because it is out of date when I open up a website that has flash. However, I don't see that plugin in my system.  My question is, is this something that I should see?  Or is this plugin only for 32-bit systems and so I need to continue using the Adobe system plugin libflashplayer.so?   If you don't want to delete the PepFlash dir, you must anyway have the libflashplayer.so as I told you (the 11.2 version, 'cause the 11.5 does not work in linux as the Adobe explain), but you have to disable the libpepflashplayer.so.",3
"Let's take the Intel Core 2 Duo P7350 and P8600 for an example. Both have a 3MB cache and a 1066MHz FSB, but the P7350 runs at 2.0GHz, while the 8600 runs at 2.4GHz. Is there a tradeoff between heat generation and performance? For example, a last few P4s are the hottest processor ever made, but they are much slower than your standard C2D which runs much cooler.  In a few generations the same might occur.  While improvement in manufacturing technology will generally reduce heat output, it also allows companies to fit more dies in the same area.  What this means is the heat output might actually stay the same, but we'll definite have faster and faster processors. Something to consider is the advances in materials science over the last 15 years.  Much effort has gone into packing more transistors per cm on the chip, reducing power requirements and increases in efficiency which also reduce waste heat.  So that being said, know that all processors are not equal at all.   Sometimes manufacturers can redesign a processor in a more-efficient way, or move to a different manufacturing process, or other factors can influence how much waste heat is generated such that faster processor could even use less energy.  But, given the critical ""all else equal"" condition, a faster processor runs hotter and there is a tradeoff between speed and heat/energy use. In fact, there's a growing market out there for processors that are intentionally underpowered to serve as cpus in computers that don't need great performance. Yes!  All else being equal (that's key), a processor with a higher clock speed will use more energy and therefore also produce more waste heat.   If you are comparing the processors using the same manufacturing technology and architecture, then yes.",4
"We've been using pound. We are not completely happy with it however. The main concern is lack of configurability for special cases and not many others seem to use it. We don't actually use it for load balancing so much as service redirection so it's overkill in our case right now. Check out WeoCEO - it's a load balancing appliance that spins up as an EC2 instance.  It works roughly the same as conventional hardware load balancing solutions like F5's. You might want to have a look at Apache Synapse. It is aimed more at web services rather than pure http proxying, but does have some nice features allowing not only load balancing, but the ability to spin up additional servers on EC2 in response to application load. Since you're looking for a server-based, rather than appliance solution, and I'm not aware of any products that run on Windows, I guess your best bet is to use a Linux-based solution. You have a number of choices: Amazon just released a load balancing product that might do what you want.  Check out aws.amazon.com/elasticloadbalancing/ for info.",5
"The Step function will round down an answer that is lower than 0.5 to 0, and an answer that is higher than 0.5 to 1. However, please note that you do not need to use a binary activation function to output 1 - I advise you to just use TanH or sigmoid and backpropagate a whole bunch of iterations. I am totally new to Artificial Neural Networks. Lets say that the model you are trying to turn into an artificial neural network has an output that is triggered only by the exceedance of a threshold: $y\geq y_{1}. $Therefore, you need to find a way to use this inequality as an activation function. Is this feasible? However, in other comments, you mentioned that you know what y1 is. That is not of importance, the network will act as a black box and will figure out a treshold itself. Don't set up your own activation function just to get the right output - that avoids the whole point of backpropagation. This is feasable. This is also called a Binary/Step activation function. You must only use this activation function on the output neurons.",2
"We're small enough that we use the spreadsheet method - there are only 3 of us that work on the servers and networks, so we do keep it up to date. If that DHCP scope ever fills (so far it hasn't but it could happen) it's time to hunt up the other admins and see what those servers are doing on the temp address like that. It helps that the DHCP server is not run by the DNS guys, so we can look up leases ourselves. Data Center IP addressing... We have a small, 8-10 address DHCP scope with a really low lease time on our data-center subnets. This allows us to get a system set up (as far as we can with a dynamic IP anyway) and then request a real address from the DNS guys. The DNS guys do all the tracking of who requested what for what. But one of our ""when we have time"" projects is to switch to using DHCP reservations for everything that needs a static address.  We do this for most of our printers now, and it's saved a lot of hassle recently.  All the secretaries and executive assistants have networked printers and there's been a lot of reorganizing recently.  With DHCP reservations, when people have swapped departments, we ""swap"" their printer IPs in DHCP and tell them to power-off/power-on their printers. We use a copy of dokuwiki, with a page for each used subnet. Whenever anything is commissioned, it's updated, althought we're small enough that if it's not updated for something, we can probably work out who did it. I use DHCP for my servers as well, with static allocations based on MAC addresses, then when we need to make changes or quickly lookup a server's IP we just go into our firewall / router's configuration. Saved having to change DNS server options on 40+ Virtual Machines when we rearranged our infrastructure a while ago.",4
"This should solve your problem. Basically, even if your windows supports IDE drives, if it was installed on anything else, the IDE drivers aren't used. In order to set them up, you need to follow the article If this is the cause, a simple repair of the install using the windows install disk should correct disk driver access problems. Check the disk type under Vmware. It could have been converted to a Scsi disk - for which virtual PC doesn't have a driver. You may need to revirtualize the original disk but choose to convert it to an IDE disk. have you used disk2vhd from sysinternals.  The only real limitation is the os boot disk should be less then 128gigs! Have you tried booting the VM with the Windows installation CD and running a repair on the OS that it finds? If your old PC uses an EIDE (possibly also SATA) disk you can probably just snapshot it and restore the snapshot to a blank VM disk and the VM should boot. See www.drivesnapshot.de for my favourite (free evaluation!) snapshot tool.",5
"Postfix, spamassassin and amavisd are all running as services, and mail is delivered with spamassassin scores.  The bayes database is in ~amavis/.spamassassin/bayes_*. amavisd-new uses its own config. These config files are located in /etc/amavis/conf.d/. In particular, in 20-debian_defaults, $sa_kill_level_deflt is the ""equivalent"" to SpamAssassin's required_score (located in /etc/spamassassin/local.cf). I am perplexed because it appears that the configuration I've placed in /etc/spamassassin/local.cf (a new score for a specific rule, for example) is ignored for messages processed by amavisd, but are not ignored when I run spamc, or ""spamassassin -D -t"" from the amavis account.  When I place equivalent configuration in ~amavis/.spamassassin/user_prefs - it also seems to have no effect on the results of amavisd-new processing. I've used Amavisd with Spamassassin for years... my configuration used to 'work' (though it may never have been ""right"") now, it appears as if /etc/spamassassin/local.cf configuration is ignored when spamassassin functionality is invoked by amavisd-new In my experience, the score returned using amavis vs. using spamsassassin directly is never the same and I haven't figured out why yet, so when you are setting $sa_kill_level_deflt you'll need to figure out an appropriate value that may not be the same as required_score.",2
"If I understand your question, logins using SQL Server authentication, no; logins using Windows Authentication, yes.  The Windows logins are placed into a Windows group (or groups) eliminating the need to each user's Windows login set up under SQL Server. This may be a bit too elementary question for this site but is it possible to control SQL Server authenticated logins using groups? So is it possible to assign sql logins under groups and give rigths for these groups? If so, are these groups only seen for an instance in question? Once the role is created you can assign specific permissions to that group and it will be applied to the logins that are member of that role. The roles are localized to the instance you are under and are not visible or accessible to remote instances. You will only be able to add the logins in that instance of SQL Server to the roles. When working with SQL Logins you ""group"" them by using roles within SQL Server. Roles are created at the database-level but with SQL Server 2012 you will be able to create them at the server-level as well.",3
"Without better ideas I will probably first try a different NIC. Years ago that has solved a (general, not DOS related) Samba problem for me. Any ideas, either for the real problem or for a precise Samba debugging / tracing that shows me what exactly the problem is in the communication between Samba and the XP clients? we have several DOS applications (Clipper) which share dbase files on a file server. The applications run under XP. This has worked for about two decades with Netware and for years with Samba (member server) without problems. Some weeks ago I upgraded openSUSE from 11.4 (samba-3.6.3) to 12.2 (samba-3.6.7) and changed the hardware (to AMD E-450 with 6 GiB RAM). To make it worse (from a debugging point of view) about that time the switch was changed (from 100 Mbit to a 48 port Gb switch). Since then (it is not clear since which change exactly because the users don't tell us immediately...) a few users face severe problems with certain of these DOS applications which are not precisely reproducible. This seems to be about access rights or (more probable) file locking. As far as we know these applications do byte range locking on the files. I do not know whether (and how) I can get this kind of debugging information from samba. There are no general problems accessing these files. Oplocks are enabled (disabling is inacceptable and does not solve the problem, too). Then I changed the server structure: Earlier Samba ran on real hardware. I made the host OS a simple installation just serving as a host for VMs) and put Samba into a VM, using the openSUSE 11.4 installation which worked without problems before. The problems have not disappeared since. An upgrade of the Samba VM (to 12.2) seems to have made it even worse. Regular Windows share access seems to not have been affected in any of these configurations. ifconfig shows that about one of every 4000 RX packets is dropped on the interface which seems OK to me.",1
"My deceision is simple, and stupid :) I've got a Master workstation, and all files from ""~"" are just copied to the second ""Slave"" one. The moment i realize I need to modify something - i do it on Master, and Slave catches these changes on sync. Ubuntu One might be what you're looking for. Unfortunately it's still in beta phase, and I have no experience with it, so I'm not sure if it'd work for you. This will use rsync to only copy the needed changes, not re-copy everything every time the command is ran. I think you can achieve what you want better by NFS mounting a common home folder.  Check out this article http://www.linuxjournal.com/article/4880 You could use a sync script like osync which can propagate deleted files and update only modified ones. If you also want to sync configs - tools mentioned above are totally helpless: configs are often changed, and many logfiles will make conflicts so they can't be merged. osync is rsync based but can handle a lot of sync scenarios between local folders or remote ones over ssh.",5
"Encryption is your friend.  Make sure that your access point is configured to use a strong encryption scheme (WPA2 is the strongest scheme commonly available), and that the passphrase you use is long and strong.  By 'strong', I mean that it should not contain any recognisable word or predictable sequence of characters, and should use a mix of letters, numbers and symbols. Some wifi hardware can be placed into monitor mode, and capture any traffic it sees.  In this case it doesn't have to be associated with an access point (i.e. connected to any particular network).  Wifi is just radio transmissions which can be intercepted with a suitably configured receiver. If you don't control the access point -- in a coffee shop, for example -- you should ensure that any sensitive traffic you send or receive is encrypted 'end to end'.  Make sure that you use HTTPS wherever it is available, and that any email client you use is configured to require an encrypted connection. I was just wondering if it's possible to monitor HTTP requests sent & received to a WIFI network. If so, how is it done and how can I protect myself from it?",2
"(PS: These same three days thinking also made me come with another answer to a problem that may rise: there should be an automated test tool to check if the generated game design is sound, eg, you should have a medieval army having a hard time to defeat a generated fortress) Only paper dealing with similar issues I could find off-hand is Stachniak and Stuerzlinger's ""An Algorithm for Automated Fractal Terrain Deformation"". It assumes you create the terrain first and deform it (or rather, let the algorithm pick the parameters to deform it with automatically) to fit your constraints later, so it doesn't answer the question directly. Still, the methods presented therein might prove useful for others with similar problems. I believe I've been staring back at this question for the last three days, while asking myself how the procedural generation of worlds or even galaxies can, at the same time, be deterministic(such as always generating the same content from the same seed), look natural, and still have unique, interesting, unusual or even beautiful features in its landscape. I keep going back to the same generic answer that this is going highly dependent on the genre of the game and its own internal consistent history within the story. That said, I wonder if a story that generates a tree of consistent plot lines, will generate the overall gameplay design you're looking for. Assuming that roads can only have a certain elevetion change per distance, terrain height would be adapted to the heigth of the road, which would lead to generation of chokepoints whenever a road passes through a hill/ravine/whatever.  Here's a great example of procedural terrain generation, using parameters like moisture, height etc...  Lighthouse 3D has a good survey of some simple algorithms for terrain generation.  If you're starting with a map that contains cities or other interesting areas, you could use some of these techniques to generate whatever terrain you would like.  For example, intelligent use of the fault algorithm could be used to create cliffs or valleys around your city that would act as choke points.  Also, using the circle algorithm would be a great way to generate terrian for turret placement.  These are just some examples, but using these simple algorithms would be a fairly easy way to generate interesting terrain around your cities. So it all goes back to your own answer where you input information into the PCG and it fills the gap, and also to the other answer about generating worlds which match plot lines and the overall story. Of course, the amount of change in terrain height would need to be limited too, or you'd have roads running straight through the highest peaks. For example, post-apocalyptic genre, why would anyone build a metropolis with massive walls in the middle of a wasteland, and make everyone inside of it live miserably? Why colonize such planets? In high fantasy, why make a city on top of a floating island? Why do dwarves make cities inside of mountains? And your generic evil enemy comes from fiery land with active volcanoes? Rule of Cool? Wouldn't such an algorithm automatically generate more 'tactical' terrain when roads are generated between cities?",5
"3). What is the interaction between the /etc/modprobe.conf module/NIC definitions, the /etc/sysconfig/network-scripts/ifcfg-ethX and the /etc/modprobe.d/blacklist functions in this context?  You could use kssendmac option to get the NIC's MAC address sent to the Kickstart script.  Parse it out with PHP and configure accordingly. 2). What is the most consistent way to maintain that ordering through repeated reboots, kernel changes (e.g. going from a RHEL mainline kernel to a RHEL MRG realtime kernel), etc. This snippet will get and print all the headers.  You'll have to come up with something that parses out the information you need.  Your network configuration lines will be conditional based on what you get out of this.   That way the known variable is the MAC of the onboard NIC and the unknown is the kernel assigned eth device. Inserting more than 1 NIC into a server can sometimes causes the new NIC(s) to boot up in their own order, not in the eth0(existing nic) , eth1 (new nic) , eth2 (new nic) order you'd expect I have an HP ProLiant DL360 G6 containing two onboard NICs as well as an HP NC375T (NetXen NX3031 chipset) 4-port PCIe card. The system was running with eth0 and eth1 belonging to the onboard NICs and eth2-eth5 on the NetXen card. I recently rebuilt the server and from the kickstart process onward, the NICs were reordered such that the onboard NICs became eth4 and eth5, while the NetXen card took over eth0-eth3. I've had some experiences in the past where I tied NICs to specific interfaces via changes in the ifcfg-ethX config files, but this is the first time I've ever seen an add-in card take over eth0 from the motherboard's interfaces. This impacted my kickstart scripts, so: In Red Hat, for example, if the driver is called ""tg3"" (the Tigon driver) you simply specify the network name order you want by specifying the eth(x) order by editing",3
"As the general tool for this job I would use Windows Device Console (devcon.exe) which is kind of a command line version of the Windows device manager. It's difficult to give exact instructions, not knowing all the devices you had attached to your system.  If you want a shortcut you can then drag the icon in the address bar at the top of the screen to your desktop or wherever you want it. To remove usb devices I use removedevices.js (which uses devcon) from https://github.com/kevinoid/remove-nonpresent-devices However there will not be a remove all the ones I don't use anymore switch. You most likely have to list all devices, filter them somehow and then remove them.  That will bring up what is basically the old printer view and you can highlight multiple devices and delete them. I have no idea why they removed that from the Printers and Devices view. This even works in the Windows 8 Metro interface. You just start typing and it will bring up the app search in Metro. It will then launch the Printers folder on the desktop. You have to play a bit with the command line, you can list devices and remove them from the system.  To remove bluetooth devices I use btpair -u from http://bluetoothinstaller.com/bluetooth-command-line-tools/",3
"Disclaimer - I've implemented DHCP in the data centre and had no issues when I was an infrastructure manager, but now as a consultant for many customers I wouldn't do it.  It all depends .... Generally speaking, DHCP with reservations is the ""best of breed"" for IP management in the datacenter, depending (of course) on the particular needs of your data center. Very rarely is it wise to run DHCP in a data center without reservations, though some blend is appropriate.  In many settings, the ""Cons"" for DHCP with reservations end up being non-issues (if the router can take out DHCP, well, the servers aren't accessible anyway, etc).  It's also commonly a decision regarding size.  A datacenter with hundreds or thousands of servers with frequent deployments and reinstallation will certainly use some DHCP, even if it's only for testing/deployment.  A datacenter with a few servers will likely be fine with everything statically assigned. ...but no, still a bad idea for servers, production ones at least - maybe in a dev/test environment I guess, or for VPSs if you had no other choice. Not only to you have to specify in the DHCP server what each machine is assigned to, you have to keep documentation of it. And you have to update ALL of the documentation any time anything changes. New network card? Update documentation and DHCP server and DNS, etc.  Having a central DHCP pool which assigns addresses willy-nilly is insane for a server block. Assigning a server a specific IP via DHCP is less insane, in the sense that having 3 imaginary pink elephants chasing you is less insane than 5.  DHCP essentially hands anyone plugging into the switch a valid lease. Yes, you can specify that only known MACs get leases, and everyone else is denied, but a better place for this is dynamic VLANs.  Having each server machine rely on dhcp in order to have its networking stack come up correctly adds another potential fault. In a server environment, where you're trying as hard as possible to achieve maximum availability, adding another moving part is not a good idea There is no other good reason to have DHCP running in a datacenter as already pointed out so well by everyone else. The following is copied from MS.  Think this through and think about why you worry so much about DHCP in the data centre: If clients receive address allocations from a DHCP server, it is important to be able to predict how they will be affected by any DHCP server downtime. In general, the longer the lease period, the lesser the effects will be if the DHCP downtime remains short. For example, if client lease periods are set to the default of 8 days, clients do not attempt to renew the lease until 50 percent of this period (4 days) has lapsed. If the original DHCP server is unavailable at this time, the client continues with this leased address until 87.5 percent of the lease period (7 days) and then attempts to renew with any DHCP server. With clients attempting to renew after 4 days, even if the DHCP server were to remain unavailable for 2 days, clients would not reach the 87.5 percent rebinding state. Therefore, you do not normally need to worry about any outage that is within 25 percent of the lease duration. Similarly, the shorter the lease times, the shorter the time available to recover the DHCP server.",5
"After terminating Cat6 cable I plug it in to cable tester to check if every ping/pair is fine and I can use the cable. The problem I am having is that after crimping Cat6 cable with either T568A or T568B termination my tester says that there is a problem with pin 3 and 6. When I get into my office and do the same with a different cable the tester says everything is fine and the cable is working ok. Is something wrong with my outside cable or am I doing it wrong? If you terminate T568A and T568B and you get the same 3/6 error, then the problem is the other end. A and B swap pairs 1/2 and 3/6, so if the error is on the same pins, then the actual pairs are good but the termination point on the other end is bad. Otherwise you would see a 1/2 error when you went from B to A. That's not a lot to go on. Are you using the correct connector for the type of cable? (stranded vs solid? They also have combination types, but make sure you're not using the wrong one) also, 3 and 6 are stripe vs solid pairs of green or orange depending on if you're using T568A or B. Are you sure you haven't swapped them on one end? I'm guessing the fact that you get the same pin error regardless of A or B standard indicates that you're probably NOT accidentally swapping them, but it's worth checking. You may not be pressing the strands far enough into the connector before crimping or your crimp tool may be damaged at one or both of those pins. Make sure the pjns are all pressed in evenly and that none of ""teeth"" in the crimp die are broken and that the strands are all flush against the face of the connector.",3
"Start by checking that when you run a program you get a copy of the dynamic linker that does not involve that path.  You should see: Assuming this all looks right it should be safe to go ahead and rename /lib/ld-2.11.2.so out of the way (though I would have rescue media to hand nevertheless). Dont delete it until youre sure everythings still OK, in case it turns out that you need to put it back! (The hex address may differ.)  This file should be a symlink pointing to /lib/x86_64-linux-gnu/ld-2.13.so which should not be a symlink and certainly shouldnt end up at ld-2.11.2.so. Anytime now that I try to upgrade or install any package I get this error. Any attempts to move files only required more files to be moved. I also couldn't fix with dpkg. I tried to update a system of mine, with which I had no problems whatsoever, and ran into problem while executing a aptitude full-upgrade. The update ran through normally till I got this error : You may well find there are complaints about other leftover files, which can be check and dealt with in much the same way.",2
"for example you would go to a CA and ask a certificate they would ask you what info and they generate a key and a cert with a CSR the key is the same they just generate a new cert. with diffrent attributes sometimes like validity dates etc...  This gets kinda tricky.  The CA signed cert is only trusted for identification because the CA is include in the pre-populated certificate store built into browsers/OS.  If I didn't have a pre-populated certificate store neither of them would be trusted. So it sounds like one would submit a CSR to a CA to get a digital identity certificate. This digital identity certificate could potentially be of the same format as a self-signed certificate (for example the Public-Key Cryptography Standards 12 format). and you usually do this by installing the CA certificates (signers) in your environment, so that it auto-recognizes the certificates signed by this CA and consider them as ""trusted"". So from the point of view of the technology the only difference is that your self-signed cert wouldn't be built into my browser/OS. well, as long as you ""trust"" the CA who signed a certificate, you can ensure that the situation is safe. as a concreet example, in my company, we do have our own CA, and we have CA certificates installed in every web browser (wether it's IE, firefox etc.), when the CA ""signs"" our SSL certificates (used in the intranet, applications etc.. listening in SSL/TLS), and we access these applications, they are automatically recognized as safe and you don't need to click on a specific warning banner saying that the certificates being used are not trusted (because they are either self-signed, or signed by an unknown CA, or a CA we do not trust) Therefore the self-signed certificate is guaranteed to work for encryption but not identification, while the digital identification certificate from the certificate authority is guaranteed to work for encryption and identification. If I downloaded and verified certificate of that self-signed key and added it to my certificate store, then I could trust it for all purposes. The key difference is: the self-signed certificate is signed by the same party that owns the private key, while the digital identity certificate returned by the certificate authority upon receiving the certificate signing request is signed using the certificate authority's private key.",4
"I know that I have to align my 4k drives by a multiple of 8 sectors, but what about md-RAID / LVM / dm-crypt? How do I tell these layers that my drive is 4k? If they don't respect the 4k sector size, the partition alignment is useless. How do I align LVM/md/crypto-layers? Thanks. So if you're using GNU parted, ensure that each partition starts on an LBA divisible by 8 (LBAs remain 512B, so 8*512B = 4KiB). LBAs originate at 0, so start the first partition at ""40s"". misaligned partitions will hurt your performance. i have done some benchmarks only on regular partitions on the top of the disk - without lvm and my results measured with bonnie++ were without proper alignment: GNU parted does not do this by default, probably because many ""Advanced Format"" drives falsely claim that their physical sectors, not just their logical sectors, are still only 512B. Most newer distributions are updated to know about the 4K thing by now. I just built a md-RAID/LVM/XFS setup on a bunch of 2TB drives with no problems. Didn't do anything special.   Be careful! gpt labels, required for disks > 2 TiB, are 39 (512-byte) sectors long. So if you create your first partition immediately after the label, it won't be on a 4KiB boundary. problem is mostly with alignment of partition beginning with structure of underlying disk. to keep backwards compatibility disks 'lie' to the bios/os that they have 512B sectors, while in fact they have 4096B sectors in case of modern hard drives, 32-64kB sectors in case of most common stripping raids/ssds. Also, if you use GRUB, leave room for its second stage bootstrap. MS-DOS labels are 63 sectors, with enough unused room for GRUB to stash its second stage bootstrap, but there's no unused space in a gpt label. So make a small partition 1, set its ""bios_grub"" flag, and then create your ""real"" partitions after that -- making sure that each and every one begins on a LBA that's a multiple of 8.",4
"I had this problem in Windows 10 until I uninstalled Logitech's SetPoint software. As soon as I had done that then my Logitech trackball's wheel started working again for vertical scrolling in Chrome and other apps. What's doubly weird about it is, it'll actually work for a couple seconds after I login, but then inexplicably stop working. I'm guessing there's some software loading up not long after I login that's causing the issue. Similar problem, similar solution: I had Taekwindow installed and configured to scroll the window under the cursor. But since this is the default in Windows 10 anyway, I just turned it off and things went back to working: Sigh, guess I should have paid a little more attention. I had KatMouse (http://ehiti.de/katmouse/) installed previously, and that was causing the issue. Leaving this here for anyone who's interested. I haven't yet tried booting into safe mode, or disabling start up software with msconfig. I'll try those and update the post with the results. Just upgraded to Windows 10 on my (~5 year old) laptop, and overall I like it; however, I've got one little quibble with it that I haven't yet found a solution too... When attempting to use the scroll wheel (either through the touchpad or with and attached wireless mouse) I can't scroll within certain windows. Specifically, the Start Menu, Settings Screens, the Windows App store, etc. Other windows work fine (i.e. Chrome, Minecraft, Windows Explorer) but not these ones.",3
"Case 2:if there is no such $e_{z}$ till ith iteration. say at i+1 iteration there make different choice say $e1_{i+1}$ and $e2_{i+1}$. Weights of both of them should be same, otherwise there wont be difference choice. now next edge $e2_{i+1}$ can be added to $T_{1}$ if it does not form as cycle, similarly $e1_{i+1}$ value for $T_{2}$. if they are added then T_{1} and T_{2} remains same till i+2 iteration, if not say addition of e_{i+1} forms a cycle then e2_{i+1} will be e_{z} for T_{1} and e1_{i+1} be the e_{z} for the T_{2}. so both 1-trees have same weight. Let $e_{1},e_{2}..e_{i}$ are the edges that are picked by both $T_{1}$ and  T_{2} upto i iterations, case 1: In this process if there any edge $e_{z}$ with weight $w_{z} < w_{k}$  for any i=1,2..n is not choosen for MST then it means that it would have formed a cycle. so e_{z} will be the edge which would be added to MST to get a 1-tree. so both 1-trees will have same weight Finding the minimum spanning tree can be solved by using the greedy approach that is it looks for the best local optimal solution at a given point of time. Your problem should be extension of the minimum spanning tree. your doubt is that we get two different MST $T_{1}$ and $T_{2}$ of equal weight w. let $e_1$ be the edge with minimum weight $w_{1}$ to be added to form $1-tree_1$ from $T_1 $and $e_2$ be the edge with minimum weight $T_{1}$ to form $1-tree_2$.",1
"Finally, as mentioned in the comments above: If you're planning to just write an image (an .iso file) to the disk, then you don't need to do any preparations at all. Except some people call step #2 ""low-level format"" now that there's no #1 anymore. And sometimes people call erasing the whole disk ""formatting"" even though it doesn't write anything with an actual format in there. Sometimes people call reinstalling Windows ""formatting"" even if all the files are left untouched. Such image writing starts at sector 0, so it trashes everything that was on the disk previously (up to the size of the image). Whether it had partitions and files, or whether it was blank, doesn't matter  it simply gets overwritten with the image's own partitions and all. ""Format"" might be one of the most confusing terms in consumer tech, really. Even your post already confuses two or three different actions... Everything beyond the size of the image will be left as it was, but the OS won't care about it  since the image's partition table says there's no partition there. Though of course discarding or blanking the disk is still a good idea if you're going to give it to somebody else.",1
"Yes they are, most days ""HR"" is nothing more than an OCR machine looking for keywords. If it can't OCR your resume, you don't make it through. A resume is like an executive summary of your experience. We can drill into the details later with a phone interview and a face to face interview.  Don't get cute, this is a document you are using to introduce yourself to a business. You should be professional, and concise.  When I'm looking though CVs/Resumes/Whatever, I'm not interested in how 'cute' or clever you can format a document, instead I'm trying to solve a problem. Be careful with ""clever"". Things that are ""clever"" often make it difficult to see the real information underneath.  When I receive your resume, it's usually part of a pile of 30 other resumes or more. I sadly don't have a lot of time to read over each resume, and reading resumes can be draining. Many sysadmin resumes are very poorly written, which is frustrating. I don't care about your height and weight. I don't really have time to read 2 paragraphs which describe what you did at the previous job. I also don't want to read a 5 page resume.  Don't be 'clever', just try to understand the problem the people doing the hiring are trying to solve and then make sure everything you send them is geared towards making it very easy for them to understand how well you can help them solve that problem. http://gizmodo.com/5489011/unemployed-man-google-mapped-his-resumesomeone-give-him-a-job-for-his-creativity-alone If you can't clearly and concisely demonstrate why you can solve the problem I have better than the other applicants then I'll put your application in the reject pile. Last time I was going through resumes it annoyed me when someone got creative with their presentation. I want to be able to quickly look at your resume, see your skills and your experience. So No. I'm in it, i'm impressed by straight lines and facts :) I'd put focus on your experience and achievements.  Who you worked for and how did proved your worth to them.  I don't consider qualifications to be a great turning point, they're all theory rather than practical, experience far outweighs them in my view. Boiling your experience down into 2-5 bullet points is a skill. It takes practice, and you'll need to go through a few iterations.",5
"The difference here is about how the arrange color values in the space and based on what, to really understand this you should study how a printer works from a color standpoint and how a display works. It's like how the Mp3 compression works, you have an uncompressed audio source, you want to compress it or in other terms, to represent only the most significant Hz, so you need to make a choice AND you need to give an algorithm, a rule; in the Mp3 case the rule is simply what Hz the human ear can catch easier than others, you discard all the ininfluent or unheard Hz and you only keep the most important ones. In the RGB case you don't have audio but you got colors, you don't have Hz/waves but color-values/hues, and you always got a finite space, meaning that you need to make a choice because you can't store infinite values, so you basically make a choice based on what are the studies behind this 2 color spaces and in a nutshell the AdobeRGB is best for printing and sRGB is for TV, monitors and everything similar. To undestand how they works just read the papers, it's impossible to explain this in a short answer but there are many resources around the web. Adobe RGB also tends to be more red-ish, it's often used when printing stuff, sRGB is more common in the digital world with monitors and digital-media. They are different color-spaces, how many values they can represent it's something that is implementation-dependant, it's not the main difference between them. In the color-world there are color-models and color-spaces, this 2 are both RGBs so they belong to the RGB color-model family but they are 2 different color-spaces.",1
"So, depending on the equipment, probably you want to size circuits to either have non-critical stuff on them or make sure that they're under 80% under all scenarios. Also there are ""80%"" and ""100%"" circuit breakers.  A 80% CB may blow if you are over 80% load for more than 3 hours.  I've also been told that circuit breakers aren't typically tested as though they're precision devices - they don't go through a test suite where it passes QA if it can run at 99% of the rated load and blows at 100% of the rated load.  Instead, they're tested such that they pass if they don't blow at 80% load and do blow at 100% load, and what happens in between is ... undefined. I'm not an electrician and certainly shouldn't be considered as an authority on such matters, but I'll jump in anyhow. Another issue is that you may be at 82% of max load now, but what if the DC is really hot?  Often servers have fans that change speed depending on temperature -- and when they spin faster they'll draw more power, so now instead of 82% of max load you're at 88% of max load.  When things go wrong they tend to go wrong in surprising and unexpected ways. There are different kinds of circuit breakers (thermal, thermal magnetic, others).  They behave differently under different types of loads.  (I've seen sketchy welders that draw 22A that don't blow a residential 15A circuit because it only ran for short bursts, for instance).",1
"Don't hesitate to refine your code into smaller methods. This will help fleshed out individual part of the code and make it easier to read the code.  It would be nice if you provide some information about the libraries used in your code. For example, if I want to read about JSONArray and JSONObject, which json library do I need to search for? The same observation goes for the java classes HttpGet, HttpResponse, HttpClient and DefaultHttpClient. Showing the imports of those classes would also be helpful. Your code is in a weird format, I don't know if you had a hard time with the format in the question or if your code is like this, but note that you're not following a standard that I know. Using a common standard will go a long way to help the readability of your code. You should use try-with-resources when you have the opportunity if you HTTP library implements AutoCloseable for their connections. I think that HttpGet do implement the interface so it would be easier to not let connections leaked. 404 is a known return code for HTTP and normally every good library will provide you with a classes that will have the constant for return code. Apache have HttpStatus with SC_NOT_FOUND representing 404 Having said that I only have one suggestion to your code. It seems like you don't need to create the array of Objects from the queue of strings (Object[] q = queue.toArray();), you could just iterate over the elements in the queue and then process each element.  This would have been a perfect candidate to be a method createJsonObject which return the created object. You could have use a loop here too since you're repeating the same line 5 times (maybe more or less later if your format change)",2
"ANTIVIRUS - Generally with antivirus you see on-demand scans. That is, when a file is accessed, the AV program scans it to make sure it isn't infected.  It will also generally run periodic scans of the OS, and will look at tasks being performed to make sure they don't match virus definitions.  This, too, is a software process that takes system resources.  Generally if you're running a Windows server, you want AV on it, case closed.  Linux you can probably get away without running one - the way Linux is architected gives it pretty good resistance to viruses. Without knowing the exact environment your parents are running on their server, it's hard to say what they should/shouldn't be doing.  In general, though, if they're running Windows, and it can see the internet (or other machines on the network can), it should at least be running AV.  Encryption may well be overkill.  Most of my clients don't encrypt data, sensitive or no, preferring to rely on NTFS access controls, good firewalls, and AV.  Your mileage may vary, though. ENCRYPTION - Something has to do encryption/decryption on the fly as you access files, save files, etc.  That's generally done in software, though there are hard drives that do encryption as well.  Since it generally IS software, there is some load on the server as it encrypts/decrypts files.  Is that load significant?  It can be.  As with all things, it depends on how beefy the server is, what it's doing, etc.  Would you see a noticeable performance hit in, say, an office of 15 people accessing Word documents off a fileshare?  Presuming the server is fairly new, probably not.  If you had 1500 people accessing the same server, you would likely see slowdown.  So it's a tradeoff. Both affect performance.  How much they do, versus how much load the server is under, is a balancing act.",1
"If you want to host a server online, you may need to contact your ISP and ask what plans they have available for people who want to host a server. They likely have a business tier of plans that do not have the port blocking. Often an ISP will offer a separate plan for people who intend on running a web server. They may enforce this by blocking common web hosting ports like port 80, and use traffic monitoring to look for ""suspicious"" traffic. The reasoning for this is that they don't want to risk someone using a lower tier connection with less bandwidth hosting a server that ends up getting lots of hits and clogging the pipe, so to speak. Often home internet plans will also be more configured for high download rates but very slow upload rates. For people connecting to your server, they likely will suffer a very slow connection rate. Usually this will be spelled out in your Acceptable Use Policy when you sign up for the service. To be absolutely sure, you should connect your workstation directly to the modem provided by the your ISP, bypassing the router and running the test again. If you're on a residential plan, I wouldn't be surprised that the ISP is blocking the ports. However, if connecting your workstation directly does work, that would mean your router is not doing port forwarding properly.",2
"It would not be correct to assume that browsers all display things similarly, even where you specify as much as possible - the idea of HTML is to be able to be rendered as appropriate on any given device - if you need an exact look and feel, thats what PDF is all about. Im designing web sites after a while and Im looking for a new laptop. And I have noticed that web site can look different with the same screen resolution in different computers. Why is it so? I used the screen resolution 1366x768px with several different laptops running Windows 7 and with one new laptop tested web sites were more narrow. Is the reason pixel density I just read about from a blog - http://blogs.msdn.com/b/b8/archive/2012/03/21/scaling-to-different-screens. Does it depends on how old the computer is,  which Windows is installed etc.. The way the website looks is determined in part by the browser - of particular notes, different browsers may have different sizes associated with various fonts (see here) as well as different installed fonts and ways of interpreting the HTML. I was looking for 14 laptop with 1600x900 screen resolution but the web sites looked different with 1366x768 resolution in that computer and in a other new 15.6 computer. And Im just confused.. Is there any tool for testing web sites etc with different screen resolution if it should be pixel densitive.. I mean I can run it exmple on the big screen and I can decide how big (how many inch) my virtual screen is and how big the screen resolution is.. If one browser has a bunch of tools bars installed that also shrinks your useable area and can distort things.  A high pixel density just makes things smaller. If the browser has been resize to a tiny part of the screen, the browser will twist and contort the content to make it fit. Screen size and traditional (4:3) vs wide screen (16:9) will make a big difference.  If you set the resolution at 1366x768 and that is not the native resolution it will either leave you with black boarders or stretch it to fill the screen.  All flat screen monitors have a native resolution and that is how many real pixels it has everything else is stretched bigger or smaller to emulate that resolution. Another pointer - the same colors can vary hugely in how it looks across monitors - and this is worth taking into consideration when designing.   (Its particularly noticeable on a laptop with a good display connected to a cheap large monitor for instance - identical window just moved across - or even same image mirrored on 2 monitors can look different). Now if you force scroll bars on then it will render the content in the correct space, but the user will have to scroll left and right in addition to vertically.",3
"Inside the DigitalOcean web interface you can rename the droplet Select Droplet->""Settings""->""Rename"" As to why the option in  previous answer does not exist, CoreOS does not ship Python cloud-init had to be re-written in a compiled language (hence it being written in Go.  To view the metadata on DigitalOcean log into the host and run the command: On DigitalOcean (and some other cloud providers; Openstack for example) they're providing metadata on each boot of the machine (either provided via cloud-config  or via the options you provided in the DigitalOcean web interface).  When you bestowed a name on the droplet/VM you signified to the metadata service that you wanted the hostname to be called that value.  cloud-config is honoring the request put forth by the DigitalOcean metadata service.  This is actually a design pattern and operating outside of it would me much like trying to avoid uing convention over configuration in Rails.  You can do it, but you're only making things harder on yourself.",1
"Also, other potential upgrades like a 7200 RPM hard drive, or even an SSD instead of magnetic platters will probably have a bigger impact on performance than the difference in memory technology. Also, the NVIDIA/ATI graphics vs. Intel integrated may be worth the upgrade, as Windows 7 and Vista both benefit from having a better video solution. Tell us a bit more about the laptops and their intended usage, perhaps then we might be able to help you more efficiently. I'd say that it DDR2 vs DDR3 is not a great difference. But if it is coupled with a higher system bus it might be well worth its money. Of course, if its only for facebook and email, it probably doesn't matter either way. For a given speed, DDR2 is likely better than DDR3 - it will have lower latency. DDR3 merely allows greater speeds. Whatever the speed bump that option might get you, it's likely not worth $100. All other things being completely equal - you will not notice any performance difference between DDR2 and DDR3 memory in typical office type scenarios. DDR2 is currently much cheaper than DDR3 (though that is starting to even out), thus you see the price difference. If possible, I would recommend using the $100 difference to purchase additional memory - either as part of the stock configuration or aftermarket from somewhere like crucial.com or newegg.com. Considering purchasing one of two laptop options which have almost the same specs with the exception that the version with DDR2 RAM is $100 less than DDR3 (there are a few other tiny differences that I'm not too concerned about, such as NVIDIA graphics card vs integrated - I'm not concerned with playing games).  (I understand the extra cost is somewhat subjective here, but what I'm really trying to ask about is how much of a difference DDR3 vs DDR2 will make). DDR3 is lower voltage and thus likely lower power than DDR2. I suspect that over time, large DDR3 modules will be cheaper than DDR2 as well (ie, 4GB SODIMMs are very pricey for either), and that may make you feel better if you think you will upgrade the ram later.",5
"Short review: The purpose of code is to tell other programmers what you're telling the computer to do. Your code here reads like you're trying show off all these cool things you can do - which makes for very difficult to read, unimplementable code. Don't perform multiple operations and checks on one line. Don't use member variables to do temporary state. Structure your code to make your intent as readily apparent to the reader as possible.  The flow of this class doesn't make much logical sense. You have a member variable mNumber, that you're modifying as you go. It's really more like an implicit argument to convert(). Same with all the member variables actually. Makes it very difficult to reason about the correctness of convert(). words[] doesn't make sense as an array. The three elements in the array have nothing to do with each other. You have the numbers under 20, you have the tens numbers (why the hyphen?) and the block identifiers. Declaring it this way means you have a bunch of cryptic words[2][0] or words[1][index] expressions. Prefer to make three different vectors and name them appropriately.  Why are you using the specific word format as a signal for something? Also, all the words in words[1] end in a hyphen so you always go into this block.  There is never any reason to write code like that. Why even pass a prompt to the function if you're going to write a different string anyway? I don't see a reason for the prompt.  So convert() takes both a units and its string representation? That's not good design since you're basically passing the same/similar information twice. Prefer to restructure this call such that you're converting one ""block"" at a time (e.g. the millions block, then the thousands block, then the unit block). Redundant information is bad.",1
"I am trying to get a rudimentary NFS server up and running. Right now the server is configured as an NFS server due to a workaround for a vendor issue not supporting direct attached clustered storage, which we are trying to get them to resolve. The vendor software is Splunk. The splunk feature we are using requires files be located on shared storage (which for us is /mnt/nfs until they support a real clustered filesystem). Splunk is configured to find it's configuration files in /mnt/nfs. However, I am running into a problem where the splunk daemon starts before nfs is finished loading, and because it sees nothing at /mnt/nfs it starts creating files there, and then when the files disappear (nfs finishes mounting the share), splunk craps out. Splunk is set to run at runlevel 3, S90. NFS is set at runlevels 2-5, S60. Is there any way to delay the startup of the splunk process further? Currently the server has a GFS2 filesystem mounted at bootup (it is the only server with the filesystem actively mounted so there should be no problems with locking). We went with GFS2 so switching over to a clustered filesystem is easy should the vendor begin supporting it.",1
"To conclude, one solution would be to make two lists of Individuals. Not only would it be more respectful towards many many people in the world, but you could also get rid of your Male and Female classes: that would make your code more generic while probably shorter and simpler too. Everywhere you have variable names containing male or female, you can replace them by person1 and person2 or individual1 and individual2. Moreover, that kind of genericity would allow your code to be adapted to more complex problems (for example, a problem of Mnage  trois speed dating). Computationally speaking, I don't have anything to add what has already been said. @amon did a really comprehensive review of your code. However, there are some serious sociological and/or ethical flaws in your code. I would say that, genderly speaking, it could be made more generic: Never forget that you should treat people as people, and consider how they define themselves before considering how you would like them to be.",1
"It sounds like you are using integrated security for your SQL Server connection string. If that is the case, you will need to add the Network Service or E-SOFTIND\$MAGNUS login to the SQL Server in question. That new login should be a Windows user and mapped to the database in question with appropriate rights to read/write/delete/etc. as needed in the database. Note the user name is the domain, a back-slash (""\""), the dollar sign (""$""), then the name of the computer the web server is installed to. The issue is your Sql configuration is using the default windows integrated authentication method running in the context of your web app (E-SOFTIND\MAGNUS$) and said user isn't setup on the SQL instance on your database server so the credentials are getting rejected. If SQL Server is installed on the same computer as the web server, use the local Network Service account. If it is on a different computer within the same domain, use E-SOFTIND\$MAGNUS as the Windows user (this is a map to the Network Service account on the MAGNUS computer, on the E-SOFTIND domain).",2
"The SSD instance storage (as well as the magnetic instance storage) is ephemeral, and I don't believe it's guaranteed to persist if you do things like shut the instance down and then start it up again later (if it is migrated between hosts, for instance, the ephemeral volume doesn't stay with it). Worth noting for future readers Amazon made 'general purpose' SSDs available across EBS. You can select then via the console, or via the apis as 'gp2'. They're a few cents a month more expensive than standard magnetic drives. Similarly ... When I try to launch an m3.medium from scratch using the ""Amazon Linux AMI 2013.09.2"" it adds an 8GB Root EBS volume by default that cannot be removed using the launch wizard.   I have an AMI that was originally created from a t1.micro linux. The ""Root Device Type"" of this AMI is EBS (8 GB) and my web application software is ""baked"" into this root volume. Regardless of what it has as instance storage, it'll boot off an EBS drive. You can mount the SSD as an additional drive - this can be handy for swap space, temporary files, caches, etc. Will my newly launched m3.medium instance take advantage of the SSD storage at all?  Or do I need to create a new AMI with ""Instance Store"" as the root ""Root Device Type""? It's really only useful for scratch space (caching, temp, etc.), for which it is very convenient and free, and doesn't consume bandwidth; EBS uses the instance's network bandwidth (except on EBS-optimized instances, where it uses a dedicated SAN interface). Now I would like to launch an m3.medium instance from this AMI but it has ""Instance Storage"" of 4GB SSD.",4
"The advantage to the new AC wireless standard is that all devices will be connected at their max level(depending on location) thru to a max possible speed in excess of 300 mbs.  Traditional Routers send to only one device at a time, where AC routers with MU-MIMO can send to multiple devices at once, making sure that all devices are connected at their max speeds and reducing the likelihood of disconnects. In addition, with range extenders we should be able to form a reliable wifi network coverage.  The reported range of the ac routers Ive been researching (high end)  have been up to half an acre. Archer C5400 http://www.tp-link.us/products/details/cat-5506_Archer-C5400.html $250  No failures, but in some cases only providing the low end of 50 mbs instead of the higher 300+ mbs that are achieved on other routers.  Im never used this company before, but just researching the top tier routers makes me more likely to buy them in the future.  Some users report HUGE range gains.  Some routers have shipped without the MU-MIMO.  It will require a firmware update in these cases.   Based on my research about real life use (reported speeds via router reviews) it seems like we can expect speeds ranging from 50mbs to 300+ mbps.  So I'd say yes, wireless can replace wired.  I'd be surprised if a single user was using more then 50 mbps even ""heavy"" applications only run about 5mbps. Ive done significant research and it seems that Linksys currently has the fastest router available.  And Im sure the ability to remotely reboot the router will come in handy. However reviews indicate that a lemon is a fairly high likelihood about 1 in 10.  There are also business versions.  However Im not sure that they offer much to us beyond what the consumer version does. Cost is high at $338.58. Best to move slow on this, as it seems that at the cutting end of technology the chances of getting a lemon router are much higher.",1
"MySQL replication (master/slave, dual master, etc.) will not help if you're biased toward write. For replication to happen, a write must be 'forwarded/executed' on all systems... this will easily lower your global performance. Note: replication could be useful if you have table contention (lock on whole table), but if you are using innodb, I'd be surprise that this happens often. Also, the performance cost of having a slave could be mitigated by the time saved in a crisis/recovery scenario - but this isn't the question asked.  Just to correct the point above, MySQL Cluster is commonly used in web applications for scaling write operations - auto-sharding coupled with multi-master replication gives very high write throughput, ie 2.5m writes per second on a cluster of 8 commodity Intel servers: We're already using the most powerful box AWS has available, so realistically it's looking like a distributed system is possibly the way we should go. Dedicated hardware might be a possibility but it's a very long shot. MySQL Cluster very rarely apply in a web accessible setup. This product is mostly available for data warehousing in dedicated cluster environment.  Would recommend taking a look at the MySQL Performance Guide (reg required) which discusses different sharding strategies: While you say this is a long shot, dedicated hardware option should be carefuly studied. Most IAAS (such as AWS/EC2) environent are prepared for a very heavy bias toward read IO. On dedicated hardware, you could leverage SSD cache and/or storage tiering. You could also leverage dedicated SAN where I/O OPS capacity are tailored to your specific requirement.  You could look into the concept of sharding. Allied with MySQL-Proxy and a carefully crafted LUA script, you could automatically re-write your SQL queries to split write to a cluster of MySQL system (careful about failure rate of AWS instances).",3
"If your network is as simple as your diagram, with no NAT surprises, then there just needs to be a static route for 192.168.0.0/255.255.255.0 pointing to 192.168.1.2.  This route can be on either Router A or Machine C, the latter being more restrictive than the former. Although you may be better off using B as a 'dumb' switch instead of a router by simply connecting everything to its LAN ports, it is possible to circumvent this issue without changing the network topology. You will need to forward ports on router B that are used in communication originating outside the 192.168.0.0/24 network. If these are standard residental ""routers"" with 4 LAN ports, then you actually have a device that is a combination of router and switch.  The 4 LAN ports are a switch.  Devices connected to that will see each other without you having to do anything other than take care of IP address setup, which is normally DHCP's job.  You can ""cascade"" switches and things will work just fine. If you are looking to just connect more than 4 devices in your home to the Internet, Router B does not need to be doing routing or DHCP at all, and the functionality should be disabled.  Connect ""router"" B to a LAN port in router A, and anything else you need to the LAN ports on router B and as long as router B is not doing DHCP or anything else, it should just work. For instance, if machine D is running a web server, to which communication must be initiated from machine C, you will want to configure router B to forward port 80 towards 192.168.0.199. Alternatively, given your router supports such a feature, you can place machine D in router B's DMZ, thus forwarding all ports to this machine, unless they are configured otherwise. Normally, this could be regarded as insecure, but in this case, the machine will still be protected through router A, unless the 192.168.1.0/24 network is compromised. Strict NAT is generally a limitation with consumer grade routers, but some of the better ones allow you to adjust NAT settings. Alternatively you can use DD-WRT, tomato, or another thrid party firmware that allows adjustments made to the NAT. You can also use port forwarding (which is a little more cumbersome) as mentioned by other users. On your unspecified  make and model of router you'll have to use a web-interface or read the manual to find it's equivalent. You are preforming network address translation twice and it's probably set to moderate/strict settings. So Router B is hiding it's private addresses from connections initiated from outside router b's subnet and vice versa. This is because it assumes the network attached to it's WAN port is the public web. Normally this is fine because if the connection is established from inside the private network the router then knows where to send the incoming traffic back to. But in your case both sides are preforming NAT so each side is hiding it's private addresses from the other. This is why you need port forwarding behind a NAT so the router knows where to route connections being initiated from outside the network. A great way to understand what is happening is to look into UDP hole punching which is what many VNC and gaming applications use to allow connections to be initiated from outside.  I am assuming 192.168.0.1 is not the gateway used on router B, but rather its own IP on the LAN interface. If router A is giving out addresses through DHCP, it is also informing B of what gateway to use, and this should be 192.168.1.1. If you can access the internet from B's subnet, this is the case, unless you have a very excentring setup that needs a far more detailed explanation. The other way around is more difficult. Neither host D itself nor its gateway, router A, knows that traffic intended for 192.168.0.199 should traverse through router B. Even if they did, for instance by defining a route '192.168.0.0/24 trough 192.168.1.2', router B would not allow the packets to pass from its WAN interface to the LAN interface. To answer your question, machines C and D are on seperate networks, but D can initiate a connection with C just fine. It can't find the IP 192.168.1.3 on its own subnet, so it passes on the request to its gateway, i.e. router B, who does know where the target machine is located.",5
"I booted into safe mode to delete the mdf and ldf files and saw that my log file was twice the size of the data file.  If I routinely need to run queries that will return aggregate, temporal information on tables of the above-specified size, is there anything I can do to prevent log bloat?  Also, I know SQL Server eats resources for lunch, but what type of specs would a computer need to have to run a query like the following with the table sizes listed above? (it takes an hour on my local machine) EDIT: this database is static in nature and will not have anything added to it.  it is also only unavailable to one user, me.  I'm not sure what type of recovery it had, I don't have that PC in front of me at the moment. I'm new to the database world and have recently started working with a large database with several tables with mainly varchar text and integers.  The two largest tables are of ~50 million and 25~million rows.  The database contains about 350,000 ID numbers for people and I often need to retrieve information about all individuals that involves joins to both of my very large tables (they're one to many relationships).  These queries are also temporal in nature and involve  the between operator to determine events that happen without a certain time frame.  It will often take 10-15 minutes for some of these queries to run (I'm still learning and try new indexes to see if I can improve performance.  After running out of ram running a particular query I had to my computer froze and I had to reboot.  Even after restarting I was unable to detach, drop connections and delete my log files to delete my database (which was in recovery mode).",1
"The cached memory is the disk cache used by the VFS. It will store files that are read there so that it does not have to hit the hard drive when they are needed, and if some application need memory, it will release some. Another parameter you should keep into account is the swap usage, as soon as your system is starting to use swap (which is on disk memory and thus slow) it means that your system is running sub-optimal and this suggest the addition of physical memory. If both answers are no, then, you don't need to do anything. Even if some applications get swapped out a bit, it may be because the part that has been swapped is never used and can safely be stored on swap. Actually should be happy in this case that your operating system is making such efficient use of your memory (by using it as disk cache), it would be rather pointless to read/write everything to disk (which is slow) while you have ram a la plenty sitting around not being used for anything. Your question was not specific, but I am going to assume the system running apache is Linux-based.  Cached memory is memory used by the kernel for caching files.  This is generally a good thing, and normal.  Having memory in your system that does nothing is silly when it could be doing something useful like speed up access to your files. As stated above, the cached memory is just an in memory (fast) buffer for the data stored on your hard disks  (slow). You will typically see that if the used memory (by your applications) increases, the cached count will decrease. So this looks like a fairly normal graph to me.  The idea is that as long as everything's working fine, don't try to fix it, cached memory, and even swapping out pages is the normal operation of a VM operating system.",3
"You can replace the \0 with something that might be a little bit easier to work with, like tabs or newlines, but that would be less safe if you have funky file names.  ThorstenS's method seems like more work then is needed to me because it runs find multiple times. For a one off,  I would just do 1 find command, and output the owner and size of each file, and then do some sort magic on that file. The find would be something like which returns username (or id number of no username) and space used in bytes, in a null-byte delimited file: I need to find out how much disk space is being occupied by each user on the network. I am aware of df and du commands: I could list the entire filesystem and AWK the output, but I wonder if there is a more standard command. If you wanted to be even more efficient, you could pipe the output to script that handles it as it runs, but that would be a little more work, and you would have to get it right the first time. Is this a one time thing, or is this information you want to be able to extract regularly? In case it is the later then one option is to apply quotas on your filesystem. Doing that the system continuously keeps track of the amount of data used by each user. That way the information is merely a query to the quota database away. We periodically bump the quota higher as serviceable disk grows -- initially it was 30GB per user, something that was absurdly high at the time. What we do in many places is use the quota system, but set absurdly high quotas. This way you get the benefit of fast reporting. At one site, each user has 1 TB of ""quota"" space.",4
"Yes. If you're on 2008 you can set up an extended event session to record logins to a file (you can configure it to roll over and set a maximum size, even 64mb may be enough). You could use a ring buffer but it seems like a waste of memory... and if extended events were not possible then an audit trace will work, as will a standard trace (created in profiler and exported to a script). They can all do rollover and be accessed one way or another through simple repeatable T-SQL. Have an agent job which reads the event file each day and aggregates data into a table (on the server or on another server - if you have a lot of servers it's good to consolidate, and you can do this in a bit of PowerShell but it becomes more complicated too). The main columns you want are the login name, application name, database name, host name (though this can be spoofed), the date, whether it was successful or not, and aggregate a count. I do this as standard across all the servers I look after because noticing a bunch of failed logins (security attack) or safely cleaning up the existing security over time is a core part of being a DBA.",1
"increaseing the log size duing the opeartion like REBUILD /REORGANIZE / Update-STATS is normal depending on how much actual data size and how much it grows. What we can reduce is unnecessary disk uasage due to the incorrect implemnation of maintanance plan. 6) As Shrink log operation is not devil if done moderatly same way REBUILD/REORGANIZE is not angel when you overdo it. you shold check if REBUILD/REORGANIZE on whole DB is really neccessary? may be weekly or every 3 day works better. 0)  Make sure the Database is in right Recovery Model. if db has ""FULL RECOVERY"" mode, then you must have proper transectional log back up process as well. if not then either ""FULL Recovery"" is not really needed or you should revisite the LOG Backup process currently implemented. 4) if answer of (3) is NO: ""Whihc is most likely because you actual DB log size is increases and you haven't mentioned anything about tempdb"" then consider using that option after looking at checks I mentioned in point (5). Also notice the infomation Where it explanins the REBULD of large index in two phases. second phase of that opeartion runs in background and does not afect db performance but the down side is disk space will be remain used untill that phase is finished. From above link you will find that Rebuild Clustered Index without disableing NONCLUSTERED index can have huge impact on DISK USAGE. Also the order of REBUIDL/REORGANIZE operation for CLUSTERED and NONCLUSTERED index is important. All NONCLUSTERED OPERATION should be followed by CLUSTERED INDEX operations.  From the table you will notice that even if you have database in SIMPLE recovery model, REORGANIZE operation is Fully Logged. but Rebuild is Minimally logged.  I am happy to go over next steps/suggestions about Statistics once you try out above and get some results. I will not expect that above steps megically solves your concern compeletly but yes, those will definetly improve the situation. 1)  check the maintanance plan and make sure you are doing REBUILD OR REORGANIZE as based on the right conditions. (for example fragmatation percentage.) in some cases you are sure that you want to rebuild index for specific table thats fine.  5)  if answer of (3) is YES: ""which is unlikely"" yet if thats the case, that means you also need to check Tempdb Configuration is doen proerly or not. like,",1
"If using Google Apps, the gmail server creates a sent mail label for every message.  Sync then copies this to the sent items folder in Outlook.  It is smart enough to handle duplicates if you have Outlook set to copy all sent mail to Sent Items but not smart enough to handle the above rule, so you end up with duplicates of every sent message. 1) Create a rule to make a copy of every message that is sent in your inbox and then switch off saving of mail into ""Sent Items"".  Your sent messages will then be threaded in your inbox and you can just move the entire conversation to wherever you please.  However, this method is problematic if using google apps sync (as I am).  See below. This issue has driven me nuts for almost a year but today I finally figured out a work around after 4 hours of trying (yes I'm a bit ocd).  There are two workarounds: 2) There is a simpler way to move all messages in a conversation including those in sent and that's what I've just found.  Expand the conversation completely.  Highlight all the messages by click the top one and shift clicking the bottom one.  Drag to desired folder.  Done.  I've trawled the net for months for a solution and haven't seen this one anywhere.  I'm just relieved I finally got a solution - the problem almost drove me out of Outlook to Googles awful web interface (gmail concepts are fantastic - the UI is just awful.  I wish they'd implement a stand alone email client so that all the problems of being in a web browser are resolved).",1
"You are correct in the sense that when tuning your model via. grid search you are technically not leaking any data. But, recall that tuning your model (via. a specific procedure such as grid search) is one of only many steps you probably took in fitting your model pipeline. In particular, areas such as pre-processing, feature engineering, imputation, model tuning, data aggregations, etc. The point of the test set is to capture the entire model building process and not just the process of model tuning. I will also note that there are other ways to get around this problem, such as ""optimism adjusted bootstrap"", but recent issues have arisen with this method which have potentially shown that for high dimensional data this method does not do well, despite being more efficient than cross validation. Since high dimensional data is the norm these days, I have my doubts but perhaps it may be of use to you. With a test set, this still won't prevent overfitting to a validation set. However, it will allow you to detect the problem and give you the true unbiased measure of model performance. Indeed, which is a major drawback of cross validation and data splitting especially for smaller datasets (with a lot of outliers/noise). Basically, the performance measure you observe tends to be highly variable with how you split the data in the first place (that is, the seed you choose when splitting your data can lead to large changes in estimated model performance depending on where your outliers fall). The solution is unfortunately, not very glamorous and time consuming. In order to gain more certainty in our estimate, we need more than just a single estimate of model performance. Thus, simply repeat the entire model building process again with a different data partition (a different test set). Repeat this however many times, and average over all repeats. Possibly, form a confidence interval that allows you to see for yourself how variable your model's performance is. Furthermore, it is highly known that validation scores reported during model tuning tend to be optimistically biased (and this bias tends to be worse with smaller datasets). This is because the probability of finding a set of hyper parameters that coincidentally minimizes the error for the validation set but not to the overall population (i.e., overfitting to the validation set) becomes higher the finer your grid is. Imagine theoretically tuning your model to one million different hyper parameter combinations. The probability of selecting a bogus set of hyper parameters (i.e. that are only optimal for the validation set) is now quite large due to the sheer number of possible candidates you have elected to try.",1
"I'm assuming this is the right place to ask this sort of question, but please don't shoot me if I should have gone to a different Stack Exchange. Anyhoo, what would you say the best way to keep servers in sync is? Obviously, using something like Rsync, but here's the issue. Say you have two servers hosting the exact same files. Your A records point the same domain to both of the servers. Basically what we are looking at here is a basic round robin load balancing set up. I figured a way to get around this could be to have a separate subdomain for each server and if the file hasn't synced, then load from whichever servers it is already on (using some magic PHP code). I'm sure there must be something simpler though. Obviously I'm talking just small scale stuff hosted on something like a couple of VPSs for instance (this is more of a hypothetical question that may come in use some time), rather than a huge data centre with custom technology like Facebook and Google This obviously works fine if you have static files, but what about user uploads? Say you host a simple board based site (like 4chan) and load balance it across two servers. Say one person uploads an image, and it gets stored onto whichever server their PC connected to (from the round robin DNS), but in that instant, another person comes along to see the file, but they are connected to the other server, and before the file has time to sync across the two servers, they are left with a broken image.",1
"I fail to understand how an IR LED can show up as blue-white. It should be red (with a little white mixed in at best). You've possibly got your camera's white balance screwed up and are also capturing visible light along-with the IR light (affecting the image color). If you are trying to highlight certain parts of your scene that reflect specific wavelengths of IR, you could try: Maybe this can help:  It is very neat trick with Wii remote control -  IR camera inside with 1024x1024 resolutions.   Bluetooth connection with your PC build-in.  Bandpass IR filters are quite expensive, but if you can afford them, taking multiple photos with different bandpass IR filters in front of the sensor would work, and you would get the images at each IR band at the full resolution of your sensor. (Assuming your remove the IR / Bayer filters first.) Not quite sure what you are trying to do - only visible light makes sense to us from a color perception point of view.  If you know the specific wavelengths you want to see, then try illuminating your scene with LEDs that emit light of known wavelengths. You would then have multiple images of your scene reflecting known wavelengths. Assuming there is no fluorescence or any other wavelength shifting phenomenon, then you'll be okay. The bayer matrix would reject the near-IR wavelengths in green and blue, so only a red image will show. Unless you are viewing in grayscale mode. If you are just trying to colorize your IR photos, then converting to monochrome with a tint or a duotone in Photoshop/GIMP is your best bet. Are you trying to make a cheap thermal camera? You probably already know that the images from those are in pseudocolor, that is, they return a monochrome image to which a colormap is applied to map intensities to color. Also, thermal cameras detect much longer wavelengths than a normal CCD or CMOS sensor can detect, so your cheap webcam sensor won't work.",4
"In my head this seems like it ought to be fairly straightforward, but I'm struggling. Either that, or I'm using the wrong search terms! Thanks for any help. I have another spreadsheet (Planning.xlsx) in which I'd like to have a dynamic replica of the data in Resource.xlsx. I.e. when I add a row to the table in Resource.xlsx, a new row is created in Planning.xlsx. I'd also like to be able to add additional columns to the duplicated table in Resource.xlsx. This isn't, as you have discovered, that straight-forward. Mainly because it isn't a job that Excel is particularly suited for! What you really want is a database! I have a table in one spreadsheet (Resource.xlsx), called ""People"". As new people join the organisation, this table grows in size. If the former, you will need to write some VBA scripting in the 1st workbook that opens the 2nd workbook and watches for change events in the People table. When a change event happens, you check for a new line being added and then add it to the second table. The VBA will be reasonably involved (too long to write here I'm afraid) but not too difficult. Having got that out of the way, a couple of options present themselves but we need to understand a bit more scope. Do you want the changes to be instant or are you happy to have the 2nd workbook update at some later point? The second case is actually a LOT easier. Now you can base your second table on a query of the first. Since you want to add extra data to the second table, the easiest and most robust answer is to install Microsoft's free PowerQuery addin. Using this, create a query against the 1st table, add the extra columns you need and that is it. Whenever you update the 1st table, save, close and open the second then refresh the query (which can be set to happen when you open).",2
"You might be able to have a user daemon listen for the status change over D-Bus. It looks like gnome-power-manager doesn't expose any signals for this, but you might be able to get something from hal or DeviceKit-power/upower. My question regards best practices on flushing keys from ssh-add on activity like sleep, suspend, hibernate, etc. I thought about writing a simple wrapper around those commands, but then wondered if are they even called? Or does the kernel initiate this activity directly? Are the PM utilities strictly userland? In the past I accomplished this key removal by running a simple script that woke up every few seconds and looked for the program that locks the screen, and if it was found it would do the ""ssh-add -D"".  Then once it went away, it would invoke the ""ssh-add -c"" again to ask me for the password.  I ended up switching to just relying on confirmation and screen locking when I suspend or leave the keyboard. This is another indirect solution that doesn't involve flushing registered identities/keys from the agent, but would locking the ssh-agent be just as useful (ssh-add -x)? I am unsure how secure this method would be (certainly no where near as secure as removing keys from the agent), however I assume this feature was implemented to offer the kind of added security you are looking for in this situation. I would like this additional layer of security beyond locking my screen, etc. and was wondering if anyone else had solved this elegantly or has best practices to recommend. Thanks. I have a little pm script that run's user-defined scripts for each logged in user on suspend/hibernate, resume/thaw.  I've used this to kill or restart processes that don't behave well over suspend.  User's can create scripts in ~/.user-pm which are run in lexicographic order on suspend and reverse order on resume.  $1 has the pm operation name. You could simply add a user-script that calls ""ssh-add -D"" on suspend/hibernate.  (you'll have to look up the SSH_AUTH_SOCK somewhere, but I assume you'd need that for any solution). If you add your key using the ""-c"" option to ""ssh-add"", it requires that you confirm every use.  This isn't as good as removing the key from your agent on suspend, if you also lock your screen it can have a similar effect, since the key use can't be confirmed until you login.",5
"But not only because of the download speed, but also because of the handshake that needs to be done between the browser and the server in order to start a new download. This can be optimized by the browser's reusing one connection to the server in order to download multiple files, but not all website administrators allow keeping connections alive, so as not too have too many concurrent connections open. Download speed is how fast the connection is from your house to your ISP. It is tested by downloading a file hosted right on your ISPs network to your computer (all speedtests not hosted locally by your ISP are testing latency). It is the physical speed of your ""last mile,"" between your house and your ISP's gateway to its backbone. Also known as throughput. Download speed was fantastic (for it's time), so if you started to download a large file (eg a service pack or cd image), then it took a few seconds to get started and then it flew. Surfing speed is a marketing term, and is meant to describe how fast the connection is, subjectively, from your house to items on the Internet. It is always variable, and is out of any one entity's control. It includes latency (as explained above, basically 'how long it takes information from servers around the world to cross the Internet and render on your machine'). Also known as goodput. Downloading usually involves only one file, but surfing involves many (html, javascript, images etc.). In conclusion I would say that while download speed is well-defined, surfing speed can vary greatly depending on the above parameters. Download speed is normally done at the maximum connection speed (at least after the first few seconds). However, when you were browsing the internet, the same delay occurred for every section that you downloaded. A webpage contained five images means you are downloading six items and the delay occured six times. So the delays mounted up. All of these things taken into account will give you your ""surfing speed"". It's really a slightly meaningless number given that it varies wildly depending on exactly what you are looking at. Hence the broadband connection was 20 (maybe more) times faster than dialup when downloading files, but for browsing the internet it was a lot slower.  Also a lot of ISPs cache certain websites to make them go faster.  This just means that the ISP which is pretty close to you holds a copy of a website hosted in a foreign country, you're request doesn't always have to go to that foreign country to get that website if your ISP is holding a local copy of it.  This speeds up surfing speed as well, though the same principle could apply to download speeds if your ISP chooses to cache any of those too. Surfing is more dependent on response time.  Downloading has more to do with bandwidth. Suppose I have an internet connection that takes 150 milliseconds for a www.google.com to respond... If I have 50 of those internet connections, it'll still take 150 milliseconds, but I'll have 50 time as much bandwidth, so 50 times the downloading speed.  Surfing still depends a lot on download speed, especially when getting pictures or flash or movies. Not necessarily the same thing. Your actual speed of downloading data will be constrained by the bandwidth of your network connection, so suppose you can get X kB / second then that's how many actual bytes of data you can slurp.",5
"Large drives have lots of extra space for moving bad sectors, I've seen hundreds of sectors replaced over the course of 2 weeks and then had the drive keep going for another month (RAID6 so we didn't rush).  In the early AM hours I had received three e-mails that looked like the following. Thats how I knew the drive had bad blocks, and was the only warning: Also, as long as you are using RAID, other than RAID 0, then you are protected in case of a failure. When drives are used in an array, the controller will set Time Limited Error Recovery. This will cause disks to report medium errors if they can't immediately read the data. This doesn't mean that they will not recover from the read error, or that the sector is completely unreadable. (Cheap SATA drives do not support TLER, and will cause the read operation to hang while the drive tries to recover the data; this is just one of many reasons cheaper SATA drives shouldn't bused in arrays; this of course doesn't apply to this particular question) I have not used SAS drives, but I have had regular SCSI drives and IDE drives that get a few bad blocks and then work for years without any other problems.  The S.M.A.R.T. status should tell you when a drive is declining and risking failure. I dont usually answer my own question, but in this case I have a definitive answer: replace the drive ASAP. The drive in question failed later the same day. If it keeps alerting you each day with a few more replaced sectors then I'd replace it before it fails. One burst of bad sectors when you first use the drive isn't scary at all but a continuing condition usually means particulates in the enclosure or a damaged read/write head. In any case, SMART pre-failure prediction has been less than helpful; a Google SMART Study backs that up. If the disk determines that the sector is unreadable, it will remap the sector. The original bad sector will not be reported up the chain, so software running on the OS has no way of knowing. The only thing you can do is lookup the SMART report and see if/how many sectors have been remapped. Many sectors being remapped is a good indication of bad things to come. SMART may also report how many times the disk has experience a soft error vs a hard error.",4
"I have an EBS image that has a task that runs upon startup from the task Scheduler. It runs a Python script that then executes the required tasks using the subprocess module. Now the script kicks off when the machine is booted as expected, but one of the tasks (specifically Matlab 2007a) returns an exit code of -1073741819 which google returns nothing for. Other tasks are also python or 7zip, but they run without problem. Also, create a new local user - Scheduler (for example) - and give that local admin and a very strong password. Then use that as the account to launch the task.  When you run the python manually, do you need to click past the UAC prompt? If you do you will need to make sure that you tick 'Highest Privilage' on the task. As you say, ""something with the permissions"".  The ""task has"" permissions of the user who scheduled the task.  You can select a different user or group account for a task to run under (and inherit the permissions of).  It's easy.  How to do it is at Microsoft's TechNet web-site. Ensure that your python executable has execute permissions for the built in user BATCH (i tend to just go with full control). If I run the python script directly, everything runs without fault. This makes me suspect something with the permissions that the task has. ""Run whether user is logged on or not"" and ""Run with highest privilege"" are checked.",3
"So I think the answer to the first question is that firmware is to software, as software. So BIOSes are also software, firmware or not. Maybe a BIOS is to firmware as a square is to a rectangle, but the first question should be what is a firmware to software? The ""soft"" in software came about in the early days when Computer programming was written via flexible material such as punch cards, floppy discs and reams of paper. The programs were in a soft, changeable state. When a program got coded into a solid state hardware the program was in a ""firm"" or fixed state. Changing the program at this point required replacing physical piece of hardware.   BIOSese were once firmware. The other answers offer more about the BIOS and shed light on why that would be, but I just wanted to highlight the firmware misnomer. Firmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory. Non-volatile memory chips were / are used to ensure the information stored persists even when power is removed. The information stored could be data containing settings used to operate / identify a device, or software that executes functions in the device hardware. (FIRM softWARE) Software instructions residing in non-volatile memory chips that hold their content without power. Firmware is found on computer motherboards to hold hardware settings and booting data (BIOS) and on myriad consumer electronics devices to hold the operating system or control program. BIOS is the main firmware required by PCs primarily to identify the components connected to the motherboard. An example of such a component is the primary internal hard drive. I think firmware used to exist as something very specific. Some chip, for example, that was programmed once, and stayed in that original configuration forever. Just being, unchanging, firmly. Early PCs used ROM (read-only memory) chips for BIOS which could not be altered without replacing the ROM chip. ROM memory chips were later replaced by EPROMs (erasable programmable ROMs), and currently BIOS is stored in flash memory chips, both EPROMs and flash memory can be flashed and upgraded. On devices with no hard drive, such as smartphones, MP3 players and tablets, flash memory chips also hold the applications and user data; however, in this case, they are called ""memory"" or ""storage"" and not firmware. Firmware is a code that makes a hardware inteface working and responding to system software and is placed below that interface but BIOS is code that configures and operates the hardware that is below itself and responds to higher system software.  Firmware in non-PC devices may be upgradeable depending on the complexity of the device and decisions made by the manufacturer. More and more pieces of hardware have upgradeable firmware these days than in the distant past. But what's like that these days? Are BIOSes still deployed on ROM chips? Do the things we think of as firmware have firmware-like qualities which distinguish them from software? Do firmware updates require professional servicing?",5
"About sound problems, I think you should post it at Ubuntu Forums and seek for help, or provide more information. Well, i have 2 problems .. not exactly problems, but one is a question, and the other is a problem .  ...so, in this .vim dir, I put color, docs and such. I create a .vimrc in .vim where actually all my configuration is. Sound isn't working. I dont know whats the problem @ all. It just doesnt play . And NO its not on mute. i Checked. If any other input is required from the terminal , please ask for it. I'm kinda a n00b in all this.  Read the :help for vimrc and 'rtp'.  It will tell you what the default location of your personal vim config and runtime is.  For Unix, ~/.vimrc and ~/.vim; for Windows, $HOME/_vimrc and $HOME/vimfiles. You can drop your .vimrc on your user home folder with your configuration. I usually create a .vimrc in my home pointing to a .vim directory. I have installed gVim on my laptop and now i want to install plugins and customize my vimrc . Where is the 'plugin' , 'color' , 'docs' directory located in order for me to drop those files in ? And where do i need to put the .vimrc file for gVim to be able to recogonize it ?",3
"Note that you do already have bugs of this form: the constructor only sets the annual interest rate, and setAnnualInterestRate only sets the monthly rate. As it stands, SavingsAccount now has a requirement for being in a valid state: It much be the case that monthlyInterestRate = annualInterestRate\12.  The problem description requires being able to do things with both the monthly and annual interest rate. However, that does NOT mean you necessarily need a field for both of them. In this specific case, though, it's not just an invariant but also a DRY violation- you're representing the same knowledge in two different places. It would be easier to just store a single version of the interest rate, and have a private method to translate it into the other version when needed.  This reduces the potential for bugs, since you aren't always having to update two values when you really only want to change one thing. It's not inherently a problem that your class has a requirement like this. They are referred to as invariants, and as long as you don't publicly expose anything that allows any calling code to break it, it's fine for a class to protect its own invariant. Also don't automatically add ""set"" when it's not needed to a name. setDeposit is a strange phrase, and would be more natural as addDeposit or makeDeposit. The comment isn't adding any information here. The line below is clearly a call to that method, there's no need to say that twice.  In cases where the code doesn't express enough, maybe it's the code that should change rather than adding a comment. For example: Is the comment because it's not clear what ""balance"" alone means? Then change the variable name to accountBalance and lose the comment. Your naming is generally good, but you switch between camelCase and snake_Case arbitrarily. You should drop the underscores. I'd also consider renaming calculateMonthlyInterest. calculate implies it's going to give me back the answer to some question, but actually it's changing the underlying state. Something like addInterestForMonth or even advanceMonth might be more expressive. I don't think the ""end of..."" comments are all that useful either. These usually only serve any purpose when you have lots of nesting and large blocks, and in that case they're more of a code smell that you're doing something wrong. Comments should be there to explain something that the code itself can't. Many of your comments just repeat information already expressed just as well by the code you're commenting. For example: You generally do a really good job of separating out concerns, the only place this falls down is in the displayData method. This should return a string rather than printing to screen. That way your SavingsAccount doesn't care about what kind of IO you're using, and you could just as easily use the same class save that information in a file, send it through a webservice, email it to someone, show it in a GUI, etc. Comments like this are actually a form of repetition, so it arguably violates the DRY (Don't Repeat Yourself) principle. To see this, imagine you had to change that line to call something else instead. Now you have two places to update rather than one- the line itself and its comment.",1
"I've seen this happen in Vista when the users profile was corrupted causing it to build a temporary profile every login. Just removing the profile folder from the Users folder corrected the issue on the next login when it created a new user profile correctly.  Look in the C:\Documents and Settings\ folder to see if the new users profile directory is being created or might be duplicating with a number in the directory name or is it duplicating with a .machine/domain extension. Knowing what is or isn't happening in the profile directory should help in determining what the problem is. Now, it's true that I set them up to be in Power Users on the PC in question. However, they're a member of ""Domain Guests"" vs. ""Domain Users"" (or a more specific group) according to Windows Server 2003's AD. Just as a brief overview... When a new user logs in, Windows first looks to see if they have a roaming profile to copy locally. If not found it then builds the users initial profile by copying a default profile from one of two locations; first it looks in the directory ""%LOGONSERVER%\NETLOGON\Default Profile"" if this is not found it then uses the local default profile ""C:\Documents and Settings\Default User"". Once the directory is copied, it then runs through a task list that various applications may have added to, stored in the registry, for tasks that need to be done to a new account. The Internet Explorer setup wizard is an example of this. When the new user process is complete the logon continues as normal. If this is a roaming account, when the user logs off Windows will synchronize the remote directory with any local changes. Have you tried getting the user in question to log on to another machine to see if the problem is replicated there?  This is the FIRST thing I always do when troubleshooting issues like this, and it will tell you if the problem is with the user account or with the machine.",4
"If you want different actions, you can now create a simple class that defines the execute method. So, let's say you wanted a Message action that doesn't do anything but print it's message. Python allows for object oriented programming. Using this would simplify your program immensely. So let's walk through remaking this. Let's think about what your game consists of. You have locations, options the player can choose from, and actions those options should trigger. In addition, you want to keep track of some state about the player (alive, any items, etc.). And so on. You can define loads of new locations, actions, etc. as data instead of with functions now. First, we want to define a game state. The state should keep track of our current location, if we are alive, and a map of locations. A few extra functions are added to make it easier later. Now we want to define a location. Locations have their description and a list of things you can do there. Simple enough, but let's also give them a name, since we want to map their name to them in the State class. We also define a method for getting user input and executing the action if it was good. Now what about actions themselves? They need to define an execute method that takes a state object. So lets define an action for moving and killing the player. Now, what should we put in options? You can see above that I want to be able to get an action from them, and I want to print some text from them, so we define the class like so",1
"Quick sort is only faster if the list is completely random, for mostly-sorted lists you're better off with an improved bubble (insertion) sort as most of the time your stormTroopers will still be in the proper order the next frame. You can also do a pseudo-binary search (warmer - colder) in your sorted arrays on the Ys as the next fireball will be close to the previous one it can help skipping to a relevant stormTroopers, then scan forward AND backward in the stormTroopers array until you're outside the Y range in both directions, start scanning stormtroopers from the range's proper extremity for the next fireball. Because everything moves at roughly the same speed in roughly the same direction full blown quadtrees might be overkill. The quadtree maintenance itself could become more intensive than the cost of plain Y-ranged, Y-sorted collision. You can probably keep your fireballs sorted as well in reverse order, especially if they all move at the same speed. You'll only have to check the overlap. By sorting both fireballs and stormtroopers, checking only the overlap, and ranged scanning on every iteration it can end up faster than maintaining a quadtree. For sorting, a bubble or insertion sort will be faster than a quick sort if you reuse the same mostly-sorted array every frames. With this you can also try sorting them in X, the sorted & ranged scanning will cause your two inner loops (backward and forward scan) to do very few iterations as it will resume the inner-scan from the values found in the previous outer iteration.",1
"If you choose to use clockwise/counterclockwise movement, there are a number of things you can do to make the system easier to intuit for your players. If you are free to use the mouse you could always use mouse clicks to move around. Clicking on a location will cause the player to move towards that point and it will automatically choose the shortest direction. Circles are ""clicks"" and arrows are shortest direction. 1) How I would approach this would take sections of the planet: the top, left, right, and bottom sides (more if desirable). And have it so after the user presses a direction, the character will keep going that direction till they release that button. So if the character was on the left side and the user pressed up, then the character would go clock-wise till release. This way you don't have to worry about changes in the frame of reference while the character is moving, only after. If the character is half blue and half red, then orientation becomes easy for players to remember.  Instead of having to identify ""clockwise"" or ""counterclockwise"", the players either move in the blue direction or the red direction.  If the character is always facing in one direction (preferably clockwise, so that movement is most intuitive while upright) it's even better, because English speakers (and anyone else with a left-to-right language) naturally tend to associate 'right' with 'forward', reinforcing the connection between the controls and the direction of movement. I assume that Nintendo invested quite some research in getting the game play as smooth as possible and this is what they settled upon. I always like to look to what the bigger companies figured out, hoping they invested something in R&D for maximum comfort and playability. Use A/D to move the character in the direction towards the leftmost / rightmost point of the planet, respectively, and W/S to move the character towards the topmost / bottommost points. You might wish to ""movement blur""  the universe when moving so it looks more natural and the eyes won't focus on the moving parts shifting positions. Another way would be to change the way your game moves. Instead of moving the player when they are on a planet you could rotate the planet instead. This is also a very common choice but it can be disorienting if you have multiple planets on screen since everything except the player will have to rotate. If you decide to use this method it could help to only display a part of the planet to reduce the amount of moving objects your eye has to keep track of: Additionally, a circular W-A-S-D or W-D-S-A pattern of input results in circumnavigation of the planet, which seems fairly intuitive. At short movements the reference frame stays the same, the universe doesn't move, get beyond a certain threshold the universe moves along/shifts along so the reference frame is back in the middle. Holding down more than one key could just cause the most-recently-pressed key to take precedence, or you could use more complex behavior such as having eight limiting points. My first suggestion would be to just stick with A = Counter-clockwise and D = Clockwise movement. It is not very confusing and is pretty much the ""standard"" (i.e. most common) choice when it comes to orbital movement like this. As an added bonus it's also very easy to translate to Mobile controls if you wish to publish your game as an app as well. As long as you are moving your character, it's easy to maintain what orientation your controls are in.  After all, continuing to move in the direction you are moving in doesn't take any action at all, and when you want to reverse your movement, there's only one other key to choose.  It's only when you start moving again after having stopped that you are faced with a choice.  The longer you pause, the harder it will be to pick up again based on what you were doing before.  By reducing pause frequency and pause lengths you can keep players from forgetting how to move. 3) How you have it designed now. It can get a little getting use too but nothing players haven't seen before. You could edit it though so it appears the planet is rotating instead so the play is always ""on top"". The game could either keep going in the same clock direction while the key is held down, or stop the character at the limiting point. If it stops, it doesn't seem like a big problem -- the player has a full quadrant in which to change their input if they want to keep going in the same clock direction. 2) Another way would like how ""Lovers in dangerous space-time"" approach it. With a joystick, rather than have the frame of reference depending on the surface, they have it so the angle the joy-stick was the desired spot for ""character"" to stop at. So their frame of reference was at the center of their ""planet"". I say character and planet but they had modular ship parts on the outside where you determine where they go, ie which direction the thrusters are pointed to move the ship. In this way, the player is always moving the character in the same direction as the input movement -- e.g. D always moves right, S always moves down, regardless of the character's position.",5
"On a Mac, the --max-depth option is supplanted by -d [depth].  So, to see a human readable listing of your root drive plus 2 levels deep use the following: I have tried with du but I only get the size of the directories I specify (du . ./f1), and ls doesn't show the size of the folders. You can ignore more folders by adding --exclude=blah. You can also specify several folders at once (ie gdu folder1 folder2 ...), and in that case, you can combine all the subtotals into a single size using option c. On Mac, you can install the GNU (Linux) implementation of du with Homebrew (brew install coreutils).  Note:  this command will expose the top two directory levels off your root.  This includes traversing one level into your Volumes, and will list the summary sizes of each top-level directory in each of your attached volumes.  Depending on what you have attached, this command could take some time to complete. Building on the accepted answer, this command will show you the sizes of the folders in the directory, and will also list them by size for you to interpret easier:",4
"The main teaching here is that one needs to grow and adapt. We will happy to run our new network with no servers attached. Next January we are launching a new school. The question of the server is out once more in the planning.  Back 3 years ago, when we moved everything to the cloud in our school, it became apparent that a small company does not need a server. We have a bunch of computers and the only thing they need to share is a printer. The fact is that MS sold us the idea that one needs a server, and it may have been true in the past before the cloud was a winner. Today we run our docs and email services on Google for free. If our school burns, we can go and work next door or next city without losing one bite of information. I give all this advice, coming from an MS ""expert"" with a long tradition of managing server environments in extremely complex networks. Now that I look at our network from a business perspective, running a server in an SMB environment is totally overkill. It's cheaper to administrate a bunch of user accounts in multiple PCs than to administrate a server, not forgetting the expertise requiered and the multiple issues associated with running one.",1
"But when you open your PC anyway just have a look at the mainboard. Typically there is some sticker or print on the PCB stating the concrete model so anyone can find detailed information online then. Assuming you currently have just one module (1x1GB) installed I would recommend to get another 1GB module so you have a pair of 1GB modules installed (2GB total). As long as you don't have any specific programs which demand more memory this should be sufficient for Windows 7 and some office use. It does not make much sense to invest a lot in such an old machine. Getting a single 1GB module should be pretty cheap though. Likely you will find some information like ""PCxxxx"" (DDR) or ""PC2xxxx"" (DDR2) which makes it easy to find the right module type. To find out the maximum memory your motherboard supports you'd have to refer to the Gigabyte support docs, which shouldn't be a problem once you have the model name.  Regarding the number of modules: Most CPUs since Socket 939 feature Dual-Channel memory access. This means if modules are installed in pairs it can access them in parallel (2x64-bit bus width) and therefore increase the memory bandwith. If you just install one module or install module with different capacity then either dual-channel feature is disabled or it just works for a part of the memory size. Use HWiNFO32 to find out details about your motherboard model name and the RAM modules you currently have. It provides very detailed information.  You might use CPU-z to get memory information. If it fails to read the information then sometimes even the CPU specification is sufficient to know the right memory type. For example Athlon 64 in Socket 754 and 939 uses DDR memory while AM2 uses DDR2 and AM3 usually is combined with DDR3 (but supports DDR2 from the CPU as well, so there are some mainboards providing DDR2 but most have only DDR3 sockets).",2
"I had the same problem, but, I found with having a lot of devices in my LAN, often a device would take the IP I wanted as static. I had to make sure the assigned static IP was outside the range the DHCP could assign.  It was set to start at .101 with a maximum of 154 dynamic addresses so I had to assign higher than .255; or since I wanted to use .200+ range, I reduced the maximum number of dynamic addresses to 100.  Try setting the IP at a higher number, I chose 8, had problems, but when I changed it to 250 I was ok. Let Windows turn on DHCP.  Next, go to the Network and Sharing Center and press Change Adapter Settings.  Right-click ""Local Area Connection"" and select Status.  Press Details. You should re-check the network settings; get in touch with the person in charge of the network and have them verify the settings for you and/or help diagnose the issue. Type in the same Subnet Mask, Default Gateway, and DNS Server as you had before.  Your subnet mask will probably be 255.255.255.0.  If so, then the first three numbers in your IP Address must stay the same as the DHCP assigned address.  For example, if you had 192.168.0.100, you must use something like 192.168.0.xxx, where you can choose xxx. The diagnostics tool wouldn't complain if it was able to make network connections, but since it cannot, it assumes that the reason is that you made the mistake of assigning an incorrect IP address on a network where your computer should receive its IP address using DHCP.  It may not be right about the last part.",4
"You may use VMs or Docker or ...  These may confuse the issues of Ports, binaries, etc, since they chop up your machine is ways that sort of give you separate logical machines. If yes, then how? I also want to understand the implication of using the single binary for running two instances. Does it affect the other instance in any way during recovery process or any other db intensive task? You must have separate data trees and my.cnf files and Ports.  Your client(s) must pick the instance via the Port number.  That's about all you need to know. Currently, I am only installing once and running the first instance using systemdctl whereas I am running the second instance manually using separate cnf files. DO I need to create separate instances of the MySQL-server binaries for both instances? And do I need to install MySQL-server RPM for a second time into an alternate folder? There are several questions on this forum relating to this, and they discuss details for what to do before 5.7.6. I have gone through this link(https://dev.mysql.com/doc/refman/5.7/en/using-systemd.html#systemd-multiple-mysql-instances), for installation of multiple instances on single machine but this support is enabled from 5.7.7 version whereas I am using 5.6.34. I want to run two MySQL instances on the same machine. The reason behind this is, I will be needing two databases with entirely different configurations.",2
"I hear Transactional NTFS is used by autoupdate, but have no clue why the system would want to place this on an external disk and then be unable to stop it upon safe removal request. But, just today I went into the disk's Device Properties (from Device Manager or from the Hardware tab in any disk's properties dialog) and found that the the disk's Removal policy was set to Quick removal: Just out of curiosity I changed the removal policy to Better performance and sure enough after I did that the Safely Remove Hardware eject function worked fine and I could eject the disk without the dreaded ""Disk is in use"" error. I have no idea why the policy would make any difference since I was trying to safely remove either way but I haven't had any problems since so give it a try. https://answers.microsoft.com/en-us/windows/forum/windows_7-hardware/cant-eject-usb-hard-drive-drive-always-in-use/f052d0e7-ee89-4946-a6ad-b6e632a65133 I've recently discovered that restarting the ""Server"" service (which will generally require Windows to also restart dependent ""Computer Browser"" service and possibly the ""HomeGroup Listener"" service at the same time) allows me to then safely remove the drive. I don't know if this is actually safe to do or not, but it does work. 64 Windows users found this to be their solution, more popular than any other solution ever provided at answers.microsoft.com https://support.microsoft.com/en-us/kb/312403 Distributed Link Tracking on Windows-based domain controllers  points to the ""Distributed Link Tracking Client has gone nuts and is groveling the entire hard drive for who knows what reason. After stopping the service, I was able to eject the hard drive. For this sort of problem, you can get more information about which service svchost.exe is hosting is behind the activity by looking at the stack for a relevant item in the Sysinternals Process Monitor list. I found trkwks.dll in the stack, which is ""Distributed Link Tracking Client"" I have had this same issue every time I used my external USB drive to my Win 8.1 desktop. The only way I could properly eject it was by going into Disk Management and then taking the disk offline. It's really painful though as you then have to put it online when you next plug it in.",4
"The tool is free for individual monitors and paid plans are available for those with multiple monitoring needs. It allows you to set both intervals (every 24 hours) and durations (greater than 10 minutes, less than 2 minutes, etc), and then receive email/SMS alerts if your cron job (or any other automated task) doesn't run according to the rules you defined.  Although, if you're really that worried about cron dying, you might want to look into rebuilding or replacing your machine.  cron should be pretty reliable, and if it's failing, it's probably a symptom of a bigger problem. Start with reviewing and/or monitoring /var/log/cron.log (or wherever your cron logs are going).  cron does a good job of logging every command that it runs, along with errors.  If you want to know what happened, that's the place to look.  If you're worried about cron dying, you can setup a cron'ed heartbeat that just logs every 5 minutes, and if you don't see the heartbeat, send some sort of alert.  If you really feel like you need a second tool keeping an eye on cron, there's a perl package (Schedule::Cron) that you could use to regularly check your heartbeat.  If you're that worried about the local machine's reliability, you can also send the logs to a second machine for monitoring/processing/alerting/etc. You can use PushMon and create a URL with a ""by 3:30 AM every Tue"" schedule. Then ""ping"" the PushMon URL when your script runs successfully. If the PushMon URL doesn't get called because the machine is off, or cron failed to run (it happens), or your script fails, PushMon will alert you by 3:30 AM. You can get alerted by email, SMS, phone, IM or Twitter, and the service is free. The ""emailing"" would be done by our internal systems where our employees are consistently logging in so it wouldn't be based off of cron itself. Alternately, you could just use some sort of system monitoring tool (SNMP, Nagios, Hobbit/BigSister, etc) to externally monitor that the cron process is running.  You do monitor your systems' health, right? I'm thinking right now of setting up a database table that stores last run timestamps for each cron command and we get weekly report for the cron commands.  Or possibly storing in the database when it should run and when it last ran, if there's a problem it'd email us. The script run by cron sends it's output to the logger command. logger sends a syslog message to the Local4 facility,  which is handled by rsyslog.  The local4.* is then sent to a remote Syslog listener - in my case,  a Splunk instance.   Splunk has a saved search which fires email alerts if the events do not happen within the expected time window. In addition to the alerts, Splunk also gives me a nice searchable historical timeline of the events.     I realize that when any cron has output it will email that output... what I'm trying to determine is that if I have a script scheduled for 3 AM on Tuesday and for whatever reason it either throws an error or fails to run, I'd like to know...",5
"Simple solution here - create new column use =datevalue(cell) formula then copy the formula into your other rows- a few seconds to fix This problem was driving me crazy, then I stumbled on an easy fix.  At least it worked for my data. It is easy to do and to remember.  But I do not know why this works but changing types as indicated above does not. I can do this for the year as well, then create a date from the three components that is consistent. Otherwise, I can use the individual day, month, and year values in my analysis. This may not be relevant to the original questioner, but it may help someone else who is unable to sort a column of dates. See Images 1 and 2 for the example... (note - some fields were hidden intentionally as they do not contribute to the example).  I hope this helps... https://support.office.com/en-us/article/Convert-dates-stored-as-text-to-dates-8df7663e-98e6-4295-96e4-32a67ec0a680 I found that Excel would not recognise a column of dates which were all before 1900, insisting that they were a column of text (since 1/1/1900 has numeric equivalent 1, and negative numbers are apparently not allowed). So I did a general replace of all my dates (which were in the 1800s) to put them into the 1900s, e.g. 180 -> 190, 181 -> 191, etc. The sorting process then worked fine. Finally I did a replace the other way, e.g. 190 -> 180. In the export loop when you hold each cell, if it's a date, convert to number using CLng(myDateValue). This is an example of my loop running through all rows of a table and exporting to CSV (note I also replace commas in strings with a  tag that I strip when importing, but you may not need this): This is still cumbersome if you have many years in your data. Faster is to search for and replace 201 with 201 (to replace 2010 through 2019); replace 200 with 200 (to replace 2000 through 2009); and so forth.   If you have control over the VBA program that exports the data, I suggest exporting the number representing the date instead of the date itself. The number uniquely correlates with a single date, regardless of formatting and locale.",5
"Now lets make our model a bit more realistic. Now suppose that we want the price the vendor pays to increase the longer it has been since a player sold them that item. Let t be an arbitrary amount of time, we can then use the function g(t) = t. Now we have c' = c * f(x - g(t)) = c * 1 / (x - g(t)) = x * 1 / (x - t).  Well why not model your economy system after a real economy? Maybe have a supply variable that for each item in circulation (i.e. if you sell ten bushels of wheat to one vendor, 17 to another, and 2 to a third, then this supply variable for the wheat is 29) and have another variable, demand, that fluctuates based on some sense of demand that your citizens in your game have (i.e. if there is a famine occurring, the demand for your wheat rises). Then, use some simple formula to determine the appropriate price for your item based on its current supply and demand values. This would prevent a player from simply selling all his wheat in one sitting because the supply variable's value would increase while the demand value would remain relatively the same since (I would assume) demand would not be that volatile. The point of all this is you have free reign and a lot of very cool ways to add depth to your games world through economy.  This is still a very basic model, lets make it a bit more interesting. Now suppose that g(t) = sin(t). By using a periodic function we can model supply and demand over say seasons or even the course of a day or week. These models can be applied locally and regionally as well to increase immersion and the intricacies of your worlds economy. I need to set up a monetary system for my game however I am unsure how to craft an internal economy for that money. I want to make an rpg with a economy system that keeps the player for a time from just selling numerous of the same item to grind for money. So when the player sells to much of a certain item the buy price for that item will go down temporarily. Also, I would like to add a system where certain items go up in terms of what they can be sold for temporarily as well. For example, in ""Story of Seasons"" I can sell items to vendors in order to get money, however if I sell to much of that type of item the price I get for it goes down. Also, there is a system where for a certain vendor or more the price I can get for the items actually increases. Overall, this is a system I've seen several times now and it does work pretty well as far as I've seen. I'll leave the actual programming up to you. Good luck! Also, as for your ""different prices to different vendors"" comment, maybe you could make the previously mentioned supply and demand variables vary according to geographic location, that way it takes into account the situations of that area (like if only one city was experiencing famine). You could then keep the same formula for determining pricing as earlier since now the both variable values change according to the location. You could also have specialist vendors that simply pay more or less. Since the player regulates supply through their actions you are responsible for handling how demand is calculated.  Lets start with a simple model. Consider the function f(x) = 1/x where x is number of an item the player has sold to this specific vendor and x >= 1 (avoid division by zero). We can use this function to modify the price a vendor pays the player, c, for a given item. The result is c' = c * f(x) = c * 1/x.",3
"I am wondering where to begin with database scaling/optimization strategies.  After reading articles like highscalability.com's facebook architecture article, and this twitter architecture article, I am not sure if by RAM they mean only memcached, or something else. Their database may indeed all in RAM, meaning that it exists in a state that does not correspond with any persistent data on the drive. The trouble with storing a database in RAM is that RAM has a nasty habit of forgetting everything when the power goes away i.e. it's not persistent. That said, making proper use of memory for high-traffic sites is absolutely essential to getting decent performance, because you get extremely good IO rates from it and that's a very useful thing if you have a high load. So, in summary, a high-volume site will have its data stored on persistent storage (spinning disk, SSD etc.) and then set up a series of layers of higher-performing caches in order to reduce (usually the reads) load on the database. Writes typically go straight into the db, but you can use a localized write-cache if you've got a lot of writes. From memory, MySQL had a MEMORY table type that stored data in RAM rather than on disk (as InnoDB and MyISAM would). Creative use of a RAMdisk would also allow any database to use RAM as a disk backing, but as above, this isn't probably what you'd want to do. As you've hit upon, a more useful application would be the use of RAM as a high-performance cache, using something like Memcached. As I'm sure you know, this gives a fast key/value store, but requires the application to know to look there first and then fall back to the persistent database if nothing's found. Sites which require a high IO rate across their entire relational DB have the option of dumping the entire DB onto something like a Fusion IO drive. This isn't going to be as quick as RAM, but has the option of being persistent so can be a useful middle ground. I believe SO runs it's database on a Fusion IO drive (see this blog post about their findings. Can X stored in RAM questions: Yes if the structures are not already cached in RAM by the database system itself, imagine just a RAM-disk as filesystem. Everything there is in memory. You got enormous bandwith from those systems. Drawback with those: Guess what happens when someone draws the plug... all your memory is lost. There are of course solutions to this, like doing regular snapshots/writing the stuff to real hard disks, or you could use persistent memory (not flash, that is far too slow and limited, but there are (really expensive) solutions like MRAM. For a simple example, when an application does a SELECT * FROM on your database, that application holds the entire database in RAM.  It's easy to visualize in Java: you've just created a great, big array of Java objects.  Where are they stored?  They're in RAM. You can store anything in RAM, it's simply a storage area. What you have to take into effect is storage size, and what else needs access to RAM to make sure you have enough. Touching on your more specific questions, technically SQL databases don't get stored in RAM, but the data from them can be.  Yes, you can index an SQL database and keep that in RAM (but it doesn't mean that your database is in RAM). Yes, all the SQL databases can be stored in RAM, and it is a pretty standard method on high performance sites like this.",5
"If you don't need maximum density, front and rear mounted shelves and sit the server on those.  But that won't be something you can screw into, and if it tips any it could all come tumbling down, but also you may not be able to get servers in the mounting space between the shelves, which could half the density. However, the servers I've seen that have these rails were years ago on Supermicro servers with the sturdy ball-bearing rail kits.  The servers in the last several years have all been missing this capability.  So mostly you'll just need to check the capabilities of the servers. There are servers with rack-mounting kits that specifically are designed for center mounting.  Usually they have ears that mount to the rails at the center, then another set of ears that will mount to the front of the rails so you can screw the ears of the server into it, so they don't slide forward as you are pushing cables in the back. Will a 2 post rack support a server (even a really heavy one) if I mount it at the center instead of just the ears? The shelf was a 4U job designed to carry 50Kg. The server was a 2U server and sags at the back, but it's been in the rack for 18 months now and it hasn't broken yet. Another option available if the equipment exceeds the 450mm width limitation is to use a universal rail kit.  This rail kit is a non-telescoping kit that will mount most equipment into most racks and cabinets. Another option I've done in the past was to get 2 2-post kits and use them for the front and back rails making my own 4-post rack.  I used a double layer of plywood to bolt the bases into, then I used a cabling ladder bolted to the top of both and then mounted to the wall to stabilize it, say for an earthquake.  Worked very well and was effectively a 4 post rack. It can certainly be done but and the kind of shelf Mark is talking about can easily be constructed or purchased but it still ands up with a server that is effectively balanced instead of properly mounted. From an engineering perspective you are placing loads where they don't belong and on parts that may not be designed to handle them safely. Mounting a server in a 2post rack can be accomplished fairly easy in both a centermount or flushmount configuration.  Depending on the manufacturer, most modern servers have alternate rail kits that are compatible with 2post racks.   There is a second option available that uses a conversion kit.  This kit has (4) support brackets that allow you to use your 4post rail kits in a 2post rack.  There is a critical dimensions that has to be determined before using this kit.  The server chassis with the 4post rail kit installed cannot exceed the opening of the 2post rack, which is 450mm wide.  These kits are available in multiple sizes ranging from 2U to 7U.   So, Possible? - Yes. Advisable? - No. My opinion is that if you cannot currently afford a four post rack you should consider not installing the server in a rack just yet and wait till you can buy an appropriate rack. You only need one bracket to break and the resulting damage could be significant. Also consider that you cannot (safely) use rack slides when mounting in a two poster, because the leverage resulting from a server slid out for maintenance will almost certainly result in breakage and having to remove a server from the rack just to replace some component is a pain in the you know where.",5
"My speed went back up to where it should be when I unplugged my cordless phone base that is on 2.4 . Using a repeater will decrease your data rate by 50%. That and the overall overhead on wlan networks (wpa encryption will cut another 20-30%) results in a real data rate which isn't capable to sustain 64 MBit/s. If you're running your AirPort Express in 2.4GHz, it will only use normal (narrow) 20MHz-wide channels so that it leaves room in the band for Bluetooth and other uses. So your maximum signaling rate is 130 or 144mbps depending on the guard interval in use. If you have a Time Capsule on the same network, and it's configured to wirelessly extend the network, and your client happens to be joining the Time Capsule rather than the Express, and it's joining in 2.4GHz (because you have one of the old one-band-at-a-time Time Capsules from 2008, or the client happened to select 2.4GHz when it joined your simultaneous dual-band Time Capsule from 2009 or later), then yes, that will cut your bandwidth in half in the best case, and possibly by much more. You didn't say if you're running in 2.4GHz or 5GHz, and if 5GHz, whether you've wisely left it set to the default of using wide channels. If you hold down option when clicking the AirPort menu extra, you'll be able to see the BSSID (the wireless MAC address) of actual AP you're connecting to, so you can determine which device you're really connecting to. You'll also be able to see what signaling rate you're getting, and what channel you're on. The rule of thumb for overhead inherent in all 802.11 networks is that even in ideal conditions, your TCP throughput will only be about 50-60% of your 802.11 signaling rate. So it's possible under ideal conditions (completely clean channel, nothing else using your network, and your client is within a few meters of the AP) for your wireless link to match your broadband link.",3
"As I understand it we need to implement a certificate on one of the DCs in the domain and the one that the password server would query it on port 636. Having never implemented this I am a bit concerned over what effect this would have on the reading of AD by the computers and the user accounts that are in use.  I have scoured many articles on the internet but cannot come up with a definite answer and I'm at a halt with the implementation as I am concerned it may break something. I have a test server setup but constantly fail to get it to read AD with the test user accounts that we have in use. The only thing that we have left now and what a lot of the forums mention is that the connection between this server and the AD LDAP needs to be secure (SSL).  If I implement one certificate on one server does it only affect that server or does the whole of AD become secure, do clients continue to communicate in unsecure move and it will only be the password management server that communicates in SSL.",1
"First thought, remove that last if statement and see if you get any print out.  It looks like it's overwriting your output. One of my tasks I need to do is Check if there are any Critical Updates. It seems like this would be easy enough but I'm having A LOT of trouble with it. I am not familiar with WSUS, I've never dealt with it but I need to communicate with it. Logic looks good; it should have to go through all of the if-else statements to reach the ""FAIL. Could not query Windows Update Server"" text. So, what is happening is it won't connect to Windows Update -- The object ""Microsoft.Update.Session"" uses the windows update agent to manage whether the updates are being handled by windows update or a WSUS, but sometimes it just doesn't do it and I don't have an explanation or even what I can do. I've been trying to resolve this a few days but it is hard for me to test it and see it failing, I just get a phonecall stating a server slated for production failed when it isn't suppose to be. I asked this question at Stack Overflow but they said I would have better luck here. I am using VBScript to automate a lot of server tasks, I'm simply checking to make sure the server build team did their job before the server actually gets used. Anyways, on my test server and on other servers I have tried it on (only two others). It seems to work fine and it passes. In production, I'm getting a lot of errors where I get",2
"Really good rendering quality can indeed be achieved without them, depending on your definition of quality. You can stick to CPU-based graphics, if you don't mind those graphics being quite slow. Or, you can use the GPU and used the old fixed-function pipeline, which gives you a subset of the shader functionality. Whether that subset is ""really good"" or not is subjective - it was certainly good enough for many years - but you miss out on some interesting modern rendering techniques as a result, as the fixed function pipeline has no way of performing the completely arbitrary transformations on vertices and pixels that shaders can perform. That doesn't mean you can't create great looking games without shaders though. AFAIK Torchlight only uses fixed pipeline and looks awesome. Also: If you're doing 2d graphics, you'll probably be fine with fixed pipeline only.   I don't really know when vertex and/or pixel shaders became available for consoles and computers, but what could we do without them ? I don't really understand what is achievable with shaders and what is not without them, and what are the true advantages while using them. In the mid 90s add-on graphics cards became popular because they vastly accelerated graphics, but at the cost of flexibility. Writing pixels became a lot cheaper but reading them became much harder and/or slower. I guess the advantages of using shaders are performance and better flexibility, but on the other hand, I am quite speechless when looking at the syntax... The fixed pipeline is rather limited. Something as simple as Phong-Shading already requires a shader (the shading capabilities of the fixed pipeline end with Gouraud-Shading). The possibilities that open up with shaders are huge. Some of them are: better shadowing-techniques, ambient-occlusion, normal-mapping, etc. Now that graphics are faster, and technology has improved (or become more affordable, depending on how you look at it), some flexibility can be added back into the graphics pipeline. Shaders give you this flexibility. The syntax is quite low-level because the hardware it is aimed at is also quite low-level. It's a compromise. Once upon a time all graphics were done in software. This was very flexible, but not very efficient. I think a really good rendering quality can be achieved without using them, but I'm not sure which one...",3
"First, if there's a rootkit, you're probably fighting a neverending fight. Take the server offline and reinstall and restore backups that are pre-infection. That's the ""best"" method of fixing. Third, what custom code is running on the server outside Plesk? How do you know that was even the infection vector? What's rather interesting though with reference to the first statement is that on the 15th of July Parallels brought another ""security"" patch. I manage a Plesk 9.5.4 server which was infected with the /km0ae9gr6m/ malware. FYI - on my server, the malicious code was added to Javascript, PHP, and HTML/HTM files. I'm posting to let the community know that I was using Plesk's IP address restriction feature at the time of the infection. To be clear - within the Plesk control panel, I had denied Plesk login access to all but two IP addresses (my own studio, and the office IP address of one of my clients). Despite this restriction, log files show logins for the admin user from IP addresses that should not have been allowed by the Plesk firewall. I have since implemented identical address restrictions on port 8443 with IP tables, and to date (approximately 48 hours) have had no further issues. Based on my experience, I say do not trust your security to Plesk's IP address restrictions. If anyone is interested, I would be happy to share my log files. We have seen this on plesk 9.5 servers despite having Parallel's security patches installed from February.  Hopefully more info will come to light on how this attack was able to circumvent both the Plesk login and IP address restrictions. After a couple of horror weeks loosing clients and being re-infected we received this reply from technical support at Parallels: Basically they are POSTing to the login page and getting in without auth, then going straight for the WYSIWYG file manager and appending the code to js files. Plesk are refusing to acknowledge this and the only option is to firewall plesk off and allow to specific IPs. In the end, the best thing to do is take the server offline and fix it by reinstalling from backups. Otherwise you can't fully trust it. And if you have any ""personal"" data (user passwords?) they need to be informed that their information may have been stolen. Then start setting up some kind of auditing on the system, and send logfiles to a secondary server over a safe channel of communication so logs can't be erased by an intruder. And run some kind of file checking utility like Stealth on another server to monitor your file integrity and warn you of changes. This will pull a list of all files in your vhosts directory containing the malware signature. I found it easiest to edit the affected files with Plesk file manager. In my case, all malicious code was appended to the bottom of the infected files. Without auditing and sandboxing, you're going to have a hard time telling what happened. If there's a database running on it, someone could have faulty code on the system. If someone else has access to the server, maybe they did something to infect it. Are the websites running with different file permissions to silo possible damage? Or are the sites pretty much sharing all the resources? Are other users involved and able to run scripts? Do they have different widgets and whatnots installed? Were the files timestamped, so you could go back into the logs to try to glean what happened?  Without knowing what else your server runs, there's little other people can do to tell you how it was compromised.",4
"The reason for this is that due to me hosting files that people can download I am with the low internet connection sometimes hitting that cap, and I want to be able to monitor when / how offen I hit that cap. I've tried different kind of tools such as Zabbix, Nagios and ntopng(ntop), all useful tools but none of them seem cable of logging the actual server speed / usage of the speed. Currently I still have Ntop(ng) installed but I am unable to get it to do what I want it to. (it is vital that the data is logged 24/7) I've been doing a lot of searching around for a tool to monitor a website's ""bandwidth"" usage, and I'm not talking about how many MB/data is being loaded but the servers actual output speed, to clarify what I mean I'll use an' example, The last element is the bytes transferred to the client. You need to keep adding this number and print or log at a regular frequency to produce the total bytes transferred to the client from the last run till that moment. My server is a Ubuntu 12.04 running with Apache 2.2, I don't know whether it's needed but in case: My domains are setup with Virtual hosts for each domain. It sounds like you are trying to monitor full network bandwidth by measuring web server output as noted in its logs - this would not be sufficient as that not include any TCP/IP protocol overheads or other traffic (any SSH, FTP, database, or other traffic that the server might take part in). What you need to be watching is interface statistics. Any server monitoring tool such as Zabbix and Nagios will have a function for that, for instance with collectd you would use either the interfaces module (https://collectd.org/wiki/index.php/Plugin:Interface) to monitor all traffic on a given interface or the iptables module (https://collectd.org/wiki/index.php/Plugin:IPTables) to monitor traffic that matches certain filtering rules (so you can analyse the bandwidth used in more detail).",3
"PrimoPDF can be used from the command line too (it has a GUI but it can be used without a GUI too). It has a free version and a commertional one called nitroPDF. If you are looking for a commercial solution, I think Easy PDF Creator can fill your requirements. I have used it to set up a shared network printer with automatic saving (creates the PDF in a predetermined directory) for a similar sounding situation as what you describe. Can setup locally in a similar fashion as well of course. It was very flexible and once set I didn't really ever have to worry about it. Its commercial, has a defined support roadmap, and is about as proper PDF as you can get, but is a lot more expensive than other apps here, and is aimed far more at 'Enterprise' solutions. Depending on your platform, the PDF printer for CUPS might be an ideal solution.  I've used it before on OS X and it's worked great.  (Just tell it where to throw the PDFs, set it as your default printer, and you're good to go.)  Apple is the official maintainer of CUPS, but I'm not sure of commercial support.  I really don't know much about how it's implemented, but it could meet your needs:  http://www.codepoetry.net/projects/cups-pdf-for-mosx. Have you looked at the Adobe LiveCycle server, it has core modules for automating PDF creation/generation/conversion.",4
"so i have old MSi GE60, its already +-6 years, monitor breaking up and what not, but the internal hardware still very much working great It depends on where you are planning to draw current from. Likely, your laptop power supply is not 5V but something like 19V, so 5V line on the PCB is fed from DC-DC step-down converter mounted on PCB. It's not safe, nor is it convenient to feed from that line. And it's especially non-efficient to connect a step-up converter to the line fed from a step-down because each conversion has about 80% to 90% efficiency and the whole chain will be losing about half the power. i already post the question notebookreview forum, but theres no conclusive answer (with reasons) as of yet  my plan is to gut all the hardware and convert the laptop into a family media center, i already planned everything , the custom case etc, the problem is, i want to put another fan on the case, and take the power for the fan directly from the mobo MSi GE60 mobo : https://ae01.alicdn.com/kf/HTB1qgTqMVXXXXcXXXXXq6xXFXXXd/MS-16GA1-For-MSI-GE60-Laptop-Motherboard-Mainboard-100-Tested-fully-work.jpg since MSi GE60 have unused PCie SSD, i'll buy the PCie to USB module, BUT, since almost all fan in market is powered with 12v , i also planning to buy the 5v to 12v step up module to put on the PCie to USB module i'm not a tech savvy, nor an electrician, barely know anything about pcb and such, thus having this question, it'll probably noob question, but please stick with me, and also i'm not sure if i post this into the correct forum Much safer way would be to take 19V straight from the power connector and use a step-down converter to get 12V you need. the step up module i'm speak of : https://www.aliexpress.com/store/product/Universal-2-1x5-5-mm-Micro-USB-Cable-DC-5V-to-12V-Step-up-Boost-Line/333670_32679700740.html my problem is, i don't know if such setup is safe for the laptop motherboard, anyone can enlighten me?",2
"Without VT-x support, you really don't want to run any VMs without support for  paravirtualization. In other words, no windows. Without VT-x support you really cant run windows as a VM. Setting up Xen and running Debian or Ubuntu paravirtualized would probably work nicely, but the only way of getting windows into the mix would be to either run it (SLOWLY) via qemu or install windows on the machine itself and run for instance Virtualbox to handle the linux VMs. But even that would be far from optimal and probably very slow. I've been looking to virtualize my atom based server (This is the mobo: http://www.asrock.com/mb/overview.asp?Model=A330ION). I've tried to install ESXi and it couldn't detect my hard drives, I've tried Xen Server but I couldn't install Windows due to no VT-x. So my question is, are there any other alternatives that I could use? All I need is to be able to install are linux distro's such as arch and ubuntu and a windows machine (xp will do). I'm not sure if A330 supports virtualization, but if it does you will need to enable it in BIOS. Also, make sure your SATA controller is running in AHCI mode, this way you will have better chances with VMware. Other then that I agree with @Chopper3 - give it up :-) well, i'm currently running virtualbox over a basic ubuntu server install. Its not as light as a standalone ESX I install but, well, it seems to work alright. without VT you're stuck with emulation and not proper virtualization. Still, virtualbox or vmware ws/player should work.",5
"If you find your RAM filling up with famous candidates like Photoshop or Google Chrome, then you may wish to upgrade that so you can run more at once. Upgrading your RAM also means that your OS won't need to free up the RAM that often, leading to smoother running of applications. If however your RAM can handle your workload but everything is too slow, then the CPU could use an upgrade. You should consult the service manual for your specific laptop to see what CPU's can fit and be supported - not all that fit may work. For example, there are a list of CPU FRU's for select Thinkpads (where the CPU is socketed) that tell you what CPUs can be installed. It's impossible right now to tell you exactly what to upgrade, because of the vagueness of your question and no information about your workload, but you can do more research on what would make your experience better and upgrade accordingly. upgrading from a  1GB Elpida  to a 4GB memory chip (KEMBONA SODIMM LAPTOP DDR2 4 GB 4G 800 MHZ PC2-6400 RAM)?  I wpuld argue that, if you need to choose, get more RAM.  After HDD, RAM is typically the bottleneck that slows down most systems. As most modern systems are designed with an expectation of > 2 gigs of RAM, with 4 being anything entry level and 8 being typical,  RAM will make a huge difference. (Even a typical low-mid range Cellphone. now sports 3 gigs of RAM). The CPU and RAM are used for different things - the RAM is the amount of random access memory that is available to the system - which directly affects how many programs it can run at once. The CPU's clock speed tells you how many operations can run per second. The number of cores directly affects how many programs you can run concurrently (truly concurrently - different to threads, which is where a CPU splits the execution of programs using a time division). If you are planning to extend the usable lifetime of your machine, and are willing to practice patience, go for more RAM. ""Swapping to disk"" is only a makeshift here and will in addition waste CPU cycles while simulating RAM. In general, the problems one has to solve by using a computer, will grow over the years (...that is, require the manipulation of more data than today, e.g. larger images in digital photography, more chunky apps, and so on). I apologize if this has been asked before, in this fashion, please point me to the previous questions.  And a follow up question: How do I know which CPU is compatible with my laptop (Dell Inspiron 1545 PP41L)? Do I just look at the socket type? I know from experience, that Socket P fits into my laptop, but I don't know: Assuming you are not doing real-time computations (like object detection in live camera images, or trajectory planning in robotics): With respect if CPU, the third CPU is only about twice the speed if the first one. (The second one listed is about 1.7 x as fast). A basic new entry level sysyem will typically be 3-5 times as fast as your current CPU, but will also include specific speedup functionality not available on a 9+ year old cpu - Im specifically thinking of graphics (video playback and rendering) and AES (encryption offload). For these reasons I conclude that neither chip will give the system the kind of speed boost you might think. Apps which are not CPU/GPU intensive will work anyway. Apps which are wont. IMHO, unless you gave a specific use case, dont waste any money on that system - its well past its use-by date. Id honestly expect a midern mid-range cellphone to be more powerful. A chromebook or entry level $200 Windows laptop will provide better bang for buck,  usable battery  and better perfornance.",4
"DevOps is a ""team"" of people that includes the development and operations teams.  (security/compliance combined).  When you are searching for candidates, imagine asking an IT guy if he plays DevOps. There are so many successful manifestations of DevOps teams that you need to understand it in the context of the business that they support, because, ultimately, this is the value of DevOps. The customers expect a certain pace and DevOps is the only way to deliver at that pace.  A baby doesn't learn to walk by standing up and walking. He learns through constant tries and failures. Even if, for an unknown reason, he gets up and starts walking, we shouldn't expect our babies to achieve the same. I usually point that out when people compare themselves to Google, Amazon or any other company with great achievements. Their failures thought them how to get stronger more than ingenious ideas all the time. Right now for example I am improving my english through constant writing and failure to make my points clearer and clearer. Something was wrong. Then something kept being wrong and that something got culturally accepted even though it is slowing down the team. Skipping failing tests, non automated provisionning management, deploying with crossed fingers are among few I had the chance to experience. These are triggers that indicates improvements are required. Failing to notice them can bring you to a hellish state where coming back will require a Genesis project. Failures will indicate your football team where improvement is required. These weaknesses will vary from a team to another. It might be in tooling, leadership, lack of vision, roadmap pressure, lack of experience, etc. No matter the reason, it is primordial that you take time to review each development cycle with everyone. Otherwise, you might enter the stage below, which is hell to come back from. What happens a lot of times is that companies throw in a bunch of buzzwords and tool names and expect to find the right person for the job. Do not dismiss perfectly good candidates just because they do not call themselves DevOps experts. More importantly, pay particularly close attention to the personnel you already have. With your leadership skills, the talent you already have can be quite successful at DevOps.  You shouldn't think about the devops ""team"" as a separate entity but part of the larger organization.  Checkout http://web.devopstopologies.com/ for organizational structure.  You can see it's about moving together.   I believe DevOps is orthogonal to your question, i.e., it changes nothing compared to a ""classical"" approach (or to hiring any team at all, not only for software development). You identify what your key needs are (for example, an ""architect"" who is able to structure large software systems; some ""hacker"" who is able to fix a Kernel driver if needed; a ""tester"" who likes to test and such; and finally maybe a ""DevOps engineer"" who excels in creating good CI/CD tooling).  I'm currently extending my DevOps team, without forgetting skills, for me, the most important thing is the ""mindset"". I tend to agree with AnoE's answer. Getting the people and giving them the vision are primordial before DevOps should be put in the picture. I would also like to add two more things on top of that. The additions come from the experience with a team that got excited about ""DevOps"". My answer is basically, go ahead for all the pros. But be aware that this sort of process will require culture and team monitoring in the sense that when  things start getting bad, your team must learn from it or it will die from it. Since DevOps is a hot trend in IT, companies are seeking top-notch DevOps talent. However, before you begin assembling your team, there are a few things you should consider: But all of them need, to stay with your image, to be able to kick the ball. I.e., they all need to work together in the context that the team is working in. They all need at least some basic understanding of what the other team members are doing; if you have a strict CI/CD pipeline, then they all need to be able to develop in that frame; and so on. When you get down to it, DevOps is all about culture. If you take a group of IT engineers and traditional developers who understand and embrace the DevOps culture you can build a successful team. However, if you hire so-called DevOps experts, who know Puppet, Chef and Docker inside and out, but do not accept and adapt to the collaborative and cooperative nature of DevOps, this team is doomed to fail.  Imagine a rock band is searching for a new guitarist. There are lots of people who know how to play the guitar, but there are so many music styles that can be played on the guitar, plus there is also another important factor, which is how well this guitarist gets along with the rest of the band.",5
"I usually run RDP on a port ""other than"" 3389 (security through obscurity).  Also, I lock down RDP to allow only one certain user account. So now I am wondering what I do for providing secure remote desktop or similar access to the machine for tweaking and manipulating the app online. taking the app offline etc. Do I ask the provider to set up some sort of whitelist allowing my office IP forward to the RDP port or do I need to provide hardware or other software to lock the box? When your server's in the colo does it just have Internet access, or do you also have a LAN extension or similar private circuit to manage it over?  If so, then it's a non-issue and RDP won't be a problem.  If not, then I'd definitely recommend locking RDP down past the defaults; different port, oddly-named Admin account with a strong password and a restriction on which IP addresses can connect to the service. I am planning to deploy a Windows server running a web application into a colocation for the first time. I have already run IIS lockdown and done a number of other security tweaks based on advise from the smart folks here on the site.",3
"If you're more interested in content that formatting for now, Google Docs would be a good free solution. Multiple people can edit the same document simultaneously. If you definitely want to use Word, you could use master documents/subdocuments to split the document up. Each subdocument could be edited independently, but would still be linked into the master document. If you choose this option, be very careful to make frequent backups -- it's rather easy for master documents to get corrupted (although this seems to be improved with more recent versions of Word). Having the document be stored in a Dropbox allows multiple users to edit it sync the changes, though I'm not sure how well it handles concurrency. I think Google Documents allows something like this, but that also may not be exactly what you're looking for. Take a look at Groove in the suite, it should allow you to do what you need - multiple people on the same document. If you have 10 people editing the same document, I am guessing you are a large(ish) company with full copies of Microsoft Office. If you don't need all of the fancy word style stuff while this document is being created, it might be easier to set up a wiki or source control server, then assign somebody to copy it to Word and add the styling at the end of the project. That way, you get the HUGE benefits of being able to track changes and go back to any point in the history of the document.",4
"You will, however, need to tell Apache you host several domains so it knows to serve out the correct pages for a given connection string. Debian/Ubuntu on a VPS should use pretty much the same amount of RAM. Ubuntu's likely to have slightly newer versions of most things which may use slightly more RAM, but not an awful lot more. Otherwise, the only difference is between the base install on ubuntu, which tends to be slightly shinier. On the server version this shouldn't be an issue though. The default configuration for Apache is ""good enough"" for most purposes, so you should not have to tweak connection parameters.   Do some load testing to see how it works under stress.  If you have static pages it should not even break a sweat. I was told that Debian is a bit lighter-weight than Ubuntu, requiring significantly less RAM. Would it be a good idea to scrap my current install and go with Debian to save on RAM? How great are the savings? Currently I've set things up as an Ubuntu LAMP configuration, which is using 137MB of the total RAM without any traffic coming in. I haven't really tried tweaking Apache2, as I'm unsure of what figures I should put in for maxclients and the related process settings. I have a VPS with 256MB of RAM that I'm setting up. I plan to eventually move the sites that I currently host on shared hosting accounts over to the VPS. The domains received an aggregate of 2,046,164 requests last month (according to traffic logs), though it can vary a bit up or down depending on the level of social media traffic that one of the sites gets.",3
"You should be able to manage the modem using http://192.168.1.1 and the TP-Link with http://192.168.1.254. One network, easy. DHCP Server is disabled on modem, but enabled on TP Link router, I have already setup DHCP to give my home PCs certain IP addresses. Anything you then connect to the TP-Link, via WiFi or LAN will get its IP from the modem because it is and should be the only DHCP server in the network, and everything will be able to connect to the Internet without extra NAT. My ISP gave me modem, that modem only has 1 WAN output (to connect to my router). I have TP Link router with WAN input and 4 LAN outputs. Basic connection is : straight LAN cable, from modem WAN output to router WAN input. To do this connect directly to it in isolation using http://192.168.100.1, then you should be able to connect to it with http://192.168.1.254 EDIT : I do have occasional packet loss, is it possible to have that due to bad network configuration? If I am not wrong they should not be able to communicate each other, because there is a different subnet mask on 192.168.100.1",2
"I would think that using Group Policy to modify the security descriptor would work fine. I have seen cases where some services don't like the default permission that a group policy-based modification puts on a service (look at this posting about the Windows Search service if you want to see what I'm talking about: http://peeved.org/blog/2007/12/07), but that has been uncommon in my experience. I'd recommend creating a group to delegte this right to, putting a user in that group, getting the group's SID (using WHOAMI or any other tool) and modifying the security descriptor this way. 5) I flipped back to my ""Bob"" command prompt and verified that I could now stop and start the service using NET STOP and NET START. This is a pretty common security descriptor for services. I've seen it verbatim on some Microsoft services. The SYSTEM and built-in Administrators have ""full control"", ""Power Users"" can stop, start, and pause the service, and ""Authenticated Users"" can query properties of the service (I'm glossing over a bit here). This SDDL string gives Bob's SID (S-1-5-21-1409082233-484763869-854245398-1009) rights to stop, start, and pause the service (RP, WP, and DT, respectively). If you want more background on security descriptors for services, have a look at http://msmvps.com/blogs/alunj/archive/2006/02/13/83472.aspx and http://support.microsoft.com/kb/914392. 3) I created a limited user called ""bob"" on my box, opened a ""RUNAS"" command-prompt as him, and got his SID from ""WHOAMI /ALL"" (a command that's on Windows Server 2003 but not on XP... don't know about Vista and Windows 7 off the top of my head). I verified that Bob could not stop / start the Tomcat service (using ""NET STOP tomcat5""). I received the same error you report in your post.",1
"However, it sounds like you don't actually need multiple names in this scenario? It sounds like only one of those two names actually work in practice (the one that exists in DNS) and then it would probably be better to just clean up your configuration such that you consistently only use one of the names instead of having a confusing mix of both. I think such a cleanup appears much preferable compared to committing further to the confusion by adding both names in SAN. I have a cloudera managed Hadoop cluster where I had installed SSL certificates on all servers. Unfortunately, the host name on the servers are not the same as the one configured over dns. Obviously accessing server1.x.y from the browser gives identity error since it trusts only server1.x I was wondering if I could add server1.x.y in the Subject Alternative Name and would the browser start trusting the certificate? The purpose of the SAN extension is to have a (typed) list of subject names that are all considered valid. You could use that to add any additional names that you want clients to be able to use.",2
"This will allow you to make queries from serverb to servera using the credentials of logina and vice versa.  From there you just need to grant the rights to the user on server1 and serverb. The downside of this approach is that anyone with access to serverb will be able to change the data on servera that logina has access to and the other way around, but the configuration should be simpler than using component services.  If you limit the grants to just read this will not be an issue. I think the easiest way to handle this would be to set up 2 SQL logins (logina and loginb) on each server (servera and serverb) and create a linked server on each to the other.  On servera you change the properties of the linked server (serverb) and go to the security tab.  Change the radio button to ""be made using the login's security context"" and then put the un/pw for ""loginb"" in the ""remote login"" and ""with password"" options. On serverb you change the properties of the linked server (servera) and go to the security tab.  Change the radio button to ""be made using the login's security context"" and then put the un/pw for ""logina"" in the ""remote login"" and ""with password"" options.",1
"The ""built-in"" functionality in Windows XP that yoy're looking for is the Network Location Awareness service. Having said that, though, you'll need custom code to take advantage of it, because it's just an API and not a finished ""solution"". Can I get / how can I get these systems to alert me when they switch from wired to wifi networking?  You might have luck monitoring your Ethernet switch ports using a tool like Nagios, Zabbix, a syslog receiver, an SNMP trap receiver, etc, and acting on port ""down"" events with a script. If you're comfortable with scripting I suspect you could cobble something together fairly easily. You could use a ""ping probe"" tool to poll the wired NIC IP addresses on the client computers and report if they become unavailable. You would probably need to use DHCP reservations or static IP addresses on the clients if your probe utility couldn't handle probing DNS names.  You could even do something simpler, like monitor via ping traces and have a script that sets off alarms if it misses [x] pings in a row... but the thought of that being used in a medical enivronment chills my blood.  As doe the question, now that I think of it. I have a bunch of medical-grade all-in-one workstations in use in a high-demand medical environment that spend their lives plugged in to power and wired networking. The systems came with a 30 minute backup battery and a wifi card for redundancy. The idea was that they'd run on wired net, and if a cable got tripped over or unplugged, the workstation would switch over to wifi, and case documentation could continue. The catch with this plan is that we need to know WHEN that switch-over to wifi happens so we can go remedy the problem with the wires that triggered the switch in the first place. The systems run WinXP (a vendor requirement). We don't run them totally on wifi because it's an electrically noisy environment and wifi not 100% reliable - that, and throughput isn't quite as good over wifi. A more precise answer will require more detail, because there are several ways to go about it,but from the sounds of things you really need a moniroing system.  At a minimum, look into setting up an SNMP server which all your clients can report to and send an alert out over SNMP if the status of their wired connection changes.",3
"So, is it any alternative for ping that use TCP connection instead of ICMP and checks internet connection quality? Generally, if you are seeing such divergent results from ICMP and TCP responses, the issue is either an overloaded server or specific TCP shaping at a firewall along the way. It's a common task to check network 'quality' - latency, number of dropped packets etc. But 'ping' has a number of drawbacks: - It uses ICMP. Many ISP has different shapers for ICMP and TCP traffic, so 'ping' will show 10ms latency, but TCP connections will experience 1000ms+. That being said, you can ping with TCP, but there are several caveats. The first is to just send the initial packet in a TCP connection, which will elicit a response from a server with an open port, but will be seen as a connection attempt. Ideally you could use the ""echo"" service (TCP port 7) ... but actually you can't because it's now disabled by default everywhere. Anyway, if you can get someone to enable it for you on the machine you want to test, a program could use that to check the round-about time for packets inside a TCP connection.  TCP cannot ""tolerate"" 50% packet loss. It will simply grind to a halt, for a simple reason: it adapts its transmission speed based on the loss of packets. When packets are lost, they are understood to indicate congestion. If you drop 50% of packets (say, with a random drop firewall rule) regardless of traffic, it will see an ever decreasing bandwidth availability. Netcat Power Tools describes how to do TCP Ping with netcat. Specifically, every unsolicited ACK packet should return RST. - It sends very small amount of packets. By default, one packet every second. Since TCP protocol tolerates packets loss (it can operate very well is half packets are lost - it's normal), it's absolutely unclear if ping's ""30% packet loss"" killing connection or if it's absolutely normal. That being said, you probably have the ""tracepath"" command installed on your machine; it's similar to traceroute but uses neither TCP or ICMP, but UDP. For TCP there are various utilities out there, you can try hping. I'm personally a big fan of mtr ( http://www.bitwizard.nl/mtr/ ), mtr is an ncurses based traceroute clone which can work using both icmp and udp. It shows you the weak spots in your link to a certain host and is in that way non intrusive. Furthermore I doubt ISPs shape ICMP vs TCP. Some might do, as there are some really stupid people out there, but it doesn't make much sense to do so. Most will shape the whole connection, or it will ""shape itself"" because of congestion. In either cases packets are typically dropped randomly. ICMP packets are generally delivered slower (if there is a difference at all), because most networks deprioritise them, especially ping packets.",5
"The problem lies in the naming. ""Design Pattern"" is an extremely broad name, but when we talk of design patterns we typically mean on a smaller scale than architectural. When using the component-based paradigm in game development with engines like Unity, is component-based design an architectural pattern, or a design pattern?  So from my experience and interpretation of definitions, the only real difference is scope of the pattern. Meaning that realistically there is going to be overlap and with how wide-spread the use of the component pattern is in Unity it could easily be referred to as an architectural pattern, though it is traditionally a design pattern. MVC is an architectural pattern because it's a pattern for the entire presentation layer. MVC states that you have a controller that will pass some model (which holds the data) to a view so that it may render. It's a very broad method, that view could be Razor, aspx, php, etc. That controller could do who knows what. It simply states that the architectural layout will be MVC. On the other hand, the Strategy pattern (which we typically think of as a 'design pattern') usually applies to having multiple algorithms for one action (e.g. You might use the strategy pattern to determine how and/or where you will log information). This could be use with or without MVC as an architectural pattern. I see architectural patterns as being more high-level than design pattern. The component-based design in game development's context (like with Unity engine) seems to fit as an architectural pattern to me. However, on some sites, I read that component-based design is a behavioural pattern, much like other behavioural design patterns, and not so much like an architectural pattern like MVC.",2
"My manager asked what is ""best practice"" on this.  I think the answer is that there are arguments both ways and have detailed some to him, but I suspect there are other reasons that I haven't yet thought of. I'll create two answers; one for ""yes, let people do it"" and the other for ""no, no, no, this is a terrible idea"" and put in the reasons I can think of.  Can I ask other people to edit in any more arguments they can think of? There's more options than just different messages to internal vs. external. In Exchange 2007, you can set external replies on a per-user basis, as well, users can select to send external emails only to a set of contacts. As a global option, External replies should NOT be turned off. It's generally accepted for things like user accounts or building access that you use the principle of ""least privilege"". (ie. Bank tellers don't have the combination to the vault, because they don't need it) Your principles/philosophy on communications scenarios like this needs to be driven by your business.  Like you said, arguments for and against.  The ""identify an email address as being valid to spammers"" argument against can seem quite compelling, but these spam emails really should be caught by a filtering system at your gateway before they even get to the users mailbox.  Other arguments hanging off that tend to fall apart as a consequence. This is a question that I think you need to zoom out and think about the overall thought process/strategy to reach a satisfactory answer. There is an option in Exchange System Manager to prevent or allow Out of Office messages to be sent to external addresses.  Technically, this is pretty easy to configure - and Exchange 2007+ lets you set a different message for internal and external use - but the question I have today is whether it's a good idea. At my company, our parent corp disabled external replies, and our sales guys are not happy about it. They have to work around it by setting up a set of rules when they go away on vacation, so it's not nearly as well integrated (and the reasoning for it was ridiculous anyways, they didn't want people to know when they weren't going to be at the HOMES because they were on vacation). Conversely, there's some employees who have no need for external contacts, so we could turn OOO off for them for security reasons. But even then, we have a solid SPAM system, so there's so few that actually make it to our users. If you are the State Department, knowledge of staff travel schedules or presence may put people in danger of physical harm. If you are a real estate agent, keeping customers informed, setting expectations and providing alternate contacts for emergencies is probably more important.  I tend to see an OOO as being functionally similar to a read or delivery receipt in certain ways, so it may be useful to look at it from the perspective of how you handle those. We send OOO to external, but after antispam feature. It don't answer to lists (I am on Linux and vacation don't answer to lists, postmaster, ...). If a spam is received, we don't send the vacation to block the knowlegement of our emails.",5
"If you're planning on using your web server for VERY SMALL SCALE use ONLY, then you can do the following: Honestly, your best bet is to go someplace like godaddy and get a cheap hosting plan - they can run you as little as $5/month these days. Unless you have a real business reason to host it at home and the expertise necessary to administer a network at that level, spend the $10 or less a month.  You'll save yourself stomach lining and probably money in the long run. I did this for a class in which I needed to host a Rails application: The website hits the wild simply by turning on the computer, and gets pulled when I get home from class to minimize the amount of time in which it's exposed to the wild. Buy a domain and use a redirection service (I used no-ip.org) to take standard http requests (port 80) to your router on whatever port you choose. As posters above mentioned, though, you're likely violating the terms of service, but if you're just trying to share some small bits of content with friends or family and you take necessary precautions to protect your network from the wild, you can probably forward safely. free web hosting turns up a lot of answers on google, you could put your home server ip/allowed port in an iframe on one of these servers Third, there are a variety of free or very low cost services for you to host your own blog ranging from blogger to wordpress.com to livejournal for basic blogs to full fledged hosting services like Dreamhost (usually under $10 for the first year) to Grokthis.net ($6 month for fairly nice services). Second, are you really prepare for exposing your home network to incoming traffic?  Are you ready to manage all the security required?  Can you handle it when a security crack creates a bot network or site hosting illegal material in your home and your ISP and/or the law show up to discuss it?",4
"ClamAV can do the same but I think for simplicity and just scanning binaries I like how Bitdefender handles this, don't forget you have options on Linux or Unix, you can install both or just opt out and use which ever you think works best for you and your environment. Bitdefender has a Rescue Disk that boots into Knoppix and scans from there. You could either try ripping BitDefender from the disk, or just use that.. I tried this about a month ago with both clamav and AVG.  I found AVG to be a lot faster, with less false positives.  The downside to both of them is that they don't, as far as I know, clean the registry.  I gather that it is possible to use a WinPE boot disk to do registry cleaning of an offline Windows install using a normal Windows virus scanning app.  There are also various linux tools available to manually hack the Windows registry, but I'm not sure how you would determine what to look for and what to do with it, or if you might risk missing viruses that exist entirely in the registry. Easy to install on Linux partition or Unix partition and easy to scan your mounted C: drive or other media, in addition to other binaries.",3
"I had originally planned for the swap space to just be three separate partitions (one per drive, let the kernel do the striping), but then I had read that having the swap as a second RAID1 can be better for the kernel when losing a drive (and as a bonus, the 12.04 example preseed covers this exact scenario). Unfortunately, documentation on the partman preseed options seems a little hazy. Is there an obvious mistake in the above configuration? Thanks for any help. At various points in my fiddling, I had it running all the way through successfully, except that the swap partitions were 1TB in size (way too big). At other times, it would work successfully except with no swap partitions created at all. With the current iteration (above), it runs, but gives errors about partitions being in use, or being otherwise unable to proceed. I have tried using dd to zero out the first 512 bytes of each drive, but it doesn't seem to make a difference. I have three 2TB drives in the machine; I want a RAID5 of ~4TB capacity, and 100GB or so of swap space. No LVM, no encryption, no fanciness. This is easy to do using the interactive installer, but I'm hoping to basically preseed this whole setup.",1
"Most files will be consolidated into one block each. That's the minimum requirement and definition of ""defragmenter"". More advanced defragmenters will move directories first (since they are, on a per-megabyte average, accessed more frequently than ordinary files), followed by files accessed recently. Files which have not been accessed for a long time (typically 90 days) are sometimes actively placed closer to the end, to allow for shorter seeks for files which do get accessed. The ""seek"" times on SSDs are minimal, on the order of microseconds. You're looking at the wrong places if you defrag SSDs for seeks. (An intelligent SSD controller could merely update WHERE the data are moved, rather than moving them, each time it detects the ""read old data + wite same data + TRIM old data"" pattern, economizing on both write cycles AND defrag runtime. Not sure if any controller does that yet.) NTFs seems to fragment only slightly less than FAT but to degrade more slowly if it is. Some of the important file are not at the front of the partition but near the middle, obviously to cut the worst case access time in half. Not sure about it, though. On a second thought, yes, SSDs distribute writes across chips on purpose, because that's how they are faster, and heavy fragmentation might slow the write patterns by making the writes LESS scattered. That might be a factor to consider if it's true. Always, and regularly, do your backups. If you defrag disks for ease of data recovery, YOU'RE DOING IT WRONG!",1
"I think someone experienced will immediately recognize the problem I mean, if not, when I return home soon, I will shoot a little youtube video demonstrating the issue! When from walking from some direction, to attacking, it starts ""jumping"", appearing glitchy, because it's not staying in the same correct position, only doing so for the right attacking sprite, due to the drawing being made from the lower left part of the rectangle. Another speculation would be that there might be some sort of method in LibGdx, the library I'm using, that allows me to change the drawing center (which I looked for and didn't find), so I could choose from where the drawing starts. It all works fine, except when the sprites are all similarly sized, but when a sprite changes from a small sprite into a big one, such as here: So the question is: what possible solutions are there? I've thought that some sort of individual frame ""offset"" system might be the answer, or perhaps splitting, in this case, the sprite in 2: the sword, and the character itself, and draw sword according to character's facing, but that might be overly complex. Thing is, I've used darkFunction Editor as a way to get all the spriteCoordinates off a spriteSheet for each individual sprite, and parse the .xml it generates inside my game.",1
"The choice of using spreadsheets to teach programming itself is kind of baffling for me. I would rather you use of the several online ways to teach coding, which are much more useful.  If these are first years, and I was a first year, I don't want to see Excel sheets. Spreadsheets make command line look like an attractive neighbour.  My personal experience of using Excel as catalyst was really good one and I think using an user-friendly/easier IDE like Visual Studio with Excel is worth a try If online is not an option and you want something simple, a straight forward combination of Visual Studio Code and GCC Compiler (which is probably already there on the machine) would be a good option.  Using Excel alone can make students thinking in terms of sheets/tables which can make them picking the concepts of arrays,etc. pretty tough later on and even if you go in some depth of Excel functions, still I don't feel its worth an entire course (even to learn programming).  I got a chance to teach Introduction to Computing & Programming course to Chemical Engineering entry students and their syllabus included both Programming concepts & Excel. I began the course with just Algorithms and after 4 weeks, switched to Excel & then on programming. Off course they are different to your target audience a bit but I found Excel('s functions) pretty helpful for them to comprehend the syntax (after a couple of weeks of Excel, I switched to Visual C++), so the pros you mentioned are really there.",2
"Hi check whether your disk is full in the installation partition. And if the storage exists manage the logs in the Mariadb server adding following lines to my.conf  This means that MariaDB is not running, or was configured to listen to some other socket (or none at all). I had the same problem and found out a few things after digging around.  MariaDB is a drop-in replacement for mysql.  On the new system, mysql is the MariaDB client (although I'm not clear on what that means).  Checking to see if the service was running: Alternatively, change the parameter for the program and MySQL to use the file socket instead.  Whatever works easiest. Based on this comment, and the error message, I'd say you have a problem with the type of connection.  The socket in the error message is stated as /var/lib/mysql/mysql.sock but you are attempting a network connect via localhost, which maps to the network address 127.0.0.1.  Your program looks like it isn't using the correct parameter at runtime.  Double-check that the parameter really is 'localhost' at runtime by having it log/write out the parameter somewhere, and double-check that MySQL is attached to network port 3306, instead of a file socket at /var/lib/mysql/mysql.sock. You could also verify that Mariadb is still running on the server when you get this error message and you could verify if you are still able to log in (in case it is running). The first step would be to look into the logfile of Mariadb, which is typically located in /var/lib/mysql and named *.err.",5
"SO, based on the fact that I got the Nvidia to work (although very poorly) with the new HIS Radeon HD 4670 PCI-e I know it's possible, just don't know what type of card I need to use. Any ideas? Are you sure though that the onboard graphics have to be disabled and that there's not a BIOS setting that would allow you to use both the onboard graphics and a PCI-e card? Either way you are going to have fun with this one, best for multiple mointors would be to have a motherboard with dual pci-x (or triple in some cases) Nvidia not ATI) it wouldn't run at more than 640x480 4bit color when installed simultaneously with the HIS card. Previously I used the Nvidia with the onboard Intel video to run a third monitor, but now that the HSI uses PCI-e the onboard has to be disabled. I tried a Radeon 9200 PCI card that I found (microsoft made I think) with it and only that or the HIS card could be enabled at the same time. When both were in the HIS card said ""The device cannot start"". I have a Dell Vostro 200 with an HIS Radeon HD 4670 video card in the PCI-e slot. I need an additional video card that will work in an available PCI slot. I Had a Nvidia GeForce 5200 that was installed but due to what I'm assuming was a compatible driver issue (because it was All I am trying to do is run a three monitor setup, two widescreens on the good card at high res and a third smaller monitor at 1024x768 that I watch continuously updating info in. http://www.newegg.com/Product/ProductList.aspx?Submit=ENE&N=50001126+1573838207&QksAutoSuggestion=&ShowDeactivatedMark=False&Configurator=&Subcategory=-1&description=PCI+video&Ntk=&CFG=&SpeTabStoreType=&srchInDesc=",3
"If you have single load balancer, it wouldn't help too much in terms of high availability, you would still have a single point of failure. CDN  used to be a way to go, but you might want to wait for few more answers... [4] Maybe in you have to reestablish a new ssl connection after the beckend if you don't trust the backend network [3] In fact the loadbalancer must do this if he doesnt want to route the traffic blindly by connection. Everything what have been posted is fine and correct. However I can see a different approach to reach the described goal (fallback system hosted by a third party hosting company):  [1] Of course the loadbalancer needs more bandwidth than the sum of the maximum bandwidth of the backend servers (this is not necessarily the maximum bandwidth of the interface depending on the application) The loadbalancer stands before all backend servers ad proxies all request to the backend servers. A secondary loadbalancer stay online as well and checks if the primary balancer is reachable. If in any case the primary loadbalancer will go down, the secondary loadbalancer will take over his IP and start serving. [5] You have to make sure that when the primary goes down, it stays down until you want it up again to prevent things like toggeling and IPs used twice It all depends a bit on the situation. A standard way of achieving higher availability is indeed the loadbalancer. The problem is that you need at least two loadbalancers I have two VPS servers from two different hosting companies. I want to make some fallback to second VPS if first is not available (preferably automatic). I understand that if I have a single load-balancer with root access I can setup it to switch between different backend servers. But is it possible to make it with VPS from two different hosting companies? Even if you have a load balancer in a third location (you can configure them in HA active-standby) you're still prone to network issues. Since your two VPS are presumably located in two different datacenters you might end with bad network performance, as your loadbalancer would contact the VPS, make the requests and forward responses to the clients, thus if the datacenter containing the balancer goes offline or experiences network issues your  2 VPS are useless. With this method the DNS provider checks the availability of your main system and switches to a fallback system in case the checked system doesn't respond properly. But - this is not high-availability! In case of an outage it needs some time to recognize the outage and adjust the DNS. But maybe it's already enough for your requirements. [2] This is not only good. If both backends are at or near their maximum capacity and the loadbalancer takes one down, the other backend  will get all the traffic and the whole thing ill fall apart. This requires good tweaking There are quite a lot of suppliers on the market providing this type of service (Amazon Route 53, DNS Made Easy ...). Additionally you will get a faster DNS system (world-wide spread, any-cast listening DNS servers).",5
"I think Bluehost uses Plesk/cPanel.  On these systems, local email delivery is enabled by default.   For email you will need MX records setup on the DNS server and they will need to point to an SMTP server to accept email for your domain. On systems like Plesk, you can simple disable the email service entirely (it does not impact the mail() function).  On cPanel, there's an option for using MX records instead of local delivery.  If your host controls these, you may need to explain, that you do not want local email services enabled on your account. As a result, email from your web form to your own web site domain will fail.   Email to external domains will work.   What is happening is the web form email is being delivered locally since MX records are not consulted. Basically, I have a PHP based contact form that utilizes mail(). Whenever a contact request is sent through with the recipient being an email address at the domain hosted with Go Daddy, the email never makes it through. However, when I have it go to a different email address, such as @gmail, the email is delivered without any issue. With most MTA's, if the email domain is listed in the MTA's ""local"" delivery list, then DNS is never consulted.",3
"It seems to me that LPT1 has a different behavior now than was before. Is it possible to change this in Windows? I've checked BIOS for parallel port settings: The parallel port is set to EPP+ECP (but also tried the other two options: Bidirectional, Output only). Maybe some kind of parallel port buffer is too small? How can I increase it? After changing the client's computer (but not the printer, or the Java program), the program does not finish the task till the card is ready, it is blocked until the last second. I have a legacy Java program which handles a special card printer by sending binary data to the LPT1 port (no printer driver is involved, the Java program creates the binary stream). Here's a bit of a long shot (untested of course). Install a generic printer driver for the printer on LPT1. Then you should be able to configure the spool settings for the printer. Windows won't know that you're not using that particular driver but should obey the spool settings anyway. if yes, you could create a new thread and let it finish it's job in the background so that the ui becomes responsive again. Is the Print Spooler service started?  It's a long shot I know, but this is exactly the kind of symptom you would get if not spooling. The program was working correctly with the client's old computer. The Java program sent all the bytes to the printer and after sending the last byte the program was not blocked. It took an other minute to finish the card printing, but the user was able to continue the work with the program. Another thing to try if possible: Right click the Java program, go to the Compatibility tab, and set the Compatibility mode to Windows 98/ME, or Windows 95.  You might also try the NT4.0 and 2000 modes.",5
"If you have the DRAC tools installed, it'll reset your DRAC to factory defaults. If it fails, you're stuck rebooting the server. I make it a habit now to install OpenManage on all servers immediately after installing ESX as well as setting up the iDrac, just in case! I know I'm resurrecting the dead here, but here is an ""half-answer"" for anyone who stumbles across this question.  (I know you've already sorted your issue, and no one wants to reboot their servers if they can help it!) ...as it does require a reboot of ESX after install, at least if you pre-install it, and if you have some unexpected downtime, when the server comes back up you can have all the access you want, as well as change the iDrac username/password. (I'm not going to re-post the entire page contents - suffice to say if anyone comments on this that the link is broken, I'll update the link) For one of our machines we lost our root password for the iDrac7 Enterprise interface. The machine is running many virtual machines and turning them off would be a huge task. For anyone who would be searching for solution. The tool you want to install on esx is called ""Dell EMC iDRAC Tools for VMware""  So, my question is: Is there a way to reset the iDrac Password without rebooting the machine? It's running VMware ESXi 5.5 and I can connect to it via vSphere client as well as SSH. Here are instructions from Dell, and supports ESX 5.5 to 6.5 - steps 1-6 tell you all you need to know to install OpenManage on ESX, just ignore the last step. Solution:  push and hold down the glowing blue button labelled ""i"" on the back of the server for 15 seconds...",5
"I don't know about AD domains, but samba domains only use DNS for dynamically mapping windows workstation hostnames from NetBIOS lookups to DNS, so non-windows workstations can reach them by name. This is by no means compulsory, though. There are some ancient clients that don't support using DNS.  But most SMB clients that do not support DNS also do not support TCP/IP. Netbios or WINS is not required to use Samba. If you disable netbios almost all SMB clients will attempt to use DNS resolution. I am looking into setting up a Samba domain controller using Ubuntu Server, for some Windows XP/7 clients, and I have one important question: Given the importance of DNS in a Microsoft Active Directory infrastructure, why do none of the setup guides mention configuring DNS to support a Samba domain? Netbios or WINS resolution is far more commonly used with Samba and DNS is optional resolution method depending on how you have configured your clients. I have installed an LDAP/Samba server in a test network that I will be attempting to join using an XP client very soon, but I'm just confused as to how the client will actually ""discover"" the domain - as I know this is how AD domains work.",3
"You'll have to put them all in one horizontal row and each frame will need to be the same width and height as all the others. Ideally frames should be square, but you can specify rectangular frames. Then you can use frame = value where value is the index of the frame you want to use for static images. I'm using flixel and flixer power tools which contains a FlxButtonPlus class that allows me to load 2 FlxSprite as each button state, the problem is that FlxSprite requires a Class a its source but I would like to have a single spritesheet for all my buttons and call each one from there, rather than having a bunch of pngs for every button and every state. You can also do addAnimation(""name"", [1, 2, 3], 29) to create named animations that you can play back with play(""name""). That said, it is probably a good idea to still do separate sprite sheets for animated sprites. You can have one (or a few) PNG sprite sheet and use it for multiple different sprites. You'll need to create the sprite object and use loadGraphic(graphic,true) to load the sprite sheet. ""animated"" must be true; if it's false the entire image will be one static frame. ""graphic"" is the embedded PNG of type Class",2
"@AliChen seems to be on the right path, actually this is a good line of thinking I hadn't considered. I never checked the motherboard specs when I posted my comment. This board uses a NEC (now Renesas) D720200F1 USB chipset for it's USB 3.0 ports, which seems to need a firmware update for full compatibility with published USB 3.0 standards, more information is available here. There is also information on TechPowerUp forums on more updated firmware information for this chipset as well. Renesas does not offer the firmware update directly from what I can see, so it might take a little trial and error. Gigabyte seems to have no official update on their site that I could find either and this firmware is not part of your BIOS.  USB-3 ports are working properly when any USB-3 devices are plugged in e.g. USB-3 Pen drive. However I notice that when it comes to USB-2 devices only my usual USB-2 pen drives & Pointing device like Mouse are detected when inserted in those USB-3 ports. Are there any design requirements or constraints that result into certain USB-2 devices just not working when connected to USB-3 ports? On my Gigabyte 880GM-USB-3 motherboard there are 2 USB-3 ports. Drivers are installed successfully in Windows 7 OS. However the other answer of replacing the motherboard should be considered given the age and the relatively cheap availability of AM3/AM3+ motherboards that are comparable or superior to your existing board.  Remember that anytime you mess with firmware, especially in older hardware like this, there is a potential of something to fail or go wrong. Just giving you fair warning, do with this information what you will.  Should I leave it here as 'behavior by design' or I need to troubleshoot further why these devices don't work when connected to USB-3? My Android KitKat OS Phones internal SD card is not detected at USB-3 port when phone is attached by its USB cable and also my TP Link TL-WN721N USB WiFi adapter is also not detected at USB-3 port. Such devices work just fine when connected to USB-2.",2
"It won't tell you how much %age disk bandwidth you're using, since it doesn't know how much bandwidth your disk has.  In any case, your disk only has the manufacturer's quoted figure for large transfers of contiguous data.   See https://www.kernel.org/doc/Documentation/iostats.txt and https://www.kernel.org/doc/Documentation/block/stat.txt for documentation. As suggested by sastanin in comments, you can process directly values given in /sys/block/sda/stat or /proc/diskstats.  This may be helpful when none of the other mentioned tools are available and you can't install them easily. I would recommend taking a look at the nmon tool. It will show you live load on a number of system parameters as well as recording data to a file for later perusal. It's a free tool available here: I think RRDtool should do what you want here it uses a daemon to dump system data and then allows you to process it however you like. I have often used it to produce graphs etc. to measure system load. You should take a look at atop, which combines the power of iotop/top/iftop, all in one place, and highlights the critical parts on your system.",5
"To know if jrunsvc.exe is restarting the child jrun.exe, check your logs.  To demonstrate, on a test machine, try killing jrun.exe with task manager and you'll see a new jrun.exe starts up right away. If your ColdFusion Application service is crashing regularly, look at the JVM settings, especially those for minimum and maximum memory.  The service will need to be allocated enough memory to handle your application, but not so much that performance is degraded by the Java garbage collector.  For larger ColdFusion applications, the JVM should be fine-tuned by someone with experience with that task, as it can really enhance the performance of your application under load. It looks like I had to restart the Cold Fusion Windows Service and that solved the problem.  Even though the service doesn't seem to keep the application alive if it fails, it does start it up on start up of the service. When you start ColdFusion (installed as ""Server Configuration"") via the Windows service you will see two related processes.  One is jrun.exe, which is the actual ColdFusion server running in a JRun instance.  The other is jrunsvc.exe which is a parent process whose only purpose is to monitor the jrun.exe process and restart it if that child process exits. The ColdFusion MX 7 Application Server service is started from the Windows Services control panel.  If the application service fails, none of the ColdFusion pages will run.",3
"2) you can switch back your DNS servers to 123reg and setup the MX record to point to your 1&1 mail server and allow, through your TXT/SPF record the Amazon SES server to send e-mails on your behalf. 1) you may try to install and configure the DNS server on your VPS host, and setting up manually your zones will give you the flexibility to setup any record you may need, such as TXT/SPF records. Plan this switching when your site is least busy, and try to lower the TTL value for a quick update. My question is ( and my apologies for my lack of DNS knowledge ) can change the name servers  back to 123reg and then create A/CNAME/MX records to point to 1and1 for my mail and webhosting ? If you go to you domain provider they can do that for you I believe. I just had to do something of the same sort. In my opinion, any domain registrar which doesn't provide you with the ability to add TXT/SPF records is not worth it. Switch to the one which will provide you with as many features of dns as possible, and the ease of modifying your domains on your own without contacting their support or not being told it is not possible.It should not be difficult to change your name servers back to 123reg, and replicate all the dns records you have in 1&1 there (A/CNAME/MX as well as TXT/SPF....).  I have a VPS hosted with 1&1 which also provides mailboxes that we use for day to day e-mailing. Our domain is registered with 123reg and the name servers point to 1&1 .",4
"I have a laptop, a TV which supports LAN connections, and a WiFi phone on my network. The Internet connection is provided from a USB WiMAX modem. [3] You probably ought to make sure you can do this before going through with steps 13. Some routers don't let you do this, I think. To be clear, I want to provide internet from Local Are Connection 2 to router, via Local Are Connection or Wireless Network Connection.  Tried network bridge, it doesn't help. Also, when laptop connects to second - router network, can't access to router by 192.168.1.1 ip. When laptop connects via ethernet cable, can't acces to router and internet Set up Internet Connection Sharing. In Vista (I'm assuming the procedure hasn't changed too much 7), you need to: [1] Also, keep in mind that the less WiFi and the more Ethernet you use, the faster your connection will be. Your access may be fast enough that it doesn't matter, but I know that mine isn't. Until last week I was getting the Internet to each device through the laptop: I configured WiFi ad-hoc between the laptop and the phone, and a cabled LAN connection between the TV and laptop. I then shared the WiMAX connection with the ad-hoc network. It worked fine for me. Recently I've bought a Zyxel P660HTW2 EE ADSL2 modem + router. I now want to configure the router for getting the Internet from the laptop (not from the phone line) via WiFi or an Ethernet connection, then share it to other devices via WiFi or Ethernet.",2
"The problem you saw is caused by some applications (particularly P2P software) uploads a lot. All Internet traffic, like HTTP, will send something outside your internal network. At least, ACK packets have to be sent to confirm your computer has already received the packets sent by remote server. And those applications will block your upload link and delay the packet you sent to external network. I am using college internet. It is shared to the whole college. My problem is that i can't even browse at sometimes because the network become too busy. But I noticed something that if I download something in IDM or axel in Ubuntu it works. When I download, I can browse also. It feels like when I start a download other applications can communicate through the busy network. Is there any way to keep the connection speed constant. without a download.  As for why you are seeing what you are seeing, there is no single rule and it depends on the networking equipment you use, but basically, when under load, the majority of networking equipment simply only delivers a few percentage of the total amount of traffic it should do - whilst UDP connections (voice/skype/other) will suffer, through the magic of TCP, it will keep resending and you will get the data in the end - you just see a lot of lag. In addition, as for why other stuff appears to be faster - all I can think of is that the equipment they are using prioritises traffic based on where the most connections already are. By hammering the connection, your machine will be sending a lot more packets and will get a lot more back - e.g. 2% of 1,000,000 packets is a lot more than 2% of 500 packets... (just non technical/random example).... which is why a download manager such as IDM which specifically opens multiple connections can give you higher speeds. LARTC has already publish some scripts to achieve this. You can try to install them on your gateway. If you have an ssh service, it should affect that much worse than web browsing.  (Some universities offer ssh shell access as an optional service to all students).  That might be something useful to point out, or to use as a demonstration of the problem. BTW: Maybe you need to buy a router which run a Linux operating system. If you can access some Chinese shopping websites, try to get a DB-120. Very cheap and efficient. If you haven't already, you could mention this to your technical support.  They won't necessarily know about everything that happens automatically.  Especially if it's only a problem at very busy times. By the way that standard network communication works, without QOS or similar technologies, nothing is guaranteed and it is simply down to best effort. I've seen this with wireless networks sometimes, where leaving ping or arping running can help keep the connection stable.  It can also be useful because you can switch to it and see if there's a problem - high latency or dropped packets.  That might be a less abusive way to get the same effect as a running download.",4
"I always thought processor's FSB was the limit of how fast the RAM could be. However, while configuring a Lenovo laptop, I see options for RAM that's faster than the CPU's FSB In short, no, some benchmarks may reveal a small advantage, but its nothing you would notice in the real world. Running your memory at 1333MHz instead of 1066MHz you will likely not see any difference. All the system will do is use a divider to work out the memory speed compared to the base clock of the CPU. (I don't think the base clock of your CPU is 1066MHz, it will be 2.66GHz divided by the multiplier). Both CPU and memory frequencies are calculated using the base clock. The manufacturer may have installed 1333MHz instead of 1066MHz simply because it was cheaper. In your particular case there will be a performance increase from 1066MHz to 1333MHz, but it is unlikely that you will notice it. Back in the day the FSB speed did limit RAM speed.  In that kind of setup the RAM is said to be ""chained"" to the FSB.  These days most chipsets allow RAM to be ""unchained"".",4
"First of all there're two loader modes in my BIOS. They are Legacy mode and EFI mode. Default one is EFI. What I tried to do is to boot from flash drive with legacy loader while I had EFI mode set in my BIOS. If you need to do this with Sony VAIO SVS1511V9RB, you should rapidly press F11 (about 1.5 minute) until notebook starts boot up from external device. But finally as far as my Debian distribution had not very stable EFI loader (or hadn't it at all, don't remember), I decided to change loading mode in BIOS and install OS in legacy mode. Now it works fine. I bought Sony VAIO SVS1511V9RB today and can't boot it from neither USB flash drive nor bootable CD. I've changed priority of boot drives to Internal CD-ROM, External devices, Internal Hard Disk, Network, and enabled external device boot. But after this only Windows was loaded. Also I tried to press F8, F9, F11 and F12, but also without success, Windows loaded from hard drive instead. I checked that my desktop booted from CD. Notebook can read CD in Windows, and it tries to read it while booting but don't print any messages and just load Windows. BIOS is Insyde H2O, version R0140C5. In summery if you don't know that EFI is and you're going to install Linux, probably you'd better switch to Legacy mode using BIOS.",1
"Leave him be, you have no control. He will soon enough be listed in RBL, after that no email would be accepted from remote server when your client will email, your client might change his view then You can ask their admin if they have SPF record, has the response back can be an answer, but not from a email sent by them if SPF record are not set. PS: I'm not the network administrator of this company (who thinks just like the customer!) just the website and email service provider. I have an important (+10 outlook accounts) but very ignorant client who refuses to accept that there is malware on the company's computers. Malware that steals Outlook data to send and receive spam. The situation went on a limit, due to the stubbornness of the client and with that I decided to raise the anti-spam filter. That started to make the customer more satisfied because he no longer receives return messages associated with the malware (lots of Chinese messages). It is not a sustainable situation because normal messages are being filtered.",2
"The license will probably work, but you'll be violating the license.  (At least, this would be true for past versions--I haven't read the Windows 7 license.)  You agree to use a consumer Windows license only once, on either a physical or virtual platform, when you buy. The VM instance of Windows 7 doesn't ""know"" that it's on a virtual machine. And it certainly doesn't know that it's on the same physical computer as any native installation you may make. So I would expect that you will be unable to activate the native installation of Windows, because your product key is already bound to the hardware configuration of the VM in Microsoft's database. I personally did this for years but then found that I could just virtualize Windows permanently via VirtualBox and then use RDP to remote into it as needed.  You lose the seamless integration but that wasn't a requirement for me. If you're goal is to have TWO DIFFERENT Windows installations, you will need to buy a second license.  However, if you are trying to have an installation of Windows you can boot to and use virtually, I you can install Windows via Boot Camp and then use VMWare's Fusion to boot the Boot Camp partition. You'll be able to install it without any trouble, but without activation, the anti-piracy measures will kick in after 30 days or however long the grace period is. Now -- I'd like to also have a native installation of Windows 7 on the same computer.  I can do the installation fine, but I'm wondering if the same license will work.  Is it going to validate OK when I activate it?  Will it ""invalidate"" the VM instance?   I run OS X as my usual operating system.  I've purchased a copy of Windows 7 which I've installed as a guest in VMware Fusion.   If I remember right, you need to buy the Professional version of Windows to use it virtually at all (again, according to the license--other versions will work fine).",4
"This way, you will see your ISP router's external IP (2.3.4.5) as opposed to your router's external interface address (10.0.0.2). I'd assume you are talking about a network like this, and you want to know the ip represened by 1.2.3.4 in the below graph: The method suggested above by others (to use a website to see your own ip address) had the advantage of being easy to use and simple, but this suffers when this network configuration occurs (NAT by ISP), where no public IP address was allocated to you: Of course. You have an external IP address on the other side of your NAT + Router device. The simplest way is to actually look at your router's configuration or status page, which will display it. Another trick is to ask someone else on the internet what your IP address is - there are numerous websites which will show you what IP is accessing them. Note that this may not be the IP address on the external interface of your router, if your ISP is performing carrier level NAT or uses transparent proxy servers. Does my wireless router have an ip address that's independent of my computer's ip address?  Is there any way to determine what that ip address is?  Note: I'm not talking about the internal router ip address: 192.168.0.1, but one that is visible from the outside. The simplest method is to log onto the router and see the interface address (which I assume you should have the credential to do so. This is by far the best method.",3
"I looked into apache, lighttpd, nginx, and cherokee, but I am not sure which one would be my best option as a load balancer. I know your question specifically pertains to load balancing web servers, but I can't help but mention Squid for load balancing. I assume that you have multiple servers since you discuss load balancing, so I would highly advise you look into dedicating a box to Squid. It even has a neat round robin function to load balance evenly across your servers. Placing a group of servers running something like nginx behind a squid box would be a pretty powerful solution... If you don't want to use Apache, Nginx or Cherokee. Cherokee is relatively new but looks promising, and Nginx is proven. I was wondering which is the best webserver that would be able to server static content with the highest performance, and also is able to perform load balancing. I can't judge your's but there are more places where things go ugly because of missing knowledge than suboptimal choice of static file server. Their core function is to loadbalance and serve static files with the highest performance possibly across different geographic locations. Here's another option for you - since its just static files, have you considered using a content delivery network (CDN) like Amazon CloudFront? As others have mentioned, its not advisable to use a webserver as a load balancer when you could use a load balancer as a load balancer :)",5
"Notebook is a more complete experience, formatting numberd lists and sublists is easier. As well as more eye candy, stickies and post-it flags, and animations.  Notebook also provides a service to clip from within any application and add it to your Notebook document. I use it for all my note taking stuff, I find it fast and responsive. It also will intergrate nicely with a lot of other applications, such as Cyberduck which I use for FTP. Maybe what you are looking for is a note keeper/organizer where the notes are easily searchable.   Something like Circus Ponies' Notebook or Voodoo Pad.  This way you don't have to save a new document each time you want to add a note, web link, image, or sound clip.  I leave VoodooPad Pro running all the time. myTexts is an application that I've been meaning to try. If other applications are too complicated, this just might do the trick - it looks promising! I'd recommend Smultron. As a plain text editor it's pretty good for replacing TextEdit. Offers a good replacement if you're not looking for anything fancy. Once you get to know some of the features in TextMate, it definitely has some time savers! I used it for note taking all last year, and it worked like a charm. Voodoo Pad Lite's price is right and provides most of the functionality of the regular version. VoodooPad has a higher geek factor, you can embed scripts in real languages (Python, Lua) and execute them from within Voodoo Pad.  The Pro version includes whole document encryption and a built it web server.",4
"I created an algorithm to procedurally generate 2d worlds based on a heightmap grid.  (I used a 2d simplex noise algorithm mapped on the inverse of the distance of each pixel from the center, to ""center"" the island and keep it from running off the screen, if you're interested). See below. Now, I want to create natural-looking rivers which flow ""downhill"" from random points or artificial lakes to the ocean.  I would like the rivers to generally travel downhill, but since you will only ever see then in a 2d view in a small window, they don't have to be 100% realistic.  Its ok if they flow uphill over small bumps. The naive approach of just checking all adjacent squares for one of lower elevation, then moving there gets stuck very easily in small holes/valleys. My next was to use an A* star algorithm to find paths from the start point to random points random points in the ocean, using the negative difference in height between too squares as the move cost.  This also seemed to get hung up and randomly fails a lot.  It's probably just an issue with my code, but I can't find the bug and its also really expensive, so I was wondering if there was any conventional wisdom about how to do this. I don't know how it will work, but couldn't you reverse the process, going from the sea and taking a direction where elevation is same or higher.",2
"I have mapped the drive to the same DFS namespace and the speed is fine this way, I've also tested by redirecting the folder redirection to the share name instead of the DFS namespace and this is also fine. So it would seem the combination of having Folder Redirection to DFS Namespace is causing the slowness. http://www.networksteve.com/windows/topic.php/Windows_7_Folder_Redirection_to_DFS_slow_when_browsing/?TopicId=47217&Posts=0 Just start over again all from the console and make a schema in the paper of what are you doing before implementing this is really helpful. During low network latency, we have faced slowness, we didn't observe it clearly at first. But when we look at the logs, we have found that the problem is the latency. http://blogs.technet.com/b/josebda/archive/2009/07/15/five-ways-to-check-your-dfs-namespaces-dfs-n-configuration-with-the-dfsdiag-exe-tool.aspx Having a strange issue at the moment whereby users redirected folder Documents is really slow (locally) even browsing through folders can take several seconds just to populate the list of files. Currently performing a project to migrate all users from mapped drive home folders to redirected folders (Documents). the do the test from any computer in the domain if the resource is accesible using the \domain.com\namespace\resource then do the replication by adding the other resource and accepting the options.",3
"You say there's an onboard video aswell? If the ati card does actually cause trouble consider just taking it out for the first few weeks till you get a bit more comfortable with the system, and then try working out how to fix it. The first two don't apply to you, and the third probably doesn't either. If the ati card you have is as old as the rest of the computer it should work fine with the default free drivers (in fact, it may not even be supported by fglrx) These days ubuntu is definatly the way to go for starting with linux, it has the best support community for new users, and some of the best hardware support (ubuntu aren't too fussy about including stuff that other distros wouldn't consider free enough) You don't mention how much ram you have, but the machine doesn't sound that powerful, so I'd consider going with xubuntu - this has all the same stuff as ubuntu, but uses the xfce desktop instead of gnome, which is a little less resource heavy. As a general rule of thumb, if you get a computer that's a few years old with fairly standard hardware it should ""just work"" out of the box with any of the major linux distros.",1
"We are all on PCs with Windows 7 Pro, also running AVG 2012 with Windows Firewall turned off (not sure if the software firewalls matter here). I installed NetExtender on my workstation, which is inside the LAN that a remote user would be trying to connect to.  Should I try from outside the network?  I know that will be the application in the end but can I not test accessibility from inside the network? The Server IP I'm using is our public IP, which is a static IP.  The username is the one I set up in the Sonicwall, with the password.  Should I be entering in something else for the Server IP?  Leave port number off?  Any help or clarifying questions are appreciated, thank you. We are a small office that has a 2-line bonded ADSL connection with our ISP (static IP).  The modem is bridged to our Sonicwall TZ 105 hardware firewall.  Off and on I have been trying to figure out how to set up a VPN connection so my boss can access our server remotely.  I don't have access to his network hardware (it's going to be just a standard modem from AT&T or Comcast or whoever).  What I can do is have him bring in his laptop and install the NetExtender client which should allow him to connect with SSL VPN. I'll preface this question by saying that I have made an effort to research this and I have been learning more about networking etc. as I broaden my IT experience, but VPN configuration still has gray areas for me.  Anyway... I Tried to configure the SSL VPN using this video, pretty much everything I did mimics what this guy does. You are correct in your assumption that trying to connect while inside the LAN is likely not going to work. There are relatively few of these multi-purpose prosumer appliances that support hairpin NAT, which is required for this to work. I click ""Connect"" and the window says ""Verifying User"" for a few seconds before giving me the error.  I don't know where I screwed up or did not put something in correctly, or if it's just the fact that I can't do this from inside the network.",2
"The thing is, though, that the plate ratings on equipment are generally the maximum rating.  In practice most equipment never even approaches this number - so those 300W servers might pull, say, 200W momentarily at startup when drives are spinning up and such and then drop back to 80W when fans slow down, speedstep (or equivalent) kicks in, etc.  For a modular network device the number may assume a full complement of ports all running optics with the highest power demand - which is also unrealistic. In researching PDU's I have noticed most cite a derated current at the 80% level. For example, 30A is derated to 24A: That said, there's a certain art to estimating how much capacity you're actually using (short of a clamp probe or similar).  At first blush most folks add up the wattage of each piece of gear being plugged into a circuit and compare it to the 80% value... So 8 servers with 300W power supplies = 2400W = 20A @ 120V which, of course, is a no-go unless you've wired in a 30A circuit.   So... if you want to be extremely conservative (not a terrible thing) add up the plate ratings until you get to 80%.  Some electricians make recommendations around dropping 30% off of the plate rating of equipment, but I've always suspected they were working from experience with motors and such rather than electronic equipment.  This is probably fine and could save you some pole capacity but ultimately you shouldn't take anyone's word for it.  Measure the consumption of actual gear under something like actual conditions and then plan accordingly.   Remember: Being careful will cost you an extra circuit breaker, or two.  Not being careful can cost a whole lot more. Various vendors will publish so-called ""typical"" power draws, but these are often marketing numbers (...a quick way to appear more efficient).  Other vendors may be a lot more conservative and publish a more realistic estimate.  The only way to truly know what's going on is to hook up some kind of meter to get an actual empirical view of the power being drawn.  Sources might include something as simple as a Kill-A-Watt to clamp on probes to smart PDU's providing graphed measurement on a per-socket basis.   The 80% rule is there for a reason.  The best case overloading a circuit is going to yield circuit breakers popping, the worst a fire.   Does anyone have anything that shows what agency this is (NEMA?) and these actual regulations? Also, does anyone have any advice, based on actual experience, on how dangerous it is to push above these limits?",2
"From the looks of the query, I surmise there is a one-to-many relationship from commits to project_commits. The problem I see is that your query is gathering data as many-on-one. If you are already have an index on created_at or if you already have a compound index whose first column is created_at, skip this suggestion. The sizes of the two tables are relatively large: project_commits contains 5 billion rows and commits 847 million rows. Also the server's memory size is relatively small (16GB).  This probably means that index lookups hit the (unfortunately magnetic) disk, and therefore performance takes a heavy hit.  According to the output of pmonitor run on the generated temporary table, the query, which has already run for more than half a day, will take another 373 hours to complete. Could I somehow increase the query's performance either by partitioning the tables, so that the join can be performed in memory, or by forcing MySQL to perform a sort-merge join? As the time involved for running alternative strategies could be many hours, I'd rather have a suggestion, instead of trying things out. As MariaDB doesn't seem to support sort-merge joins, I ended up exporting the required fields into two sorted files, performing the JOIN and DISTINCT with the Unix join and uniq tools, and importing the result back into the database. The operation took fewer than 12 hours to complete. Full details are here. Both join fields are indexed. However, the join involves a full scan of project_commits and an index lookup on commits.  This is corroborated by the output of EXPLAIN. Note : I have no idea what to expect from my suggestions. After all, your LEFT JOIN is like an iterative Cartesian Join with the potential of make a temp table that is the following",2
"This is a very step-based, procedural approach.  It doesn't take advantage of Java's object-orientated nature.   Try to avoid generic names like askInput and showResult.  They don't read as well as more specific names, and they can clash with future times when you need to ask for input or show results.   Or you could generate the initials in the constructor if you use them frequently.  That doesn't seem necessary in this case though.   Also, getInitials requires seperateFullName (sic) which requires askInput, but if you switch them, the compiler won't notice.  You'll eventually get runtime errors.   You pass class fields into each method, but you don't need to do so.  They're already there.  You already use the fields rather than returning the values.  It would be better to separate on some other basis.   This isn't how static variables are usually used.  You're essentially limiting yourself to do this only once in the entire program.  More common would would be to make these instance fields in a Name object.  Then adding more names is as simple as adding more object instances.   This way, greet and showInitials know how to display; askName knows how to get input; and the Name object knows how to store and process data.  And if you change the order, you'll get a compile time error.   Sneaky.  You start all over again.  However, you'll still have the rest of the previous run to finish.  So you will call showResult twice.   Note that my revised version doesn't require this.  You'd instead mark an error in that case, which will be handled in the caller.  That way Name doesn't need to know how it was called.  As it is, this is a separation of concerns violation.  You could only call this code from start.",1
"Other users reported that having IPv6 enabled in the network adapter can cause delays, but IPv6 is disabled. I am unsure where to continue my investigation. Does anyone have any insight? I can provide any required information. Explicitly specifying credentials on the CLI makes no difference. I also tried disabling the system's 'Detect network settings automatically' as some users reported that this impacted SVN performance, but no impact. I used Wireshark to inspect the packets, and no traffic travels between the server and SVN server during the 30 second delay. After the 30 second delay, the server sends two packets that wireshark identifies as Application Data, to which the server responds promptly. I enabled verbose logging (using --config-option servers:global:neon-debug-mask) and the last thing that happens before the 30 second delay is the SSL identity is confirmed. The issue occurs 100% of the times that I have observed, across all of our internal repositories I tried. I cannot test this issue with an external repository as the server does not have internet access. If I update a specific file on my workstation, the operation takes less than two seconds. If I try it on the server, it seems to connect, wait 30 seconds, and then perform the update. It doesn't seem to be related to network issues, as I can browse the repository from the server through a web browser without any delay. The issue also manifests across both SlikSVN and TortoiseSVN. I would like to add more info to a possible solution.  We have had a very similar problem with subversion for a long time, our delay is about 18 secs.  All of our Dev work is disconnected from the internet.  We captured some traffic using wireshark and found that some app is requesting Windows Updates.  Since microsoft.com is not accessible the requests always need to time-out.  We believe subversion is requesting the updates but we are clueless as to why subversion would do that.",2
"I'm rather ignorant on related standards, programmatically altering bios behavior outside the typical interface, hacking routers to force sending this wakeup packet on requests, or other methods.  However, any info or leads to obscure methods and standards, hack or otherwise, would be appreciated. I have a non-professionally/non-commercially purposed server - a 5yo laptop actually - on the www with low traffic, accessed no more than perhaps twice a day, and of course at unpredictable times.  Being a laptop and so rarely used, I'd like to conserve power (battery or plugged in) and heat generation while not in use rather than having it on 24x7.  Is there a way to awaken a system from a request of any protocol of any kind, load up, and pass the packet to the usual server software stack when up and ready?  Is there an OS that can handle this case over any others, by listening on the ip yet still be technically mostly ""asleep""?  I'm getting discouraged by the apparent need for a special packet to awaken the machine. Edit: if this question is more appropriate for superuser.com, please someone with permission to do so move it there. The Magic Packet you're talking about is the standard way of issuing a Wake Up command (WOL), and it's issued to a MAC address, not an IP address. To be listening on an IP address instead of a MAC address, then the operating system needs to be running, which means the machine has to be on.",2
"I'd imagine they don't bother investigating unless they have reason to.  They might potentially automate some of these checks if they really cared, but these checks might only flag the obvious uses of the tech... A simple (but not foolproof) way might be to use the strings utility to examine the binary for known strings: version stamps, error messages, etc. You could suspend the app and scan its memory region, potentially, to do similar checks, looking for known patterns (akin to the way a virus scanner might work). Of course there are a number of ways to attempt to guard an app against such checks.  For example, you could compress and/or encrypt the majority of the executable and/or data.  But if your app can decrypt it, likely so can an investigator... If symbols aren't entirely stripped (not likely in a submitted build) you can use something like nm or objdump or the like to see the names of symbols.  Some symbol names may be a good indication that a particular tool, compiler, library, or framework was used. You could look for machine code or data headers that are known to correspond to a specific version of a compiler, library, or tool.",1
"The 64K block size comes from the fact that SQL Server does its I/O in units called ""Extents"", which are each 8 pages - and a page is 8K so the basic unit of I/O is 8 X 8K = 64K. If you decided to test the performance under different cluster sizes, you would probably discover that the difference is negligible, especially on high-end storage systems. The method is the one used in the article that you linked in your question: test it. You can capture a representative workload for your application/database and replay that workload on servers that have different cluster sizes. What you will find out is that SQL Sever works best with 64KB clusters, because of the way it reads data from disk (read-ahead). However, you don't have to do that: setting a 64KB cluster size is an established best practice, as clearly stated in the article that you refer to. From the article: There are many articles on what storage blocks size should be used for sql server e.g. Disk Partition Alignment Best Practices for SQL Server. The right blocks size should improve the performance of a sql server database. Im looking for a recommendations and methods to identify which storage blocks size is appropriated for a database. Is there a guide on how to identify an appropriated block size?",3
"Alternatively, you can also turn off the UAC through the registry. For instructions, you can check this article. This article is about Windows 8 but this method works with Windows 10 as well.  I believe C: is locked by default since Windows Vista. So this type of behavior is normal. You will need to take the ownership if you want edit this folder while user account control (UAC) is active. To do this press Win+X keys, select file explorer, open computer, find drive C, select properties, go to Security Tab, select your user group that you want to have access and finally click ""Edit"" to change the permissions.  So, all I am trying to do is to change my cursor files to cursors I want. I did cursor schemes but I have to go to settings and re-change them upon every restart of my computer. So I am trying to replace the cursor file in my windows folder instead. However, I can't edit anything even though I am admin. I tried going into the properties of the folder, but all of the permissions I should be able to change are greyed out. How do I fix this? Keep in mind that this will not work if you have full-disk encryption (such as Bitlocker) enabled on your Windows drive.  If changing permissions or disabling UAC (as outlined in Neal's answer) don't work, you can alternatively try booting into a live OS (such as Ubuntu), and then modifying the required files on your Windows disk.",3
"Obviously they want them to be cheap to make. As for the PS3 specifically, memory prices were very high at it's release, but they dropped soon after. The Playstation 3 now has only 256MB ram and 256MB graphic memory, and I'm sure that the day the console was released, even laptop's ""standard"" capacity was at least 1GB. In addition to what others have pointed out here, I'd like to say that the consoles have very little overhead when it comes to memory (XBox 360 only reserves ~32 MB of RAM for system use, the rest of the 512 MB is for the game). Once you figure out what is already used by the OS and background apps on the majority of consumer PCs, you will realize there isn't too much spare RAM available. So why do they put so little memory in their machines, while developers would benefit a lot by having more ? Or is the memory that much faster than desktops and thus more expensive ? Or is it not that much worth it for developers ? What are the Sony/XBox/Nintendo engineers thinking that seems to be the same reason ? There ain't a big clunky operating system running alongside games, so unlike Windows developers can actually utilize all the memory, you easily have more ""room"" on a PS3 than on an XP machine with 512 MB of system memory. Knowing the exact hardware specifications means that it is easier to go to the limit. Most memory eating PC games can to some extend be attributed to lazy programmers, there really ain't a lot one can do with more memory, except not care as much about filling it up. That isn't to say there is no optimization in PC games, but it is in different areas than consoles, and in general due to the wide range of supported hardware, it is less effective. If the amount of memory was higher on console launch, they either would have to raise the price or take an even larger loss per console sold. While the console memory is generally fast I doubt that it is a lot more expensive, it is to a great extent the development of the technology that cost, the production itself is probably not a lot more expensive. So when Sony and Microsoft make or purchase a big pile of fast memory it doesn't get a lot more expensive than slower memory would have been. Another factor that goes into making consoles very performant on what appears to be very limited resources, is that since every console is identical, we as developers can take advantage of platform specific optimizations. On a PC you can't do that as much, and thus you are required to have a beefer PC so it can brute-force its way through a similar unoptimized task. As @AttackingHobo said, the memory is very fast and expensive. Also, you must take into account that these consoles launched years ago, when memory and hardware prices were higher.  Luckily modern OS's have paging systems for their RAM so we generally don't need to worry about memory usage, but even on a PC game it would be a good idea to stick to a limit of 256 to 512 MB of memory so you don't start thrashing the paged memory and ruin game performance.",5
"VirtualBox would be good in the special case you have, where linux is on its own disk. You can create a vmdk image that uses what they call ""raw hard disk access."" 3) Is it possible to do the P2V conversion while I am logged into windows. I can see the actual linux drive loaded in disk manager, but windows doesnt read linux file systems so im confused as to how to access the linux drive if this is possible. Linux P2V you can use http://www.madness.at/blog/2008/10/p2v-with-mondo-rescue.html and for Win you can use symantec backup tool which 30 days trail both tools are very easy to use any one can  I have used this method several times and it is fairly painless.  Once your done simply create a new VM and attach the VHD. If you want to stick with windows virtualisation technology then I recommend the following guide to convert your existing disk into a VHD file: http://blogs.technet.com/b/enterprise_admin/archive/2010/05/13/linux-p2v-with-dd-and-vhdtool-easy-and-cheap.aspx 1) My linux image is around 80gb, do i need to take this into consideration? The linux drive is around 180gb in total. All my other drives are NTFS non writeable in linux (as I use them in windows and ntfs is dodgy in linux), so probably not possible to move the image over to my ntfs drives As im having problems understanding how this is done, I would really appreciate a step by step guide (for a newbie), or any simple tutorials that you can point me at. A nice feature in Virtualbox is the shared folders, which lets you keep the datafiles on the host FS, reducing the VM filesize and simplifying the entire experience. It's much more risky when the Lx partition is on the same disk with Windows. But your case is perfect. I have two OS's installed on different drives in my PC. One linux (Centos 5.4) and one windows 7. Its getting tiresome to constantly have to stop and restart the PC when I want to use either OS. I would very much like to use Windows 7 as my host OS and access my linux OS from within Windows. However, im having trouble deciphering exactly how to do this (many of the articles seem confusing and a bit overkill)  From what i have seen its possible to use VMWare converter to convert the physical linux image to a virtual image so that I can use it in windows.",5
"just unplug the mouse from your pc and reconnect it. I do this whenever I face this issue(I have guest additions installed but still has the problem).  Well for me, the problem was solved by selecting my external mouse under USB in the devices menu. Maybe it installed the driver or something. (Sorry if I sound like a noob, but these are my 1st 30 mins into virtualbox.) I am using latest version of VirtualBox (VirtualBox 4.3.6 for Windows hosts) and after installing any OS and clicking inside of it mouse disappear. It is not visible until I press right CRTL. Anyone know what can cause this problem. I have also installed extension pack.  One stupid thing no one mentioned, the actual key combo to release your cursor is SHIFT+right ctrl, not right-ctrl. It doesn't say that anywhere in the interface, docs or website. I'm not sure this is your problem, but others might run into this. It's not a problem in Virtualbox. RCTRL is set the default button for mouse capturing inside virtualbox. So you would only be able to use the mouse inside VB after enabling mouse capture. If you don't enable mouse capturing you can use your mouse but it will be invisible as the mouse is being used by the Current windows. You can change the RCTRL button by going into File -> preferences and then you can enable mouse capture with other button. Also check when you enable Mouse Capture the mouse will disappear from current windows and show in host windows.",5
"You can see that where storage redundancy/speed/integrity are an factor, I feel a Linux OS is the TOP choice as I've had bad experiences with hardware raid which is the only real commercial level raid solutions available for MS Windows, including their server.  It depends on the server and what it will be used for.  If the server will house a particular software package, such as Oracle, ask the vendor what they recommend.  What will be easiest for them to support?  What OS has the best benchmarks for the software in your use case?  Do you have staff who can work with that OS? File Servers: If performance and data integrity is a key requirement, Linux (Redhat, Oracle, Scientific Linux, Debian, CentOS). Many large organizations like to use Linux Software RAID 6 for their servers (I do not recommend RAID 5). You may find iSCSI and Apache Hadoop of interest.  Security/Firewall Server: OpenBSD or FreeBSD are my first choice. Or a standalone produce based on of these. when it comes to security, better to get someone with the expertise in security and these OS's. Worth the investment.  Many people I know swear by performance of Gentoo or Slackware. For situations that require commercial software these I find don't work well. And its harder to find server administrators that know these OS's. Hope all this helps. For file sets where ease of access is key, keep a MS Server for easy role based security schemas and integration into the MS windows File explorer, but would keep critical files backed up on an RAID 6 Linux file server. MS windows is not great on software raids and hardware raids can be a disaster when the controller inevitably fails and someone forgot (or too cheap) to buy a spare card ahead of time. Linux software raids are proven rock solid and not hardware dependent.  Groupware/Mail servers: This comes down to software solution choices. If simple MS outlook integration is all that is required and the server administrators are a not intermediate-advanced, its a simple MS Windows server. If you need more advanced options and want to use IBM Domino server for Lotus Notes I recommend a Redhat based distro (Redhat, Oracle Scientific Linux, CentOS) to increase performance. For support purposes Redhat and Oracle are safest to avoid the ""we don't support that OS"" tag line from people at IBM (depending who you talk to, some are helpful with centOS as well). Novell Groupwise had serious security/stability issues when a team I worked with was testing various Groupware servers; we could kill it with a single e-mail every time and no fix was available at the time of testing but probably fixed by now. The choice here I think comes down to features required, cost allowed, and security. IBM Domino server is expensive but has more features and TONS of encryption options. You can run it on MS windows servers as well, but performance won't be as fast especially with encryption features turned on. Since you ask about issue besides software required that only runs in a specific platform I'll add a few specifics to what Justin said which was an excellent answer. In all functionality evaluations, skill sets available are an important factor in selecting an OS platform. That said, any business environment needing more than simple domain controllers/small network server I believe requires at least one person moderately Linux/BSD literate. My forumlae for selecting servers for functionality are as follows:  The software that you will install for the most part tell you what the best OS is, for example PHP, Apache, even Oracle I think are much better suited for Linux What are criteria that you should be aware for choosing an operating system for a specified server (apart the fact that software needed is only available on 1 platform) ? Web Servers (Especially PHP based solutions): If have at least 1 moderately versed Linux staff member: (in order of preference) Redhat, Oracle, Scientific Linux, CentOS, Oracle, Debian, FreeBSD, OpenBSD. More server admins know the Redhat based directory structure which are shared by CentOS and Oracle than Debian or OpenBSD. Hence the order of preference. Its faster, more secure,  is easier to find answer for any issue via Google search and MS Windows and easier to integrate other technologies scripting for text/file processing. In response to Justin and FreeBSD: Yes its an excellent choice I agree, especially on security although its sometimes not easy to find someone comfortable with BSD environments.  Hopefully, you can see how important it is to efficiently selecting the OS that you manage to initially find an unused server or VM with an OS already on it.  Otherwise you could be looking at a 6 step process to finding the OS that's right for you and your environment.  Therefore,  the smart, forward-thinking SysAdmin who wants to reduce his workload will spend dozens of hours a week finding spare servers or VMs and installing various operating systems on them, in order to avoid having to do that down the road, which would increase his workload, and interrupt his work-time nap-schedule by involving some approximation of the process above. for storage I don't think it matters as much, I would say Linux, but I would just go with whatever infrastructure you have in place, if you have Linux servers, then Linux Virtual Server: Here performance is critical and recommend Linux hands down. Redhat,  Scientific Linux, CentOS, Oracle are easiest to support the widest variety of open-source and commercial options. There are some for Debian and BSD distros too, but not as many that can be installed, and some commercial, closed-source solutions that may be ideal cannot be compiled by hand. Know several companies using VMware, XenServer. Proxmox looks very promising as well. VirtualBox is good too. For generic servers, go with whatever your staff is most comfortable supporting.  For basic web servers, if your staff knows their way around FreeBSD, use that.  If they prefer CentOS, go with it.  This will also somewhat depend on the software (you're probably not going to deploy Linux for an ASP.Net web site). For example, if I'm building an Oracle Database server, how do I choose between Linux (and which Linux) or Windows ? Database Servers: If you don't need to integration something into MS SQL Server 2008+ (including Oracle Database), its Linux again. Oracle actually does run significantly faster in Linux. Redhat, Oracle, Scientific Linux, CentOS. I'm not as keen to recommend Debian based distros because I haven't run Oracle in Debian and Redhat based distros are easier to support. If you have mission-critical apps that require top performance suggest configuration on Database partition of RAID 10. Reduces disk bottleneck. If there is a particular application well suited for a particular OS, such as a firewall, consider an OS with a reputation in that area.  OpenBSD has a very good reputation for security, so they are an obvious choice to base firewalls on.",5
"I had this problem and after check for leading spaces and getting rid of the filters, I finally solved it by copying the non-sorting cells and paste special them as ""unformatted text"" into Word and then copying and pasting them back into Excel.  No idea how they picked up some kind of un-sortable formatting as they appeared normal. You might have some of your rows in a table and some not. If you have banded rows, then those are probably in a table.  Check the Design tab, Properties group, click Resize Table and make sure the range covers all of your rows. I just encountered a similar issue.  The answer ended up being that some of my cells in the sort column were entered as text, and some were vlookups from another page.  The cells with vlookup formulas were not sorting properly either through a filter or through a sort.  I copied the spreadsheet and pasted as values over the top of it and it sorted properly after that (although I lost they dynamic nature of the lookups. I copied and pasted my data into a new sheet, without the headers.  It sorted properly.  Be sure to save your work prior to trying anything new! You've not told us what type of data you are using. Mixed data can cause all kinds of issues with sorting. If it is mixed, it should be formatted as text. Here is more info about how to troubleshoot sorting. If you've reentered the data it is likely you've gotten rid of blanks. It is possible the rows are not included in the sort because they are not selected. If those 43 rows are not included in the sort grouping, they will not be sorted in the order you choose and will appear in their own group. Verify they are being included and retry the sort.",5
"All of these services should provide a way to stop them from sending you further e-mails. You should move them to your spam folder and report them as spam to services like SpamCop.  (Of course, none of these services provide any way of opting out, except possibly by registering, which they will then take as an invitation to stuff your mailbox with even more crap) While traditional UCE (get rich quick, enlarge your body parts, Nigerian barristers) are handled adequately, I'm still receiving a lot of not quite unsolicited spam.  This is typically from commercial services forwarding ""invites"" from my ""friends"" and then ""reminding"" me of their services.  Typical offenders are Facebook, Linkedin, dropbox, bebox, etc etc. If you're using a system like Google Apps for e-mail, then marking something as spam will pretty much force all the following e-mails from that service to land in your junk folder. What is a good way to deal with these?  I can of course junk them using procmail, but is it a better idea to e.g. bounce them, or at least send a reply informing the sender (and ""friend"") that I am not interested in their service nor their spam.  Any solutions to this?",2
"My client is mac os x 10.9.5.  I have a bastion host (linux) that I am ssh'ng into.  I have agent forwarding working so I can ssh to the bastion host and then ssh into another machine without having to store my private key on the bastion.   I have tried setting up a config on the bastion server to force it to use and identities file.  But since I do not want my private key out there, this fails. My problem is I have hit 7 keys in my ssh store (ssh-add -L) on my client machine.  When I try to use agent forwarding for machines with the 7th key, it fails for ""Too many authentication failures for '.  With -vvv on, I can see that in the send_pubkey_test that it gets to the 6th key offered and that is when it stops.  7th key is never offered up. Below is snippet of the 6th key and failure.  In addition, I tried taking the public key for the pem and copied it to the bastion server in the .ssh directory.  Then used -i name.pub on the command line.  This did not work.  It seemed to treat the pub key as a pem. My workaround is to write a script to load/delete the keys as I need them.  I keep thinking for ssh agent forwarding, there must be a solution that I am missing. I have tried setting up the other servers in the client in the config file.  Below is the example.  When on the bastion server, I try ssh internal.IP and it fails ""too many authentication:"" Why do you have 7 keys?  I've seen people do this thinking it's more secure but it's not.  If your client is compromised all keys are compromised.  Maybe you have a legit reason but just want to make sure you're not creating more problems than you're solving.",2
"Note that this answer doesn't account for the required routing infrastructure.  When forwarding packets across interfaces, it's assumed that you're traversing to a different subnet.  If you're bridging the NICs so they're on the same network; this solution will not work since you're no longer routing packets. Add an entry to the forward table to allow traffic from already established connections.  This will make sure that the returned traffic can be routed back to the wireless LAN. I am attempting to install a MAC filter in between a switch that handles my wireless network and the core of the network. The idea is to have a centralized MAC filter bypassing the filters built into the individual access points. My proposal is that wireless devices would still be able to use network resources. How can I effectively do this using Ubuntu 10.04? I have a old dell that I am testing on right now with 2 NICs. I know you can do anything in the world with Linux... soooooo how should I go about this? I would greatly appreciate tutorials.",2
"I've recently run into this issue on Amazon Linux. My crontab outbound email queue /var/spool/clientmqueue was 4.5GB. If you are running an EBS boot instance (recommended) then you can increase the size of the root (/) volume using the procedure I describe in this article: It could be coming from Jenkins or Docker. To solve that, you should clean Jenkings logs  and set it's size.  If you are running an instance-store instance (not recommended) then you cannot change the size of the root disk.  You either have to delete files or move files to ephemeral storage (e.g., /mnt) or attach EBS volumes and move files there. That file, / is your root directory. If it's the only filesystem you see in df, then it's everything. You have a 1GB filesystem and it's 100% full. You can start to figure out how it's used like this: Here's an article I wrote that describes how to move a MySQL database from the root disk to an EBS volume: ...and consider moving to EBS boot instances.  There are many reasons why you'll thank yourself later. You can then replace / with the paths that are taking up the most space. (They'll be at the end, thanks to the sort. The command may take awhile.) and a lot of old packages were removed, freeing up 5 gigabytes, for instance there was many packages like this ""linux-aws-headers-4.4.0-1028""",5
"is being displayed whilst trying to connect to github using SSH. I've also tried using https:// as the remote path but to no success.   go to your home directory and create a folder called .ssh, and a blank file called known_hosts inside of it.  then set the permissions on the file to 644. In your Documents and Settings folder of your local machine, create the folder home\. For some reason, cwRsync won't create these folders for you. I didn't change my cygpath, so I created mine in C:\Program Files\cwRsync\home\. Use Sysinternals Process Monitor to log the file system accesses that take place when you are trying to do this.  then try ssh again and see if the result is any different.  (note: you'll have to say yes when asked if you want to continue connecting the first time.  then that server will be added to the known_hosts list and you shouldn't be prompted anymore after that.) maybe the permissions to your home directory are interfering with ssh's ability to create the known_hosts file?",4
"An ADSL router with 4 ports (they are routers, not modems), typically contain an ADSL port which is routed to the 4 ethernet ports.   The 4 ethernet ports are typically individually accessible but bridged to behave like a dumb switch in software.    [ I know this because I have reconfigured a number of these devices to have different switches on different VLANS and route between them by changing the software to OpenWRT ]. An ADSL modem is the bit in the router which actually captures the raw frames.  In earlier times, ADSL routers were internal cards (which could go into a PC) which did this - they did not have routing functionality on board.   Remember too that a ""modem"" (or ""conventional modem"" is actually a device used to modulate a digital signal over an analog carrier - typically associated with speeds of 300 to 56400 bits per second and used over the phoneline in days gone by). A single port ADSL router (Its still a router even if its called a modem) routes packets between the ethernet and ADSL interface, but is still routing the packets.  We know this because the device will typically strip off the PPP overhead (ie the PPP bit in PPPoE or PPPoA) and then make that available to ethernet side of the connection.  The devices also often run a DHCP server so they can hand out an IP address.  These devices are generally slightly cut down version of 4 port routers.",1
"You can even create different vSwitches and route between them using a VM that has interfaces on both vSwithes. VMs cannot let you test hardware compatibility or that it works on a particular hardware configuration, because it isn't there. So keep that in mind that if some configuration or software has a hardware dependency, VMs will not be able to help you with that aspect. But if you want to test whether the Windows deployed with IE configured a certain way, and group policy implemented correctly and such, then VMs are useful and they make testing faster. As far as Windows and Deployment Services is concerned, VMs are fairly transparent. VMs can help you test deployment settings up until you need to make sure that the hardware settings work. In my company we had to deploy over 100 Windows Server installations over the network and VMs helped me iron out most issues with it. I say most is because there are a few things you have to keep in mind that VMs cannot test for you, namely the very beginning of the deployment process where the system has to first transition from network boot.",2
"At the bottom of the shown PivotChart, it shows the two levels of Data Labels at the bottom, for the X-Axis. Thanks in advance. I've done a bit of searching and so far can't find anything to help me with this. The Table now has special formatting, with a colorful header row and alternating bands of color. It's a little overformatted, but you can select it and choose a less (or more!) formatted style. Excel charts work by plotting rows and columns of data, not just a big long row. So arrange your data like this: If you don't want the total (it might overwhelm the rest of the data, simly select and delete the total columns in the chart, or select only the first four columns of the table before selecting the chart. And a bonus question... Let's say I want to add a new ""week"" every now and then to the right of this table (expand it with more data). Can I do it so that the chart automatically adjusts and includes the new data? Or will I have to manually edit it each time? I'm trying to create a fairly simple chart from a fairly simple table. But I'm having more problems than I thought I would have and spending too much time on it. So... X-axis has labels from the top row, Y-axis has values from row 4 and bars are grouped according to labels in row 3. Also, bars are colored and there's a legend on the side linking a color to a specific sub-label. Select this range of data, and on the Insert ribbon tab, click Table. It won't insert anything, but it will convert your ordinary range of data into a special data structure known as a Table. Nothing to be scared of, Tables are pretty powerful. The dialog will ask if your range has headers, which it does (Week, A, B, C, Total). (By data labels I mean ""field"" labels along the bottom below the axis, not actual data labels in the chart.) If I right-click on the Axis labels and select Format Axis, I get the option to format the Axis, but it seems I can only format the ""A, B, C"" level labels (from the example here), but can't figure out how to or if I even can format the ""1, 2, 3"" level of labels. Now comes the magic of Tables. If you have a formula somewhere that relies on the whole column in a table, then if you add or remove rows in the table, the formula will update without any effort on your part. These formulas include the Series formulas in the chart. So add a row to the table, and the chart will automatically include the new row of data. My question: How do you format BOTH LEVELS of the X-Axis data labels? Both the ""A, B, C"" and ""1, 2, 3"" levels of axis data labels in this answer's example. My question is related to the ""There is a not-so-simple way to do this with normal charts, but the real skill here is to use a PivotChart"" PivotChart answer.",3
"Outside of Firefox/Chrome/Opera/IE Privacy modes, I wouldn't bother with the rest of the hacks.  It's debatable how effective they are at protecting your privacy. On that note, I suspect you are shooting for privacy rather than anonymity.  IE9 is probably the best one with its No tracking feature.  While other browsers are at the mercy of third party compliance, IE9 give users a bit more control by forcing compliance upon third party trackers. I remember reading about the politics and drama that went to implementing this feature.  When it was first announced, ad companies threaten to sue.  That tells me this is a good feature when it threatens their business model. Unfortunately, browsers are complex and trying to implement too much anonymity can create havoc with website compatibility. Just ask anyone who keeps cookies off.  It wreaks with functionality. Other people recommend booting an entire operating system from a USB stick when doing online banking. http://blogs.msdn.com/b/ie/archive/2010/12/07/ie9-and-privacy-introducing-tracking-protection-v8.aspx I'd note despite turning off nearly everything that could identify me(plugins, useragent, etc) the profile that pantopticlick gave me was unique, but things like supercookies and other client side tracking would not work. However this is the closest i could get to your requirements Another feature I would strongly suggest is a password manager.  LastPass is my choice here.  Along with strong plugins, two factor authentication, my favorite feature is the one time passwords.  Along with your master password, you can also use the one time passwords if you are logging into a public machine, rather than using your master password. In particular, the Fedora ""Live USB creator"" tool allows you to set how much of the USB stick to reserve for ""persistent storage"". Some people use a ""browser on a USB stick"" ( a b ). Since all the cookies, etc. are stored on the stick, when you unplug the ""stick I use for banking"" and plug in the ""stick I use for browsing Youtube"", there's no way either site can access the data stored in the other unplugged stick. Its a little quirky but qtweb has the most finely grained privacy controls of any browser i've seen, and does user agent spoofing out of the box. You can disable most features easily including user agents, plugins and and cookies. I have Ubuntu on one USB stick and Fedora on another USB stick. I hear that other people like Puppy Linux or Chrome OS for their ""OS on a USB stick"".",3
"2.The fn_dump_dblog function is used to read transaction log native or natively compressed backups. The result is similar: Unfortunately, no official documentation is available for fn_dblog and fn_dump_dblog functions. To translate the columns, you need to be familiar with the internal structure and data format, flags and their total number in a row data 1.Here is an example using fn_dblog to read an online transaction log, with a result of 129 columns (only 7 shown here) 3.DBCC PAGE is used to read the content of database online files  MDF and LDF. The result is a hexadecimal output, which unless you have a hex editor, will be difficult to interpret There are several SQL Server functions and commands (e.g. fn_dblog, fn_dump_dblog, and DBCC PAGE) that potentially provide a way to view the transaction log file content. However, significant knowledge of T-SQL is required to use them, some are undocumented, and the results they provide are difficult to be converted to a human-readable format. Following are the examples of viewing LDF file content using SQL Server functions and commands: So, there are different ways to open a transaction log file, and most of them do just that  opens it. Its tricky to get any human readable information and make a use of it though",1
"What I found to be my problem was that my dragging my mouse to highlight the table content was actually selecting part of the table border in the last cell I was highlighting.  So when I was careful to highlight and copy just the cell-content (tip: stop highlighting one character short in the last cell) it would paste into Excel with the column/row formatting I was expecting. So Column A, Row 1 is Year, Column A Row 2 is All Families, etc. I used to be able to paste the text and it would stay in the table format (even if they weren't split into cells). to copy the table and paste it into excel, it will only paste the values into one long column in Column A (and each row has a value). Save the PDF to your computer and open it in Adobe Reader. Hold the Alt key while selecting it (this allows you to use marquee/table select), then copy and paste it into Excel. I'm not sure if this is still an open issue, but I had the same problem when attempting to copy the contents from an HTML table into excel.  I would drag my mouse to highlight the content from the HTML table and copy the selected contents to my clipboard.  Then I would open Excel, click into one of the cells and paste my clipboard content which would paste it into one long cell instead of formatting it into the column/row format as it appeared in the HTML table.   Try looking at the delimiter on the ""text to columns"" function.  I had this pasting problem recently and my delimiter was a ""comma"".  I changed it to ""tab,"" and tried to paste again.  Voila - the data was all in columns",4
"With a method like this, you could calculate these values on the fly, for whatever percentages you want and with whatever name that you want. You could still save the percentages that you want as consts, or you could load that information from another source such that you may not need to recompile the calculator if you want to make changes.  I have recently begun using property lists to achieve this. One step further would be to save these values together as a class, and pass an object of that class into the calculation method or class, and return the result in whatever format that you need.   Definitely pay attention to nhgrif's recommendations, he knows what he is talking about.  I would like to talk about something that you might or might not be able to change, so please take it with a grain of salt. What I do not understand is why you need to set things up in this inflexible way?  It seems to me like it would make more sense and also be more extensible if you could create these calculations on the fly.  I assume this is just a program for personal use, but what if you decide later that you want to calculate these values in another way?  You will have to go back to the code and add more properties or alter the const values that you are declaring, and recompile the code when you are done.",1
"Press enter and enter username and password.  If RDP is enabled and the workstation or server is connected and on you will be logged in. Then you may need to follow a guide like the following, to disable NLA on the client as well. http://blog.backslasher.net/using-remote-desktop-client-without-network-level-authentication.html Also some vulnerability scanners will try to connect with network level authentication disabled and take a screenshot of the login screen - which is useful to determine the OS edition, OS language, whether the machine is part of a domain, and (in some cases) some valid user names. On the server, you will have to allow RDP sessions with network level authentication disabled (which is in the control panel remote settings), and either your RDP client must be old enough to not support network level authentication (i. e. from WinXP or before) or you have to connect via a .rdp file that contains the option enablecredsspsupport:i:0. If you are looking at the remote access settings on the host, you would enable the setting to Allow connections from computers running any version of Remote Desktop. If you are referring to remote access to your server, you might want to try, from Windows 10, ""type here to search"" RDP.",3
"Fore pure cold backup I'd say RAID5 is fine. RAID5 will take a considerable amount of time to restore on 8x1TB SATA drives, but given the chances of something breaking at low IOPS that's probably not critical. If you want the added bit of safety, go for RAID6 which will give you the ability to survive two simultaneous disk failures. On my filer I run a software raid 5 across eight 500 GB s-ata drives, which works great. Read performance near 160 MB/s write performance around 90 MB/s. Recovery time is about 20 hours. Athlon X2 4200+ and nvidia chipset. If it is purely for backup purposes and IO doesn't matter, raid 5 should be fine, raid 6 would be even better but you'd be losing storage space. How critical is your data? You could also do raid 5+0 comprised of two 4-disk Raid 5's. This would let you lose 2 disks, give you better IO, and leaves you with about 6TB usable, but there's a good possibility your controller doesn't support it (usually requires higher end stuff). Raid 6 is slowest of all of the options, lets you lose 2 disks, and will still leave you with about 6TB usable. You'll need to figure out how much your data is worth, in real world dollars, and how annoying or costly a failure will be before you can think about storage strategies.  A hint is that the answer can't be ""no loss ever,"" because that has infinite cost. As this is to be used for backup storage, I don't see the need for raid 6. It's unlikely that two drives on the backup raid will fail within a few days, at the same time as production data are damaged or lost. Use that cost/time/data to guide how long you can tolerate a rebuild, or how many hours the system can be down, or how much money you want to spend on hardware.",4
"I write the user name into the computer description property using a logon script, which lets me see everything in AD Users & Computers, do searches on it, and so on.  Very handy. I'm surprised nobody has mentioned loggedon2 yet, which I've been using for quite a few years. It's the GUI implementation you asked for and is available here. You can detect a user being locally logged on to a workstation by querying WMI through the following PowerShell script.  It returns the name of whoever is logged on locally or the empty string. If the servers are running Terminal Services, you can use Terminal Services Manager to view the servers in a domain and who is logged on to them.  It is GUI and can be found under I'm not sure where I got it but I have this code laying around that shows users on a machine.  You can wrap this in a for each loop to scan a bunch of machines.  I would say that if you want to know who's logged on to a system the simplest way is to turn on login auditing and look at (or query) the security log.  Here's the code to see who's on at any given moment:",5
"Basically - once my PC is 'ON' it stays on for hours or even days with no trouble.  When I shut it down (none of that hibernate stuff - 100% off)...it won't turn back on when I hit the 'power' button. Do you know how hot your computer runs on average?  I've seen cases where someone wasn't careful when processing video on their laptop and ended up sustaining 200F for a minute or two, causing erratic behavior from then on until it finally died 3 months later. This kind of fault may be diagnosed if you apply a electronic 'freeze spray' to selected parts of the PC one at a time, while it's running, so the faulty component or connection is cooled and behaves as if the PC has been switched off for a while. No, no, more seriously, it may be a faulty connection (a dry solder joint?) caused by cooling from operating temperature to room temperature. What's the ambient temperatue there? I've had this problem for a while now and it seems to be getting worse.  I've replaced cases and the problem remains - so I'm fairly confident it's not a bad power-on button.   Does this sound like a PS issue?  Is it possible my motherboard is at fault?  Is there any way for me to test these things without purchasing new hardware? It seems like the way to fix this is to give the computer a good smack...after a few hits, the power button will once again turn on the PC. All the parts you mentioned would be potential candidates. Either take the PC to a repair person or try yourself with 'freeze spray' from an electronics DIY retailer. I want to fix this - but I don't know what is really going on.  My guess is that it is a bad power supply.  I've completely taken apart and reassembled my computer - it didn't help (so I don't think it is simply a loose cable somewhere).",3
"I guess if you were trying to tunnel ssh within ssh that might make sense, though I can't off the top of my head think of a need for that! Alternatively, SSH does have a -D option too that acts as a SOCKS proxy that can do HTTP[s] , but tell the web browser to connect to a SOCKS proxy rather than a regular HTTPS.  So I suppose then like the line David said ssh -fNg -D 8888 -p 1000 username@server-address  -v  The -fNg  and -v isn't essential of course. If you put the http[s] proxy on port 22 of the destination machine(server-address), and configure your web browser to use proxy 127.0.0.1:8888 then your web browser could connect to port 8888 and that will be forwarded to the http proxy on port 22 of the destination machine.  When you did -L 8888:127.0.0.1:22 it means you have to have something listening on port 22 of the destination machine. And you might want to change 22 to something more sensible like 8080  I haven't tested it but I think it'd work OK so the client program would be connecting to port 8888 and whatever it sends there will be forwarded to port 22(22 being the port commonly used for ssh).  No, you already have SSH opening a port listening on port 8888.  You can't have another thing listening on that port.",1
"This combination of technology is not a great idea. See https://www.postgresql.org/message-id/54F7F625.6020907%40hogranch.com - particularly because the db is not small. I know how to create Windows file shares. I know how to mount file systems in Linux. How do I mount a regular file system in Linux that is supported by a Windows file share?   I can use the smb prompt interactively between the Linux guest and the Windows host.  But I do not know how to have regular Linux operations on files on a Windows share.  I only know how to use the smb client.  These interactive commands require special smb commands initially. vboxsf does not support the folder permissions necessary to set up Postgres.  That is the Postgres data directory cannot be on vboxsf file shares in my experience.    My Linux guest is Centos 7.3 with no GUI.   What do I do to use Samba without an interactive prompt or otherwise house the Postgres database on a Windows file share?  There may be a solution that is outside of Samba and Oracle VirtualBox technology (e.g., vboxsf). I am using Oracle VirtualBox on a Windows host.  The Windows host has a regular Windows file share.  I want to use this folder for storage (of Postgres databases).  You might want to look at an alternative like NFS instead - or even better (i dont use Virtualbox so dont know how practical it is) create a block device or use a file as an additional block device for the Linux VM and put the data on that. You have a few options for setting up the mount - ensure smb-utils is installed and modify /etc/fstab  I have installed Postgres on the Linux guest.  I want to have the Postgres database files be stored on on a Windows share.  I do not have storage capacity on my Linux server.   To clarify, my goal is to have Postgres databases of several gigabytes reside on a Windows server that work through a Linux server.  The Linux server will have Postgres installed on it.  The Postgres databases do not need to work with the Windows server.  The Postgres databases cannot be on the Linux server.   If you are going to do it, mount the windows share to /var/lib/pgsql/x.x/data  (where x.x is the version of Postgres",2
"If you're just mapping drives, why not just use a script w/ credentials stored in the script to map the drives: I visit a number of different Customer sites each week, and I prefer not to join my personal machine to any of my Customers' domains (to not have thier group policy apply, etc). Sounds like a really bad idea to join customer domains with equipment that isn't owned by the customer. That sort of thing has been a serious security policy violation at every employer that I have worked for. Have you considered using VMs for this purpose? Create a VM for each of the domains and switch between them on demand. The simple answer is that you cannot. A machine can only be in one domain at a time. The Microsoft way to deal with this would be for you to have your own domain, and for each of these other domains to trust yours. Good luck convincing your customers to do this, even for one-way trust. I'm an independent contractor with domain accounts at multiple client locations. I do my development work on a laptop and would like to be able to join the domain at each site. Under Windows XP I joined one domain and mapped drives/VNC'ed into the others. There are two that I visit at least once a week, so I would like a painless way to switch between the two. Anyone know how to do this in Windows 7 Pro?",5
"I need to implement Linux-HA configuration in two servers. I've decided to use DRBD for block level replication on both hosts, mainly for MySQL data replication. A better solution for MySQL HA is to use circular MySQL replication. This article explains it further, but the basic premise is to make each server the slave of the other. You will need to set up some sort of load balancer in front.  One thing to worry about is that database connections may be long-lived and you will need to configure applications to reconnect if they die.  As I understand, in DRBD configuration there's always a primary server, others are slaves (which can have slave of their own).  Replication is only passed to from masters to slaves, not the other way around. Provided that this configuration will go in conjunction with Heartbeat, it would be Heartbeats' job to ensure that MySQL runs only on the master, but lets assume for the moment, that Heartbeat failed for some reason. ""Can't happen"".  DRBD will ensure that only one server has the block device active at once.  If you get a ""split brain"", where both DRBD hosts think the other is dead (so they're both active), they won't reconnect and you need to resolve the inconsistency by hand (usually discard the changes on one master or the other). So what happens, if I have MySQL processes performing writes on both servers at the same time, one of which is master, the other is slave? If you need DRBD ""mainly"" for MySQL replication, maybe it is easier to use the replication features built into MySQL?",4
"You don't mention what protocol you want to use, i.e. TCP or UDP - and it's also important to realise that ""port"" isn't quite as granular the system supports to disambiguate sockets. E.g. if your system has multiple IP addresses then port 80 might be in use on all IP addresses (either the application has bound to ""0.0.0.0"" or ""::"" or to each IP address in succession), or it might be in use only on a subset of those IP addresses. Next, to tell if the port is truly ""open"" as you ask - you need to start looking at potential firewall rules. Again the easiest thing is to try to connect to the port. Use netcat as above, on the server, and from a client use netcat to attempt to the connect to the port you opened. (make sure you have node installed and that it works with node not just nodejs or change the program accordingly) will attempt to bind to TCP port NN on (optional, the default will be all addresses) a.b.c.d. Add the -u option to do the same in UDP. will connect to port NN on a.b.c.d, using UDP if the -u flag is specified. You can then type input into the client end, and it should show up on the server. If it doesn't, you need to look into system and network specific tools.  A lot ways of doing it gave me issues because they didn't work on linux and osx, and/or because they didn't show the ports used by docker, or processes that were owned by root.  Now I just use this javascript program: The best, and surest, way to determine if a port/address is free and available is to attempt to bind to it. Netcat is handy for this.",2
"This smells slightly odd to me. Why would the class of customers know about what customers are actually participating in the model at the moment? Is there a need to even have knowledge of all the customers in the model overall? (Even in a simulation, you'd have that knowledge as part of the harness, not the customers themselves.) No, I'd instead model the collection of all potential customers as an instance of another class (the catchment? the clientele?) and would avoid having public static methods in any class at all. That collection might well just be a simple wrapper around a core collection class instance, but I'd be making it a proper member of the model simply because the collection of customers is a model participant. (Well, if it isn't then you have no reason to keep a list of all customers at all.) The other advantage of working this way is that you  can then easily adapt to a growing business that opens a second shop far enough away from the first that virtually nobody goes to both shops",1
"Then install Parallels 7 again, and during the installation insure that it downloads the latest version of Parallels. Once installed, double click the VM file and the error should be gone.  If the error still exists, I think the problem is not related to Parallels, but something on your specific machine and OS specifically. After a long session with Parallels' support the solution is to drop the Network from the VM's hardware configuration, recreate it, and restore default networking configuration. The last step would be to manually load the network extension. In the past, I have gone into the System Preferences -> networks, and removed the Parallels network adapter.... Then simply running the installer for Parallels should resolve the issue.  (A reboot maybe required....) Make sure you remove all traces off Parallels apart from the VM's themselves. Then using Software Update to ensure you have the latest OSX Lion updates loaded. I suspect that something corrupted or an update has removed some of the required files for Parallels. My best suggestion would be to completely uninstall and remove Parallels 7 following the instruction at KB112189. The complete uninstall may be overkill, but it is a safe method of doing this...  (It may just take a few extra minutes...)",3
"You may find it helpful to enable the Administrator account (disabled by default).  From an elevated command prompt run the following command: I'm surprised how often this happens. There is a reason why Microsoft makes the effort to block user's access to their own drives. . . I was recently modifying the security settings for my drive C. I did not notice that i was logged in as a user and changed the permissions for users to ""read"". Later i realized that there was no admin set up on the pc (when switching user). Now with the security settings changed  I can't do anything on the computer. I can only read the files that previously existed. And this applies to every drive on my computer. Now the user accounts do not have the privileges to change the permissions. How can i allow users to have full control without having admin on my pc  What you can do now it to use another system to change the permission, e.g. a Windows PE CD or another Windows installation. You might also use some tools like ERD Commander to create an admin user. edit  Specifically, you need an admin account to take ownership of files.  Only with ownership can you grant other accounts permissions.  /edit If there's really no admin user on your computer (I don't think that is possible?), you will have no way of changing that permission back under your current Windows installation, because if you can the security model of Windows will be a complete failure (M$ had it certified, so it's probably sound). http://answers.microsoft.com/en-us/windows/forum/windows_7-performance/how-to-reset-all-user-permissions-to-default/9da312d2-c99b-4283-a275-e74d93dcc366 change windows registry to derivated 43 hex on default registry settings under the bios reconstructor, there you could get spicy and add some new windows 7 features at least the trigonal methods for registry ascii mode binary down-loaders, on windows registry bios relations.",5
"Of course this is a trivial example and I could do this calculation much more easily on the CPU without having to worry about mapping UVs to pixel centers... still, there has to be a way to actually render a full, complete [0,1] range to pixels on a rendertarget!? (I modified the pixelSize calculation by 0.001f - for smaller textures, e.g. 1D lookup tables, this doesn't work and I have to increase it to 0.01f or something bigger) That is simply beacuse you are using the texCoords from vertex output which is interpolated by hard drive after processing vertex shader. Now I know about the half pixel offset you should apply to your coordinates when rendering screen-aligned quads in Direct3D9. Let's see what kind of result I get from this: Your problem is that UVs are designed to be texture coordinates. A coordinate of 0,0 is the top left corner of the top left pixel in the texture, which is not where you normally want to read the texture. For 2D you want to read the texture in the middle of that pixel. This is certainly being caused by linear sampling.  If you look at the texels to the right and below the full-black top-left pixel, you'll see that they have 63 in the R and/or G channels, and 2 of them have 2 in B.  Now look at the muddy-dark-yellow you get; it's 31 in R and G and 1 in B.  That's definitely a result of averaging texels over a 2x2 group, so the solution is to set your texture filter to point. It looks like I have to move the right and the bottom border of my quad exactly one pixel up and to the left, compared to the first figure. Then the right column and the bottom row of pixels all should correctly get the UV coordinates right from the border of the quad: The same thing could also be achieved by expanding the UV coordinates for the geometry outside of the 0-1 range. That is you subtract the appropriate offset to get the top left pixel at 0,0 and then apply a scale to get the bottom right correct. I render to a 4x4 texture and the shader in this case simply outputs the texture coordinates (u,v) in the red and green channels: However this didn't quite work out and caused the bottom and right pixels to not render at all. I suppose these pixels' centers aren't covered by the quad anymore and thus won't get processed by the shader. If I change the pixelSize calculation by some tiny amount to make the quad a tiny amount bigger it kinda works... at least on a 4x4 texture. It doesn't work on smaller textures and I fear it subtly distorts the even distribution of uv values on bigger textures as well: I have some trouble rendering a bunch of values to a rendertarget. The values never end up in the exact range I want them to. Basically I use a fullscreen quad and a pixel shader to render to my rendertarget texture and then intend to use the texture coordinates as basis for some calculations in the shader. The texture coordinates of the quad range from (0,0) in the top left to (1,1) in the bottom right corner... the problem is, after interpolation, these values don't arrive at the pixel shader as such. Red and green have gotten brighter but still aren't correct: 223 is the maximum I get on the red or green color channel. But now, I don't even have pure black anymore, but instead a dark, yellowish gray with RGB(32,32,0)! The top left pixel is black, but I only get a relatively dark red and dark green in the other corners. Their RGB values are (191,0,0) and (0,191,0). I strongly suspect it has to do with the sampling locations of the quad: the top left pixel correctly samples the top left corner of the quad and gets (0,0) as UV coordinates, but the the other corner pixels don't sample from the other corners of the quad. I have illustrated this in the left image with the blue box representing the quad and the white dots the upper sampling coordinates. What I want to get out of it is a texture where the pixel in the top left is RGB (0,0,0) and thus black, the pixel in the top right is RGB (255,0,0) and thus a bright red and the pixel in the bottom left is RGB (0,255,0) - a bright green.",4
"I have a bunch of RSS feeds in Firefox. If you've ever subscribed to one, you'll know it creates what looks like a folder, which upon hovering, pops out to show the contents of the feed. I use Google Reader for this. I get to see all of my feeds combined in one chronological list, either newest first or oldest first. Not native to Firefox, and you would have to go to another site, but it does get the job done of a combined feed. It also has the benefit of being able to see them individually too, or in arbitrary groups, as well as search the history of every subscribed RSS feed. Is this possible? I guess the only way it could work is if RSS feeds have timestamps, do they? I've practically no knowledge of how they work. Similar to FEEDcombine is RSSMix, which I've used in the past, and can vouch for. You just give it a list of RSS feeds and it spits out a URL to a combined feed. Very convenient. I want to have it so that I simply have one ""folder"" on my bookmarks tool-bar, which, which expanded, shows them all in one - as in, if I had 3 feeds I was subscribed to when I click on the folder it will show: I use Sage to read RSS, and while it doesn't offer the functionality you want, you might be interested in filing a feature request to do so. You probably won't get this behaviour in plain firefox, but in an add-on like Sage you might have a chance. I would add  Yahoo Pipes to the List of Programs - a page where you can add, combine, filter different RRS Feeds and put it on a Page in a single Feed. if you are not sure how to do so you can serarch for pipes of other Users - and often you can copy an existing pipe for the Feeds you want to have. If it does turn out to be impossible in firefox, what's the lowest footprint software that can accomplish it?",5
"Ok- so I've tried it out. So far, so good. No noticeable performance losses yet. The one hangup I hadn't considered was that I have to manually implement what was previously taken care of by GL_CLAMP regarding texture bleeding. So I think the answer so far is ""yep, this is a good thing"". (Unless this is just a ticking time bomb waiting to bite me in the butt...) I have a very low res mobile game I'm working on that takes multiple rendering passes followed by a pass the composes them. Currently, I have a tex for each pass- each of which gets given to the shader for final composition. I want to keep # of textures used low, and I'm only able to sample from so many textures per-shader anyways (esp. for mobile). So I'm thinking each render pass would instead render to a portion of a single, bigger texture. Then the composition pass would sample the same texture from multiple places. Are there any red flags that pop out regarding this strategy? Also, note that my game is incredibly low resolution (128x256), so a single ""giant"" texture that could hold maybe 6 screens would still be pretty small at the end of the day. My biggest worry is it would totally thrash the texel cache (since every frame, every pixel samples the exact same texels for all the textures (each texture is a 1 to 1 mapping of the screen) = incredibly predictable/efficient caching. However, I really only have a very basic understanding of how that works so I could be totally wrong.",1
"It can be hard to learn writing drivers when interfacing with complicated devices and/or complicated buses. Because of that I would recommend either using some simple and well known devices (ignoring existing implementation of their drivers in the kernel) like IC/SPI devices that are usually used with microcontrollers. For example you could find any device that is ""supported"" by Arduino community (meaning there is a library/documentation for it) and try using it with RaspberryPi. You need some microcontroller programming skills for that but it's not hard to learn programming with Arduino and I believe it's useful knowledge for driver programmer anyway.  If that's not enough or you don't want to buy too much hardware, you can create one yourself. Just get some microcontroller (like atmega or something), create a program for it to become some device and then try interfacing with it using Linux drivers. This way you can easily create programs that will emulate different classes of devices. And since you will write your ""firmware"" yourself, it will help you debug problems.",1
"We finally resolved it by swapping around different negotiation methods between auto, 10/half and full, and 100/half and full, for each of our locations, until either auto or 100/full stuck. You may also want to ask your provider to temporarily remove the 13Mbps cap, to see if its an issue with their bandwidth limiting. AT&T blamed it on the switches they used (also Cisco), but wouldnt swap them for alternative models. We stopped caring as long as we stopped getting errors and 100/full worked (either by hard coding or auto negotiation). To this day, we still have some offices auto and some 100/full, just because it worked and we dont want to break it. Do the interfaces on your router show any errors? We use Cisco and we would see CRC or input errors, depending on the interface. I had a similar problem with filling the ethernet buffers on a Cisco 857 by maxing out the connections on the switchports.   The spec claims that it can shift 140Mbit, or 30k Packets per second.  So might not be that, but perhaps a beefier router could cope with the traffic? Just an idea.. But what's the speed of the fibre?  Can the backplane of the router actually shift packets at that speed?",2
"I have the same error on a table refresh - the error ""This operation is not allowed.  The operation is attempting to shift cells in a table on your worksheet."" I am not sure if it was the same problem, but I had a similar issue on a large spreadsheet where I had many rows and columns hidden.  I would try to add or delete rows or columns and got a message similar to what you had.  In many cells I had comments, I discovered that although the comments were linked to a specific cell, you could move them and they could be anyplace on the spreadsheet.  If you tried to hide, delete, or insert rows/columns that had those hidden comments you got an error message that would go off the table.  I made all the comments visible, and then moved them to a spot I was not trying to affect and no more problem. If your table is a linked table (via ODBC connection, linked to Access, etc.) that's causing the error, you can change your connection properties on the linked table to ""Insert entire rows for new data, clear unused cells"". This solved the issue for me where I had several consecutive linked queries on one worksheet.  This also frequently happens when your table has too many rows. You cannot add more rows when the table is maxxed out. Now, another column, fill 4 lines, select the COLUMN (by clicking on the column header) and make a table. Rightclick the table: you cannot insert any more rows. The root cause is the refresh is adding rows or columns in the table and detecting there's no room to do so - could be empty looking rows but have been used before?  Not certain. My result - I've converted my SS to have table on top of table (10 in total) on each worksheet - no spaces in between.  The updates now occur without error, and all rows shift around to accomodate what is needed. The fix - edit table properties (click in the table, click design tab, click properties) -- change selection at bottom OFF from ""insert cells for new data..."" and onto either of the other options, my preference is ""Overwrite existing cells..."" I was getting the same error.  ""This operation is not allowed. The operation is attempting to shift cells in a table on your worksheet."" Caution - this will overwrite any standard information in the expanded rows or columns.  However - if there is an immediate adjacent additional table - it will MOVE the table, not overwrite/remove - thanks be to some reasonably minded engineer at MS! Simple Example: make a new spreadsheet, fill 4 lines, select those, and make a table. Rightclick the table: you can easily insert a row above. Tried the suggestions, but everything looked correct.  Ended up just converting all 4 Tables to Range and then back to Table.  Don't know which Table was the problem, but it's all good now.  ;)",5
"If you need to check if x has only passed the spot, you can just do the comparison without rounding: Your feeling is right, don't round your x at every frame because you need that value as an accumulator and it should remain accurate for a consistent simulation. Instead, round the x as a temporary variable when you need to compare it to an integer.  Right now, I'm thinking that just rounding 'x' at end of each step is all I have to do to fix this, but I also have a nagging feeling that I am possibly missing something right in front of my face. One reason being that, If I round the x-coord at every frame, does that not make a lot of the adjustments made by Delta Time pointless? Is there a (preferably easy and/or concise) way to do comparisons looking for values inside a certain range of digits after the decimal? I am working on a game in Game Maker Studio: 2 and am using Delta Time for the time. I understand what it is, how to get it, and how to use it, but I am running into issues when I need to make comparisons that contain values that have been modified by Delta Time. You can also use the floor method, which will always ""round"" down, and never ""up"". This could give you a more consistent behaviour: you're not at ""specific spot on axis"" if you haven't reached it. Period. Example: an item from my game is moving along the x-axis at 2 pixels-per-sec. at 60 frames. The speed the item is moving is then modified by Delta Time, with the x-coord of the item effectively being 'x = 2 * delta'.  The ""specific spot on axis"" that I am looking for is an integer (no digits after the decimal), but when using delta time, the x-coord is always off by usually miniscule amounts due to adjusting its position relative the speed of change of each frame. Instead of 'x = 12', it is 'x = 12.00387562' or 'x = 11.99943345'.  Now, I want to check for when the x-coord of the item has reached a certain spot, i.e. 'if(x = ""specific spot on axis"") {do something}.",2
"On the server end I configure the server correctly, and then locally I use the 'ghost' rubygem to modify my hosts (well the OS X equivalent), so I can test all is correct before flipping over DNS Finally, you can setup a domain in your local hosts file, and point it to your server's IP, and use that domain to hit the server. However, I do not yet have a domain pointing at my server, so I'm not sure what I should put for the ServerName parameter.  I have tried the IP address, but when I do that, restarting Apache gives You've got a few options.  First, can you just browse to your server hostname, ie:  http://efate/ ?  If so, you're set. I might be wrong in thinking that the reason I have been unable to get this to work is related to not having a domain name.  This is the first time I have used Apache directly - any help would be most gratefully received! The versions of Ubuntu I've been using have the virtual server config files in /etc/apache/sites-enabled. I suspect that the ""It works"" page is being served from /var/www/htdocs and is specified in the /etc/apache/sites-enabled/000-default file.  Once you've edited this config file you might be able to access it just by using the machine name. I sort of had a similar problem with my rails, except that in this case, I didn't need a domain name. What I did was to setup my vhost contain on a different port all together: The Passenger installation instructions say I should add the following to my Apache configuration file - I assume this is /etc/apache2/httpd.conf. where ""efate"" is my server's host name.  But now pointing my browser at the server's IP address just gives a page saying ""It works!"" - presumably this is a default page, but I'm not sure where this is being served from.",5
"In each game tick, worker's grow a year older. I want to appreciate/depreciate the values of their physical attributes such that, for example, they increase until they are at their maximum value around age 35, and decline with age after that. What is an efficient way of defining this function? Would this involve something like running their age through a quadratic function with its maxima at the age value 35? I'm sure there's some elegant math/statistics here that's escaping me.  Instead write the value pairs in a table and linearly interpolate between two age values and grab the corresponding other value. If you have for example only 10 rows, for ages 0...90 years, with 10 year intervals (ie. age in column 1 and for example strength in column 2), the curve will have a bit of sharpness, but otoh it is easy to insert more points AND you can use for example  Newton interpolation (C code sample) to smooth between the points; this will create the nice roundness you'd get when drawing between the points with a pen, by hand. I would suggest that you do NOT try to create a mathematical formula for this, since you will sooner or later find that the curve needs special shapes - it will be very tricky to achieve these with an f(age). My workerclass has physical attributes such as physical_fitness and sexual_fitness which are intended to be functions of their age (the attribute is, unsurprisingly, called age). When the class is initialized, the initial values of XYZ_fitness are computed by a randomized normal distribution.  Above all, you'll open for further development and maintenance this way. Doing it with some polynomial formulas seems to me like harakiri. It is easy to create and adhoc-view a suitable table in Excel or OpenOffice, then copy/paste it. Or simply write it into a .txt file.",2
"It is quite frustrating as it stops me from controlling my volume from the keyboard or access other shortcuts which forces me to go out of my current session and do it manually. I also tried contacting Lenovo and looked up on their forums, but it seems they only offer to replace the whole keyboard which is not an option for me. (except my battery which may randomly switch states and turn my computer off at random times, but that's another issue.) If you need any additional information, I'll be happy to provide. I'm mainly looking for what the cause of this could be and hopefully a solution for this and if anyone else has experienced something similar before with Lenovo Thinkpads. The FN key on my Lenovo ThinkPad has been non-responsive since some time last year and is just showing this constant green light which never turn off except when the laptop is turned off. The cause of it getting ""stuck"" in such way is unknown, but I am almost certain it is not due to physical damage as everything else works perfectly fine.  I have tried various solution on the internet, like for example trying different combinations also attempted to disable the FN key entirely but nothing works.",1
"The first thing to do, before investing a bunch of time in a programming project, is to see what is already done.  Don't uselessly repeat a bunch of work (and possibly do an inferior job), only to later learn that there is a popular re-release that already accomplished your goals.  Many organizations have released source code of some popular software, including ""id Software"".  (A comment by JakeGould, mentioned earlier on this page, refers to the ""open"" source code for ""Wolfenstein 3D Engine"".  The Blake Stone game simply used a minor enhancement to that engine, so check on that engine.)  Other popular re-releases/re-makes include Descent ][ (Descent 2 @ DDN ; and D2X project and hyperlinked projects).  For WarCraft ][, there is War2.ru unofficial port/remake.  Clones may include Stratagus (WarCraft 2), or FreeDOOM.  Those are just some selections based on some games I've enjoyed (which were popular around the time of Blake Stone, or shortly after).  Many others exist, as seen by Wikipedia's list of commercial video games with available source code, and Wikipedia's list of Open Source video games. Sure.  There is nothing technically preventing you from re-writing the whole game.  Note that I'm just referring to what is possible, from a technology standpoint.  Many popular games were ""intellectual property"", so there may be some issues from a standpoint of legality. I don't know where you got that idea.  Having unique-per-game ""Copy protection"" methods seemed to be more common back in the DOS era than today. I was wondering if I can use the old files and rewrite some code (or the whole game) to create a version for modern windows to process.  I noticed that back then piracy wasn't as big of a problem as it is today so I have access to all the files I need from the original disks.",1
"So, basically, all these arbitrary limitations you're learning to deal with are because we're nursing a data structure from the 1980's along.  >smile< All this ""primary"" and ""extended"" nonsense is based on old, old, old disk partition structures from way back in the day. Basically, the master boot record (MBR -- a data structure stored on the first sector of the hard disk drive) has ""slots"" for up ""pointers"" that describe the on-disk location of up to 4 partitions. Simply put you need a primary partition in order to boot.  Extended partitions cannot be the boot partition (beacuse it's just a container to get around the old limitations Partition table can have maximum 4 records. To have more than 4 partitions several partition tables are organized into chain using extended partitions. Historically partitions described in first partition table of the chain are named primary while all other partitions are named logical. It is pretty normal to have only logical partitions without any primary partition. The only drawback is inability to boot from logical partition with ordinary BIOS. Somebody got the idea that an entry in the master boot record, instead of ""pointing"" to a partition, could ""point"" to yet another list of up to 4 partitions. This is the ""extended"" partition table.",3
"I get the feeling that you're trying to use cron as a process supervisor. (@reboot is once while * * * * * is every minute). Yes, this is absolutely possible. Cron is an important service on every system and will likely start before something like MySQL. The script only does not work if I do @reboot. Is it possible cronjob is starting before MySQL does? The custom wrapper script wait for MySQL process to become active, then execute your python script and exits. If you had to be absolutely sure that mysql process started before your script runs, you can write a shell wrapper script like this (let us call it custom_wrapper): While it might be possible to alter the order, this has the potential to screw up your system if you are not extra careful. A better alternative would be to just add a delay to your cron script so it waits long enough that MySQL will be running (e.g. 60 sec.). A crude solution is to instruct your script to sleep some time. For example, change your cron line as below: Then, from crond (or whatever configuration management system you are using, such as cfengine, puppet, chef, etc.) call /sbin/service SERVICE ensure-started   (where the ensure-started would be something added by you. I can't remember if condrestart is meant to do that or not....). PS. documentation for init-scripts can be found in /usr/share/doc/sysvinit* or similar on RHEL systems. Yes, the cron daemon could quite easily startup before mysql has completed (or even begun) its startup process. Have a init-script that starts the service and manages a lock file (/var/log/subsys/SERVICE would be best, as an init-script is expected to manage that on a RHEL box (well, at least prior to 7, not sure if that is still true in 7).",5
"At any rate, I suspect that these applications are also hurting tomcat's overall stability.  About once a week, our server stops responding entirely and is unable to serve even static pages.  After restarting the service, everything works again for another few days or so.  Looking through the logs reveals hardly any uncaught exceptions, so I'm not sure what would be causing tomcat to crash.  Nothing remarkable is listed in the error logs before the server quits responding, sadly. If serving the static pages does not require any database access, then it seems unlikely that it is a database resource issue as such. It may be that all of the pooled threads are stuck somewhere, such as waiting for the database drive or in a deadlock. The first thing I'd do is get a snapshot of the stack traces with jstack. You can further look at the process with visualvm or jconsole. My advice, there is no tomcat process left because they're all stuck waiting for a resource (maybe a database connection, or they're just an infinite loop!). The tools I listed before will definitely help you understand why your server is dying slowly every week. We are also considering switching to JBoss, as it is a bit more ""enterprisey"", but I'm not convinced it will solve these problems.  Is there any compelling reason to switch web platforms, or should I debug further within our own webapps?  Also, is it possible for the webapp to crash the application server by doing something bad? If locking is the problem, you will want to see if changing the storage engine on the problem tables from MyISAM to InnoDB is feasible. We've got a number of web services running through Tomcat which use hibernate/mysql.  I suspect that some of them have incorrectly configured connection pools, as after several hours some of the individual apps run out of connections and stop responding.  We have been making changes to the connection pool service (in this case, C3P0), but we still need to leave the old versions of the applications on the server for backwards compatibility. Just wanted to add that it's quite common for table locking issues with MyISAM tables can easily cause DB connections to pile up and cause the application waiting for those results to sit and sit. If you install the lambda probe webapp (get the 1.7 beta) you can get thread-level monitoring; keeping an eye on this will tell you when threads are stuck waiting for the database, as well as a host of other useful diagnostics. You may want to check out the MySQL process list to see if there are a lot of queries sitting in a locked state.",5
"Another sign to change something is if you take a good hard look and if you notice that you are creating a ""DevOps silo"", in which all DevOps know-how is concentrated in those two guys/gals, and everybody else just leans back because those two are ""doing DevOps"". That is not the point of DevOps. If this is the case, think about the cultural aspect, and modify them to be more evangelists/teachers/coaches for the other teams. In both cases, the deeper reason of why having DevOps in the first place is a good thing (the general Good Stuff) should be clear to the upper management. If you cannot bring that message across, then scale down the work which your team is doing, by shifting it onto the regular Devs/Ops (as should be the case, anyways). In this case, a sign of shortage is simply too much workload for those two; too many projects requesting their services; too many tickets; overtime; stress, burnout. These factors should be reasons enough for a responsible leadership to add more capacity. I don't see a DevOps specific sign in this, it's just a function that is understaffed. I assume this team of two is going from project to project and establishing DevOps stuff there (creating CI/CD pipelines, supporting the other devs creating Dockerfiles, or whatever technology you are using). In other words, type 3, 4, 5 or 6 as per http://web.devopstopologies.com/ .",1
"It is quite possible to domain a linux machine. Samba has supported this for quite some time. Some Linux distributions have a wizard that'll guide you through the process. You'll need some information to make it work, such as the name of at least one Domain Controller, and a domain username for the domaining process itself.  there is a requirement to be in windows domain in our network. It will be monitored. Up till now I just got ip from dhcp without much worrying. What do I have to do in order to have my linux station in domain. Or is it possible without? And not to be discovered? Thank you very much. All that being said you are likely to be monitored using WMI calls- which will fail on a linux machine Linux systems can authenticate against a domain but cannot participate in the functions provided by active directory management. For most current distros there is a kerberos wizard.  Windbind (part of samba) allows linux to see AD users and groups as unix user and groups.  Samba provides file and print services for all clients using the SMB/CIFS protocol.",3
"You can extend on tic-tac-toe to have a computer opponent, which would introduce you to very basic AI (even if the computer just places its piece on random squares) So, I was thinking about working in some place else, while I study it. What I really want to know is, a good place to learn, and a country that developers are well payed to be able to pay my course and still have money to do something else. After you're comfortable with 2D, start making simple basic 3D games. It doesn't have to be the next big shooter, or an MMORPG, it could potentially be 3D versions of the 2D games you've already made. DON'T get caught up in all this talk of game engines and C++ and don't try and get ahead of yourself. When you're too ambitious when starting out, you end up getting stuck on a problem, get frustrated and give up on it. Once you've made very some simple games like that, move on to 2D graphics. I'm sure you can find OpenGL wrappers for Java and use that. You could make Pong or (slightly harder) Tetris. Personally I learned with this book: Sams Teach Yourself C++ in 24 Hours. it's in 24 chapters, and you can get through it in a week or two and have a good foundational knowledge of C++. As for countries to learn game development, the United States, Canada and Europe have universities or art institutes that provide programs focusing on game development; I don't know any off the top of my head, but you can do a search for them. Get some books on what you're interested in (Game Coding Complete is a good book that introduces all of the fundamentals you need to make games. Amazon Link: Game Coding Complete). Of course when you do get stuck, you've got sites like this to help and places like www.gamedev.net. Firstly, I would start very small. If you know Java, then use it. Don't worry about people saying you need to use C++, there's plenty of time that later. If you're comfortable with AWT or Swing, use it, rather than looking up OpenGL libraries. For starters, you'll want to learn to program in general. Once you know how to write code, you can work on applying ti to games specifically. I would recommend learning C++, some think it's harder to learn than others, but I personally didn't find it too daunting and it is something you will need to be proficient in eventually. Personally, I think this site is best for specific game programming questions that you have. Since learning game development is quite a broad subject, there are plenty of varying opinions on it. The only way to learn and progress is to keep making games. You can use bits of reusable code in your later projects, but focus on making a playable game before you start refactoring code and optimizing. I really think that the best way to learn is to try it yourself and learn from your mistakes (this is why you should make a mall game first). Once you finish you'll have a learned a lot and will have a whole lot of ideas on what to do better next time, so, make another simple game. Start by making a simple word game (E.g. Hangman, Word Jumbles, etc). This will give you experience on the fundamentals of game play. Then a tic-tac-toe game, this will introduce you to very basic graphics. You could also use C# and XNA for this. C# is very similar to Java and XNA is a fairly all-inclusive API for making both 2D and 3D games. I don't want to be a downer, but I think that before you have learned game development and built up a portfolio, it doesn't really make much sense trying to think about what country has good game companies, because they will most likely not even look at you without proof of experience. Thus, my recommendation would be to start building up your portfolio. If you still want to learn from an educational institution though, I would recommend in no particular order: I'm brazilian and I want to learn how to develop a game in some college or something like that, but I don't know any place here to learn it. Here is not that good either to develop games, as we don't have many companies to do that. Next, I would try to make a game. A small game. Make Pong, Tetris, Tic Tac Toe or something similar. Focus on making a complete game. Try to figure out everything that the game will need, graphics sound, pause menu, controls, etc, and do everything. I wouldn't completely re-invent the wheel, try using an open source library, I think SDL is great.  Sloperama - good, sound advice, geared more towards the business and design end but stil very helpful. I don't know if you have thought about it, but I wanted to make sure that you are aware that just moving to another country is not that easy. For someone that has had the privilege of studying and working in the U.S. for around 9 years before moving back to my country - Bahrain - there are a few points I wanted you to be aware of: Don't get too ambitious, make something small and simple, and focus on finishing it. Here's the thing, you will probably do a lot of things wrong, that's OK. Don't worry about getting every little thing right or making it seem professional or polished, just finish it.",4
"The problem is... The OS can answer only on the base of what the device itself answered. Usually both the device firmware and the OS drivers are closed source, so you have to rely on their documentation and on the License Agreements. The problem is that both sides are often valued considering their performances. They may rely on the fact that between the go-ahead of the system and the average human reaction time needed to unplug the device there usually is enough time for the modern devices to finish the internal operations in progress.   To your precise question, no, it is not safe until the blinking stops. Normally I do not see any blinking, but sometimes (not often) I do and wait. That works.  Moreover, the responsibility of each side is limited by the License terms. Limitations written to safeguard the company as much as it is possible, even if they knew or should have known about the possibility of the damages (see below for Windows 10, but you can find similar for the devices producers). In my personal experience only a few times in several years (and specifically not with windows 10) I remember a data loss or file system corruption if removed quickly immediately after the go-ahead of the system. But it only happened with a second person ready to remove the USB and run away (immediately) after the go-ahead of the system and when writing operations were required at the very last moment. You ask to remove this drive from the system. The OS stops/refuses the new I/O operation requests from its programs, waits the ending of the current I/O operations and checks if some lock is active on the device. When the device communicates that all the I/O operations are ended the OS will release the device declaring that you can safely remove the device. So, since they discharge the consequences in advance and are valuated on their performances, it is up to you to decide to what extent you can trust them and you will feel safe.  The blinking indicates some activities in the device, or at least an hardware (not OS related) communication to the user. The blinking of some Seagate models is an error code [avid] communication. You may check if, with your model, that way of blinking (short or long pulses) has a special meaning. Usually 8 short pulses mean the byte zero, and 0 is the exit code of the program with no error at all...  Premise. In general the scope of the question is so wide that there cannot be other single universally valid answers: too many models and brands may be involved. You must refer to both the manual of your HDD and the instructions of the operating system and drivers, consider the most restrictive and yet you will not be able to be absolutely safe.",2
"So I have multiple hard drives encrypted with the same password and instead of having to enter it multiple times I'd like to have it set up so that after booting I automatically get a password-box displayed into which I enter the password once and which then mounts all the encrypted hard drives and securely removes the password from cache/memory afterwards. I'm not sure how secure this is but better than using the --password option since that shows up in ps. Can this be done somehow? If it's not possible via the GUI maybe via a script? I'm using Debian 9.1 with KDE. In TrueCrypt this WAS possible. If, for instance, 4 volumes are loaded at logon and these volumes have the same password, then the password only needs to be filled out once in the text box. Obviously this feature has delibarately been removed with VeraCrypt. I do not understand why, because if you want to avoid this behaviour you can give your volumes different pass words. There is a button available in the UI, or the following command will ask for the password/keyfiles/PIM once, and automatically mount multiple volumes, if they exist:",4
"I suppose that XNA vs the world ( aka OpenGL and DirectX as real alternatives ) can easily look like a religious question especially when targeting a new developer, but i find that the right adjective for XNA is simplified and not powerful. I should also remark the fact that OpenGL is a rendering technology when DirectX is a complete framework oriented to all the main aspect about multimedia, but the thing is that today there are frameworks available for every purpose that you have in mind, most of them are free and really well tested, with an history of several years of testing and they are really reliable, they are also cross-platform and with a really good license most of times if not in every case. Especially as a hobbyist developing for yourself, if you want to develop games in managed code, C# and XNA will serve you very well.  You will be able to accomplish everything that you could at a baser level of coding, and you will save countless headaches. The ANX.Framework is a framework which is source compatible with Microsoft's XNA framework 4.0. Source compatibility means, that you can ""translate"" a XNA game project to be a ANX game project by simply replacing all namespaces from Microsoft.XNA.Framework to ANX.Framework. The advantage of ANX is simply that you are able to swap the RenderSystem, the InputSystem and the AudioSystem. By swapping this systems you are no more limited to run your game using DirectX9 which XNA is using. ANX comes with a DirectX10 RenderSystem as a default. A DirectX 11, DirectX 11.1 and a OpenGL 3 RenderSystem is currently in development. This will make it possible to run your games on Linux and other plattforms which are supported by OpenGL etc. simply by swapping the namespaces.  The last part about what framework you are supposed to learn, again, i will go for the most business friendly and the best choice as an investment these days is OpenGL with a programmable pipeline approach and this means OpenGL 3.0 and above. You have also to consider one aspect both C# and XNA ( or DirectX ) are technologies that are available only on Microsoft platform, this is important to stress, my point is that today Microsoft is not a big player in the industry, if you consider on how many platform you can make games, including desktop and mobile, you will easily realize that Microsoft is simply struggling in the market and more and more Microsoft will see its own market share resizing to a smaller size. If you ask me the language for you is the C++ because learning C# or C++ is the same thing for a newbie, it's always a new language, but what the C++ can offer the C# can only dream about it; you also will develop a real know-how that you can spent on as many platform as you want and on all levels, from small games to serious development. You may want to check out ANX, it wont let you publih on WindowsPhone and XBOX360, but it opens Linux, MacOS, PSVita and Widonw8Metro (or ModernUIStyle) to you. Good thing is you may use XNA documentation and samples to get yourself started. XNA is a subset, someone can argue that, but in the end is just a subset of DirectX, this framework is well suited for small project and not for serious gaming. As soon as the OEMs will stop installing Windows on their machines, and most of them already do that in practice, just see the market share percentage of Windows Phone, the entire Microsoft ecosystem will collapse and just looking at what is now, it's not that good anymore. Look at the marketplace, Microsoft has a marketplace by years until now and there is absolutely nothing, it's empty compared to what you can find on the marketplaces of the competitors. Look at the technologies, C# lives thanks to the web development, if it was for Windows Phone 8 development it were dead by years; Silverlight is a big flop, find 1 technology that Microsoft has introduced recently and see how good is performing on the market, even Internet Explorer have lost its throne. I personally believe that C# and DirectX/XNA will have a really hard future, i think that during the lifetime cycle of Windows 8 many many people will realize how bad their choice was about the language to adopt as main language, C++ is much more suitable for the performance, for the business and for cross-platform compatibility. I would recommend trying out those resources that seem to only applicable to windows phone or xbox.  Nearly all the xbox example projects provide pc versions, all of which have liscensing open to free use.  Look at this search list:  http://xbox.create.msdn.com/en-US/education/catalog/?contenttype=4&devarea=0&platform=0&sort=2.  Also, XNA generally performs better on the PC than on the xbox, because the second stage of the CLI peforms inlining.  (And the infinitely better garbage collector doesn't hurt, either.) Most of the resources for XNA seem to revolve around XBOX or Windows Phone. I have successfully built some prototypes for Windows but am not sure what the limitations are in terms of distribution, etc. I'm new to game development and want to implement a few ideas I've had as a hobby over the next few months. I want to use only C# as the development language for 2D and very light weight 3D games. Yes.  It certainly is, despite naysaying advocates of Linux, Mac, OpenGL, and C++.  They tend to ignore the main criterion of your question: ""on Windows"". no  limitations that i'm aware of although you will need to access MonoGame if you want to sell on the windows 8 store I know that most of the people that will read this, especially advanced and senior developers, will get this as a ranting against Microsoft, but when even the founder doesn't believe in its own Corporation that is performing so badly before an important launch, that is losing important assets and presence on the market on every field, i think that you just have to consider the facts.",5
"I personally do the OS deploy with a stock WIM image, inject the drivers and make any other customizations (such as making a copy of the install image on the local drive) via MDT and deploy the software stack via EminentWare.  To integrate the two use the boot WIM image created by MDT as a boot option on your WDS server. You can use the WIM created by the capture process at that point as part of a scripted install.  My personal preference is to totally avoid using captured WIM images. While they work I have found the process to be more trouble than it is worth - fighting with chronically outdated images is a royal pain in the butt.  For us, the ability to keep applications separate from the OS image has made management of applications and images MUCH easier, but if you're in a situation where you have identical configurations for ALL machines and only do image rebuilds on an infrequent basis, then rolling it all together would be a better option for you. Having followed a whole bunch of documentation, tutorials and white-papers etc, it seems that MDT will always go through the full processes of Before you capture your WIM, configure your reference computer with whatever applications you want to have pre-installed.  When the WIM is applied to a target, all that is left to do is join it to the domain and apply any updates. With that said, the beauty of using MDT (and by extension, SCCM) is that you can create separate application packages that can be updated independently of the OS.  Adding/removing/updating applications doesn't require a new image build every time.  In exchange, you increase the time it takes to deploy an image to a target. Whereas what we need is a simple ""take image of reference machine"" followed by ""clone image onto target"".  This should be much faster than the full installation routines followed by MDT. Personally we've had times where things go wonky or not quite right, so the bare simplest way we use WDS is this: Should I be doing something else with the captured WIM from MDT?  Can I simply put it into WDS somehow as a ""ready to copy"" image? There are ways of scripting the installations and post-installs right down to having your systems pre-injected in AD and auto-join and everything. So there's ways this could be improved but this method meets our needs and keeps it flexible enough to be usable, while not complicated enough to end up swearing in front of the kids when something decides to not cooperate at the last minute. It takes more overhead in managing the images; they get outdated, etc. and we're not scripting answer scripts for everything. We mainly use it for the occasional lab or deployment to get a decent base installation, then run the smaller post-image updates and join it to the domain.",4
"If you're looking at a one-time/one-off/adhoc backup that you plan to use in the short term for recovery, then it's less an issue of reliability and more a question of convenience.   Assuming that you're looking at a corporate server backup solution (e.g. ongoing backups, some media rotation schedule and some retention period requirement for each backup), it's still less of an issue of reliability (since you'll assumingly have at least a daily backup) and more one of convenience.  So assuming your backup process is rigorous (done according to a schedule and verified for errors) and frequent, I see no issue taking advantage of the larger capacity of Blu-ray disks. I saw some longevity tests from a year or so back when this first became an option that suggested that Blu-ray disks are about as reliable as regular DVD - certainly not significantly better or worse. I'll try to find the article and update this post. Under no circumstances though would I rely on any optical media for long term storage.  For long term, tape will be most reliable.  To really reduce risk of long term backup storage and avoid restore failures I think it's important to have multiple backups stored in different locations. I have dvd backups from 2000 still going strong, over time these have definitely been the most reliable of all the different methods I have tried. Whatever method you choose, if you have a backup of a backup, in theory you should never lose your data. Guess it all boils down to good management. Making backups is really a game of probability.  Assuming the data is successfully written to any media (as confirmed by the backup program's ""verify backup"" function), the weak link becomes the shelf life/survivability of the media.  Backup tapes can break and be demagnetized.  Hard disks can crash.  Optical media (like DVDs and Bluray disks) degrade over time.  I view the question of ""Is it safe to back up to media X?"" less of a yes or no question and more one of your goals and retention requirements. I would be interested to see how much more or less reliable the actual burning process is for Blu-Ray vs DVD.  That would be an expensive coaster.  Of course, you'd need to get quality media and hardware for burning.",4
"In the Algorithms in the Field workshop a month ago in DIMACS, Graham Cormode was arguing in favor of teaching sketching techniques from streaming algorithms to undergraduates. Moses Charikar said that they do teach them in Princeton, I think @Suresh Venkat also mentioned he teaches things like the Misra-Gries algorithm for heavy hitters. I think some basic streaming results would be great for high school students too: they rely on basic but important math tricks, the problem formulations are like puzzles, and the solutions feel like magic, and magic is a great way to inspire high school students. You can make sure to emphasize the dramatic difference between the scale of the problem and the amount of resources you can use. A silly example: suppose that you can ask every person entering or leaving JFK airport their zipcode. Can you keep track how many different zipcodes have been represented in JFK during month, only using a notepad? One of the most appealing aspects TCS is how it uses abstract mathematical ideas for day-to-day practical applications.  A presentation can focus on the abstract ideas that lie one step behind what they see daily on the Internet:  Shortest paths becomes exciting once they are put in the context of friends-of-friends on Facebook.  More graph algorithms can ride on Pagerank; Amazon recommendations raise the challenge of machine learning; and buying-stuff on the Internet is certainly a good lead for the public-key crypto.   In my opinion, to be sexy to high school students you need to be some kind of magician. That's why I think that randomized algorithms are very good as a student attractor. For instance property testing is really something intriguing, and also something that can be explained (not the technicalities, but the idea) to anyone.  One protocol is the following. Charlie puts a ball in each hand, then chooses to either switch the two balls behind him, or not. Then he presents the two balls again. If you can always detect whether he switched the two balls or not, then Charlie is increasingly convinced that you can tell the difference between them. If Charlie does this shuffle at random and you really can't tell the difference between the colors, then you will only guess correctly with probability $1/2$. After $k$ trials, Charlie should be convinced that you can tell the difference with probability at least $1-1/2^k$. A good source for education purposes in general is CS unplugged, which has lots of neat CS ideas translated into high-school and middle-school activities. Now while Charlie becomes increasingly convinced that you can tell the difference, he frustratingly never learns which ball is red and which one is green. There is a neat way to introduce zero-knowledge proofs to students, which I think is originally due to Oded Goldreich (please correct me if I'm wrong).  You have a red ball and a green ball, which poor colorblind Charlie believes are the same color. You want to convince Charlie that you can tell the difference between the red ball and green ball, and you want to do this in a way that Charlie does not learn which is red and which is green. (You want to prove something is true, in such a way that no one else can turn around and claim a proof of that something as their own.) How can you do this? Or is it impossible?",4
"Instead of replicating state in each new node you could just keep state differences between a node and its parent in the tree. By traversing the tree from the root node to a node at depth n you can calculate the new state at n. My first try was to determine all allowed actions and generate a new game object for each action the player or computer could do. Then I would repeat the process for all resulting game objects. This would yield a form of tree with all the future possibilities, from which the AI simply could select the optimum. It would be useful to know which language you're using, as memory management changes a lot. Some general ideas: Any help is appreciated. If this question is more appropriate for somewhere else (for example StackOverflow), please move it there. Thanks! But it turns out that this approach is cruelly slow, mostly to me deep copying the game object thousands of times and holding thousands of copies of the game object in the memory. If the cost of copying is bogging you down you have a lot of game objects and thus the tree you are describing will grow very fast.  I doubt you are going to get enough depth to the tree to be of much value here. While there have been several answers focusing on the question you asked I think you're barking up the wrong tree here. I am now searching for a better approach to calculate a good move. Are there technics for this type of problem. I am aware that this probably depends on the kind of choices the player or AI has to make in the game. But I am more interested in general technics used to tackle this kind of problem. Mem-copy large blocks of data, rather than calling copy constructors for each one. Much easier to run in parallel and you shouldn't run into any cache misses that way.  Exhaustive trees are only an option in games where the choices are simple.  After all, even chess has a tree that is beyond computation. Suppose in a turn-based game, I have an object holding the current status of the game (like players, board information, positions of pawns, things like that). Now I want my AI to calculate the best move (or at least a good move).",5
"I'm going with typo in the vhost's ServerName, or else the machine's ServerName points to www.site1.com and that's overriding the vhost's definition. I have two domains, site1.com and site2.com for the purpose of this post, pointing to this one server. The www subdomain is a CNAME to the main domain. Pinging all four entries (the two domains with and without the www subdomain) all point to the correct IP address. Having enabled both virtual hosts, and restarting Apache, site1.com, site2.com and www.site2.com all point to their correct sites. However for some annoying reason www.site1.com points to the default site (/etc/apache2/sites-available/default) for non-matching virtual hosts. Can anyone explain why this is happening? I've checked and re-checked the configs, logs, directories and even re-built the server cleanly numerous times and fail to understand Apache's behaviour in this example. I know it might be a late response .. but maybe you should check if you have the NameVirtualHost Directive Set up .. if you have too many Virtual Hosts with it set, Apache will override to either the last one in the list .. or give you an error and not start the service at all ..",3
"I'd recommend instead trying to script a way to change the password on 900 servers.  Obviously, I don't know anything about the application in question or where it stores the password, but it's more likely to succeed than trying to get the old password.     I need to know the password of an User ID in Active directory without resetting it. Is this possible? If so how?  There's more information in this old Server Fault question, but... no.  I wouldn't recommend attempting it. I also wouldn't recommend attempting to brute-force the password (basically what strongline is recommending).  Depending on how you do it, you might end up offending your IDS.  If you do it without written permission from your boss it could go badly for you.   Surely you could just check the password used by the 900 applications in order to determine what it is? Have you checked if theyre stored in plain text? E.g web.config sql connection string?  Also, could you not setup a new user and password and deploy it to the 900 applications? I know if I had 900 applications running with hard coded user details I would have a deployment system setup.. Unfortunately no. As a desperate measure, you can trial and error, hoping you can find it back... use ""runas /user:domain\id cmd.exe"" command referencing this account, a correct pwd will let you open a new command prompt. Unfortunately, as strongline says, that would be no.  Passwords are stored with non-reversible encryption.  I understand that you feel like you have a use case for this feature, but it would be a bad thing for security.   Resetting password is not an option because almost 900 servers have the current password saved in their application. Changing password will fail that application in all those servers.",4
"The first step would be to try uninstalling the nVidia driver and software: Right-click Start and select Programs and Features. Find the nVidia software in the list, uninstall it, restart the computer. Windows 10 will install the missing drivers after you restart. Test by watching YouTube. It sounds like you're encountering the error when Windows tries to switch from the integrated Intel GPU to the dedicated nVidia GPU. The issue may have originally been caused by an automatic Windows 10 update to either graphics card driver (you can check this in Start > Settings > Update and Security > Advanced Options > View Update History). If that doesn't fix the issue, you may wish to visit the Lenovo support site and download the latest nVidia drivers for your laptop and install them. However, they may be an older version than what Windows 10 had installed...but at least they're stable. You may also need to (less likely) do the same for the Intel video driver. If you run into difficulties installing the Lenovo drivers and/or the problem reoccurs, check the Windows update history (as above) because you may need to disable the automatic update of that driver by Windows 10. I have a Lenovo Y50-70 with a NVIDIA GTX 960m and lately I have been experiencing this weird error ""Display driver has stopped responding and has recovered error"". Usually it happens when browsing youtube and watching videos, but sometimes while coding in Visual Studio as well just scrolling down a class and the screen freezes for a second and then I get this error. What can I do? I've heard of using MSI Afterburner but I am not sure what I am supposed to edit?",2
"In scale, unless you are expecting to receive only a particular format, it is machine learning. For the first task, you should first parse the text and then scan it, probably with a Named Entity Recognition (NER) system to extract the information you are after. Having a NER system would work, as you can manually code different types of features that will greatly improve the performance. If you just want to perform candidate matching, then standard bag-of-word approaches would perform decently.  On the other hand, regex can be great ways to go with, especially if you can predict/adapt to the variability of the incoming data. In any case, they can be used to create your first training data.  For the second case, things are similar. You can rely in some syntactic analysis of the sentence to obtain the invitation and the time/day of the proposal. This again can be coupled with NER systems. Lately, for both tasks neural networks yielded promising approaches. But, in any case, you need labeled data, which can be cumbersome to obtain.",1
"Let's say we have two processes: Process A, and Process B.  Process B depends on the results from Process A, so there is a relationship that needs to be defined between a Process B run and a Process A run.  This is how the relationship is defined: I see three principal disadvantages of using GUIDs as database keys, and would avoid them unless you have some compelling reason to use them.  Note that the globally unique property doesn't get you any advantages over using a key that is locally unique within a table, because you already know what table the data came from. I recently learned about how relationships are defined in the database at work, and was wondering if this is a standard practice. 3. Size: Less of an issue these days, but a GUID is 16 bytes long.  It takes up more space in the table and any indexes that include it. Inconvenience: It's not terribly easy to type in a GUID if you want to do an ad-hoc query of the database, for example: My question is, how standard is this?  I was brought up on the idea that each table should simply have an autoincrementing id that is unique just to the table and not the entire database. As an aside to your question I would strongly suggest not using GUIDs as primary or surrogate keys as it takes up 4x the space of an int. Now in the days of multi gigabyte disks you might says it's not important but when you have a few million rows of data it still takes resources to read from disk or transfer across a network, Even more so if they get included in any indexes. While I'm on the subject indexes, GUIDs are bad in primary keys due to there random nature and usually causes significant fragmentation.  In principle having unique Ids across a database isn't too bad but its rather unusual in my experience. Unless there is a specific requirement for maintaining a globally unique Id keys, using the normal identity should be fine as the entity types or objects tend to be type specific and doesn't usually result in any inappropriate joins. Now, up to this point, things make sense to me, but then things get a little strange to me and my understanding of table design.  Whenever a row is created in TableProcessA or TableProcessB, a function is called that creates a globally unique id for each.  So basically, all of the Id fields in TableProcessA and TableProcessB will not contain any matches because the Id's are not just unique to its table, but to the entire database.   Your processes can create records that contain the GUID as a reference to the process instance, but using the GUIDs as a key is not necessarily optimal. 2. Natural ordinality: An autoincrementing primary key has he side effect of naturally ordering your transactions in the order they were entered into the system.  This is often quite useful.  GUIDs are not normally generated in order, so they are of no use in this regard.",3
"I'm going to answer no - it is not possible.  There appear to be similar questions on Logitech's site so you can search there as well.  I suspect the answer is that when you pair, the transceiver and the device exchange several bytes of random data and form a key to both allows the devices to recognize each other and to encrypt their wireless transmission. I have seen some post on superuser regarding softare such as input director or synergy but thought I would see if anyone has hardware solution first. The problem is that the transceiver may have the ability to recognize something like 5 devices, but the input devices can only recognize and pair to one transceiver.  That's because the transceiver only has to sit there and accept a signal it recognizes, otherwise it is passive.  Sit, if you see a signal with the correct encryption and ID, accept input, otherwise ignore. If you added the ability to have the input devices (mouse, keyboard...) pair to multiple transceivers, you would require some sort of additional buttons on the device to switch between them.  You'd also need to have the input device store the state (caps lock, num lock...) it was in for each transceiver. This can be achieved easily using a $6 USB manual switcher...That is sharing a single keyboard/Mouse etc via a single Unifying receiver, you need a power USB hub as well I have a mouse and keyboard with the Logitech Unifying receiver.  I have two of those receivers, one in my desktop and one in my laptop.  It is quite a pain to actually unpair and repair the mouse+keyboard from one PC to the other.  (I think logitech did this on purpose so you would have to buy a second mouse+keyboard, who knows) . The only Logitech device that can do this is the new K760 keyboard for bluetooth.  None of the Unify devices appear able to do this. In any event is there some hardware method of resolving this or some ""hack"" to be able to easily switch from one unifying receiver to the next quickly? I had been successfully using a KVM switch to pair 3 unifying devices into one unifying receiver.  There are one type of KVM switch technology (USB DDM - Dynamic Device Mapping) can pair multi unifying receivers and share unifying keyboard or mouse to many systems.",4
"Whether you choose to report such unsuccessful attempts really depends upon how much time you have upon your hands. The accepted convention is for network operators to operate an abuse@ address on their domain and detail this, plus any procedural information, within the inetnum entry of their RIR for any address ranges which they are responsible. I'd say the vast majority of these ""botnets"" are used to send spam and run scams, the volume of which is entirely overwhelming. Your response depends a lot on the vector they were using. For example, I had a VM on Rackspace Cloud that was being constantly hit with SSH attempts. So, I added some IPTables rules to only allow SSH from my IP's. Now, if the vector was web serving that you want everyone to see, obviously restricting it would not be the answer. The reality is that most attempts will be generated by distributed and automated utilities operating from networks which simply won't follow up abuse reports. If you're feeling particularly vigilant then you can send reports. But chances are, unless the IP resolves to a well known organisation, your time would be better off spent protecting yourself. The appropriate response is to make sure that you're not vulnerable (which is more than likely if you can actually detect the attacks), and if at all possible, restrict access to only a few IPs of those people who are allowed to have access in the first place. I can't even count the number of times our servers receive hacking attempts per day. It's basically a normal part of incoming traffic on the internet these days. Not only can you expect this traffic to be coming from compromised home computers, but you can expect entire networks of them working together just like SETI@Home, except bigger, and on an involuntary basis. You need to audit any change to your files, audit your code, make sure you sanitize any inputs. you can limit the risks by running your http server chrooted/jailed or/and use a IDS plugged in a monitor port of your switch. You can retrieve this information from the relevant RIR by performing a whois against the would-be attacker's IP address. Don't trust the reverse-DNS of the IP because it can be easily spoofed to an unrelated domain or hostname.",4
"Some file formats (such as PDFs) have data in them that allow you to determine if the file is complete.  But you have to open and read pretty much the entire file to find out. One alternative I was thinking of was to have the file be copied as a different file extension (like .tar.gz.part) and then renamed to .tar.gz after the transfer is complete. But I figured I'd try to figure out if there is simply a way to determine if the file is whole at the command line first... Any clues? Using inotifywait can achieve what you're after - it has the capability to wait until a file write has finished before executing a command. lsof will just tell you the file is no longer open - it won't tell you why it's no longer open.  Nor will it tell you how big the file is supposed to be. Because determining that the file is closed is not the same as determining if the file is whole.  For example, a file will get ""closed"" if the connection is lost partway through the transfer. The problem that I am expecting to run into: If it takes > 1 minute for the tar file to be copied to the server, and the cron script runs once every minute, it's going to see the .tar.gz file and try to do untar it, even though the tar file is still in the process of being written to. But all methods have to rely on the sender somehow signalling that the transfer has completed successfully.  Because only the sender has that information. That's a perfectly fine way to communicate that the file has been fully and successfully transferred.  You can also move files from one directory to another as long as you stay within the same filesystem.  Or have the sender send an empty filename.done file to signal completion. You are on the right track, renaming the file is an atomic operation, so performing the rename after upload is simple, elegant and not error prone. Another approach I can think of is to use lsof | grep filename.tar.gz to check if the file is being accessed by another process. The tar files are automatically copied to this server over SSH from another server. In some cases, the tar files are extremely large, with lots of files. Is there any way (via bash commands) to test if a file is currently being written to, or if it's only a partial file, etc? I need to deploy an automated process (via 1 min cron script) that looks for tar files in a specific directory. If a tar file is found, it is untarred to the appropriate location and then the tar file is deleted. The following will continuously watch a folder for new files and execute the command in the loop when writing to the file has finished.",4
"Edit: It's currently charging and it did something similar: the charge jumped from 2% to 7%, then it started increasing steadily as one would expect. I just noticed a few days ago that Windows 7 hardly refreshes the battery charge (or current capacity, expressed in percentage) as the battery is discharging or charging. I had it jump from 100% to 80% then to 20%; today it really bit me as it jumped directly from 96% to 0% - as in Windows didn't even had the chance to notice the battery was almost dead, so the laptop ran out of power (I forgot to plug it in to AC). If the battery is relatively new (i.e. < 1yr) then it could be a faulty charging circuit causing the odd readout. My old HP Compaq nx6325 was doing the same thing. I used this tool from hp to test the battery. This tool shows you if your battery is ok, defect or needs to be replaced soon. Mine failed... but, after replacement, everything is working as expected. What is the problem here? The BIOS has no option even remotely about the battery and the diagnostic options only have memory and HDD tests; the log is totally empty. I'll give HP a call, but I'm not looking forward to it, as the customer service is horrible, at least in my country. I use the free BatteryBar utility instead of Windows own battery indicator, which I first found through Gizmo's Freeware, and posted about on my blog Steve's BPCA Blog back in April 2010.  How old is the laptop? As the battery ages the available runtime lessens but also the reported value becomes less accurate to the point where my old laptop always registers as 0% charged. Your battery is probably just not calibrated right. The battery after a lot of usage forgets what is 100% and what is 0%. What you want to use is to charge the battery until it's full and then keep it plugged in for a couple of hours to establish 100%, then disable the critical and low battery functions until the battery dies to establish 0%. Then recharge.",5
"on windows now, though, so i can use it for gaming when i'm not developing.  but i can say for cuda development opensuse worked great. Looking at the CUDA website I would try getting hold of one of those distros - including the version specified. Of course, when you want to install a newer version of something you get into a whole new world of pain... I tend to think that the best distro for this sort of thing is that which the developers of the thing use: any instructions and downloads are far more likely to work on the distro they were written for than on any other. I would use Arch Linux since it comes with the very latest stuff and you can install CUDA-SDK/everything you need from AUR directly. (Also NVidia driver is directly avaliable in the repository.) But yeah you can use anything if you build up there the environment.   i used OpenSuse and it worked pretty smoothly and was easy to set up.  extra benefit is OpenSuse is a pretty nice desktop environment w/nice package management tools that doesn't suffer from limited functionality such as ubuntu's over-stripped down-ness (and brown? really?).",3
"which would also do just fine, especially if the SQL thread is busy and you do not want interrupt it. If your monitoring has the same time granularity as the MySQL timeout values, you have nothing to alert you when it does happen. You would have to poll MySQL more frequently. As an alternative, you could probably create some kind of SNMP setup to monitor MySQL so if SNMP info from does not update in a timely manner you could detect MySQL being down or not responding without ever connecting to MySQL. Given that scenario (that I eyewitnessed between two Amazon EC2 servers in two different AZs (Availability Zones)), the solution back then was to check the Security Groups and get port 3306 open in the Slave's AZ. Why talk about the network like this ??? You can be victimized in the form of data drift. Back on Jun 17, 2014, I answered the post I have been tasked with Mysql Master-Master replication?. I briefly mentioned the network as an unsung hero in data drift: The connection between Master and Slave requires that the firewall be open. Unfortunately, I have seen occasions where the firewall was open on the Master and a Slave would connect as usual. The Slave would have the I/O thread show up in the processlist like nothing was wrong. The Master would do the same. All of a sudden, 60 seconds later, the I/O thread disappears from the processlist of the Master, but remains visible on the Slave. Look into your max_allowed_packet setting. Many times in the DBA StackExchange I have affectionately called the MySQL Packet the Silent Killer of DB Connections. The I/O thread is just as much as DB Connection as any other. I would make sure max_allowed_packet is always set to 1073741824 (which is 1G). Based on these paragraphs and the default value for slave_net_timeout (60 Seconds), it appears that the I/O thread should heartbeat every 30 seconds. You could change the heartbeat period to 10 seconds like this: You running STOP SLAVE; and START SLAVE does not find the root cause but does indeed solve the problem at hand. How ??? All this does is disconnect both the I/O and SQL Threads and then reconnect from scratch.",1
"If you can get into the ISP Archer modem/router, you should be able to configure it to only act as the gateway. But I'm also wondering since you note having your own DSL modem/router if you've considered having that be the gateway as well?  Most ISPs will let you use your own.  They usually have a list of supported/allowed -- I suspect for them to push fw as well as test to verify.  The times I've done it, it's been a fairly easy call to give them the MAC of my modem & sometimes they ask for the MAC of their's.  I initially did this to avoid having to pay a modem rental fee when high speed internet was finally available in my area back in the day. I have an ISP provided DSL modem/router (Archer) that is currently configured as 192.168.1.1 and acts as the gateway, DHCP server, and the DNS settings are also stored there. I also have my own ASUS DSL modem/router currently configured as 192.168.1.2 (and some additional routers that allow for ASUS' AiMesh networking) that I'd ideally connect straight to my ISP, but I can't get it to connect, so I'm going to disable wifi on the Archer and manage all of my Wifi from the ASUS router. With DHCP sevrver disabled on the Archer modem, should I still consider the Archer the default gateway, and leave gateway/DNS/NAT settings as they are (i.e. only change the DHCP server setting) or should I now treat the ASUS router as the gateway, having updated the DHCP server settings only, or do I also need to configure the ASUS router to handle all gateway tasks? But that would leave me with a DHCP server configured on the Archer modem and on the ASUS router. Can I just disable the DHCP server on the Archer and enable it on the ASUS, and reboot? In short, can the Archer remain the gateway, and it's just changing the DHCP server settings, or do I need to do more)?",2
"Going above 'dual head' at the moment is moderately complex under Linux, though dead simple under Windows sadly.  You'll probably be looking at using Twinview on each 'Device' within xorg.conf and then tying them together with Xinerama. This may hurt performance. I'ev got three  myself.  The optimal way is to have one monitor per video card.  Just be careful what your motherboard supports.  Some only support ATI cards, the others just support NVidia. On Windows, a large virtual desktop spanning multiple monitors can be set up no matter which way you connect them in your circumstances. I don't have any multimon experience on Linux and I've forgotten what i knew about OS X. However, performance depends on the particular combination of factors in a specific configuration. The OS, drivers, slot type(s), bus speed etc. all interact. The only way to tell for sure without doing a well-designed benchmark. I would find something that does a lot of OpenGL and displays the frame rate. Run it so that it spans the two monitors (if it will let you) and try it in each potential configuration. Multiple monitors is pretty standard for a NOC environment, and my experience setting it up and configuring it under Linux varies wildly. Some of the new support under Ubuntu Linux can be quite fantastic, they make installing the proprietary nVidia drivers very simple indeed. Invoke 'nvidia-settings' as root afterward and you'll spy a very simple way to configure the right orientation for your monitors, works perfectly for two monitors or similar cards. I can't be sure on this, but one to each card would be the logical answer.  Distributing the load across both cards.  I should think it makes minimal performance difference, but that's the way I'd do it. Regardless of the OS, I would think that some operations that span the two monitors would have better performance if they stay on one card and don't involve the bus or the OS. So I would recommend that you use one card until you get more monitors.",4
"On the hypervisor run kpartx which will scan the LVM for a partition table and create device entries for each partition found (where I assume have a Volume Group Guests and a LVM guestname-diskname which is the virtual hard disk for your domU) With the losetup command you can mount a file, or a part of a file as if it were a normal partition. It is practically the same as daemon tool for the windowsers' tribe. This time we use this losetup to map a part of /dev/d0_vg to a new block device, to /dev/loop0. If I read your question correctly you have a Linux hypervisor (a Xen dom0) with Linux LVM volumes which are used as virtual disks for your Xen guests (domU). And you want to access the data in that LVM directly from the hypervisor, bypassing the Xen guest.  After the losetup command, you can check if you found a real filesystem with the command file -szL /dev/loop0. It becomes a little bit more complex if you used LVM inside the domU as well; then the partitions are of the type 8e Linux LVM and formatted as physical volumes which you can't simply mount yet and LVM trickery is needed. Instead of the mount command the steps become: If you are mounting something so, the mount command first calls a losetup, and finally mounts this losetup device. It is normally /dev/loop0 (or bigger). Shut the guest domU down. (accessing a filesystem from two different locations is asking for data corruption)  If you had three partitions in your Xen guest this should result in 3 new device entries: /dev/Guests/guestname-diskname[1-3]  And a lvscan should show the logical volumes that were created in your domU and the device mapper entries are typically created now as well, allowing you to do something like: IIRC it wasn't possible in Xen but can happen with KVM, the inconvenient case where the name of a volume group used in the hypervisor is the same as one assigned in the guest.",2
"The nginx ingress controller can be replaced also with Istio if you want to benefit from a service mesh architecture for: Yet another option is to expose Nginx Ingress controller over NodePort (although not recommended for Production clusters). If you would have more than 1 replica of nginx Pod in your Deployment spec (example here), you could control pod to node assignment via pod affinity and anti-affinity feature.   See a full example with source code at Building a Kubernetes Cluster with Vagrant and Ansible (without Minikube). NodePort type still gives you the LoadBalancing capabilities, and to which  specific Pod (backing the Service endpoints) the traffic should be sent, you control with 'service.spec.sessionAffinity' and Container Probes. In order to access you local Kubernetes Cluster PODs a NodePort needs to be created. The NodePort will publish your service in every node using using its public IP and a port. Then you can access the service using any of the cluster IPs and the assigned port. If you want to expose service to outside cluster use service type either LoadBalancer or ingree. However is you use LoadBalancer approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose. Here is example of LoadBalancer.",3
"Is there anything I can do to get the vm to boot? I am lost for ideas, normally p2v of a basic IDE pc works flawlessly. When I power on the VM in VMware Workstation 6.5 (or Vmware Player 2.5) it gets to the Win 2000 boot graphic then I get a BSOD with the classic 0x7B Stop error: inaccessible_boot_device.  It sounds like you have p2ved the individual drive partitions, not the whole disk. If this is the case you're probably missing the partition sector, and the boot block(s) This blog post explains how to fix the issue on Linux while it's still a VM image. If it's too late for that, my advice is to get yourself a bootdisk like BartPE and run an NTFS rescue program. In instances where I've seen this before, I boot off of the Windows CD and choose to reinstall.  It will then detect there's an existing installation and give you the option to repair it.  Take that second repair option.  You'll need to reinstall all the service packs again. To be clear on the 'reinstall Windows 2000' you boot from Windows 2000 iso, don't follow the first Repair link, do an install, F8, then when it detects you already have Windows installed, do a repair from there. See http://support.microsoft.com/kb/263532 for details. I am using the latest VMware Converter Standalone to p2v a physical Windows 2000 Professional SP4 PC. The PC is a standard Pentium with IDE disk from circa 2001. The disk is 20GB partitioned logically into C: and D. It converts with no errors (I did both disks into one VMDK).  the reason this happens is the physical disk geometry has changed but the NTFS disk label is still using the old layout.",5
"Look for 'Catalina' labels within JMX items (MBeans). In this context 'Catalina' is synonymous with 'Tomcat'. My question: Is there a way to change the parameters dynamically (while the system is running) without restarting so that I can autotune (using machine learning) the parameters of the server by looking at the runtime characteristics. I'm trying to do online parameter tuning for the TPC-W benchmark. (It is a 3-tier web application and a standard benchmark used for performance.) I have managed to set up the TPC-W benchmark. I have used tomcat 7 for the web+application server (does tomcat have a web server inside?) and MySQL for the database. I read somewhere that to change server configurations (say MaxThreads parameter), we need to change server.xml and restart the server. From my memory, MaxThreads cannot be dynamically changed via JMX on tomcat7, which for me would imply it's not possible at all. But take a look, because some other dynamic setting may be interesting for you. I'm relatively new to this field and not sure whether I'm posting the question in the correct forum.",2
"I use DenyHosts to monitor my logs for suspicious SSH traffic, and I have it configured to automatically firewall off hosts at a certain point. There's nothing here to suggest that your box has been compromised.  Even if your passwords are pretty weak, a dictionary attack that only throws one login attempt at each username is extremely unlikely to succeed. Non-executable data is probably salvagable, but anything with executable content (this potentially includes such things as MS Office documents and some configuration files) has got to go unless you're willing and able to either manually examine it to ensure that it hasn't been turned hostile or to accept the possibility that it could be hostile and either damage your system or provide an avenue for a future re-compromise if you keep it around. So that doesn't look good. Now that I know I've been targeted by an attack and that some of my username, password combinations are weak, I'd like to know how can I... However, if there is other evidence to show that your system has been compromised...  Nuke it from orbit.  It's the only way to be sure. If you're only going to do one, monitoring system load is a very effective way of detecting compromise, because most machines when compromised are used to do things like send massive amounts of spam or otherwise receive a lot of traffic.  Perhaps not useful if you're a high-value target and people may be trying to specifically break into you for reasons other than to turn your host into a zombie, but valuable nonetheless.  Plus monitoring load is needed for profiling and to figure out when you need to invest in more hardware or better software. I recently read an article about analyzing malicious SSH login attempts. This got me thinking, are the SSH username, password combinations on my Debian box that uncommon? Had I been targeted by a brute force dictionary attack? Let's take a look at /var/log/auth.log.0: Valid login attempts are logged in as well, so if you see a brute force attempt followed by a success, that's a good indication something bad has happened. You should also do comprehensive log analysis, looking at auth.log and others for things that are unexpected.  Log file analysis is a competitive market and the problem isn't yet solved, but there are free tools like logwatch which can be configured to send you summaries daily. But, if you know your passwords are weak, then strengthen them!  I personally like pwgen (which is packaged by Debian).  On each run, it generates a large number of strong, but relatively-pronouncable password candidates which (for me at least) are fairly easy to remember, such as yodieCh1, Tai2daci, or Chohcah9. Note there are a variety of other ways you'd want to monitor your machine to see if it's compromised, including load patterns, login activity, periodic traffic sniffing,  monitoring running processes and open ports, and ensuring file integrity with a tool like tripwire.",3
"read reads from standard input. Redirecting it from file ( ./script <someinput ) or through pipe (dosomething | ./script) will not make it work differently.  The Bash builtins man page has a pretty concise explanation of read, but I prefer the Bash Hackers page. Arguments can be accessed via the variables $1-$n ($0 returns the command used to run the program). Say I have a script that just cats out n number of files with a delimiter between them: In this case, we are passing a file name to cat. However, if you wanted to transform the data in the file (without explicitly writing and rewriting it), you could also store the file contents in a variable: As far as reading from stdin, most shells have a pretty standard read builtin, though there are differences in how prompts are specified (at the very least). You don't mention what shell you plan on using, so I'll assume bash, though these are pretty standard things across shells. Basically, if the variable (reply) exists, return itself, but if is's empty, return the following parameter (""N""). If fewer words are entered than variables, the leftover variables will be empty (even if previously set): for an example where the tool being used already behaves this way, we could reimplement this with sed instead: read will then place one word from stdin into each variable, dumping all remaining words into the last variable. Now the rest of your script can just read from stdin. Of course you can do similarly with more advanced option parsing rather than hard-coding the position of the filename as ""$1"". All you have to do is to loop through all the lines in input (and it doesn't differ from iterating over the lines in file).",4
"I want to write plugin (library) for Unity3d (it doesn't matter which framework I will choose for this, question is ), for cutting arbitrary mesh with plane (for simplicity it will be plane for beginning).  2) Split intersected triangles into triangle and quadrilateral, triangulate last one and put them in corresponding lists (TRI_ABOVE or TRI_UNDER) 1) check every triangle whether it lies above, under plane or intersected by plane, assign all vertices to VER_ABOVE or VER_UNDER lists, recalculate triangles, put them in TRI_ABOVE or TRI_UNDER lists Where it is better to calculate intersections, triangulation (for example I will use Constrained Delaunay Triangulation or Sweep Line Non-Convex Polygonal Triangulation cutting into monotone polygonals) and other stuff: on CPU or on GPU (using Shaders), if someone could explain me general pipeline from loading cashing mesh to memory and so on on GPU and on CPU, so I can better understand the most time consuming actions and can optimize some stuff.",1
"A new user may not recognize the differences betwen KDE and Gnome in the screen captures above.  Look at the position of the default panels (top with Gnome and bottom with KDE) not the color of the desktop or the apps that are open. While installing inxi for just this purpose may seem overkill, inxi can be used to provide a lot more system information with inxi -Fxxxz (where -z masks stuff like your MAC address). On Mageia linux you can have very easily both environments and also because of that script is written so. According to original post, the ideal solution for me (and hope someone else) is demonstrated with example: a) Graphical way, with HardInfo: the answer is normally in ""Operating System"" > ""Desktop Environment"", but if not you can look to ""Environment variables"". HardInfo is ready with all the tested distros, except the one with KDE, but it can be easily and quickly installed (only 2 packages in Linux Mint 13). The item that appears in more lines should be the answer (if there is a draw the item with ""session"" should be the solution).",4
"Alternatively, backup, repartition the entire drive, then restore.  Which would be quite a bit safer. Since your post is short on detail of your partition table content, it is impossible to do anything more than guess, however.   If your partition table matches the picture, then deleting those two primary partitions involves writing zeroes to exactly two bytes in sector #0 of your disc.  This is a fairly routine partition management task, and most partitioning tools are capable of it without incident.  There's an old Microsoft Knowledgebase article that shows how to do it with Windows NT 5 and the FDISK from MS-DOS, even. I want to delete the first two partitions (a 243 MB Linux swap partition and a 5.87 GB root partition). Problem is, every time I deleted them, Windows would also delete Drive G: and H:, leaving only drive F: intact. Deleting the two Linux partitions using GParted from a Live CD also gave the same result. I've attempted this multiple times now and, after each attempt, TestDisk always managed to recover all the deleted partitions. My gut says that it has something to do with deleting both of the primary partitions from the disk, leaving only the extended partition on the drive. But my educated guess is that the pretty picture is hiding the ugly truth that your partition table is in fact mucked up, especially when two different partitioning utilities from two different heritages behave identically.  I wouldn't be surprised  given that it is a known failure mode of some tools  if it turns out that your container partition doesn't have the correct length set.   What you might need to do is delete 1 partition first, create a new primary partition in its place, then delete the 2nd partition.",3
"What a data center does when you reach capacity really depends on the data center and the equipment they connect it to.  I mostly deal with full circuits, say 20A, and for those it's not uncommon that if you exceed it by too much for too long the circuit shuts down, just like would happen at home if you run the microwave and toaster at the same time.  :-)  But at a 1.5A loading, it'll depend on what the equipment you connect to can do, and how loaded the circuit is. Give them the specs of the server you are looking at and ask them if you need to reduce your server power draw or they're ok with it mostly being under that.  It's really up to their policies and procedures, but expect that they've seen people come in with over-powered servers before, saying ""we'll only use 0.5A usually"" but then a process gets stuck and they start using 2A all the time. As far as the power supply specs, most power supplies are at their peak efficiency around 50% of utilization.  Meaning they convert most of their input power into output power.  So that 570W power supply will only produce around 175W to 200W of usable power to the components at that 236W peak load you are talking about.",1
"On your computer, you will need to add the printer as a TCP-IP printer. For this you will need to know the public IP address of your Google Wi-Fi mesh. This can be found as the WAN IP Address in the Network tab of your Google Wi-Fi app.  Set your printer up to allow the TCP-IP connection on port 9100 if this setting is not already enabled. Check your printer documentation on how to do this, as it will vary by make and model. In the Google Wi-Fi configuration, set the printer to have a static DHCP address by doing the following: I would suggest using port forwarding instead. If you can print to your wireless printer over TCP-IP, then you dont need to connect your desktop to the wifi at all. Next, you will enable port forwarding. Specifically forward port 9100 to the DHCP address you just reserved for the printer.  You should now be able to print directly to your wireless printer from anywhere you have internet access. Even if youre not on the same network as the printer, it should send the print request to the printer. You can now disconnect your computer from your Google Wi-Fi, ensuring all your traffic is going through the wired connection.",1
"As far as connectors go, you'll be wanting whatever suits the switch. There are (or at least used to be) patch cables you could use to change from ST to SC presentation, wouldn't be surprised if that's available for the smaller connectors commonly seen today. Fibers are normally used in pair (one transmitting one way and the other transmitting the other way), so thinking in pairs isn't a bad idea. We don't currently use any fiber (nor do I have experience with) and our switches are all copper, but I'm thinking to get some between the two racks, to have the possibility to use that as 'backbone' in future expansions (network cables are distributed between the two racks, so I get some switches in one rack and some in the other). There's some 30~40 meters of conduits between the two racks. In my experience, the cost of the fiber is pretty small in comparison with the cost of getting it installed, so if you're wanting fiber, you're probably better off installing ""what you think you need, plus a little extra"" (so if you think you'll just need one pair, you might as well install 2 or 4, if you think you'll only need 3 pairs, you may as well install 5-8). I'm helping a customer in moving his company to a new building. There we'll have two racks on two different floors to accomodate various servers and switch infrastructure. We'all also use the opportunity to migrate the last remaining 100mbits portions of the network up to gigabit speeds, the whole setup will be Cat.6 certified. Fiber is an excellent way of avoiding electric coupling between floors. For that short a distance and within a building, I'd personally consider multi-mode (usually cheaper GBICs and slightly cheaper cable). However, if you're not concerned about galvanic isolation, it's absolutely fine to go for copper. I'm mostly thinking about upgrades in some years when 10gbit will get into companies, maybe just starting as backbone between gigabit copper switches when gigabit will start to get pushed by end user systems. Seems like the most common and price-wise option is to think about switches with GBIC or the like slots.",2
"I have not used Unity in about a year now so I don't remember a lot of the specifics, but I was just skimming this article earlier today and it looks helpful. (scroll down to moving platforms) You could increase the friction when player enters the collider of the platform, you can increase friction like this:  I have a few basic platforming mechanics going on in this scene. I have a prefab platform with an animation that loops back and forth between two different x positions, but does not begin looping until OnCollisionStay2D activates with my player object with the tag ""Red cube"". Note that the content below was copied from the link above, and the wording slightly tweaked with Unity in mind. But for some reason the Red Player cube still falls of... Is there an error here that I am missing? Thank you for your time! I would like my Red cube Player object to not slide off of the platform when it begins to move. This is solved through OnCollisionStay2D as well; however, the Red cube Player remains kinematic even after I have left the platform. I tried to solve this with",3
"I have a similar setup, I generally have a lot of open windows.  I can't imagine life with out the alt+mouseLeft window move and alt+mouseright window resize.  There is a windows port of this capability called WinMover. Perhaps try sidebar software, such as Desktop Sidebar, Google Desktop, or Vista/7's built in sidebar. Join Twitter, and keep a client always-open to keep track of news and current events. Basically, a widescreen monitor, in the right hands, is a wonderful thing. As for using window space effectively, I can't bear to have more than 2 things open on my screen at once, so I tend to use Aero Snap (Win-Left and Win-Right) often, but not much more. On windows 7, use Aero Snap, on Vista and below use software such as GridMove or a suitable Autohotkey script (It's really not my area, but AHK serves my needs brilliantly) Wide screen is very good for watching movies on VLC or Youtube, and even though I have one 1680*1050 moniter I still find myself running out of space. Does either monitor have any fun ports apart from VGA/DVI, such as RCA or coax? If so, you can use the secondary monitor as a periodic TV-watching or SNES-ing output or the like. Buy a 5 button mouse, you're spending lots on your output, so you should beef up your input. Autohotkey, or software such as Logitech's SetPoint, can empower additional buttons, or combinations, to help you with your workflow. I'm slightly amused.  I've also got two wide-screen monitors and find myself wishing for more space.  I guess I just fill up all that space with lots of applications that I use for my work.  I suppose if all your doing is browsing the web, not many sites are catering to widescreen yet. Start using multiple applications at the same time! Wheras before you might have a document on the secondary screen to refer to while you work on the primary, have both on one, and then use the secondary for either even more reference material, or something completely different (I keep IRC channels always-open, cover them up with the docs to whatever language I'm using at the moment, so I can look up functions and arcane chants very quickly). Then again, it's only a 19-inch 1366*738 (or something similar), so that might have something to do with it. You're thinking ""How can I modify existing software to better use a widescreen monitor"", when really you should be thinking How can I now use my computer more efficiently. Stop maximizing. Maximizing stopped being useful about 5 years ago*, we have screen resolutions in the 1000s of pixels both ways now, and enough hardware to run countless programs at the same time.  *at least, programs stopped being easier to use maximised. It's still great for those times you just need a little extra space.",5
"Alternatively, it would be user-friendly if I could run the method against command line arguments, for example: The argument to palidrome is a pointer to text which is not altered within the function.  To show that to any calling program, the declaration should instead be like this: Here are some observations and suggestions that may help you improve your code.  In all, it's not bad at all for a first effort. Another thing that puts me off is that the main method just runs palindrome in a loop for the same input string. If this code is ever used elsewhere, you will want to both reintroduce a header file (but give it a decent name!) and also document how the code will handle certain kinds of input.  Right now, it will crash if handed a NULL pointer.  palindrome is not a good name. It does not tell the user what it does. I suggest simply isPalindrome or, if you prefer, is_palindrome. Since C99, the compiler automatically generates the code corresponding to return 0 at the end of main so there is no need to explicitly write it. For a small program like this, it can all easily be in a single file. Unlike Java, C does not force you into proliferating bunches of files for every program.  The declaration can either be in main.c or even simpler, just place the definition for palindrome above that for main and you won't need any forward declaration.",3
"It prompts for confirmation. It would be about the same speed as the xargs version (except that it's prompting for each file). It would handle filenames with spaces if that were applicable. It will handle any number of files. The disadvantage of the other answers is that they're either don't prompt to continue, slow (but prompting for permission makes this moot), would fail if there is a large number of files to delete (exceeding command line length maximum) or would fail if there were spaces in the filename (which does not apply in this case). Is this shell command safe to delete all of them (that I have permission to) from the drive? i.e. no chance or false positives or other mishaps. None of the answers given so far have addressed the safety issue and I can't comprehensively, either. However, I can say that the command you show will work as you intend (plus you have the extra safety of being prompted before each file is removed). On Mac OS X, .DS_Store files seem to contain custom view settings for the directory when it is viewed in Finder (file manager GUI). I want every directory to use my default view settings.",2
"Whatever it is, either the power adapter or motherboard is not delivering enough power to charge the battery when the computer is on. HPs, similar to Dell, often use a third wire communication with the power adapter. That allows the computer to know what size power adapter is plugged in. The easiest thing to try here is a different power adapter. You might also be able to go into BIOS setup and it might tell you if it recognizes the power adapter or not. I think HP is a bit more limited in this regard though. Tapping F10 during power on usually gets you in to BIOS setup for HP. There is an off chance the battery is just bad and taking too much power to charge. But I really think this sounds unlikely with your description. Youve indicated the battery charges when the power is off, and I am assuming it runs on battery properly with a reasonable amount of run time. Your problem could be one of two problems. The power adapter could be bad. In which case, just try another one. Or, your motherboard could be damaged. If that is that is the case, its probably the power jack.",1
"I have two spheres in Unity. One is 1000x in size other is 1x in size. So I want the smaller sphere to be attracted to bigger sphere. So how do I do it. I know that it can be down through gravity using rigid body. But how do I change gravity angle towards the big sphere? Else you can do it the other way around. But in any case this would probably be better since you most likely want to make gravitational pull different from planet to planet.  Have the planet/big sphere with an circle collider, give it a child. Now, the planet should either be tagged planet or static cosmic object, and the child should be labeled Area of influence. There's no need to resort to the gravitational equation.  Acceleration due to gravity is constant regardless of mass, so all you want to do is accelerate your small objects towards the large-object each frame. In fact, I'm currently working on a game based around gravity: simply, you place stars to move your player through eliptical orbits and the slingshot effect. I disabled gravity, and used this code: PS The code originally looped through an array of all the planets: this is edited and therefore may not be fully correct. It should be okay though. What others said, gravity on unity is only towards a certain direction, so maybe you should instead disable gravity completly and script a force that moves the small sphere towards the huge one. Put a script on the planet with a on enter/enterstay trigger, if it is a sphere of influence it should be pulled towards that.",5
"More funnily, a full scan could be done from every completion attempt on a shell if a mountpoint would appear in client's PATH. Also some file change notification systems on clients will automatically poll your NFS server, because there is no callback mechanism provided by NFS for this purpose. NFS is mostly transparent for clients, thus the ways I've seen people hitting NFS mounts - even without them know it - are countless. You should really make sure those clients are 'idle'. It's not uncommon to have some kind of crawler (mlocate/slocate, desktop indexers and so on) on a client machine not smart enough to avoid an NFS mountpoint. We had a NFS server at work which was working with slight lag. However as more mount points were getting created at several clients(serviced by the same server) we noticed the performance coming to a screeching halt with more users not being able to mount or cd to the mounted NFS. My question is can several NFS points on several client machines (even if idle) affect the performance? If yes, how it would affect? Yes, having more clients can affect IO.  NFS have a finite amount of NFS IOD's, limited by RPCNFSDCOUNT= (location varies with distro).  The NFSD Count has a point of diminishing returns however, depending on how many clients are mounting with async vs. sync and what they are doing.  On your NFS server, you can see them blocking with ps auxw|grep D to find the NFSD's in uninteruptable sleep.  You can also see your run queue increase when this occurs.  You will also see blocked processes (second column of vmwstat).  You mentioned that they are idle, but are you certain of this by watching network and rpcinfo stats? Idle client mountpoints should not affect your NFS server performance since they cause no I/O. They only use a very small amount of memory to hold the mount state.",3
"Million rows, with, let's say, 2k of data per row (which seems a lot of characters for a survey), is 2GB of memory. If you can throw a bit more hardware onto your problem, maybe you'll be able to keep your data set in RAM? A lot of applications are more read heavy, so typically you'll have a master-slave replication setup where all writes go to a single master, but reads are distributed to the slaves. For us this doesn't work because we're doing writes most of the time. Which leads to the next question: What's your load in absolute numbers? Customer requests per second, translated to I/Os per second, divided into reads and writes per second, how many gigabytes of data, with what growth rate? How does your load scale with number of requests? Linearly? Exponentially? You don't have to publish your data, just write it down and think about it. What is it today, how do you think it is going to look in a year or two.  There's no such a thing as a Grand Unified Database Layout. If there are custom questionaries, there, really, need to be custom tables. Otherwise you  are on a quick path to a single-table-of-200-columns of VARCHAR(128)-with-no-primary-keys monstrosity out of thedailywtf.com, which is inefficient, unsupportable and will hurt you in the future. Making sure our application server (PHP) and the web server (Nginx) scales is quite easy, the trouble is scaling the database server onto multiple servers. On some similar questions I've seen mention of the Tungsten Replicator and how it gives you  a lot more flexibility with replication. Would this help me at all? What kind of benefits would this give me that MySQL's built in replication can not provide? This sounds like a good case for sharding.  If the data in one survey doesn't need immediate access to the data in another survey, then sharding your data will be easy.  You'll setup a database that has basically a user ID key which points to a Survey DB.  You can then setup multiple Survey DBs.  Hopefully you'll also choose to set those up in a replicated tuples as well.  Your application will need a bit of re-working. Run your reports and do the joins in software.  If that's also an option, sharding is the way to go. I've seen mention of a master-master setup, but this typically hits a snag with auto incremented primary keys. The solution is typically to have one server do odd numbers, and the other do evens. I want to avoid that. If currently you need 1k IOps, but have three 10k rpm HDDs in RAID 5, then there's no way your hardware will be able to satisfy your requirements. OTOH if your app has a user request per second and brings a 32 core 256 GB of RAM beast, backed by an enterprise-class storage to its knees, then chances are the problem lies not within hardware capabilities. I have a write-heavy application. The application is best compared to surveys - the customer creates custom questionares and this is saved to the database. Most of the requests are from their users submitting these forms. Later on our customers do complex reports and graphs on these submissions. Wikipedia says a 15k rpm SAS drive will give you 175-210 IOps. How many do you need in RAID 10 to satisfy your current and projected load? How big is your data set? How many drives do you need to fit your dataset (probably a lot less than to meet the IOs requirement). Would buying a pair (or a dozen) of SSD be justifiable? Is local storage going to be just OK, or are you going to saturate two 8Gb fiber links to a high-end storage subsystem? Sharding, as recommended by toppledwagon may be a thing to consider, but first, double check, that your database  is rationally designed. If it is not normalized, then have a very good, preferably backed by testing, reason, why it is not. If it has hundreds of tables, it's probably wrong. If it has single table, it is definitely wrong. Look at the ways you can divide your problem into independent sets. You will spend more effort up front, but the system will be better for it. There is also MySQL Cluster, but this typically hits a snag with very large databases and complex queries (joins). I need to be able to run complex reports, so this probably won't work for me.",3
"Next I tried sitting at my server machine and logging in directly with my username and password. Then, while still logged in to the host machine, I try to SSH into the server from the remote machine using the RSA key again.  In this case, the login process works. I have an RSA key pair set up for authentication into my SSH server.  The server is an Ubuntu Server, and I'm trying to log in using Putty on Windows.  I load my private key into Pageant (the Putty authentication agent), and I attempt to log in.  The server asks for my username, which I input.  Then, I receive the error message: I've seen this happen when the user's home directory isn't local, but on an NFS automount on the network. If your home directory isn't already mounted when sshd goes to look for your ~/.ssh/authorized_keys, it won't be able to access it in time, so public-key-based authentication will fail. Can anyone please help me figure out why the key-based authentication only works if the user is already logged in to the host machine.  Thank you. If you stop the sshd service (service sshd stop) on your server you can run it interactively to get some debug output (sudo sshd -D).  It may be related to the file protections on ~/.ssh and ~/.ssh/authorized_keys.",3
"You're subtracting 1 from x and y every time through the loop. You could calculate the values with the 1 already subtracted out in interp2() rather than in interp2_mx(): That takes you from 8 multiplies to 3, but from 9 adds to 10. So that's likely from 17 instructions to 13. My function interp2 requires a T (intended float/double type) pointer to Z (mz by nz matrix) and interpolates the points in XI and YI into ZI (which are all the same size mi by ni). Likewise, mz and nz are confusing, too. And even worse in iterp2_mx(), mm2 and nm2 are not only confusing but easy to mix up! mz and nz should be something like imageWidth and imageHeight (or matrixWidth and matrixHeight, or inputWidth and inputHeight), and the other variables and arguments should be appropriately named as well. which is 2 adds and 1 multiply. Doing that also removes the need to calculate x1mf and y1mf. So your inner loop would become: You might also have luck by keeping track of a pointer to the next input pixel just incrementing it instead of calculating line every time through the loop, too. It would be nice if you could pick more easily understandable names for your functions and variables. Why is this called interp2()? What is interp1()? You should name it bilinearInterp() or something obvious like that. And what does interp2_mx() mean? Looking at your error handling, I'm not convinced it's sufficient. You check if xp is less than 0 or greater than nm2, and the same for yp. But what happens when xp exactly equals nm2? line ends up pointing to the last pixel in a line. But then you calculate line + 1. That gets you the first element of the next row. And for yp, it means reading a row that doesn't exist when yp equals mm2. Why do xi and yi have an i in their names? That makes them look like integers, but they're declared as type T, which you say will likely be float or double. Using x and y to represent horizontal and vertical offsets is fine. Those are well-established traditions. But you're naming the start of your image z which is really weird. z is usually reserved for depth, at least when used with x and y. That makes your code very confusing to read. I wrote this under VS2010 and it is intended to be called from MATLAB as a MEX function (thus the minus one subtraction in interp2_mx because indexing in MATLAB is from 1:end as opposed to 0:end-1).  I have spent countless hours trying to speed up my bilinear interpolation up. I even tried implementing an SSE version (a double version and a float version), but that was even slower than this version.",2
"If that is still too high a budget (btw., I run a setup like that - very happy with it), you can use a Thecus / QNAP etc. NAS / SAN combo. The QNAP SS839 looks really nice - as does the 859. put in fast discs WD Velociraptor) and it should be ""good enough"" in a positive sense. For a much better price. AND noise - actually given 2 decent servers and a QNAP - you can make without server room. Any rack equipment is NOISY. BTW, i've got a quote for an MD3000i with 4TB+ of SATA II 1TB drives (with sparing, RAID5) in the $7K range. Before you commit to a SAN, look at getting a NetApp NAS (can use iSCSI or NFS).  Deduplication technology, plug and play off-site hot/cold failover down the road. I haven't priced an EQL PS4000 recently but I'm surprised that it's significantly less expensive than the MD3000i. If it is (and if they will support you reusing your existing drives) then by all means go for it. They are great entry level iSCSI SAN devices that would be ideal for your use case. As far as configuration and on going management are concerned they are about as easy as it gets, and if you do decide you need to grow\expand your SAN at any stage they are really easy to extend.  15 drives in the basic setup, and you can expand it with up to 2 (or maybe even 3, can't remember) MD1000 boxes, 15 drives in each. As far as cheaper options are concerned you could go the NAS route, there are quite a few entry level NAS devices that are certified by VMware that are quite cheap. Their performance and availability wont match either the PS 4000 or the MD3000i (unless you buy a NAs that costs more or less the same) but if you are willing to live with that there are cheaper options. The best strategy is to search the VMware Compatibility Guide for entry level (but business grade) NAS products (e.g. from Iomega, QNAP, Thecus etc) and then check out their support forums to see if they are known to be problematic. I can't give you much better advice than that as I've never used one of these in a production environment.  In your planning make sure to take into account the cost to expand beyond the basic SAN unit. If the basic SAN unit holds 15 drives, how much does it cost to add the 16th? Can you add a cheaper disk tray/chassis, or in the case of the Equallogic do you have to buy an entire additional SAN unit?",5
"For the other two nonclustered indexes, the optimizer has decided it is best to save the keys of these indexes to a tempdb worktable (the Eager Spool), then play the spool twice, sorting by the index keys to promote a sequential access pattern. You could consider performing the deletion in smaller batches, hoping to reduce the impact of the excessive memory grant. You could also test the performance of a plan without the sorts using OPTION (QUERYTRACEON 8795). This is an undocumented trace flag so you should only try it on a development or test system, never in production. If the resulting plan is much faster, you could capture the plan XML and use it to create a Plan Guide for the production query. That said, the xml indexes are particularly problematic. It is very hard for the optimizer to accurately assess how many rows will qualify in this situation. In fact, it wildly over-estimates for the xml index, resulting in almost 12GB of memory being granted for this query (though only 28MB is used at runtime): The top levels of the plan are concerned with removing rows from the base table (the clustered index), and maintaining four nonclustered indexes. Two of these indexes are maintained row-by-row at the same time the clustered index deletions are processed. These are the ""+2 non-clustered indexes"" highlighted in green below. The final sequence of operations is concerned with maintaining the primary and secondary xml indexes, which were not included in your DDL script: There is not much to be done about this. Nonclustered indexes and xml indexes must be kept synchronized with the data in the base table. The cost of maintaining such indexes is part of the trade-off you make when creating extra indexes on a table.",1
"You should see the service account listed in the security settings, and can adjust the permissions accordingly. I encountered this problem a couple of times. The issue is not with SQL Server but with Windows Permission of the folder. You have to add appropriate permission to the folder where you copied .bak file (I think System Network role). After adding permissions (I added Full Control, to match the default SQL Backup folder.) and restarting the SQL Server Management Console, I was able to select and restore my backup file. If you can find it in Windows Explorer, then you should write a RESTORE DATABASE command instead of relying on the clunky GUI to find it for you. Who knows what code is going on in there and why it might not be able to find the file - sorry I only have a workaround and not a solution. After spending some I discovered that it is permissions issue. SQL Server Service is running using Network Service but Network Service account did not have permissions to access the folder. After granting the permissions to Network Service. Backup file was visible to restore. Adding the service account is not as simple as it sounds. Here is what worked for me. (Server 2012 R2) Also ensure that the file is actually something.bak and not something.bak.txt (Windows may be ""helpfully"" hiding the extension from you. I hate that default behavior.) This happened to me today on my dev box. In my case, the service account had permissions to the folder, but my user account didn't. Once I granted my account permissions to the folder, I was able to see the .BAK files. I also came across the same issue today. I was given a backup file to restore but it was not visible to me while browsing. However I was able to acess the in file system but in in restore wizard using SQL Server management Studio.  We had the same issue today. It turned out to be a permissions issue, as illustrated in some of the other answers. The difference is that the account we needed to add was NT SERVICE\MSSQLSERVER. I identified the account by comparing the permissions of the default SQL Backup folder permissions to the folder containing the backup file. The easier solution is, to move the file to the default back up folder in Program Files. It has all the necessary permission. For SQL Server 2012 it is",5
"I read, that the Hull Shader, Domain Shader, Geometry Shader and Pixel Shader can be used optional. So, is the Vertex Shader optional too? If no: What does a basic Vertex Shader look like? Just like a simple pass through? Is the Vertex Shader necessary to tell what kind of datastructure (Van Stripes or Meshes) are used? What can I do, with just the vertex shader? Vertex shaders doesn't know what kind of data structure it is, merely that you're taking in a vertex with some input data. From this you can infer that a vertex shader is absolutely required to write SV_Position but that's all, so the most basic possible vertex shader will just write an arbitrary value to SV_Position; in the pass-through case this just means copying over the input position to output, but (since D3D10+ allows drawing without input buffers) you can set it to any value you like if you wish (this is commonly seen in code for drawing full-screen quads). If you use compute shaders, or a GPU compute API like CUDA or OpenCL, then you don't need any of the graphics pipeline shaders.  Depending on your application, GPU compute may be a better fit for you, e.g. if what you're doing has nothing to do with rasterizing triangles.  Compute allows you to simply launch a large block of GPU ""threads"" running in parallel, and what they do is up to your program. They tend to be used to transform vertices from world to view space and then project them onto a 2d plane (screenspace). As for the graphics pipeline, you can't do much with just a vertex shader.  You can rasterize triangles to the depth buffer, and that's about it.  If you want to do some computation at each pixel of an image, a common approach is to put the computation in a pixel shader, and do a ""full-screen pass"" by drawing a single triangle that covers the entire image.  This requires a very simple vertex shader, but the real work is done by the pixel shader.",4
"Kd-trees and Oct/quad-trees are both good general purpose solutions, for which a platform friendly implementation can be written, but in the end you are going to have to balance the depth/complexity of your spatial partitioning tree against the cost of iterating it, and the overhead per model (i.e. draw call cost). If you're targetting XNA, I'd recommend you keep it simple and high level, and if there is sorting problems with some of the content, then strongly consider changing the content before trying to improve your engine until it can cope with it; the returns diminish very quickly after the most basic render sorting is implemented. BSP systems are good when you're subdividing the world based on individual polygons, but for larger objects a volume based approach makes more sense. The answer highly depends on the amount of models that you are planning to use. I haven't really worked with these kind of things for XNA, but if you use AABB's for the tests and don't have too many models you might be good enough with a brute force test. Perhaps even throw it out on a thread. I'm not sure how/if there are any SSE/VMX optimizations that can be used for C# but if you manage to get the AABB's linearly in memory (so that you get no cache-misses for the CPU), you should be able to push through a high amount of tests in quite little time. Octtrees (or even just quadtrees) and Kd-trees are both good general purpose spatial partitioning schemes, and if your build pipeline and/or engine is generating them, then you will find they come in useful in all sorts of different ways for optimising queries/iterations. They work by sub-dividing a fixed volume in a hierarchical way, which makes queries like raycasting into your object space very cheap to query for (great for collision checks). Above all though, it's worth noting that none of these systems are perfect for determining render order for transparency, even the BSP approach. It will always be possible to construct geometry which breaks your algorithm, unless you are able to subdivide polygons on the fly. What you're most likely looking for is a 'best effort' solution, where geometry can be ordered correctly in the majority of the cases; and the art team can subdivide models for any of the cases which don't work (because the model/polygons are abnormally large, long, or self-intersecting). Smaller models/nodes are always much easier to sort 'correctly', but you pay for it in terms of iteration overhead. Bounding Volume Hierarchies work in a slightly different way (they aggregate the volumes of the objects in the tree rather than sub-dividing spaces), and are a simple way of trimming unnecessary things from being iterated over. But because BVH doesn't put any restrictions on how two sibling nodes are related, it's not such a good scheme for figuring out rendering order, or for arbitrary collision queries. McCranky covered amazingly all the data structures you mentioned, however I see that you havent considered R-Trees. The body of knowledge on those structures is huge, and they are very suitable for spatial range queries (most of the work have been done by database theorists and practitioners) in the last 30 years. Recently Sebastian Sylvain from Rare has given a course on the GDC on why they work for games too (specially on consoles with in-order processors). You can learn more from its pdf: http://www.gdcvault.com/play/1012452/R-Trees-Adapting-out-of  A naive and simple implementation is pretty easy, and the basic structure gives you lots of room for improvement (better management of splitting heuristics, prefetching oportunities, parallel searching, etc).",3
"As D. Kasipovic points out, you should check the file permissions on the files you have restored, it could be as simple as a file permission problem. The first thing you can try is to completely replace your MySQL data directory with the one from the backup. I would move it out of the way (mv /var/lib/mysql /var/lib/mysql-orig) so that you can easily restore it. Do this with the MySQL server not running and preferably on a testing server so you don't accidentally break something on your production machine. This will depend a bit on your table type. With MyISAM tables, you can generally just copy the folder and files and everything will work. With other table types, it sometimes doesn't work as well. I had a bad server crash in which I was able to restore my MySQL files, but I was unable to do a dump in order to get the .sql file. In other words, I have each individual part that was saved in the database. (i.e ""databasename.table"") You should make sure you're connecting as the root user or another user with administrative permissions; otherwise it's likely that a MySQL permissions will keep you from being able to see the databases. Depending what method ends up working for you, you may need to create any users that had permissions on that database. So, I have all my information, but after resetting the server, I am now unable to put my database back online. When I copy the files over, phpMyAdmin and MySQL Workbench show the table names, but act as if they don't exist. So the files are there, and the names are read, but the information is not there. (i.e phpMyAdmin says something like, ""databasename.table does not exist"") I am unsure of what to do and I hope I was able to describe it well enough for you guys to understand it as well. If anyone is able to help me, whether it be describing how I can compile those into a .sql, or moving them over for the new server to read them, I would greatly appreciate it. Since you have the old hard drive data but can't get the drive to boot, it may be relatively painless to start MySQL from that drive (you can do this in a couple of ways; chroot to the old drive and try to start it there, copy the files to a server with a working MySQL instance, stop mysqld, edit the configuration file to point to the old mysql data directory, then restart MySQL, or finally just follow the steps above to completely replace the mysql data directory on the working instance with the one from the old server. Note: I have replaced the hard drive in the server, so I still have everything that was saved on that hard drive, but I am unable to get it to boot, so if you guys find that I need to go through it again and find some other files, that can be done.",2
"I tried to remove it from their page and it says ""That IP Address is not listed."" but when I checked it again, it is listed and gives me the above message.  SpamRats is a cheating ""company"" that is adding whole IP ranges and after than they try to sell you commercial services provided by their official companies ... if you write them an email with demand to stop illegal discreditation of the good name of your company or you will sue them they will remove it within an hour ... personal experience There seems to be no way to persuade spamrats to unblock your IP.  Their removal tool simply does nothing, and attempts to contact them go unanswered.  You either need to persuade the people you are emailing not to use spamrats, or get yourself a new IP.  Just so you know, the 'blunderbuss' only comes out when sufficient IP(s) in the class C all are detected as being a spam source, however the SpamRats! team is very good about helping you get unlisted if your server is properly configured, even if it is in the middle of a class C full of spammers.",4
"I have a distribution list. If a message bounces back, the intended recipient is flagged. If they get flagged 3 times consecutively, I stop sending them mail. The purpose is so my server does not waste resources constructing and queuing emails to bad addresses, while they sit and gum up my mail queue. I want to make the ban ""probationary,"" where I test the address in the future to see if it still bounces. Maybe their inbox was just full, and now it isn't. Or maybe their server was blacklisting my IP, and I have since been whitelisted.  When you send an email that gets rejected by the server for some reason, you may get a bounce-back message with useful information about why the email was bounced, such as an RFC status code. Is there a way to get this same information without actually sending an email, like some kind of ping? So that, I could say: If I were to hypothetically send an email to address@domain.com, can I know definitively if it will bounce? And if it will bounce, what would the bounce-back look like? Is there any way to determine this without actually sending them an email? If it bounces again, it could then potentially continue to gum up my mail queue and waste server resources, which I am trying to avoid.",1
"When you restore the backup in an emergency, you don't want to wait for the indexes to rebuild, and you're going to suffer abominable performance until you do. Honestly, you really don't want to do this, even if you overcome the other issues others raise here. Then you stagger your backups so that you're doing filegroup backups of the primary every night, and transaction log backups every X minutes. If you switch over to full recovery mode, you can do this with filegroups, but it's really, really clumsy.  You leave the data in the primary filegroup, and put the indexes in a separate (non-default, that's the key) filegroup. I can't think of a situation where you'd want to restore a backup without indexes, so in all cases you'll really want to back them up at the same time. When disaster strikes, you restore the primary filegroup by itself.  The data is suddenly online, but the indexes are not.  However, to get back to normalcy, you'll need to export that data into a new clean database and add indexes from there.  You can't bring the database completely online without restoring all of the filegroups, and you can't say ""I don't need that other filegroup anymore anyway.""",2
"You need to set up and install a Proxy Server on your VPS. The most commonly used one is Squid and it can do everything you want. To set up a SOCKS5 proxy on my localhost via the SSH tunnel I am able to establish to my remote VPS host but it seems it's still not as secure or as anonymous as expressvpn.com. I also tried https://www.some-web-proxy.com, or similar web proxy sites with HTTPS / SSL enabled on them, and they worked. However, the problems with them are Without knowing what you are trying to achieve it's difficult to say how you can achieve your end result, however that statement doesn't make a lot of sense - and you've provided no explanation of how you arrived at that conclusion. Given the confused nature of the rest of the question makes me suspect that your basis for making such an assertion may be flawed. This opens a port on your local machine to connect to. The next step would be to change the proxy settings in your web browser to the following: Question is a bit confused. The point of SSL is that nobody except the browser and the server can read the data stream - so all the (HTTP) proxy has to do is pass the data back and forth. In order to connect the client to the server the client needs to tell the proxy where to connect to (not using SSL). The proxy does not need / cannot use a SSL certificate. I don't want to pay for this kind of VPN service because I believe I have everything they have, I just need to set it up on my VPS server via SSL. Right? How to install it varies greatly based on your OS of choice, so I suggest you research installing squid on your platform. Some you'll have to build from source, others will have packages you can install. You say you want security and anonymtity. The former is very generic but adding a proxy does little to enhance security. Anonymity is something a lot more complex.",4
"2. Windows Vista introduced multiple local GPOs which achieve the same result without a domain controller; The biggest advantage for me is the ability to whitelist signed executables by publisher. Have a look at this http://technet.microsoft.com/en-us/library/ee460943(v=ws.10).aspx At the same it has one BIG disadvantage that make it pretty useless: it is not extensible, and you cannot add custom file extensions that you want to restrict. As the documentation says: Applocker relys on the Application Identity Service (make sure it starts automatically). I have read many articles from Microsoft and others saying that the new Applocker feature is 100%  better than the old Software Restriction Policy and is recommended as a replacement of latter. I use Applocker within my company. The strategy we use is: Deny everything as a baseline (in fact: the Applocker defaults), and then do what was suggested: make a rule that allows for only signed applications (office, adobe, wintools, ax etc.). Most, maybe all malware is not-signed software so won't execute. Further I cannot confirm one cannot use UNC-paths. In some extra safety deny-rules I use UNC-path's succesfully. The pitfall is in using environmentvariables: they don't work for Applocker. Use * wildcards.",4
"Say, I have an executable file that I want to pass a parameter to from a .bat file. The catch is that the parameter value must have a newline in it. One possible workaround is if the program recognizes an escape sequence, such as recognizing \n.  (JScript, supported as .js files that CScript can use, supports the unescape command.  WScript.Echo(unescape(""%0d0a""));) The file format of a batch file assumes that the newline character ends a line.  There is no supported escape sequence.  So, what you're asking for cannot be done. Similar to why the chown Unix command uses a colon as a separator: chown assumes usernames end with colons.  Likewise, batch files have a file format. Another common approach to handle this is if a program supports a data stream, which could be a specified filename or the ""standard input"" stream.  Newline characters/sequences can be provided to a program that way. Those approaches are used by multiple programs.  I'm not offhand recalling any way to just have batch ignore a newline character, nor any other standardized way to try to encode that character in a way that all/most programs will recognize the character(s) that you're trying to refer to.",2
"The middle water tile has a double transition in it's top right corner and bottom right corner. So since swamp goes over plains we first draw all the plains transitions. Then we draw the swamp transition. And thus the 2 corners will end up with 2 transitions drawn over the original water tile. It is up to you to set precedence and the art but this should give you good results. This solution does not offer different transitions for each other type but it is possible with a simple if statement, however your needed assets grow exponentially and that may not worth your while. But I can imagine a transition from swamp into water looks very different then into plains. This is essentially what you are showing in the comments, the mountains blend in very well with the grass but they won't blend in with the water that well. The reason you don't see the grass blend with the water is because the mountain overlaps it. Personally I think it looks fine but I see a lot of room for improvement, the tiles used are not much better then your average programmers art (Sorry to offend the artist, but I'm sure he agree's if he practiced more since then :) ). Jungle over forest -> forest over rocks -> rocks over hill -> hill over swamp -> swamp over desert -> desert over grass -> grass over water. I know this is old but this has not been mentioned. Your assumption of the transition tile is wrong. Your transition tile gets 8 transitions instead of your 4. On top of that it can have multiple overlapping layers in order of precedence. In the article they mention they have the following precedence. Ultimately, though, I think this is very much about visual design. There's an art involved in getting a limited range of tiles to look correct, and I suspect that shading is extremely importanta boundary with no appearance of depth will be more jarring than one that's carefully designed. The ""base"" for any given tile type would be a single solid tile, and then you'd have the 16 or 32 transition tiles, like Artifact. But you could also add a customized transition tile set (another 16 or 32 tiles) between a specific pair of tiles that need additional help. The transition tiles could either be done alpha tested/blended, like the generic transitions, or they could completely replace the lower-precedence tile. This still isn't perfectsay you have a custom water-grass and water-hill transition, but what happens to water that has grass on one side and water on the other?but it does at least allow you to prioritize. Since you're presumably in a situation where you don't need an extremely low-budget solution for technical reasons, and you are also presumably not interested in creating the 200+ unique different tiles that you would need to handle every possible case, you may want to consider a solution that allows you to use alpha testing or blending to handle most cases (like Artifact's solution), but override specific transitions that look poor. Of course this comes with it's limitations but not in the scope you are asking for. I'll take your example layout but cut off the eastern tiles so we end up with 9, we don't need more since there won't be overlaps with tiles not being direct neighbors.",2
"For larger networks, I usually follow the same principle: using 10. addresses you can have 256 subnets of 65534 hosts, or 65536 subnets of 254 hosts: more than enough for any network, without the need for fancy /13, /28 or /27 subnets. There can of course always be exceptions, but this is my general rule. .11 and .12 (and maybe .13, .14 and so on) are domain controllers, DNS and WINS (if in use) servers. Usually I dump all the servers on their own ghetto subnets, so I can keep an eye on them, and make sure they don't mix with crappy desktops. If I have to have them mixing, then, yea, I reserve the first 50 or so addresses for servers/anything that needs a static ip. Again it's just a matter of less typing. Desktop users seldom care what their IP is, and you don't often need to type it in. If the network is larger and more segmented, I like to put servers on one subnet and clients elsewhere; client (and even server) subnets can of course be more than one, if the network is big enough to require VLANs. I strongly believe in order when it comes to network and systems management, because computer systems tend to be chaotic in their essence (as in chaos theory): the smallest error can have unpredictable results. In network addressing, I try to always use the same ending addresses for the same roles; this is my typical breakdown of a class-C network: I usually use 1 for the gateway, just because it makes it easy to remember. Either way, as long as you use the same number on every subnet, it doesn't matter. Other than the gateway, I don't reserve specific ips for specific types of servers.  I prefer ""round"" subnets, because with them it's a lot easier to remember subnet masks, networks and broadcast addresses, and to know which subnet an address belongs to. I like DHCP (we have tons of laptops), but you need to couple that with registered MAC addresses, or any shmuck off the street can come in and plug in and that's a no no. MAC's aren't secure, but they're at least as good as statics, as far as security is concerned. I don't use ""registered"" DHCP; I'm not a windows DHCP person. If I'm going to have statics and dynamics on the same subnet, I just set the DHCP range to be 51-255 or similar, and put the statics in 1-50. After 10 I usually work in /16's, but I plan them by /8's (which are usually a nice size for a business unit). Working in 8's is nice because (unless your company is massive) you can just assign a business unit 10.1.0.0 and you won't have to worry about them running out of space any time soon. Obviously, if you have more than 255 business units, ymmv. I tend to choose 192.168.X class-C subnets for small networks where 254 addresses will surely be enough; I'm usually quite conservative here, and go with the simplest of them: 192.168.0 and 192.168.1; I also like 192.168.42.0/24 a lot, for obvious reasons. This hint greatly improved my VPN life: for example having the same subnet might be annoying when 192.168.0.1 might be your home router and remote server you're trying to fix. Then you'd have to add a manual route through the VPN interface, etc. The subnet size is of course to be choosen based on the size of the network, with enough room for future expansion, because re-addressing is always a big pain. That said, my favorite subnets are those beggining with 192.168. and 10.: I really can't stand 172. ones, and of course this doesn't have any rational reason: it's purely an aesthetic concern. I usually use ""low"" addresses for servers and ""high"" ones for clients; the former are always static ones, while the latter are assigned using DHCP. I'm a great fan of DHCP and dynamic DNS for clients, but I'd never use it for servers and other ""fixed"" systems, like network printers and scanners. Besides all the wise suggestions given here, one that I found useful: for sake of comfort, avoid having the same network as your office or other LANs you might have to connect to (remotely).  10.x.x.x; anarchy and crawling chaos (/22 is actually a damn useful subnet, not too big and not too small, so keep the same irrespective of the size, second octet defines a primary location, third defines a sub-location); gateway is always 1, servers start at 11 (with primary DNS being 11), then clients (starting at 10.x.1.x/10.x.5.x/etc using a /22 subnet), finally printers and other devices (starting at 10.x.3.x, 10.x.7.x, etc); servers with the same roles in each subnet have the same address where possible; DHCP for client PCs, static for everything else, reservations used for certain ""special"" clients where there are legacy apps and legacy security models that rely on a specific IP address.",4
"None of these suggestions will outright prevent bad words, but they can go a long way to helping you keep your game clean. There are, however, several techniques that can help. The idea is not to prevent offensive behaviour, but rather to take the fun out of it. Users determined to use inappropriate words can be very creative in getting around even quite sophisticated automatic systems. Imagine you want to block the word, ""pear"". Can your filter deal with deliberate misspellings like ""pair""? And what about false positives, like ""appear""? What if the user's real name is actually ""Pearson""? But offenders start using the player's name in place of ""bad words""? If input is ""Apple"", the blockedWords array read in from the text file will contain it and throw up the message. It's effective in these kinds of systems to show the original words to the user who made the input, but the scrubbed version to everyone else.  One simple option is to have a text file that contains a list of all words and phrases that you want to block. Scrub the input prior accepting it by checking it against the list of words in that file.",3
"When you're at home and connect (for example) from HOME to OFFICE with agent forwarding enabled, you should see $SSH_AUTH_SOCK pointing to a socket owned by the sshd process which accepted your login  not to any standalone agent process. (You can check with sudo lsof $SSH_AUTH_SOCK.) If the local agent refuses this request, the troubleshooting process is mostly the same as with normal direct connections: if it's gpg-agent, make sure you have $GPG_TTY set; run gpg-connect-agent updatestartuptty /bye if necessary before making the initial connection and be careful to not use any gpg commands from other terminals. When you then try to connect from OFFICE to yet another server, the ssh client on OFFICE sends a sign request through that socket directly to your local agent, mostly indistinguishable from a local request. (The local agent only sees the local ssh client as making this request, so there is no option for it to ""not support"" agent forwarding.) If your check shows that the remote system has actually pointed $SSH_AUTH_SOCK to its own ssh-agent or gpg-agent process, that is a problem: either the server hasn't accepted the agent-forwarding request, or your login scripts (~/.profile, etc.) are unnecessarily overwriting the environment variable.",1
"Just changing the name will make a cosmetic change only and not affect the underlying files/folders in the profile. You have to make a whole new user account named the way you want. Then you can copy your old profile to the new one. Note that the user account for your daily work should be a Standard account, not Administrator. Here are instructions for doing that (assumes you don't already have an extra administrative account made and are using a single administrative account for your daily work): Let me know if you want any more details about this. Of course you don't have to copy the old, incorrectly named account if you don't want to. You can just copy over the data using Windows Explorer. Uncheck the option ""Users must enter a user name and password to use this computer"". Select a user account to automatically log on by clicking on the desired account to highlight it and then hit OK. Enter the correct password for that user account (if there is one) when prompted. Leave it blank if there is no password (null). In that view you can change the account name simply by right-clicking on the relevant account and selecting Rename. It looks like the answer is no, not through the menu - but you can manually change the values by editing the registry values described in this KB article.",3
"If the users are part of a domain, you can use a GPO to define a software restriction policy which allows only your application to run. Create accounts for the users and give them local administrator rights to the machines (preferably with this same GPO). This is absolutely possible, PowerBroker Desktop: Windows Edition allows you to elevate the rights of specific applications without elevating the user, you can define what additional elevation is given down to the SID(s) involved. http://www.howtogeek.com/howto/windows-vista/create-administrator-mode-shortcuts-without-uac-prompts-in-windows-vista/ I'm not sure if it will entirely work for you though as if you give the user sudo permission I don't think you will be able to stop them elevating other apps of their choice. Might be worth looking at though. Take a look at Sudo for windows. It allows you to elevate normal users to admins for the scope of a single application. I am not sure how suitable it is for a work/school environment.  but at home this works for me fine for a small number of admin utilities i use regularly. The best way I could thing of would be to use a runas replacement that saves passwords encrypted .. something like supercrypt.",5
"By the time you've made this question u probably already know the answer... But here goes the answer: This port is related to uPNP services. I've made a port scanning on my home router and found this ""park-agent"" port opened in it. After some research on the internet I read some people talking about uPNP. Then I decided to disable the uPNP service on my router and bingo! The port has been closed!  I just don't know how exactly uPNP works but i read that it allows devices that ""talk"" uPNP to search and find each other in your network for communications purposes (i.e. media sharing, and other stuff which idk how it's being shared). So it looks to be insecure if enabled on your gateway or router since you don't want to share through internet your media or anything else you don't know how exactly is being shared to the world. ;) If its your own system, you could probably run netstat -b (on windows) or netstat -p (on linux) on the system its running on to identify the process- netstat tends to use behaviour to guess what sort of software is running, and what OS, and that description sounds rather generic.",2
"I don't see where routers come into it. Because you'd have the same question if there was just one router. Then R1 and R2 have to bqe doing NAPT. (NAPT is NAT but also one to many).  So the SSH server communicates with R1's IP address and R2's IP address. Each connection is at a different port.. And if one computer can initiate multiple connections, then you can say each connection is distinct by being initiated by a different IP:PORT So each sSH session, from the SSH server's perspective, which is a good perspective to be looking from.. as it sess all the connections to it.  Each ssh session can be identified by IP:PORT  where IP is R1's IP or R2's IP. And whatever port. If you have multiple computers behind a NAPT router and they're accessing the Internet, that doesn't require port forwarding, but it does require port translation. The website just sees the NAPT Router's IP. The NAPT router distinguishes each connection, as it opens local client ports up to communicate with the websites, and any local client port it opens up is going to be associated with one of the private IPs connected to it.",1
"For option #2, you can just put it up there and link to it in some online forms (though not here, maybe in chat though). For option #1 you'd probably need to modify it a bit to conform to the rules of the marketplace. Depending on how your game works, this could be a minimal effort or a huge task, but you have the opportunity to earn a bit of money (probably not much, but you might eventually buy yourself a very nice dinner, and maybe even more, without seeing your game its hard to say). If your game is really good, you could even try publishing it to Steam, but their requirements are much more strict than Xbox Live Indie games. You are in the catbird seat. You have a complete and working game. Your options are limited only by how much additional effort you'd like to put into this game. In my mind you have three basic options: Using monogame you could publish it to the Win8 store and even the Apple store. Perhaps you could make some money on it there. If you just want to share it and you don't want to set up your own website for that purpose, you could always use any file hosting service and share the link via gaming forums. It doesn't get much easier than that. You might get some added attention from the XNA community by uploading your project to CodePlex. It's Microsoft-hosted and thus you can expect a lot of C#/.NET stuff. A lot of open-source XNA games, engines and frameworks are easily found there. However, as people are wary (or at least should be) of downloading executables from an unknown source, you could always share the code itself through something like Sourceforge. You can quite easily set up an online repository, forum, wiki, all sorts of things, depending how much effort you want to put into it. That way, people can still download the executable, but can also verify the code that it contains nothing harmful/malicious. Also, if your game contains something that piques someone's interest, your game might, with your consent, get developed further into something even better.",4
"Since deep learning can not be run on spark, does that mean I would need to copy the aggregated data back out to the local file system to use some of those techniques? 2nd is some 13 million rows of text data (several paragraphs for each row)  for classification. (currently in solr database) Regarding data storage, from my experience CSV files or similar work best to load the data into both Pandas and Spark. There are Spark connectors for databases (e.g. Cassandra), but I am not sure about MySQL and Solr. For processed data and intermediate results (e.g. models) I would always use Python's serialization framework pickle, so you can directly get the object instances back into Python without the need to parse file formats and reinstantiate objects. Actually deep learning can be run in spark using h2o sparkling water feature.Also you can use h2o.deeplearning to run deeplearning on your data in cluster or single node.Spark is good for munging the data in cluster as it does so distributedly but in memory,otherwise h2o has limited functions for data munging and that too it can't distribute data munging. If I take the time to simulate a cluster using virtual box/hadoop/spark (for my learning experience, not performance), can/should I do the aggregations there and write the results to the distributed data store?  I expect to be using python for file processing, scilearn, pylearn2 and word2vec for general exploration and training. R for getting a taste of the language. For data set 2, I want to run the word2vec algorithm as found in the kaggle tutorial https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors. In that example, that is a deeplearning method, so I should just leave the data in the solr.. right? In general I am looking for appropriate applications and insight into data flow from app to app to help me get to the part where I start trying various ML techniques. 1st project has 10,000 time series with 24 float data points each day for 10 years (876 million points). I will be creating a bunch of calendar and weather features for the data, then trying to forecast using a variety of machine learning techniques. Clearly data set 1 will require joining weather and calendar data to date/time and aggregation across time and location. I know how to stuff it all into a MySQL database and do the aggregations and joins there, but I have been reading about spark and wondering.",3
"What Jeroo doesn't have is variables. You can't do things like loop 5 times. You can only loop or if while a Jeroo hasFlower(), isFacing(EAST), or something like that.  Yes. Programming languages that don't have variables still have conditional branching. The paradigm is called ""functional programming"". They're not directly using variables, although they're using boolean conditions. I've found that starting with methods like hasFlower() makes the idea of x == 2 easier to wrap their heads around when we get to it.  A list of pure functional languages can be seen at this link: https://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Functional_languages We only spend a few days in Jeroo because students start to get frustrated with what it can't do. You can't tell a Jeroo to loop a set number of times. You can't ask a Jeroo what column or row they're at. You can't ask them how many flowers they have. Any of those frustrations work really well as a hook into why variables are useful.  The first 6 or 8 days of the year we work in Jeroo instead of straight Java (there's a Python mode as well). It serves as a very quick introduction to syntax, ifs, and while loops. I've found it helps when I can go back later in the year with ""remember in Jeroo when..."" Here is a link to a list of programming languages that support functional programming: https://en.wikipedia.org/wiki/Category:Functional_languages",2
"There are a number of tools that will handle Access->MySQL transfers. I like Navicat, but you can download the free MySQL migration toolkit and it works fine with Access. If your looking to convert JUST the tables, then I would suggest you export them as CSVs and then import them to MySQL from the CSVs. I am doing the migration from a MDB database to MySQL doing some transformations and data validation on the way and the best tool for me right now is Pentaho Kettle (Community Edition). It's free and you can load the MDB database as input, browse the table, make transformations on the data and output as CSV, SQL, TXT, ... If you have a DSN connection set up, you can import all the tables using the import from external data source.  The exact menu commands vary from Access 2003 to Access 2007. I have an .mdb file that a coworker wants me use to convert to tables on our MySQL server. I'm familiar with MySQL, but have very little experience with Access (nor do I really want to learn). I don't recommend using comma separated values or fixed width.  You will waste a lot of time re-establishing the meta data (what fields are text, which are dates, etc)",5
"What devices/software can you recommend? We need to have fully working ~1000mbit/sec link with the minimal system load. For example, if the offices will be connected with OpenVPN or similar software, will it be able to handle a gigabit connection? What server load should we expect? All servers are equipped with a similar CPUs - Core Quad 9440 but of course I don't want to use the whole CPU for VPN. It may be that the CPU cost of a VPN solution is too high, but I highly recommend at least trying it out.  It's the simple, low-cost solution, and it's very likely that performance will be fine. We have 2 offices connected with a fiber optics cables. On both sides FO is inserted into the fiber ports in the switches but we can install fiber pci-e cards in some servers if required. The fiber cable goes about 500m thru a public areas, it's easily accessible and could be used by someone to get access inside our LAN. Worst case, do exactly what you suggested: get a machine at each end to act as a router and forward packets over an openvpn link between them.  No extra load on any other machines, and openVPN is pretty light, so it should be reasonable even on the routers. I would definitely recommend a hardware firewall device at each end with a persistent VPN connection between them, if you are worried about fiber splicing. Having some separate device would be better, especially if the bridge will be transparent to TCP/IP. A slightly better case would be to check out what Cisco or Juniper has in the way of a small office router that can handle gigabit.  A quick google shows up SafeNet as having a product that claims to be able to handle up to 10gE, but it's hard to find a price for it online, so YYMV. The offices are small, all people generally trust each other, the LAN is insecure inside and we would rather leave it this way than building a strong security. Have you done any testing with common VPN solutions?  Try it.  I think you'll find that the performance impact of the encryption is acceptable.  Modern block ciphers, like AES, are very efficient, and modern CPUs are very fast.  For example, one core of my 2 GHz (Intel T2600) laptop can encrypt 120 MBps.  I expect that the CPU cost of encrypting a gigabit per second on your 9440s would be less than 50% of one of your four cores. You don't need to buy fiber cards for the server, see if you can't create a VLAN on the switches which includes the fiber link port and a port to a secondary ethernet card in the server.  Then set up a VPN between them and route internal traffic over it. The offices have no dedicated server rooms so we would like to avoid installing any loud industrial switches/routers.",5
"If square brackets are used around the name, the result is an error basically saying the name can't be blank. So the value of the token is seen as an empty string.  When running an agent job, the tokens which are supposed to expand to provide information such as database name, server name, etc. are not expanding. The result of the token expansion is an empty string instead of the desired value. I am trying to use tokens in my SQL Server agent jobs as described here, but some of the tokens are not working as I expect.  The token $(ESCAPE_NONE(A-DBN)) is supposed to expand to the name of the current database, so I expect this job to produce a backup file with the same name as the database, like this: A simple job called ""Backup"" has a single T-SQL step called ""Create Backup"" which uses the database MyDatabaseName If I change the token in this step to (DATE) then it works as expected, and a backup is created with the name being the current date.  I have spent several hours looking for an explanation for this specific behavior, but can't find anything relevant.",1
"You might try Synergy. Can't say if that's what you want but there's no extra hardware necessary. Here's a link (URL): This could be a cheap answer, but what about VNC.  Looks as those somebody figured out how to go from Mac to Windows, you just have to decide which will be ""Master"". If the monitor only has 1 input as you have described then you will require a Video Switcher often known as a KVM Switch. From the hardware point of view, you may try to purchase a Monitor Switch e.g. Belkin Switch2 KVM, Belkin SOHO KVM Switch or some cheaper version TRENDnet 2-Port USB KVM Switch Kit. It's also called KVM switch. When I tried it a few years ago, it used a fairly large chunk of CPU power on one of the computers (don't recall if it was the master or slave).  I think you'd probaby be better with a hardware option (multiple inputs on monitor or KVM switch). You might be able to make it work by installing the software on the PC, connecting the monitor to the mac and en/disabling it on the mac to switch the monitor between computers.  It's not clear from the documentation if the software will auto-reconnect or not. This also means that you will be able to share 1 keyboard and mouse for the 2 computers although if you prefer to keep the 2 (A better solution for a MAC + PC combo as in your case) then you can. If you really want a software option, MaxiVista might be able to do what you want; but is primarily intended to slave the screen of one computer to a second, not to let you swap a monitor between two computers.  Reading the documentation I think you might need to have a dedicated screen for your PC to control it though, since it only supports using a mac as an extra monitor for a PC and not the other way around.",5
"I appreciate that you're a gamer. You must know that the number of possibilities are already jolly high! As a programmer, I have to say any game you choose to program for the RPi will run on it! As such the options are infinite. I understand the Pi 3 graphics output on HDMI is superior to its predecessors, but what sort of game did you have in mind? 3D first-person shooters are probably a bit of a stretch. I recently got my RPi 3, even though I had planned to use it to control an autonomous robot, being a gamer at heart, I'm really very  curious about whether or not it can run games, and if it can (like it runs the the Minecraft Pi edition) how will be the performance? Can regular Linux games be run on the RPi 3? Moreover, I don't have much knowledge about Windows 10 IoT, but can it run the usual windows games? Quake III is easy to install on the Pi and it looks awesome. This shows what that little processor can do, though, my challenge is to get Quake II running. I'm making an Amiga in mine, but I expect that with so many units sold, new games (like Q3) will be developed to buy and install in the Pi. Obviously older games with 320 x 200 screens in 16 colors don't translate well to modern hardware but by the mid 90's RTG boards and 32 bit processing made high resolutions and huge color palettes programmable and I still get a kick out of Metal Slug and Cruisin' USA on the Pi. (It's hard to believe that was 25 years ago!) Win 10 IoT don't provide desktop interface. If even it do you was aren't able to play games because most of games for x86 architecture and RPi based on ARM one.",4
"On a side note, some browsers will even cache data for pages you havent even looked at yet.  This is called pre-fetching.  While you are looking at a web page the browser will navigate the links in the background and start downloading those pages into its cache, just so they will load faster in case you do click those links. This is due to your browser caching the files.  Caching temporarily saves the data you view to your hard drive.  This includes the text and images on web pages, and therefore the icons in question. What is the purpose of caching data?  Speed.  By downloading the files locally, you dont have to download the data repeatedly as you navigate the website, or go back to it at a later date/time.  In some cases, the time saved is insignificant, but in others, you can visibly see pages load instantly, as you already have the data. As people mentioned in the comments, if you clear your browser's cache, the icons will disappear.  However, if you open a different browser and test it, the icons wont appear, as they were never cached by that browser.",1
"As best I can tell, what happened was the cube moved at an incredible speed, seeming to teleport, but then was restricted to its maximum velocity in the second frame, then had its velocity reduced due to the gravity, making it stand still then fall. The slowness was also caused by the drag, which I managed to overcome by writing my own function to simulate the acceleration due to gravity, though this stopped working properly when I re-did the entire project, causing the cube to drop at incredible speeds as it no longer needed to overcome the 'invisible air resistance'. I deleted everything and started fresh, on doing so, the original code worked fine. During my attempts to replicate what I originally had, I found that adjusting the drag to something > 1 while either turning on/off the ""Use Gravity"" tickmark or changing the gravity from 0 to 9.81 or reverse between two runs caused the problem with no way to undo it. After several more hours working on it, I've narrowed it down to a bug/peculiarity in the Unity engine. Even after reducing drag back down to 0 this problem persists, meaning that although the surface to surface friction was returned to 0, the air drag was not.",1
"Your difference (2) is erased if your clones are ""linked clones"" rather than ""full clones"". Linked clones use differencing disks just as snapshots do. Then the real differences from snapshotting are that snapshots can be made from a running VM; and furthermore your point (1), which occurs because each snapshot has the same disk ID (UUID). In contrast, each cloned disk is assigned a new disk ID (UUID). The distinct disk IDs permit cloned disks to run concurrently. I have a VirtualBox machine representing a ""clean"" install of my OS.  I would like to use this VM for several types of machines - one for simple games, one for programming, one for my audio editing.  So basically I want three or four (maybe more later) VMs which I can start up depending on what I'm doing.  From what I understand, both clones and snapshots could serve my purposes.  I could either make a separate clone for each VM I want, or just branch off several  snapshots of the base machine and install all of my tools on each one. Note that I'm not concerned with running multiple VMs at the same time, I only want to run one at a time. Both: Start with clones, then snapshot them before you make crucial changes, or whatever changes you might want to roll back. Example, your programming and decided you need to add some library that might kill your VM. I would make a clone of the base VM for each purpose you need one for. What I don't understand is why you want separate vm's if they are all base images you're creating... then you could have one vm to worry about, and snap shots would then work best for you The only basic difference I can think of is (1) I can't run multiple snapshots of the same machine at the same time (or can I?) and (2) clones would take up more space on my HDD (since the whole virtual HDD would be cloned multiple times).",4
"Another aspect is that you cannot reduce the size of your iSCSI/VMFS partition on your SAN, however you may be able to reduce the NFS partition depending which SAN you own. A Netapp can reduce an NFS partition (if there's space free of course). NFS data-stores have been in my case at least susceptible to corruption with SRM. NFS speed used to be a bit better in terms of latency but it is nominal now with all the improvements that have came down the pipe. NFS in my opinion is cheaper as almost any thing can be mounted that is a share. I obviously prefer iSCSI but, iSCSI solutions or even FC are a bit more expensive. I would mount a NFS and iSCSI DS and run VMwark and see what your IOPs are, that would probably be the best way. As far as NIC bonding, where would you bond at ? The appliance level ( your NAS ) or at the VKernel level ? All of the benefits of having shared storage in ESXi are available with both iSCSI and NFS.  Beyond that, which is ""better"" to use is subjective, which doesn't belong here. A small side effect on your VM's, when you use NFS, you will not be able to see the disk performance of your VM's. iSCSI bandwidth I/O is less than NFS. iSCSI uses MPIO ( Multi Pathing ) plus you get block based storage & LUN Masking. More than a single initiator can connect to a single iSCSI target, if the target is configured to allow it.",3
"Don't put two subnets on the same VLAN. That's trash, and there are really good reasons not to do that. One could also use an entirely separate physical network for this (or in addition to), but since this is in a datacenter, all networks (no matter if they are on the same fabric or on an entirely different one) should have separate VLANs. What you want can be accomplished with VLANs, and / or a separate physical network. VLANs will give you the ability to isolate the broadcast domains entirely from the public network and the private network, while staying on the same network fabric if need be via tagging. Yeah, definitely keep the NFS stuff on a local private network, if only for security reasons. That's a pretty nasty thing to have on public addresses. Do make sure to restrict NFS to use only the private interface on the NFS server itself (if that's to be a host you manage and not a shared storage solution from the datacenter). Ideally, if you were managing the NFS server, it wouldn't even have a public IP. Setting up VLANs on your hosts is extremely easy. You create a sub-interface with the proper vlan number (such as eth0.300 for vlan 300). Then you use it like any other interface. I imagine the datacenter will be able to provide you with two or more VLANs.",1
"Personally I probably would be cautious when using it on a production machine where having a 100% reliable firewall is important. But it's worth a look nevertheless. In the interest of continued evangelism of the cause, I suggest leveraging Puppet to do your lifting. There isn't presently a good script for handling iptables 4 and 6 rules, but it wouldn't be too much of a challenge to write one either once you adjust to the language. The ultimate bonus at the end of this is that the firewall rules for a service can be written into the service definition and automatically deployed and removed when a service is deployed or removed. While looking into this question I stumbled across ufw (Uncomplicated FireWall) from the Ubuntu folks.  With IPV6 enabled in the ufw configuration, ufw will manage iptables and ip6tables rules in parallel.  This means you can do something like this: Ufw also supports application profiles, which allow you to create named groups of ports.  You can do this: Mediawiki's public git repository is a fantastic mine of configuration patterns and includes an iptables class that will provide a good base to start with. You could edit it to apply rules to both stacks at once by default and have a flags for different rules when you're basing things on IPv4 or IPv6 rules. Since it is still missing from the list: Shorewall is a widely-adopted and regularly updated feature-rich packet filter configuration tool. It had IPv6 support for a while now. Additional documentation can be found at the official FireHOL documentation or at the additional Readme about IPv6. I was also recently confronted with creating iptables-rules for both, IPv4 and IPv6. After some searching I ended up using the IPv6 FireHOL branch by Phil Whineray. Firewall Builder has exactly what you need. You can create one rule set, mark it as ""combined ipv4+ipv6"" and place your ipv4 and ipv6 networks assigned to the same segment (such as ""database network"" etC) in the same rule. The program then generates two separate iptables configurations one for ipv4 and another for ipv6. This chapter of the Users Guide illustrates this, although it uses Cisco routers access lists as an example. It works exactly the same for iptables. Unfortunately the official version lacks support for IPv6. But Phil Whineray has added support in an unoffical branch.",5
"and i'd be tempted to present them with an invoice for ""loss of privacy"" as if it's a product or service they are purchasing from me.  purely symbolic, of course, there's no way anyone would pay it. I'm a Domain Admin, and I have not been subjected to a background check of any kind.  I also work in the government sector.  From what I understand most universities are not doing background checks of their admins. The company I presently work for did a background check on me when they hired me. Standard practice for all new hires. and, as mrdenny suggested, there's a difference between a check at time of employment (where you know up-front that that is a requirement of the job) and a check later where it is an additional condition added afterwards. I would say in this day and age and due to the power that a domain admin has, it's not an unreasonable request. i would resign if my employer insulted me like that, but then i don't work for the military, the police, banks, or anywhere else where a security check would be justifiable. I think it's all completely warranted. There's a lot of bad people out there. If you're one of the good guys, you have nothing to worry about. At a company I previously worked for, I had to undergo a more extensive background check for a federal security clearance because we did some work for government. The only time I've ever had to go through a background check was at time of hire.  However I know that some companies to like to do them every once in a while just to be safe.  If it's in the terms of your employment (which I'm sure that it is) then the company feels that it's warranted and in their best interest.",5
"I don't know if I'd press that it's not a sales call (they always say that) so much as that you're a network administrator who has found an issue originating with their company that is affecting your network and you need to speak to someone as soon as possible to resolve the issue. I don't know if I'd hint that you might need human resources or legal to contact them, but you'd really like to get this resolved with their network or system administrator. Depends on how the conversation goes. Before I've had to deal with other larger companies, client of ours, when we have had e-mail issues with them.  Most of the time it's when our messages have been tagged as spam, although right now it's because one of their systems has gone haywire and is repeatedly sending us e-mails.  Anyway, my question is how do you get in touch with the email administrator.  I've found at some larger companies unless you have the name of the person, the receptionist will refuse to connect you to them (I'd imagine they're acting as the gatekeeper from salespeople asking for ""the person responsible for dealing with your printers"").  I know we could always try to deal with our contact at the company, but sometimes that can be slow and difficult and these issues are usually time sensitive. I would call the main line and ask for someone in IT security.  That usually will get you someone that the receptionist probably won't want to block and is hopefully somewhat technical.  Once you explain your issue they should be able to help you get to the email admin. I also think that knowing a name doesn't necessarily help. Most sales calls come from someone who knows a name, whether from a mutual contact or a conference or someone dropping a name while they were at a conference or just someone passing the sales call on to you. We get plenty of calls that know a name here and they more often than not insist it's not a sales call, but then proceed to ask questions about our network infrastructure, things we purchase, etc... The first thing to do is to make sure that you have covered all your bases. Jeff Atwood wrote a good article about this here.  I would at least make sure you have the ptr record and spf entry before I would bug anyone... If no one is listed when doing a reverse lookup on the domain name, I'd call them and ask to speak to their information technology department. Introduce yourself as an IT administrator for your company and inform her that you have reason to believe there is a network issue where someone at their company is having issues with your network and you need to speak to the person in charge of their email server.  You might get really really lucky and someone might be listed by name on the company's website, too.",4
"What's the proper way to run pry in this Linux?  On Windows and Mac OS X I can simply type pry at any command prompt or terminal. there's a pry file in ~/.rvm/gems/ruby-1.9.3-p194/bin.  I have to call it with ruby_noexec_wrapper in the same directory: If you had installed pry from the distro package, apt install pry, then likely it would be in a normal system bin.  However, the distro version (0.10) is probably older than the one installed by gem, and will not get updated as often (presuming gem, or you, update stuff it installs). bin directories generally contain executables and if you echo $PATH you'll see a number of them there. This prefixes that directory.  It is important not to replace the rest of it, which is why there's :$PATH at the end. I installed rvm and Ruby 1.9.3p194 on a Raspberry Pi using Debian Linux.  When I installed pry using gem install pry, everything installed perfectly but typing pry in a terminal didn't work: To make it permanent, put that export command in ~/.profile.  Note that it applies only to the current shell session, so if you have added it in one GUI terminal it won't magically apply to a concurrent one.  Also note that .profile may not be run if you use a GUI login.  To check just login again and echo $PATH.",2
"If you want more detailed control of boilerplate, etc.  you could do custom %INCLUDEs based upon setting of variables.  The %INCLUDE could set spreadsheet variables, which can then be incorporated into any of the pages. Then in any other page you can insert the statement with %CALC{$GET(DOC_STMT)}% and you could insert %CALC{$GET(DOC_FOOT)}% in places like  When a user creates a new topic, they can choose a template. I've made several templates for Functional and Programming specs. The functional spec and programming spec require different document numbers. I would like for the software engineers to be able to create a new topic, choose the template, then be able to generate a PDF from the wiki page, pulling the appropriate document number, and some other text into the headers and footers. I am not very familiar, and haven't been able to find any examples on doing this. Any help would be appreciated! Then in your PDFHeaderTopic,  you define your standard coverpage.  You can reference thinks like the DCN, and DTYPE set in this topic. In your Document TemplateTopic, you can insert control information for the PDF generation as well as define Foswiki variables for use in other topics.  For example - the template - and root document could contain: I have a foswiki installation for keeping ISO and other documents. I would like to create a PDF from each page. How can I create a topic template with different headers and footers for each topic template?",2
"Anyway, one additional thing you might try, just to be really fastidious about the whole process, is cleaning out your distfiles.  There are scripts out there if you search. I would guess that the next major ""problem"" update will be the migration to openrc once it goes stable (it was done on ~arch middle of last year iirc with only a few issues, primarily with networking changes). It seems like you've got it covered.  The only other tip is to maybe take it a bit slower than a blind wholesale update.  You can always update the packages or dependency chains individually with emerge -DNuav <atom> instead of world (use --oneshot for things you don't want to add in to the world file, like system libs for example). If your goal is to have the absolute latest of all packages, then yes, that seems to cover all the bases.  I'm surprised this has worked for you for a long time, though, because I would imagine you'd get bitten often by blocks, broken deps, etc.  In the last couple years I've become much more careful about wholesale updating, electing instead to do it piecemeal after viewing the update tree.  Gentoo development seems to have passed its heyday.  Maybe it's because of the rise of Ubuntu. Note that it's always easier to do if you keep up to date regularly rather than going for monolithic batch updates every couple of months.  Also, to avoid too many surprises, you might find it helpful to subscribe to one of the mailing lists like gentoo-user (WARNING: ~100 mails a day on this list) since pretty much any major update issues are likely to show up on the list with plenty of discussion explaining how to solve them.",2
"I know for a fact VMWare has certain extensions, or drivers, that you can install to increase performance; try those. As an extensive user of VMs myself, their performance can't match native speeds; just get used to it. I tried programming in VMs, then I got sick of it and got another computer, one for Windows, one for Linux. Install and play any one of the latest PC games (Call of duty, NFS, etc) on your VM, since almost all PC games makes use of DirectX. These games are the perfect test for your hardware and underlying software as-well. In short, your fancy hardware isn't doing anything with respect to what's in the VM.Dualbooting is really the only way to make use of your video cards. I've been a software developer for 27 years. (Yes, back when it was just text and color ascii characters...) Today I'm a Sr. developer for Windows/Web/Mac, and do my primary coding on a MacBook pro (with two external monitors- DVI and USB). If the game play experience was not good, then there should be some problem with VMWare/hardware.  If not, then it might likely to be problem with WPF itself.  I remember reading somewhere that WPF is having few known performance issue under Citrix environment. hence there is chance that the problem you are facing might be related to that as well. While I do 95% of my WINDOWS coding (.NET) through Windows 7 on Fusion (VMWare).  I still have a 100GB partition with BootCamp. Oh and BIG TIP --- Don't launch your Windows 7 BootCamp through Fusion, even though they say you can.  Actually it'll foobar the Windows 7 license, and you'll be calling Microsoft to re-register (because it'll tell you it's an illegal copy of Windows...)  I think there's a fundamental misunderstanding of VM technology here. A VM can never completely pass through information to physical hardware without a layer in between. When you're 'running' DX applications, they're getting run on a virtual video card, that, if you're lucky is having its output accelerated by hardware(so direct X -> virtual video card -> opengl-> actual video card), or if you're unlucky, software emulated (in which case everything runs on the CPU, which, naturally dosen't excel at such tasks- else we wouldn't need graphics cards).",4
"That's all you need; the only difference between this and your proposed solution is that it uses the entity's local directions, so you don't need to repeat those states for all four cardinal directions. Assume we're already following the wall to the right. The entity moves one tile at a time, and at each step: 2) While the ball is falling, as soon as it touches a tile we set the engaged var to bottom (the ball is engaged to a tile that is at the bottom) and we make it move to the left. I'm implementing something like the electric balls seen in this video https://www.youtube.com/watch?v=MWyB56AlAWw (skip to 1:55 and you will see this electrical balls follow tile edges in the tilemap). You can simplify the problem, and try to implement a tile-based entity that follows a wall, a.k.a. the Wall Follower algorithm. There's very little state required - you only need to know what direction you are currently going. I would like to know if there's a more clean way to do this without all this ""mini cases"" or a better way to accomplish this. Instead of coding individual cases for every single state of being, a finite state machine provides a simple, and more importantly, expandable approach to coding such awkward case structures. This method is even more advantageous when used in conjunction with an object-oriented language as states can be represented as class instances.",3
"Every time when there is a forced garbage collection of entries, last_flush field is updated in the neighbor table, neigh_table. When a forced garbage collection of entries is requested, entries that meet both the following criteria are discarded: A periodic work, neigh_periodic_work tries to free unreferenced entries if the total number of entries is greater than gc_thresh1. Now, if I understand correctly, if the number of arp entries goes beyond gc_thresh1 but remains below gc_thresh2, the excess will be removed periodically with an interval set by gc_interval. My question is, if the number of entries goes beyond gc_thresh2 but below gc_thresh3, or if the number goes beyond gc_thresh3, how are the entries removed? I'm trying to configure sane values for the Linux kernel arp cache timeout, but I can't find a detailed explanation as to how they work anywhere. Even the kernel.org documentation doesn't give a good explanation, I can only find recommended values to alleviate overflow. In other words, what does ""actively"" and ""aggressively"" removed mean exactly? I assume it means they are removed more frequently than what is defined in gc_interval, but I can't find by how much.",2
"The big downside to barriers is they have a tendency to slow I/O down, sometimes dramatically (around 30%) which is why they arent enabled by default. In addition to this, things become doubleplusungood when you start to add logical layering on top of standard disks like LVM or Raid. LVM (relatively recently) added barrier support for most LV configurations and mdadm seems to have had it for a little while. It is possible the write to the journal on the disk is delayed because its more efficient from the head position currently to write in a different order to the one the operating system requested as the actual order, meaning blocks can be committed before the journal is. Modern disks typically do re-ordering of requests that are used to speed up performance (by re-ordering writes to make the entire list of requests less seeky), this is called Tagged Command Queueing. The way to resolve this is to make the operating system explicitly wait for the journal to have been committed before committing any more writes. This is known as a barrier. Most filesystems do not use this by default and would explicitly need enabling with a mount option. Most journaled file systems (ext3/4, ntfs) only transactionally protect the meta data. If a power outage occurs, user data could be rendered inconsistent but the meta data is fine.",2
"If this is something you are doing often for some reason, then you could simply create to sets of configuration files and database files, then simply make your named.conf be a symlink to whatever version of the file you want active.  When you need to switch, change the symlink and restart. If so, then I found that the easiest way is to simply remove all but one copy of all these zone files, and symlink all the names of the removed zone files to the single one left.  In the said single zone file, make sure to avoid all absolute references to any of your such domains, e.g. use ""@ IN AAAA ..."" instead of ""example.com. IN AAAA ..."" etc.  Viola, you can now edit a single zone file, and have the changes be applied to all the domains at once. It also wouldn't hurt to go to your directory of zone files and set up version control there.  Something like: Working from the command line can be very fast if you use the right tools. For example, you can use a command like the following to find specific files and do the needed replacement. You need to change the search criteria according to your needs and files naming. Also, I think it will be a good idea to run sed without -i option at the beginning to make sure you are doing it right, or at least take a backup of your files. This will cause perl to create a .bak file for each file it runs against.  You can, of course, come up with more complicated perl in the expression, if that's necessary.",4
"Download a hardware monitor (HWMonitor, HWInfo, Speccy, etc...), check your temperatures.  If they're too hot, get a new heatsink.  ""Too hot"" depends on your CPU, so look it up on the manufacturer's website.  There you will find the maximum operating temperature (either on the spec sheet or datasheet). Heat sinks will get hot. Heat is relative. If your computer is running fine with no abnormal reboots and shutdowns then everything should be ok. And as long as there is thermal paste between the cpu and heatsink. You can use a cpu monitor to check the temperture of the cpu. There are widgets if you are using Vista/Win7 to keep an eye on cpu temps too. With a monitor you can make sure the cpu is running within the correct temp range. Again, if the heatsink wasn't working (read: not attached to the CPU anymore), it wouldn't get hot at all.  All that heat would be trapped in the CPU die instead. My computer gets extremely hot where the fan and heatsink are. The fan works, so I think the problem must be with the heatsink, even though that seems very unlikely, since it's passive.  Thought I'd see what you think before I buy a new heatsink/fan unit. And indeed it should.  If it didn't, I would begin to worry where that heat is going.  Your CPU generates a finite amount of heat, so your heatsink can't magically ""get hotter"" then normal.  It does, however, get colder if it doesn't work.",3
"So - you need to get specs from the vendor for what PHY they'll specifically support.  If we assume it's SR (not unreasonable) then you can procure standard breakouts to OM3 LC connectors.  You'll have a live TX and RX for each of the two ports.  To connect to another server (or a server and a workstation) cross the respective TX and RX links.  It's just a back-to-back 10G Ethernet connection, after all.   As far as what can be connected?  You can find QSFP adapters that break out to 4 twinax or AOC cables (fixed length .5 -> 30 meters with SFP+ connectors).  You can also get a few different fiber options which will be MPO/MTP 12-strand connectors.  Depending on what the vendor supports this will likely include multimode (10GBase-SR) which can support up to 300M on OM3 and possibly some single mode options.  I know of 40G optics that break to 10G LR (10KM).  10GBase-ER is rated for 40KM but I've never heard of a QSFP breakout module that would support these.   The Intel 82599 is a 2 port 10GE controller.  QSFP is a 40GE connection mechanism.  QSFP is a 40G connection mechanism but can operate in a mode where 4 10G lanes are broken out.  Absent any other specs I assume that they're just using two out of the four.  Honestly it seems like an expensive and inefficient way to produce what probably should just be a couple of SFP+ ports.",1
"If it does not, then your next call should be to Amazon to exchange this purchase for another of the same.  Yes, I would take this to be some kind of fluke.  I would describe the issue to them, and rather than demanding a refund I would just ask them to provide another of the same.  This will make them more likely to actually just ship it out.  They might give you some flak about it being software, etc... just be clear and sure about the fact that Microsoft is repeatedly telling you that it isn't a valid product key. From what I'm reading, there are plenty of people installing OEM versions of Windows 7 on Macs via Bootcamp... it actually seems quite popular.  Most of their posted issues are with attempting to use the same copy for the installation AND a VM installation, and having to activate it twice in that instance.  This is NOT the issue you are facing, but I'm including that information because they have no issues with an OEM copy of Windows 7 for the first activation.  They run into issues when attempting to install it again, in a VM (or Parallels) and try to activate it a second time on what may (and can) appear to be different hardware.  Their first activation (just like yours) works like a charm. From everything you describe; the seller, the disc, the product key sticker... it should still activate.  You may have to use the phone activation system however.  I had to recently, when I used the OEM key from a dead Toshiba, to install Windows 7 on a desktop. It is possible (even if unlikely) that you mis-typed the key when you attempted to activate.  An 8 instead of a B, or an O instead of a 0, an H instead of a K, etc.  I know that's a long shot, but I had to include it on the off chance. I can positively say that I have successfully activated OEM installer versions of Windows (up through and including several Windows 7 installations) with OEM product keys.  Most of them were fresh installs of Windows using the OEM keys on the bottom of laptops, but there were also a mess of desktops in there.  In the heady days of Windows XP, I was recycling those keys of dead machines and re-using them.  sigh If the product key was for OEM Windows 7 Home Premium 64 bit, meaning if all that is spelled out above the product key on the sticker, then it should have activated with the disc you have shown us.",1
"It would need to have agents that run on the servers, and ship logs back to a central host.  The host would then process them, and draw useful graphs such as requests/sec, average response time etc.  The idea is to have one ""console"" webpage, which would present an overview of all the webapps graphs on one page, with the ability to drill into specific times on the graphs etc. If you are set in stone on not using the above solutions then mod_apache_snmp & cacti should give you exactly what you want. http://mod-apache-snmp.sourceforge.net/english/ On the other-other hand, you might find Newrelic more to your liking.  It's not free (as in either beer or speech), but they are much more reasonably priced than Splunk.   Is there another tool that could provide something like this which I've missed?  It feels like a fairly common requirement, so I'm surprised I've found nothing so far that does it directly! Currently the best candidate is Logstash & eleasticsearch, but it seems pretty complex to setup and I don't know if it can present a console view, the demos seem to be more based around searching rather than presenting an overview. Cacti is also an option, but it would require some way of shipping the logs onto the central host, and writing custom scripts to parse the logs. I've had great success with Logstash and Elasticsearch myself, and I would put it at 'low to medium' levels of complexity.  Check out the 30-minute video on the Logstash homepage, or drop $15 on the Logstash eBook.  Both were very helpful in getting my environment set up such that Logstash parses the logs, throws them to Elasticsearch, which Graphite then queries to build charts-n-graphs. You seem to be unwilling or at least very hesitant to do any scripting (as per your cacti comments) so a simple out of the box solution like the two you ruled out seem ideal. Alternatively, check out Piwik - it's ostensibly a replacement for Google Analytics, but I believe they have a local-file-parse mode that is similar to how awstats works.  You could set up rsyslog to throw all your http logs to a central server which is then parsed by Piwik in local-file mode, giving you your 'modern' interface. To clarify - Things like AWStats & Webalizer aren't really suitable (and pretty dated now as well!).  I'm more interested in the health of my servers, than where my visitors came from. To clarify: I wouldn't call things like AWStats & Webalizer ""pretty dated"". They sound almost EXACTLY like what you need. IS there something that these products don't do that you need? I'm looking for a log analysis tool to monitor & analyse the logs from an Apache & Tomcat webfarm.",3
"An idea not yet mentioned is to reject the backscatter. All of it that I've seen comes through open mail relays, and there are two blackhole lists which you may find useful for reducing the amount of backscatter you receive. Having said that, if you establish SPF records for your domain, there's a better chance that receiving systems will recognize the forged email as spam.  An SPF records identifies systems that are allowed to originate mail for your domain.  Not all receiving systems pay attention to SPF records, but larger email providers will use this information. In summary, you can use SpamCop to report the senders of these bounces. It doesn't (directly) stop the initiators of the problem, but it may reduce these bounces. Adding these in (along with several other more traditionally focused BLs) reduced the amount of backscatter that I receive by over 90%. Sender Policy Framework (SPF) can help. It is an email validation system designed to prevent email spam by verifying sender IP addresses. SPF allows administrators to specify which hosts are allowed to send mail from a given domain by creating a specific SPF record (or TXT record) in the Domain Name System (DNS). Mail exchangers use the DNS to check that mail from a given domain is being sent by a host sanctioned by that domain's administrators. It is the nature of SMTP (the protocol used to transfer mail) that no validation is done on the sender address listed in an email.  If you want to send an email that appears to come from president@whitehouse.gov...you can go ahead and do that, and in many cases there's nothing anyone can do to stop you. They're using spoofed sender data to generate an email that looks like it's from your domain.  It's about as easy as putting a fake return address on a piece of postal mail, so no, there's really no way to stop it.  SPF (as suggested) can make it easier for other mail servers to identify email that actually comes from your domain and email that doesn't, but just like you can't stop me from putting your postal address as the return address on all the death threats I mail, you can't stop someone from putting your domain as the reply-to address on their spam. As the other answers have mentioned, you're receiving bounces from someone else's emails. SpamCop has previously not called this spam, but these days it accepts reports for this. E.g. I copied the message you quoted (and I've included my Gmail account to  determine my mailhosts) and got this result (which I cancelled).",5
"I have 3 desktops all running Windows XP SP 3 connected to a ADSL Modem-Router. On 2 systems I have assigned static IP addresses and am not facing problems with Internet connection or LAN sharing. On the third system, if I assign static IP address then I am not able to visit ALL web sites. In particular I am unable to access an essential business portal. But if I use dynamic IP, then I am able to access all web sites required. But the problem with dynamic IP address is, I lose connection randomly (I get Limited or no connectivity error). I have to disable and enable my network adaptor to regain connectivity. Some times I have to repeat the procedure a few times or restart the system. I even tried to use a LAN card in place of the onboard LAN, but the problem continues. Some times I do not face any problem for 2 - 3 days and then suddenly, it surfaces. There are no proxy settings and I have tried different browsers like Firefox, Chrome, IE 8 etc. I am also using McAfee Security-as-a-Service. Any suggestions?? Please do not suggest to upgrade the OS as my business software is not fully compatible to other versions of Windows as yet. If you can, set the third machine to a static IP, OR change the lease time to a week or more (604800 seconds = 7 days). It also seems that during the lease renewal, windows (at least XP) disconnects/drops the address during the renewal request, making all the connections drop too.  Booo!  Where did you want to go today? What is the lease time on your DHCP addresses supplied by the router?  I have had issues with Windows machines dropping connections if you're using the network across a lease expiration.  Some home routers default to 1 hour or several hours, most seem to have 24 hours as their defaults.  Even a staticly assigned dhcp address is vulnerable.  This would explain why your two static IP addressed machines are stable, while the dhcp machine is flaky. Oh, and if you try the ""0 = infinite lease"" on the router with a windows machine, I can almost guarantee issues, since windows doesn't parse the given value (-1 = 0xFFFFFFFF) properly, it actually ADDS the current time to the lease time (in seconds)... but doesn't check for the special '-1' infinite value.  So the expiration is one second in the past, causing windows to re-request the lease.  Stupid Windows! Standards are for Following!",2
"You might have a problem with the Server and/or ""Computer Browser"" services, in which case just restarting those services will solve all your ills. Most likely it is an issue with the 128-bit encoding that Windows 7 uses. XP is not compatible with it and can't decode the folder contents. this enabled me to find the shared folder, use shared printer on win 7 machine but it does not appear on the list of machines in the Network Places To do it straight forward you must have the same account name with the same password on both computers, then that'll work. I did this for my wife laptop that is running Windows XP and she can access shared folders on my Windows 7 computers without any problems. This is probably because the ""Password-protected shares"" option is disable that it doesn't work... Also, do you have a password on each computer? If there is one computer that have no password on admin account, sharing won't work, for security reasons. Ran into the same problem, this solved the problem for me although it might be unsuitable for your environment. To solve this, in the Windows 7 computer under Network and Sharing Center. in the Home or Work profile, set it to allow computers with a lesser encoding to access the shared folders. In the Network Sharing Center on Windows 7, you need to put on the compatibility mode for Windows XP.",5
"I don't know what your voltage is, but 90 A would only sound sensible with something like telco-rack 48 V equipment. APCs switched rack PDUs do support a ""staggered"" powerup - you can set a powerup delay on each outlet. Other manufacturer's gear might do that for you as well, but I only ever worked with APC stuff. Apart from that, most server systems and disk arrays (or rather their controllers) also would not power up all drives at once - this helps keeping current spikes down. The nameplate draw is going to be maximum configuration worst-case draw. I'll second the power advisors that ewwhite suggested (I use the APC one when building out racks myself), and using a power meter to figure out your actual configured power use, if you have the equipment already (or if you can get eval equipment to test with).  I use the HP Power Advisor and the APC UPS Selector tool to provide a good estimate of power requirements. The APC tool has a great database of common telco/server equipment, and provides a way to build in some headroom for expansion. I've used things like Kill-a-watt (which plugs into the wall and your server plugs into it) to evaluate single power supply load, or dual with a split power cable. Some of the metered/managed PDUs will report on current draw (pun not intended to be funny) so you can figure it out without unplugging/rebooting your server. But if you have dual power supplies into separate PDUs, which is sensible, you may not get the whole picture.  Also, the storage array and switches may vary widely in power use. I had enterprise storage arrays that warranted 2-3 dedicated 20A circuits each; if your array has multiple controllers and dozens of drives, it's different than, say, a 1u 4-drive storage array. Often the vendor will have recommended practices, and the vendor SE should know how accurate the published specs are and what people are actually doing as far as power distribution. Ideal is not always practical.  Rather than looking at the power supply rating, you should be looking into the data sheet for the specific system for a better estimate on power consumption. The supplies already come over-engineered, so just because a system runs with 2x 1200 Watts PSUs does not mean that it will suck down 2,400 Watts at any point in time - especially since those would be running in a redundant configuration which are meant to provide enough power for every situation even in the case of failure of one of the PSUs.",3
"I googled around looking for solutions, and found some sporadic anecdotes about similar situations.  The consensus seems to be that... seriously, this can potentially be solved by removing the 3ware card, blowing on it, and reinserting it.  (Apparently this solution was devised by Nintendo.) Don't know if that's going to help anyone but on my side this issue was resolved by booting offline. I have no idea what could be the relation between the NIC and the controller but it's the only cycle that worked. I have a RAID array using a 3ware controller which has failed - (not simply degraded, but multiple drives failed, so the array needs to be discarded.) I rebooted the computer to try and get into the 3ware BIOS Manager.  However, when booting, the system says ""Waiting for 3Ware Controller to Initialize..."", and simply hangs forever. Anyway, I actually tried that - I reseated the card and reinserted it.  After rebooting, I still have the same problem - hanging forever. We were having a similar problem, but different in that the card was new and would not work with more than 5 drives. Happened to have a second card on hand used in a similar machine, that one worked in the same machine when swapping it out. Ended up needing to update/re-flash the firmware on the new card, this fixed the issue.",3
"My friends who acquired a china-Pi have had absolutely no complaints at all, and I haven't seen any signs of ""lower"" quality on them. The only ones in my peer and friend network who had complaints about china-bought Pi's are the ones who 1) haven't owned or worked with any Pi's at all and the ones who 2) bought their Pi's from Finland for twice the china-Pi's price :) They do not pay royalties to the designers, use hacked software or microchips, avoid TAX'es and usually come with 1 year ""silent"" warranty- In other words, you will be lucky if you get a reply when things go bad Since you're from Czech Republic I would highly recommend this shop. They sell the RPI2 for about 40 USD. Very professional and enthusiastic. I bought my RPI2 from them. Website is in Czech but you can email Mr. Prenner your queries in English. Accurate and clear explanation: http://goughlui.com/2015/02/19/the-raspberry-pi-2-not-all-from-the-uk/ Personally I have had good experiences ordering things from China through eBay, cheap price work as expected, quick delivery. But they items are often noticeably lower quality than I would expect for the full prices item. None. Not even in packaging or in documentation. Same box with the same contents. The other was 60 euros, the latter 32 euros. Caveat emptor ""Let the buyer beware"". Does sound too good to be true. But the only way you will find out is to try. Can you afford for them to fail and have to buy new ones? If you really support the Raspberry Pi community it is worth buying locally and paying a few dollars more. Not only does it directly support the charity, it supports your local business and global trade, while supporting 3rd party licenses, like h264. I would advise you to buy from a reputed seller locally since you have the option to replace it or obtain warranty later on if something happens to your Pi.",5
"The catch is that the number of sets in the partition is an exponential tower in the parameter of pseudo-randomness (See here: http://en.wikipedia.org/wiki/Szemer%C3%A9di_regularity_lemma). Here is a recent result from FUN 2012 paper Picture-Hanging Puzzles by Erik D. Demaine, Martin L. Demaine, Yair N. Minsky, Joseph S. B. Mitchell, Ronald L. Rivest and Mihai Patrascu. Although the run-time for such algorithms has been subsequently improved, the original algorithm for sampling a point from a convex body had run time $\tilde{O}(n^{19})$.  The Robertson-Seymour theorem aka Graph Minor Theorem establishes among other things that for any graph $G$, there exists an $O(n^3)$ algorithm that determines whether an arbitrary graph $H$ (of size $n$) has $G$ as a minor.  The proof is nonconstructive and the (I think non-uniform) multiplicative constant is probably so enormous that no formula for it can be written down explicitly (e.g. as a primitive recursive function on $G$). News from SODA 2013: Max-Bisection problem is approximable to within a factor 0.8776 in around $O(n^{10^{100}})$ time. The regularity lemma of Szemeredi tells you that in any graph on $n$ vertices you can partition the vertices into sets where the edges between pairs of sets are ""pseudo-random"" (i.e., densities of sufficiently large subsets look like densities in a random graph). This is a structure that is very nice to work with, and as a consequence there are algorithms that use the partition. Algorithms based on the regularity lemma are good examples for polynomial-time algorithms with terrible constants (either in the exponent or as leading coefficients).",4
"The observant reader will note that this is a screenshot of Ardour running on Linux, not Windows. In my case, I'm offloading the DSP to a Linux box; jack has network capabilities, so you can link two jackd instances and have them share audio. This way, the audio from my Windows system gets sent over the network to my Linux system, mixed by Ardour there, and sent back to Windows to be sent to the hardware and/or programs that record audio (OBS, Discord, etc.). This all happens with ~5-8ms latency, which isn't perceptible. I recently wanted to use OBS Studio to record game footage while playing with a friend, though that friend doesn't want his voice to end up in the recording. A quick search on Google led me to test two programs: Virtual Audio Cable (what we French call a ""usine  gaz"" [""gas factory"", a very confusing and hard to use program]), and JACK Audio Connection Kit. Jack can use ASIO drivers, and there's a few pieces you can put together to do this -- in fact, this is the setup I run on my streaming/gaming rig for everyday use. I also tried to use the Connect feature of QjackCtl, but obviously, neither OBS or the game appear there. Note that the config can all be pretty finicky so you may have to play around a lot to get it to work well. Technical details: I downloaded JACK2 1.9.10 64-bit from the official site, and I run it on Windows 10 64-bit. The thing is, OBS Studio cannot record audio from a program, but from a device (the sound that gets output to the speakers, for example). While I found some tutorials on the Internet (this one is the best I've seen), I'm always stuck at the same point: the selection of the JackRouter audio ""device"".",2
"I looked through the registry and found a ton of entries for this update. None of the values or keys could be deleted in order to trick the system into reinstalling the update. Same goes for the actual package files at %windir%\servicing\Packages\ that were referenced in the CheckSUR.log. Windows complains that it needs permission from SYSTEM to modify or delete files even when I am logged in as an Admin. At least that narrowed it down to one corrupt update. Easy enough, I'll just go into Programs and Features and uninstall it. That's when I realized it wasn't even listed as being installed. Something must have gone horribly wrong with this update at some point. I also tried to open up the cab file for this update and manually replace the .mum and .cat files that were said to be corrupt but ran into the SYSTEM permission issue. I've also tried running sfc /scannow in safe mode which gives that oh-so-helpful message that it found problems with files but couldn't fix them. Over the weekend I had the task of installing Windows 7 SP1 to any applicable servers during downtime. All of the servers updated fine save for one pesky Win7 Pro 32-bit virtual machine we use for testing. Since it is a non-critical box I ignored it and went on completing other checks figuring I would get to it later. Today I've spent about 3 hours troubleshooting why exactly this thing won't update to SP1. The error code I get using Windows Update is 0x800F080D. I did manage to find the stand-alone msu installer here: http://www.microsoft.com/technet/security/bulletin/MS11-012.mspx None succeeded. I get this error using method II and III which seems pretty generic: 0x800B0100 No signature was present in the subject. Using google I stumbled upon the CheckSUR tool that can be used to check for any errors or missing features that would be needed for updates: http://support.microsoft.com/?kbid=947821 I then went through the list of all 3 of the manual methods for a manual uninstall listed on this page: http://www.sevenforums.com/tutorials/109213-windows-update-uninstall-console-level.html Is there something I'm overlooking or is this Win7 box basically stuck without SP1 until I can reinstall? Maybe use a LiveCD and manually replace those .cat and .mum files outside of Windows? Would really like to do this in a way that won't make windows update throw a fit later.",1
"Of course, if you have Word installed you can use it as well (even automate it using a macro or VBScript), but don't expect great things in terms of HTML quality. I edited a document, made all my styles now I want to save it as HTML, is there anyway (add-in or extension?) to save it as HTML right from wordpad instead of looking for an online free tool that does it? Try downloading RTFConverter, then use using the command line application Rtf2Html.exe in the bin\release folder. I have found that a good way to code and save files is the Cloud 9 IDE. It is internet based easy to use with live preview options. The files are saved on the cloud therefore be used on any computer anywhere and can be saved to your computer. If you are on a PC the best option is to just copy and paste all your code into 'Notepad' that allows you to save the file as .html You should be able to mass convert your files easily this way, although as with all automated conversions be sure to visually confirm that the final results look similar to the original sources.",3
"The most common configuration for Linux is to have twelve virtual consoles and use the first six of them for terminals (run getty on them to show a login prompt). The rest of them are just blank until they are eventually used for anything. X usually runs on the seventh virtual console because it's configured to either use exactly that one or use the first unused one (which might be the eighth if the seventh is considered to be in use for some reason).  Then init changes runlevel it kills all processes belonging the the current runlevel and prints this message to the system console (it is perfectly normal behaviour that the runlavel changes during the bootup process). The concept of a system console is a generic Unix concept. In Linux the system console can be configured to work in different ways. One way is to use the currently selected virtual console as system console. So if init happens to say something while the seventh virtual console is active, it ends up there. You are seeing this message because you have done something that shuts down the system (halt, shutdown, reboot), changes runlevel (init ...). Hitting Ctrl-Alt-FX does nothing but change the console on which you are on. Usually graphical mode starts X on tty7. At the moment all I see is the message INIT: Sending processes the TERM signal. What does this mean?",3
"If you are trying to find both the clusters of weeks and the clusters of hospitals where outbreaks happen then you might be more successful using  The techniques I have used till now find me clusters where in some cases weeks are away from each other, for example the 7th and 37th week fall into the same cluster as can be seen in the output given below but I want to achieve continuity in weeks because outbreaks span over a few weeks I understand the reason of the results I am getting but I want continuity, if anyone could help. I have used Hierarchical and K-Means clustering for both determining clusters of hospitals as well as clusters of weeks but my real goal is to cluster in such a manner that outbreaks can be detected using data from consecutive weeks and at the same time the cluster of hospitals in which the outbreak is noticed is also found out. I have a dataset from some health institues. The data contains information about malaria cases on a 52 week range. The dataset has 52 columns, one for each week and about 16 rows one for each hospital reporting the number of cases in the particular week diagnosed in that hospital. Example of the dataset with 9 weeks entries :- 1) a simple moving average filter on the weeks,  playing with the number of weeks you are averaging - trying 3-5 weeks as the averaging window.",2
"In the image below, you can see that there are multiple instances of Outlook.exe running (which is to be expected), but that the CPU usage is 0%. Upon investigation, I can see that for some reason it just stops processing requests. For example, there were approximately 15 people logged on to the server this morning, and all of a sudden I was getting calls about the server 'freezing'. When the server does this, I cannot do anything. Processes will not end (or start), and I cannot even restart the server (I have to forcefully power it off). In the performance tab, the same is evident of the CPU usage, and the PF usage is at around 3.2GB (4GB of RAM installed). I have seen this behavior in other server software where nod32 is installed (as well as random crashes). This included all version of the software up and including 5.0.x. At my place of work, we have 4x Windows Server 2003 Terminal Servers, and lately one of them has been very crash prone. Disabling the real-time scanning component solved the issue for us. ESET support wasn't able to help us in any other way. We have not however, updated or tested version 6 so maybe it does fix it.",2
"Note if you're using Amazon's web panel, when you open the ""Launch Instance"" dialog you can filter the list of AMIs by ""Free Tier Eligible"", and the CLI has a filter param as well. That was the quickest way I found to see what AMIs are available for the free tier micro-instances. You can get Ruby on Rails, Django and many other EBS AMIs from our project BitNami  The EBS are completely free and we try to keep them up to date You might want to check out ami-c2a255ab. Its a 10GB EBS-based Ubuntu Lucid server. You will have to manually install what you need but it seems to be the only 10GB Ubuntu server right now. Amazon web services recently introduced a free tier, where you basically get free stuff to try out AWS and run tiny sites and projects. Basically it's free as long as you remain below a certain limit of bandwidth, disk storage etc.  Since going over the limits can quickly become quite expensive (for a hobbyist) I would like some recommendations or suggestions about which AMI's I can run on the free tier, for the purpose of trying out Ruby on Rails and/or Django.",4
"Personally I've used ActiveXperts Network Monitor for this - it's a no-nonsense Windows-based monitoring service with a very easy to use configuration tool... it has built-in checks for stuff like web site replies, sql queries and can handle most custom scripting and snmp if needed. It's not free, but it's extremely cheap imo. I monitor a few server ports with the freeware version. For easy setup there's a nice web interface included. There, you FIRST create your servers (""devices"") and THEN (on the ""device"") you set up the ""sensors"" you want.  Have you tried PRTG Network Monitor? It can monitor web servers. It also has a built-in SQL database monitoring (mySQL, MS-SQL, Oracle). For SMTP/POP monitoring, you can set up a ""round-trip-sensor"" that sends an email via SMTP and checks whether it reaches your POP server. Maybe the freeware version is already sufficient for you. It supports 10 ""sensors"", which could basicly be 4 ping sensors, one email sensor, one ftp login sensor and 4 SQL query sensors or similar. There are a bazillion products that do this. I've tried half a bazillion of them. They range from the very simple to the very complex. Based on your responses to the answers posted so far, your best bet would be to peruse some of the vendor's web sites, download the ones that look promising to you, and try them out. http://www.google.com/#hl=en&source=hp&q=server+monitoring+software&aq=f&aqi=&oq=&fp=baa94940edcea411 PRTG also sends notifications when one of your servers is down. And you can see nice statistics and graphs for up to one year in the past! You see, I love it! ;-) I can't believe I am mentioning ""Whats up Gold"" in the same thread as Nagios but wug is about as simple as you can get.  Also every network is different so any product you install will require some work. We used to us OpManager which worked nicely for us. It's very easy to setup and run a simple monitoring scenario on (it looks complicated, but it's not just point and click web interfaces, automatic discovery and configuration, etc). In the end we decided to go with Nagios for all monitoring (previously nagios monitored our datacenters, and OpManager monitored our office servers)",5
"Solution two: Don't do it on autopilot at all. Make a mini-game where the player has to use thrusters to approach the planet, and if they hit it at too high a relative speed, they blow up, but they have limited fuel as well. Make the player learn how to solve the intercept problem! I am creating a 2d space game and need to make the spaceship intercept a planet. I have working code for straight line intercepts but cannot figure out how to calculate the planets location in a circular orbit. I would fix the location at which to intercept (graze the circle, at the ""outgoing"" side of the orbit.) Solution one: Deny the premise of the question. The quantity that is ""slippable"" in the question is the angle. Instead, fix that. Point the ship straight at the center of the orbit.   Now you just have to adjust the spaceship's speed so that planet and ship reach that point at the same time. Note that the rendez-vous could be after N more orbits, depending how far away the ship is, and how fast the planet is orbiting the star. The game is not scientifically accurate so I am not worried about inertia, gravity, elliptical orbits, etc. The question is: given that the ship moves in a straight line at a given velocity, and the planet moves in a circle of given radius at a given angular velocity, and the starting positions of the planet and ship, determine what direction vector the ship's straight line should be in to plot an intercept course.",3
"Putting it in /home is a bad idea, as that directory is for home directories. Your shared media directory is not a home directory. Consider that if you use /home/media and later need to create a user called 'media'. According to the Filesystem Hierarchy Standard, /usr is for ""shareable, read-only data""; /usr/share is for ""read-only architecture independent data files. ... for example, a site with i386, Alpha, and PPC platforms might maintain a single /usr/share directory that is centrally-mounted"" Similarly, avoid directories that are managed by your distribution. /usr/share/media is a bad idea, since a package in your distribution might well use that path. Less pedantically, /home (as suggested by raphink) seems like a good choice. You probably have /home on its own partition, both so that you can easily blow away the rest of the OS without touching user data (eg, when doing an upgrade or reinstall) and for ease of backup (as everything you care to back up is stored under /home), and for space management reasons (on most home boxes, the partition with /home ends up being the one that runs out of space first). Above all, remember it's your filesystem so there's nothing wrong with creating a new top-level directory. /usr/local, /opt and /srv are defined places where you have control of the namespace underneath, but /opt is usually for application directories and /srv for data that the host is serving up. /usr/local/share may be appropriate, but that's just too long a path for my liking. FHS compliance is not an issue. FHS is a standard for your distribution and applications to follow. A distribution or application should never create a top-level directory, but as a user and system administrator, the filesystem namespace is yours to use as you wish.",2
"I'm trying to understand the hype around cloud computing and whether it could be a better alternative to a traditional data center in terms of costs. I can see the benefits of cloud if you are a small business, but what if you were lucky and became a big giant? A website that hosts videos, images, music, games, etc. would need an incredible amount of storage. If you were aiming at the next big thing on the Internet and if you eventually had to store, say, 200 petabytes of data, would you pay 200 petabytes * 0.15 per GB = 30M a month or would you buy your own storage? I know the latter comes at a price in that you need to pay for electricity, IT staff, hardware replacement, housing, data replication, etc., but I still feel like the former seems more expensive. OR maybe I'm wrong? Of course storage is not the only variable that costs. You need to pay for bandwidth (probably the most expensive), HTTP methods like GET, PUT, and DELETE, and virtual machines. A website with high traffic and a large user base would consume those in high doses.",1
"If it's acceptable to have a few seconds of DNS failure before the swapover occurs, you can create a simple shell script to do this. Non working pseudocode follows: Have a dns service address at each site that is a VIP on the loadbalancer.  Then active/passive loadbalance each VIP across the two dns servers, favoring the local one. If you are using load balancers anywhere in your site, you should be able to configure them to  have DNS as a virtual service.  rotate makes the resolver pick one of your nameservers at random, rather than using the first one unless it times out. timeout:2 reduces the dns timeout to two seconds, rather than the default value. My Kemp Loadmaster 1500s can be setup to do round-robin with failover. That would use their service checking to make sure that each DNS server is up every few seconds and divide the traffic between the two servers. If one dies, it drops out of the RR pool and only the ""up"" server gets queried.  So timeout is quick and rotate make him use both to round robin the load, without any VIP/VRRP/staff to manage, just 2 dns servers doing their job...",5
"If there are some files that you know you want to delete (i.e., you want to bypass the Recycle Bin and permanently delete them), you can select the files, then press Shift+Del or hold Shift while you right-click and delete. Just going this myself right now.  I have not seen the word defrag mentioned yet so for future readers I throw that one out there.  Chkdsk to see if there are issues and do a defrag if necessary / if it has been awhile since the last defrag. If none of that works then get Process Explorer from SysInternals and see if you can tell what is going on when you delete. Does the performance change based on your recycle bin being empty or full?  Even at 1%, if you have a 500 GB drive that is still 5 gigabytes of files in your recycle bin, which can be a lot of files if they are typically pretty small files. This allows you to selectively bypass the overhead of using the Recycle Bin without completely disabling it. http://answers.microsoft.com/en-us/windows/forum/windows_7-files/windows-7-deleting-large-files-is-extremely-slow/f2b32bf0-bab6-4935-9002-8127d9ca066a Anti-virus is another possibility.  I have also seen other 3rd party tools that install themselves in place of the recycle bin.  Check for that as well and try disabling them.  If nothing else you could just start disabling all those applications running down in your system tray and see if that makes a difference.  Click Start, type %temp% in the search bar and press enter. Delete all the files by manually selecting. I find it works a lot better to remove files from the recycle bin based on how long they have been there, not size.",5
"If you're not running any CGI from /usr/lib/cgi-bin/, then it's unnecessary.  That said, it's a production machine, so tread carefully. Here is the part of my current apache config that refers to cgi-bin stuff.  This info was included in the default configuration in debian.  Should this be removed on a live/production machine? To break of a flamewar (sorry, that should really be discussed elsewhere but I just need to say it): My opinion is that even upon installing a daemon it must not be started automatically. For any decent site the default config is almost always wrong. (Yes I know that I can set dpkg options so that it won't be started, I'd rather like it to be the default) Personally I always remove the default vhost that comes with debian from the sites-enabled directory. I also replace the apache.conf with something that is sensible for our environment. Those a are config files and dpkg (backend to apt-get) should expect them to be changed. Tools like puppet run apt-get in a way that will keep the ""old config"" in case a package updates it's configuration, and if you run apt-get interactively it will ask you what to do if it detects a change in the configuration files. That is any change not just a conflict that can't be merged automagically. After all even if it could be merged it could yield some configuration you don't really want in your system.",3
"Now double click it. To open>see, read or play and edit a folder or its file you need to drag>copy or move it to desktop. You can move it back when you are done. The simplest way is open Computer folder where you see all your drives. Here you need to create a shortcut for ftp server. To do this, on empty space in computer folder right click and click on 'Add a network location' click on it and follow the prompt to create shortcut for your network ftp server. If you have password for it or is anonymous do that. You can give it a name too. It will appear in Network location on Computer folder. probably the ftp server process does not have the right to write to c:. Try c:\somedir\somefile.txt instead ftp.log will capture the output. Adding the 2>&1 means you capture both standard out (normal output) and standard error (any error messages from ftp.exe).  This is probably because FTP.exe take control over the shell. So redirections, that are handled by the cmd.exe have no effect. What you can do, is use the -s:filename option and redirect the whole ftp output to a file. It will contain more then you want, but you can take care of that later. This doesn't really answer the question, but have you considered writing a script to do it, then redirecting the output of a script?",5
"If the systems let you connect via telnet or ssh or something similar, you could script the password changes in a relatively straightforward manner. If the password changes have to be done via a web interface, writing tooling to deal with the variations would probably be more work than it'd be worth but I'd at least try to make sure the new password was pasted in from a reliable source rather than hoping I could accurately retype it 400 times. NOTE: Changing passwords on a regular basis does NOT improve security as much as is commonly believed.  If anything, it may encourage folks to pick inferior passwords, because picking a new good one every N months is a pain in the patootie. It only helps if you believe someone is reasonably likely to have stolen your existing password and if that password's getting out exposes something you care about or has a risk of being leveraged for rights amplification. I myself use different pass for different sites, unless they are all leaked out, I don't need to change them at one time. Also worth noting is that you should make sure the sites have been upgraded before changing your password. Federated ID systems simplify this somewhat by providing a single point to change the password for multiple systems... but of course you have to decide whether you trust those ID hubs. LastPass have heard you and posted a blog entry explaining how this can be done. And the bottom line is: you need to do it by hand site by site. You can try to use Dashlane utility which includes Password Changer feature (it's free) which can change dozens of passwords in a single click. you do a batch of password changes , and log the session using AHK. next time you want to change password. take out the AHK script and change the password only. you only need to play the AHK script again. Since your question probably doesn't lend itself to an easy answer, I would propose that you change the passwords of websites based on how vulnerable they make you (loss of money, loss of privacy, loss of reputation, etc.)",5
"It seems that after the animation has completed one iteration and all of the BitmapImages have been loaded from the source URIs, the animation runs smoothly.  I suspect that the BitmapImage is not loaded until the Image who is using it for the source is loaded.  Consider using WriteableBitmaps instead of BitmapImages and Invalidate() them after setting the Source to ensure it's loaded. Anyone have any good ideas on how I can use preload those images so that the flicker does not happen? Anyone have any good ideas on how to get these into a sprite sheet so that I do not have to have use separate files?  I haven't found a good way to do it with Win8 XAML / C#. I'm working on a project that requires doing sprite based animation in XAML / C# for a windows store app.  I've worked out a method for displaying the animation, but it causes flickering during the initial load of the images.  The method I used basically involves creating a class which subclasses canvas that loops through all children during the its CompositionTarget.Rendering event.  If any of the child objects are of type SpriteImage (another class I created that subclasses content control and is a container for the image control that displays the current frame) it calls the update method so that the SpriteImage may display the appropriate bitmap image for the current frame.",2
"I am unable to run both simultaneously (services will fail to start) due to port conflict, so is there a way to run both simultaneously that allows unbound to provide DNS service to the machine and VPN, as well as OpenVPN to also listen on port 53 UDP? Ports below 1024 are best left alone because they generally are used for something important. None more port 53. Go back to using port 1194.  Otherwise, if you are really bent on using port 53 twice (are you behind a firewall you do not control?), place either service inside a VM, and make sure that the VM is not behind NAT, but has instead an IP address on your LAN.  I am trying to run a OpenVPN tun server that listens to port 53 on UDP. I also am running unbound DNS on the same machine to provide DNS and filtering to the VPN and the machine itself. You mean that you changed the regular UDP listening port for OpenVPN, 1194, to 53? Why did you do that???  You don't want to use OpenVPN for this, if I understand what you're trying to do...  you probably want Iodine If a VM too much of a hassle, you may try a lighter solution, a Linux Container (LXC) if you are on Linux. Ubuntu has some scripts that will do all of the work for you. It may be possible, but it's a Really Bad Idea. In order to accomplish this, you may very well have to do some C coding to patch this functionality into OpenVPN. Don't do this. The only way to do this without two different ip addresses, is to use a different port for openvpn, since dns cannot operate over any other port, unless you're using a custom compiled dns server and client that uses a different port  (although I've never seen such a thing in real world usage although I'm sure it's possible). A common use case for tunnelling traffic over port 53, is to break out of captive portals. Is this what you want? Then you might want to look into DNS tunnelling.",5
"The point is: The method as provided is effectively one line. For me it makes perfect sense that ""A cached report bean should only be used if it is the requested one"". All other stuff I think does not belong to this method. Therefore it must be located in the environment I made up. As only one method for review is provided I can only guess how the environment looks like. So I made up a setup to include the method with the semantic I analyzed. Throwing an ExceptionInvalidReportRequest is not the responsibility of this method. Why ask for the usage of a cached ReportBean when it is not even passed in as a parameter? This should be handled before this method is called. If ""strRequestedReportID == null"" and """".equals(strRequestedReportID) have the same semantic this should be unified as early as possible in previous code (maybe where the request occured) so the provided method and maybe all other methods need only to handle one case. I suggest to use an empty String that is the neutral element for String operations. Try to avoid multiple returns. Breaking the control flow can hinder you to apply refactorings like extract method. So you should be sure that your code does not violate SRP or it will never change again when using multiple return statements, break or continue.",1
"tl;dr Previously I was using RD Gateway and it worked pretty well, I didn't have to install anything in the client. However, my ISP has recently blocked port 443 and 80. After some investigation, I determined that our proxy also allowed communications on port 8080 (I was able to open a site I hosted on that port) so I figured maybe I can change the RD gateway port from 443 to 8080. But after some digging and days of troubleshooting, its seems like its not possible. and that brings me here. Is there anyway I can remote my computer at home from another location (work) where the only way I can access the internet is through an http proxy? I know you mentioned without installing additional software on the client, but you said it's only a preference.  If you can stomach it you could try logmein which is free and accessible from a web browser. Is there any other way I can remote my computer through an http proxy, preferably without installing additional software on the client? I don't really care how a ugly hack it might be. or maybe some web app I can install that will function like RD using a browser?",2
"6) Your copy will be gone, but you will have a blendshape1 node for your base mesh in the channel box on the right (under Inputs). Click it. You should then have two fields beside it, Envelope and  (when I tested this just now I created 2 cubes so my copy was named pCube2). DON'T delete history after this. You will delete the blendshape1 node from your model. Creating the blendshapes deformers should be the very last thing you do. So if you have more than one shape you want it to animate to, create all of them from copies of the original base mesh, then select all of them (base mesh last) then do step 5 onward. You will have a field in the blendshape1 node for each shape you combined.  Notes: Blendshapes are supported in Unity, but if you combine joint-based animation and blendshape animations together on the same mesh the joint-based/keyframe animation takes precedence, and your blendshapes will have to be hand keyed because your driven keys won't work in Unity. If you go that far. 8) If you are exporting as fbx then importing in Unity (instead of just importing the Maya file) then I think you have to bake the animations first (in the fbx animations section). When you do that it will let you select the frames you want to export as well. 4) Modify your copy to be the shape you want in the end. Delete its history again. (When you modify stuff you keep a history stack on the object. I'm telling you to delete them because they will bite you later if you don't.)   7) Change the value of that second field to something between 0 (meaning not applied) and 1.0 (meaning fully applied). You can key those values. So goto frame 1, set pCube2 to 0, right click it and select key selected. Goto whatever other frame you want to goto and set it to 1, then right-click > key selected.  You have not used joints to animate it I see, but did you use a blend shape, or nothing at all (?). From your question, it sounds like you are keyframing mesh edits, which is not ideal.  I saw your post on Stack Exchange and followed you here. I can't comment yet as I just linked y SE account. You could also ask this on answers.unity.com, but whatever. 5) Select the copy, add select the base. Go Animation Shelf > Create Deformers > Blend shape options > Advanced, select ""Front of Chain"". Click Create. The ""standard"" approach to this would be to use either joints or blendshapes in Maya or just animate it in Unity.",1
"In addition to the other recomendations I like to use Fiddler to see exactly how long individual requests are taking and how much badwidth is used.  It runs from the client so it shows me how long each request takes from that persepctive.   Seems to me you need to know network connections for each process, the amount of traffic for each socket connection, the response time for each socket and a way to visualize the data. Can't hurt to perform actual load testing from multiple locations as well. Run a local load test, and then run one from various locations outside of your data center. The local tests should give you a good baseline of what to expect at LAN/local conditions. Then you can compare that to what you get externally, and you can see what sort of loss you're getting on the networking end. Obviously, the external tests are going to have higher load times, but you have to determine what's acceptable in that realm.  To see if it's the DNS server, you can try to bypass the DNS server.  You can do that by adding the relevant host names to your hosts file.  If, when you have the addresses in the hosts file, the web server seems to be responding much faster, then investigate the DNS further.  If site performance still sucks, the problem is most likely elsewhere. More advanced; You can also query the DNS directly with nslookup or dig.  Make sure you query your web site domain's authoritative DNS servers directly to see their response time, as opposed to the response time of the local DNS server, which may have a cached copy of the entries.",4
"Method 1 (Look-ups) would be that the DrawComponent knows it needs to check state before drawing. It calls ""GetComponentWithType(Type t)"" to get the StateComponent (or more likely an IState interface), checks the state, then draws.  Ditto position, rotation, etc. A lot of people are still getting used to component systems, and there can be debate on the good or bad ways of doing this. But why not use a reference to PlayerEntity? Instead of having a reference to the Drawable in the Entity, I'd have a reference to the Entity in the Drawable. I'd also make the Drawables observe the Entity so that when the Entity changes, the Drawable is notified and can pull any relevant changes. If the PlayerEntity has state that controls entirely how it is drawn, pass the PlayerEntity state into the SpriteDrawables draw function. That does mean that there's a coupling between PlayerEntity state and SpriteDrawable. Note that the PlayerEntity state could be a separate class (e.g. PlayerEntityState). If don't want this coupling, or you want to control arbitrarily how it is drawn depending on other state in your game then have the object that aggregates PlayerEntity directly aggregate the  Drawable so you can control it there. I would suggest giving all components a reference to their parent Entity. Your player entity does have a base class, right? And that class should also have a method for adding components, where it sets the component.Parent = this. The property setter for the component can handle all the subscriptions. If you need, you can have a method on the entity that fetches any desired component. Myself, I used a generic method and stored a dictionary from types to components: I suggest you have a look at the Mediator pattern and messaging. This is a nice way to avoid coupling. One thing is for sure: whatever is responsible it must know about the specific type of Drawable it is operating on, you can't change the texture without knowing it has a texture. You did this in CreatePlayerEntity, because at that point the type of drawable is known. If you want to do it elsewhere  then you may need to rethink your design. I think I've mostly come into agreement with Gerold Meisinger though, and have been looking into F# as an alternative.  Because I think a post he made on StackOverflow is very accurate: the more componentized I've tried to make the design, the smaller and more function-like each component becomes until I've just got a messy bag of states and chained functions trying to pass the states around. According to Gerold Meisinger in a blog post, there are basically two ways to address the problem of inter-component communication: looking it up on the parent, and message/event passing.  A third would be reference holding, which is what you were debating.  My own attempts to implement such a system basically showed the same thing.   I've been researching component-based designs pretty heavily myself, and while I'm still in some pretty heavy confusion, I think I can offer some thoughts on this. But that is to say that there's a spectrum: at present you've only got 1 entity containing some state and one component.  I'd probably opt to have the drawable component receive packets of ""DrawableInfo"" through events.  But just keep an eye on the complexity or pretty soon you too will be wondering if switching to a functional language might not be better. The basic foundation is that you have an Entity class whose only job is to hold parts and manage them.  If you wanted to change the animation (say the player's state changed) then you'd have a StateComponent or maybe a PlayerState component, and it would need to communicate ""My state has changed"" to any other parts that need to know that. You could also use the parent Entity as a kind of message board, holding a log of posted messages that each component checks for interesting messages. This is a slight optimization since it makes both the sender and the receiver ""lazy.""  Each sender only posts one message (to the host) and each receiver only asks for messages when it needs them to update, and then only gets the latest message, ignoring any irrelevant in-between states. DyingComponent -> {send message via Entity/Mediator} -> DrawableComponent {receive message and switch to death animation}. You'll note that each method has complexities and problems that would have to be worked around.  If you stick with OOP though, I would say that's just part of what you have to pay for flexibility though.  I was very confused about the whole thing until I realized that the Factory pattern is essential because the factory is the only thing that actually knows how to hook it all up in the first place. Method 3 (Reference) is an optimization of method 1.  Instead of asking for a reference to the component, it just has one already.  Either the factory gave it one, or it stored the reference after it asked for it the first time. eg. Your entity changes action, so it notifies all observers that something has changed. The Drawable calls entity.getAction() to find out what action is underway, and sets its own animation accordingly. Method 2 (Events) would be that the StateComponent emits a message when it changes, and any subscribers to that event get alerted.  Typically the factory would wire that up, adding a DrawComponent, then setting it to have message handlers to watch StateComponent for changes.",5
"DNAT happens in the POSTROUTING chain, that is, after Linux has made its routing decision. Linux selects the outgoing interface for an IP packet by selecting an entry for the next hop via a lookup in the routing table. However, I do not know if it is possible to implement this ""different interface for every outgoing TCP connection"" with it. It would require TCP connection state tracking to make sure a single TCP connection would always use the same outgoing interface. Even if it would be possible, it requires lots of effort to implement and good understanding of TCP/IP networking. This means, that the interface which to use for the outgoing packet is already selected when the packet enters the POSTROUTING chain. The, the DNAT rule cannot be used with an IP address that is not bound to the outgoing packet's interface. This is why your packets will have the original interface IP address. To actually get the behaviour you want, you need to somehow alter the routing decision to get Linux to use another interface. Policy routing is the mechanism used to alter the default routing behaviour of Linux.",1
"Depending on your routers you could also use dynamic routing protocols to provide some WAN redundancy for egress traffic to the Internet at least.  For example if LAN 1's Internet connection was down the traffic could be sent to LAN 2's router and use it's Internet connection and vice versa. Your note indicates that these LANs are located at the same location.  If so this could just be a routing problem.  With some NAT if both networks share the same subnet.  But you'll likely need routers that are more capable than average consumer equipment or a layer3 managed switch that supports static routes if you don't need NAT. Basically just need a static route in each router to the other routers subnet.  This can be done over cat5e type cable if they two locations are close enough.  For longer distances this would require either fiber or metro ethernet if that's available in your area.  This will provide much better performance compared to a VPN solution and leave your Internet connection free to do other things. You would install a box with openvpn on both subnets, setup openvpn with a static key, then on your routers setup a static route to direct traffic to openvpn.  Both LANs will need to be on different subnets. You need a VPN.  I suggest openvpn.  Its very easy to setup, compared to other VPN solutions.  I would start with the simple static key method. The best solution if your routers support them is a site to site vpn connection. Then all the routing is taken care of for you when the connection is setup",3
"Will you be able to cope with the limitations of 1 processor, 1 Gb RAM and 4Gb of storage as per the Microsoft site A single CPU license costs $6000.  If you have a dual cpu server with 6 cores that would be 12,000.  I'm willing to bet you are going to eat up more staff time mucking with these additional integration efforts.  You will also need to buy more windows licenses if you're thinking of scaling. Tune your existing application or spend some cash for the tools.  Doing some basic query tuning is not that difficult - look into sql profiler, query execution plans and getting a handle on what your worst performing queries are.  The company I work for has an old sql server based legacy system, it's been around for about 7 years old with a 13gb database. There are two servers, web + sql. If we were to partition the database on dates and make a copy for each year, each copy would be <2gb for that year. This would make the system faster for a smaller amount of effort than trying to performance-tune it. Also it would allow us to scale it out onto more than 1 sql server.  My question is, if we do this we won't need SQL Server Standard Edition licenses, ie Express should be ok as it's under 4GB. Are there likely to be any problems by using Express as opposed to standard? Are you prepared to handle the negatives, like the fact that you loose referential integrity and query performance? Please note that Express Edition miss scheduler, so you won't be able to schedule a backup inside SQL for example or any other script. You'll need external scheduler that runs sqlcmd.",5
"Your best best would be to look at monitoring software such as Nagios. Any good monitoring solution will be able to poll this information, although you may need to install agents on your target machines. If you have SNMP on your server then you can poll it and collect the data.  There is no network port that just already has the data streaming off of it ready to be harvested.  Setting up a monitoring system is probably the way to go if you want things like process monitoring, CPU load monitoring, service monitoring and that sort of thing.  If you want hardware info like temperature, and fanspeed that is more commonly available from SNMP and can be intergrated into a monitoring system. Is there any way to get system information(cpu load, mem usage) through the network for example through a raw TCP socket or over xml-rpc, both in nix based systems and win32? Tried looking for some software that provides this, but does windows provide this natively and are there any nix distributions that will do this natively? You can get this information from SNMP for both windows and Linux. With windows you have the additional option of using WMI.  Not through ""a raw TCP socket"", as some form of agent will always be required to send that information, be it built-in ar add-on. I use Nagios for this, as well as many other monitoring jobs.",4
"1Off-topic: back in highschool one of the math teachers ran a chess club during lunch. Against one friend I almost always won games that were ranked against him and almost always lost when unranked. Irked him pretty bad, as he was ranked quite a bit higher than I was. Another option (which works best when most players are ""top tier"") is to use a  Chess Rankings-like system.    When two players go up against each other, determine their ranking difference. If the higher-ranked player wins, the point change on both is small (and the greater the difference, the smaller that change is). If the lower ranked player wins, the point change is large (and the greater the difference, the larger that change is).  For evenly-ranked players the point change isn't very large, but it would be the same for both players (i.e. it doesn't matter who wins, the point shift is still X).  The EFC Grading page goes into detail on how to calculate the point shift, but you're free to come up with another algorithm. For lower level players I would suggest a floating bracket with a hidden ranking number underneath.  The level is more important during this time, while also having the system track wins and losses in the ranking system, then when the player hits the point at which they're allowed to fight players at the level cap, their ranking can unhide and be a better determination value. By tracking their ranking up to this point you don't dump them into the Big Kids pool with a default ranking score: afterall, you could have data about their performance up until now, so you may as well track and use it. So a level 15 player is a decent matchup against both a 10 and a 20, even if a level 10 isn't a fair matchup to a level 20.  This gives a ""floating bracket"" around which to match players together. So even at the ""end game"" where everyone is level 100 (the cap in your game) players will have different skill levels and this rating system will match up evenly skilled players. You can also allow any two players to face off against each other, regardless of their rating (although you may wish to give them the option to say ""this game is unranked"" so they aren't wagering points1). Generally such systems use [player_level] +/- Y, where Y is some value that is roughly ""fair."" E.g. 5. How wide your bracket is would depend on your game, how important +1 level is, and other factors, which would probably require some testing. There's no easy way to know that the lower-level player has a decent chance against the higher level player as there are a lot of factors: max hit points and damage output are only one.  Status infliction abilities, max mana, gear, etc. etc. can all influence the outcome.",1
"When I try to add the plugins, it asks me for my FTP details. I've setup the server entirely in SSH via terminal. I have not used any FTP related thing in it. So I'm clueless about it Change the owner of wp-content to be the same as whatever user Apache is running as - it might be ""apache"" or ""nobody"". You're having this issue because WordPress cannot update itself unless Apache has permission to change the contents of the wp-content directory.  But, the problem arises when I install wordpress. The installation was smooth but I can't add any plugins or media from the dashboard As Nada stated you need to change the owner of that directory to www-data which should be the user apache is running as. I have a rackspace cloud server running Ubuntu 12. I've setup the web server, PHP and MySQL and it's all working with my custom PHPs and databases performing as expected Next, when I try to add media, it says it does not have enough permissions even though I've granted full permissions to the database user. Besides, I cannot change the wp-content folder to 777 from security point of view. The images thing is pretty common, I've run into it myself before. I am sure you'll find an answer in their support: I also had to recursively change ownership for a directory and it's files to wp-data as mentioned above and all was well.",5
"If you configure VLANs with each having their own subnets, is there a point to have them if inter-vlan routing is enabled? Since VLANs will have access to other VLANs, the security argument is not applicable. I guess broadcast traffic isn't routed between the VLANs? Also, layer-2 protocols (STP, CDP, LLDP, etc.) are considered security problems, and should be eliminated, isolated, or minimized as much as possible. Layer-2 protocol attacks are stopped at a layer-3 boundary. (See LAN Switch Security: What Hackers Know About Your Switches.) Spanning tree loops and broadcast storms affect the entire layer-2 broadcast domain, but are stopped at a layer-3 boundary. Limited broadcasts (to 255.255.255.255) are generally limited to the VLAN = broadcast domain. Directed subnet broadcasts (to e.g. 192.168.0.255/24) may be routed if your routers are configured that way. The modern best practice is to push the layer-3 boundary as close to the hosts as possible. You isolate a VLAN to a single access switch (you can have multiple VLANs on the access switch, but those VLANs do not extend to any other access switches). One way to do that is to connect the access switches to the distribution switches via layer-3. Any layer-2 problems on an access switch are isolated to that access switch. Today, there is almost nothing that requires hosts to be on the same layer-2 network, and layer-3 separation solves several problems. You can place security (ACLs, firewalls, etc.) between VLANs, but that is much more difficult or impossible on a layer-2 broadcast domain. In the layer-2 LAN, every host has direct access to every other host on the same LAN, but not across a layer-3 boundary.",3
"I'm just going to go ahead and post this since i spend three or four minutes on it, but after looking at this code, there is no way this was working as you had it. This site is for reviewing working code, not fixing broken code. If your code doesn't work you need to ask on StackOverflow in the future. Use a prepared statement. It's not cool to throw variables into your queries all willy-nilly. You will anger the code gods. Your first condition (i there are records in the table) does not close the thead or the create a tbody. Edit... you're not closing your table row in the loop either. I'm not gonna bother updating the code snippet. Your delete button is no button at all, it's an input wrapped in an anchor tag (<a>). That's why it's not working as expected. Either remove the anchor and give the input a type attribute of ""button"", or remove the input and put some text in there instead. Here's a quick re-write (assumes you're using PDO - If you're using MySQLi, see the docs to convert the prepared statement to MySQLi)",1
"*Similar to the Per Core licensing model in physical OSEs, all virtual cores (v-cores) supporting virtual OSEs that According to the documentation released by Microsoft I would need to license each core on the host. This documentation does not clearly explain if I need to license every core on every host on my server farm. It also appears I will need SA to allow me to vMotion the servers across the farm. You should call 1-800-426-9400, Monday through Friday, 6:00 A.M. to 6:00 P.M. (Pacific Time) to speak directly to a Microsoft licensing specialist, and you can get more detail information from them. Outside North Ameriac you can use the Guide to Worldwide Microsoft Licensing Sites to find contact information in your regional MS location. per VM. For licensing purposes, a v-core maps to a hardware thread. When licensing individual VMs, core factors do not apply. Note: Licensing individual VMs is the only licensing option available for SQL Server 2012 Standard Edition I am looking to license SQL Server 2012 Standard on my VMware Vsphere server farm. I currently have 3 hosts in the farm with the following spec: Can someone please clarify how many core licenses I will need to correctly license my farm? And confirm that I also need SA to use vMotion. That having been said, with Standard Edition, you also have the choice of licensing Server and Client Access License (CAL), as opposed to per Core licensing.  Do you know how many user connections you are expecting?  If so, it might be more attractive to go down the Server/CAL route.  Otherwise, you will need to license every CPU core, in your farm, that could possibly be used by a ""Production"" SQL Server Instance.  As I understand, you are entitled to Disaster Recovery instances, as long as you are not using them as a standby read-only instance. To license individual VMs using the Per Core model, customers must purchase a core license for each v-core (or virtual processor, virtual CPU, virtual thread) allocated to the VM, subject to a four core license minimum At the risk of sounding dismissive, you should really ask your licensing vendor this question.  They usually have staff dedicated to understanding licensing.",3
"If you don't have any decent jumper wire handy, see if you can find an old chunk of Cat5 solid ethernet cable. Strip the outer casing, separate the strand pairs into individual strands, smooth the ""twisties"" in the wire out (this can be rough on your fingertips if you're making a lot of jumper wires) and strip off each end of the wire. I've made lots of jumper wires from old Cat5 and even some old 25-pair phone wire (which has some really cool color combinations) and it works very well. Be careful with Cat6 though - it's actually one gauge bigger and it might be a bit tougher to insert in your breadboard holes. If whatever cable you find, it there's a plastic '+' separating the 4 pairs of wires, it's probably Cat6 or higher. For LEDs that are farther away from the GPIO pins, let's say the left-hand Red LED, if that was plugged into Row 3, hole D [[ I can't tell for sure in the picture ]] remove the LED from there, run a jumper wire from Row 3, hole D to Row 23, Hole D, put the anode of the Red LED In Row 23 Hole B, the cathode in Row 24, Hole D, one leg of the resistor in Row 24, Hole B and the other leg of the resistor into the ground strip. Looking at your picture, why not make use of the extra 10 unused rows on your breadboard? I am going to assume that you've figured out that ""row 1, holes a-e"" are all interconnected, ""row 1, holes f-j"" are interconnected, ""row 2, holes a-e"" are interconnected, etc. Your unused rows 21-30 aren't used, so, for example, the anode of the left-hand blue LED is plugged into row 19, hole b. Plug the LED's cathode into row 21, hole D, plug one leg of the resistor into row 21, hole B, then plug the other resistor leg into your ground (-) strip.",1
"And nothing else concerning this (meaning, as it states: ""Default title is the name of the file"", as you didn't pass the -h parameter). So it looks like a misbehaviour, at least at first sight. On a second look, I notice your -p parameter: My guess would be that this ""centered header"" is only calculated once, instead for each file separately. So you could try your both approaches without the -p parameter to check whether that introduces the problem -- or try a third approach: As an example, one could print three files ""file1.txt"", ""file2.txt"", and ""file3.txt"" by either of the following commands: When printing multiple files using lpr and the -p option, the name of the first file is printed in the header for all files. How can this behavior be modified such that the correct file name is printed in the header for each file? In both cases, however, ""file1.txt"" is printed in the header line of each file. I would like ""file2.txt"" to be printed in the header for file2, and so forth (without having to print each file separately a la lpr -p file1.txt; lpr -p file2.txt; lpr -p file3.txt).",2
"If creating classifier for your images is your final goal then you can simple get some images for non vehicles. You can try using pretrained model and take the output. You might need to apply dimensionality reduction e.g. PCA, to get a more managable size input. After that you can train novelty detection model to identify whether the output is different than your training set. Either download image dataset like Imagenet, CIFAR etc and sample images that are non-vehicle like dogs, flower etc. Just make sure to include sufficient variety of classes and around 1000 images. Another method would be using autoencoder. So the idea is if the tested image is ""similar"" to the training set the network will have no problem reconstructing the input and hence lower reconstruction loss. But I think the issue might be the lack of training data since I believe 1000 is small for your network to generalize the concept of a vehicle, but who knows if you dont try. I only have around 1000 images of vehicle. I need to train a model that can identify if the image is vehicle or not-vehicle. I do not have a dataset for not-vehicle, as it could be anything besides vehicle. I guess the best method for this would be to apply transfer learning. I am trying to train data on a pre-trained VGG19 Model. But still, I am unaware on how to train a model with just vehicle images without any non-vehicle images. I am not being able to classify it. Guess there's already vehicle label in ImageNet, so the easiest way would be not to train anything at all. You can also easily scrap both vehicle and non-vehicle images from the net. Check this post. 1k images is a bit too little to train DNN. Feature space is hyper-dimensional, so you need something deeper, this means your net capacity will grow. Training with little data will result in overfit. Try also to augment your data set heavily. You can easily make it two orders of magnitude bigger.",4
"I have a lot of threads that writing to DB some data in two tables. tbl_raw_data and  tbl_parsed_data where tbl_parsed_data have foreign key to tbl_raw_data. Im using SQL server, the threads are coming from TomEE server that writes data that came via HTTP requests via JDBC and JPA (ORM). An ORM requires information about the first write (SCOPE_IDENTITY or such) to complete the second write. This means 2 (with an OUTPUT clause) or 3 database (with SELECT SCOPE_IDENTITY) calls in general in a client side transaction. While checking the options on improving the writes (asuming reading time is nu so important) a friend of mine told me that I need to check the Transaction Isolation Level that is appropriate for my logic. If you want more performance, then the best concept stored procedure to do an atomic write (in a transaction) to both tables in one database call. This means 50% or 66% reduction in database calls Bulk writes indeed are fast but will not be logged. If that is not what you want, consider procedure calls with table valued parameters. That way we have realised significant write improvements. The only isolation level that influences writes is SNAPSHOT (and the READ_COMMITTED_SNAPSHOT). Snapshot isolation requires row versioning and row versioning requires extra writes. Read Understanding Row Versioning-Based Isolation Levels. after reading some articles regarding this issue what I understand is that this property influence reading. Now about the 'super-fast' part of the question: the 'super-fast' option for INSERT is the bulk insert path. This requires a client API that understands bulk-insert: IRowsetFastLoad in OleDB, ODBC send_row or managed .Net SqlBulkCopy, EnableBulkLoad=true for JDBC. You do not want 'lots of threads running lots of connections', you want one thread doing lots of inserts, in bulk API mode. Use a producer-consumer pattern in the app to funnel all your app threads to one single bulk writing thread. Is there a Transaction Isolation Level that affect writing? Which isolation level is ""best"" for lots of Threads running on lots of connections? Then there the design: are you using IDENTITY columns or GUIDs or Hibernate style nvarchar keys? Has the ORM designed the database for you? For example, should you be using SSDs for your transaction log files: the drive hosting the log files determines overall write speed because of write ahead logging.",4
"In my first tests with a Raspberry Pi running radvd I found out that the majority of connected devices have privacy extensions enabled, making it impossible for the uplink provider to block a misbehaving IPv6 address for longer than its validity time. Unfortunately, the university providing the uplink requires us to be able to identify the users behind an address. They're currently running a firewall blocking IPv4 addresses in case of misuse (viruses etc.), which is a viable solution because every user currently has only 2 fixed v4 addresses. We already have a list of all MAC addresses that belong to a user (to configure dhcpd) and want to selectively allow/block users ourselves (network fee payment, misbehavior etc.). Is there any rather simple solution using existing software? Should we change our plans and use DHCPv6 instead (said to work not as good as SLAAC)? I already thought about requesting a bigger subnet (/48) and giving a separate /64 to each user, but this would require having a huge radvd.conf with 200 prefixes (+ probably 200 VLANs) and unicast RAs changing a few times per hour, as far as I see. We're operating an IPv4 network (on ethernet & Wifi) in a student dorm with ~200 users and want to deploy IPv6 in the long term, with stateless autoconfiguration on a single /64 prefix.",1
"This is almost exactly why Linux control groups (cgroups) were invented.  This allows a group of processes (for example, all the processes decending from your initial interactive login) to be treated as a single entity for purposes of resource control -- such as limiting the total amount of memory consumed (or cpu resources, etc). I am the unpopular guy who has brought down our high-memory linux machine twice this past week because some processes I have been running ate up way more ram then expected. One way around this would be to set a per process ulimit so if each process goes over a certain amount of ram usage that process will be killed. Sometimes though I need lots of ram for a single process rather than some ram spread out over a bunch of parallel processes. Rather than relying on me remembering to actively manage my ulimits whenever I run a different kind of job at a different level of parallelism, is there some kind of equivalent to ulimit that looks at the summed ram usage for a user, and then kills all or some of that user's processes rather than sinking the whole system?  Cgroups are relatively new, and convenient support for them is largely missing in most distributions.",2
"Below CPU, the us, sy, id and wa values correspond with 'user', 'system', 'idle' and 'iowait'. Like all other tools listed, vmstat shows you the status at a single point in time. Be sure to refresh often ;-) I need to remotely retrieve, parse, and report in a Java program the CPU and memory usage of a Linux machine.  These should be done in two separate commands instead of one to decouple them to make it easier if one command needs to be changed in the future. Apart from that, the ""free"" command shows numbers about used memory, ""df"" about disk usage, and ""top"" about different values. If you really must parse the output from a console command, you probably want to do as less piping and grepping in it as possible. These are pretty much questions from a newbie.  (I have looked at the other related questions and answers, but none seemed to be exactly answering my own original questions.) The ""Java way"" is to use a library, SNMP comes naturally to mind first but it could be too big a hammer to crack such a small nut. The ""parsing"" part of the answer involves playing with standard output and string formatting anyway, unless you use SNMP which (if it is an option) could give you a lot more informations. The ""True Unix way"" is to make scripts using available system information commands: free, ps, w, vmstat, iostat, ifstat, netstat, etc (hint : don't use ""top"", it's for interactive use, dammit!) As far as memory is concerned, mind that the 'free' above is the amount of memory that is free, without counting buffers and cache as free, so you probably want to add buff and cache to free to get the 'real' free amount of MiB's (i.e. physical memory that is directly available to the system for use). If you want to call it from a script or application use the '-n 1' flag so that it does not run in interactive mode.",5
"I've just run my info through both those sites. The first returned only 2 out of 5 domains. Interestingly, those are .com domains, whereas the other 3 are .com.au domains. The second site appears to just do a revers DNS query and is therefore only showing what I have set, not the domains being hosted. Edit: I thought I'd try the gwebtools site against my secondary DNS server, just out of curiosity. It returned zero domains. As far as I know they're just building a database of domain names and the associated authoritative name servers. You're just searching that database with their web interface and seeing a list of results that, through ""normal"" DNS channels would be rather difficult to get (w/o generating a lot of queries). It's a little bit like a telephone ""reverse directory""-- it's the same information that DNS gives out to normal SOA lookups, but given to you in a bit of a ""backward"" manner to facilitate types of searches that would normally be difficult. It's not actually querying the nameservers. It has a database of domains and just looks to see what the NS records for each domain are.  Your original assesment is correct, don't doubt yourself :) Does this not work off what domain robtex has ""seen"" before? It doesn't detect what domains are on a NS, it just lists what domains it's looked up before that have that NS. Spyonweb.com also provides this information. It also show all dns servers registered in certain IP address.",5
"So rm will warn you if you try to delete a file you don't have write permissions on.  This is allowed if you have write permissions on the directory but is a little weird, which is why rm normally warns you about it. I may get errors because one of the files doesn't exist.  Is there any way to tell rm to ignore these files? I'm way late to the party, but I use this all the time. In a makefile, add - to the beginning of a line to ignore the return value of that line. Like so: The -f option means that you will not be prompted if something is not as expected. It does not mean that permissions are not taken into account. BUT, if you have enough privileges to change privileges, you file will be removed. This is the case when you are the owner of a file with readonly permissions for owner (-r--------). As owner, you can chmod u+w, then remove it: rm -f will remove that file. That sounds like almost what I want, but I'm not really sure about the permissions part.  Is there a way to do this? I'm writing a makefile that will clean up some useless files at the end of the compilation.  If a target has already been made, it will of course skip that target and the useless file may not be there.  So if I do this: you newer get error message Problem occurs, if error generate not to STDERR 2 but STDOUT 1... It's take place when command generate not error but warning.",5
"I don't know of any way to do this natively to bind9 if you're using flatfile backend. There are various DB-backed systems which can help automate it.  Or you can script it: For the amount of zones I have, syncing manually ended up being easier than getting any other solution to work. If I had many more zones I'd look into the proposed solutions. I've also experimented with stuffing all the domains to propagate into a special zone, and used a simple script on the slaves to rebuild the named.conf based on what they see in the master zone. Basically the same deal as the text file above, but feeding it from DNS to keep everything in-band. I should probably publish the script before I end up losing it =/ I populate a text file with a list of zones and the primary NS IP for the zone, and stick it on a website that I allow my slaves access to. The slaves fetch this file periodically, and if it has changed they parse it generate a named.conf, and tell bind to reload configs. It's ""automatic"" in the sense that I don't have to manually ssh to my secondaries and update configs, but it's still external to bind9. While this won't go back and clean up any mess that you have currently, it does make it really easy to synchronize machines that you are able to manage using ""rndc"" going forward. Using rsync on your entire /var/named tree works pretty well if you write your zones correctly and make sure named.conf lives in /var/named. It won't work with dynamic updates though, and is sorta against the grain for ""how things should be done"". In the days of everybody and their mom having their own domains, it surprises me there isn't a good solution for this integrated with Bind by now =/ Maybe you're looking for a configuration management system like Puppet or CFEngine? There's extra infrastructure involved, but they can handle distributing a lot of configuration stuff, and could easily include this too. Look at BIND 9.7.2-P2 in which you have the ""rndc addzone"" and ""rndc delzone"" statements that allow you to ""remotely"" add and remove zones from a running server. You could also use a higher level configuration management system such as puppet, to manage your entire DNS infrastructure. That's a bit more complicated though.",5
"If I click on advanced and go to the owner tab, it tells me that it is ""Unable to display current owner."" I have a folder that I can't access from an account that is part of the Administrators group on windows 7. It is not OK to sue the ""Replace all child object permissions entries"" checkbox highlighted in red.  The latter one will replace existing ACLs (permissions) rather than just changing the owner. To see it, you'll need a directory (c:\SomeFolder in this example) that is owned by a different user account and to which you and the administrators group have zero access. You'll need to use the ""security"" tab of the properties window to alter anything via the GUI. This gives you two buttons: ""continue"" and ""advanced"". Advanced gives you a window with the four tabs: ""permissions"", ""auditing"", ""owner"" and ""effective permissions"". Continue gives you just the ""owner"" tab. It is very easy to not read this message box fully, assume it's just another box asking you to confirm something non-destructive and just hit enter to OK it. It's also very easy to assume they couldn't possibly really mean replace because nobody sane would ever want to do that. If you select a new owner and tick the ""apply to sub-folders"" box, hitting OK or apply gives you a ""Do you want to replace the permissions"" message box that, again, really does mean replace permissions. If you don't check the sub-folders box, you don't get the warning and everything behaves as expected. So, is there some way of giving admins access to the folder without taking control and destroying the current permissions. If I bring up the Security tab on the properties, I see ""You do not have permission to view or edit this object's permission settings"". Some careful digging reveals that taking ownership sometimes destroys existing permissions and sometimes doesn't. It all seems to depend on whether you try and do it recursively. Note that Windows does warn you when it is going to replace the existing permissions, but (in the GUI at least) it's very easy to just OK the message without reading or understanding it fully. Note that if you answer yes here, it really does mean replace the permissions. Any existing permissions will get destroyed. If you answer no, you still have no permissions on the folder but are now the owner so can give yourself permissions normally and without destroying any that already exist. I can get access to the folder by re-assigning ownership, but this destroys the current permissions on the folder. If you don't specify the recursive flag (/R), you don't get the warning and the owner is changed without affecting any other permissions.",2
"Run the installer. Unselecting as many of the packages as you can. Noting, that if you select a package that has requirements, it will auto-select the needed packages. Another option is to install these packages, but the first time you do it, save the packages to another location like a USB drive, then if you have to do  I love using Cygwin, but I hate all of the extra disk space it seems to use caching stuff I don't need.  What can I delete to keep my installation footprint as small as possible? Note, there will be a bunch of basic library packages that will be required in even the most minimal install. I have in the past (about a year ago), gotten it down to under 100 megs while still having everything I ever threw at it in terms of bash scripting. I can't use Windows without installing Cygwin, but I've never really had too much of a problem with it using disk space though. I think you can delete the folder Cygwin stores the packages in but every time you update Cygwin it will download the packages it needs. Some people appear to be under the assumption that it is easy to swap out the hardware on whatever platform this person is using. It may very well be that 500MB is actually a large amount of space to sacrifice if they are perhaps using a CF adapter and a 4GB card to run windows on a netbook or similar. Or they may be installing it to a virtual image and this extra information means the difference between it fitting on a DVD unzipped or having to span multiple DVDS and dealing with the issues that come with that, especially if it is being sent to a client that isn't particularly technical for example; In-house training material for the application your company develops. Having one defined image that is the same for all trainees works wonders, makes the environments homogenous and makes the trainer's life easier. Virtual images are a simple way to do this in most situations. My Windows XP machine has 1.1 GB in service pack uninstall data.  I'd delete those files well before trying to trim down Cygwin.  Try using the WinDirStat program and see where all your hard drive space is going - it's probably not Cygwin.  Also, from time to time, you should probably delete C:\cygwin\tmp, as this doesn't get cleared on reboot. Instead of Cygwin, you could also go with more packages that are not intending to solve all the problems Cygwin aims to be able to solve.  There are packages of unix utilities with just the a limited selection of the most commonly used programs.  I note that that my current install is under 250MB, It would have to be four times that size before I started to care about it on my laptop.  Install as normal, run it to check that it operates correctly. Delete the folder the packages are stored in, if required for updates Cygwin will download it again, not the most effective use of bandwidth but depending on your requirements it may be better than keeping the cache locally. When installing Cygwin, on the screen where it asks you where to store your packages, point it at a specific location, say c:\Cygwin\Packages.  On a related note, what is a good barebones set of packages that will give me the essentials, without fluff that I'll probably never run? I would start with the basic cygwin package, and 'page-fault' in any additional commands you need.  I keep a list of common utilities which I use, but might be quite different from what you use.  Do NOT select ALL packages when you install Cygwin.  I did this once and it took about 6GB.  My thinking was then I'd never have to worry about not having a program.  It was a bad idea.  As for a barebones setup, it really depends on what you need. I start with the base Cygwin install and add OpenSSH, Cron, RXVT, Screen, Vim, Git, Curl, Zip/Unzip, and Wget. Those are most all the tools I need, but I just install something else if the need arises.",5
"You can perhaps try this to reinitialize your TCP/IP stack: http://support2.microsoft.com/kb/299357. If still not successful, you might have to reinstall Windows. You definitely have a DNS resolution problem according to wireshark (see http://www.wildpackets.com/resources/compendium/tcp_ip/unreachable for descriptions on the errors), and your virtual switch only has default IPv6 DNS servers and your physical adaptor only has google IPv4 DNS servers. Do you still get the same behaviour when DNS is working correctly? This sort of broken IP stack is usually caused by a firewall. Assuming the trace was produced on 192.168.1.5 itself (which seems like the case based on timing of the packets), then the broken firewall must also be on 192.168.1.5. You probably have some kind of firewall installed on your system, that blocks the outgoing icmp packet. Disable the ""Carte Ethernet vEthernet (Internal Ethernet Port Windows Phone Emulator Internal Switch)"" adaptor (Run ncpa.cpl, right click) and try an IPv4 ping again. If you can ping correctly, it's probably the configuration in your vEthernet adaptor. Re-enable the adaptor. This is not standards compliant. First of all ICMP is a mandatory part of IP, so not supporting ICMP is not valid. Secondly it makes no sense to be sending ICMP packets and simultaneously saying you do not support ICMP. As it appears that you can't ping your own computer, the problem seems to be purely software, as the loopback interface is not passing through your network card. Either Windows has a corrupted TCP/IP stack or configuration for some reason, or something is preventing TCP/IP from correctly working (this can be a legitimate software, like an anti-virus, that for some reason is not working as it should, or it can be a malware that has severe side-effects). Perhaps it's worth verifying if it's enabled.  If it is, try disabling it and then seeing if that helps your problem.  It's helped me in the past with funky issues with VM's and vpn connections. Try disabling IPv6 and enabling IPv4 on the vEthernet adaptor. If still not working, under Advanced set your metric to 1.",5
"With the versions of Samba found in current versions of common distributions you can certainly have a Linux machine act as an NT-style domain controller (this has been the case for some time). If you don't mind commercial linux software, Novell's Open Enterprise Server 2 (SP1 and later) has a component called Domain Services for Windows that'll do exactly that. Cost-wise, however, it'd probably be cheaper to purchase a Windows 2003 server and create your own AD tree. But if you really want a Microsoft-free solution OES2 will do it for you. I believe that taking part in an AD based domain is being actively worked on but not yet ready for production use, though it isn't something I've looked at recently so the support may have moved on. My answer is going to be ""why would you want to?""  You're probably far better off implementing a Windows DC; it won't cost you that much, and you'll be on a supported and more predictable environment.  AD isn't difficult - there's a lot in it, but it's not difficult.  So long as you don't do bizarre or wacky stuff with it, it's incredibly robust and has low maintenance overhead.  If you want the Linux boxes to authenticate against it, you can set it up in Mixed Mode and it will present itself as an NT4-like DC where appropriate. Until Samba 4 arrives, you can use Likewise Open.  We have had a great experience with this on Ubuntu.  Another option that provides a lot of the advantages of AD is Novell's eDirectory.  I have zero direct experience with it so I can't personally vouch, but it's something more than one of my sysadmin friends have raved about -- one using with Linux servers and an all-WindowsXP client base.  Samba4 is going to be able to do that, but it's still in alpha.  If you're adventurous you can play with the latest releases.",5
"I'm having this weird behavior: Firefox (4.0.1, Debian version) is showing the same exact page with two different font sizes if the page is hosted on localhost or a remote server. Is it possible that the hostname isn't resolving from inside the network? Look at resources that can't be loaded in Firebug, you might find that your CSS isn't able to be loaded. I had exactly the same problem - even to the point that some pages behaved and others didn't!  Clearing the cache solved it. I run a bit of JavaScript that grabs the screen resolution from the browser and then sets the font size accordingly.  This allows text to remain legible with mega-resolution screens without it being ridiculously large on low-res screens.  Maybe I'm missing an easier way to do it, but redefining font sizes in em rather than points or pixels didn't work. Is there any configuration variable that could increase font size based on host? Or anything else that might cause this? According to Firebug, rendered font in localhost version is 14.3px while the remote server version is 13px. The size specified via CSS is 13px. Chrome shows both pages with the same size so it's not a server issue.",3
"I found that the firmware update (HP Broadcom NX1 Online Firmware Upgrade Utility for Windows Server x64 Editions (cp025019) in the latest support pack was causing the fans to keep ramping up and down. Very annoying. There might be some settings in the BIOS.  But from my experience it takes a good amount of heat for those fans to kick on so high...  I have only heard them full bore while rebooting or the drivers are not working in the OS correctly. I also noticed with VMware ESXi the fans run a little faster then the slowest speed after I did a BIOS update... Check your CPU usage to make sure some rogue process isn't running the CPU 100% of the time, increasing heat, causing the fans to run full-bore. Some rackmount HP machines that I've worked on in the past were known to run their fans at full tilt until a proprietary driver or software package is loaded in the OS. Then they would be relaxed down. Put it in a cooler room or feed it more cool air. The fans typically ramp up as more cooling is needed. Are you monitoring any of the temp sensors to see how warm it is when it gets loud?",5
"In what seems to be a just released Firefox extension, the original question now has an answer which is both a direct solution to the problem posed (not how to also remove cookies, passwords, etc from the selected domain which happens through the use of ""Forget about this site"") and will handle websites which utilise a page redirect rather than a page content change that is undesirably cached. Great design also. Quick and efficient. If you want to do a force-refresh, which is what I think you want to do, hit CTRL+F5. And of course, Cmd+F5 on a Mac. I'm kinda late to the party here, but I wanted to leave this answer for people who found this thread through googling like I did. The problem was solved for me by accessing Firefox's profile selector (Windows -> Run -> ""Firefox.exe -p""), using a new profile and visiting the website from there. Then when I went back to my regular profile, the website began working normally again! Opening it on the other profile must have refreshed my regular profile's cache with non-corrupt versions of the files. Another approach would be to use Firefox's containers - create a fresh container tab/window using (using a container add-on e.g. Temporary containers, or Mozilla's Multi-account container add-on) and load your site in that container - a fresh/temporary container will have empty login, cache etc so will act like you've cleared these.",4
"Might be an idea to keep you eyes on RedBadgers XPF project, but its going to be a long wait :P (the company was founded by the guy who did our wpf/xna implementation at work) Not really, there is no elegent way to make silverlight/wpf and xna live side by side like that. At work we reimplemented some of WPFs basic controls to do in game 2D layout, but thats over most peoples needs. You better off rolling your own game specific UI code. Yeah doesn't sound like that would be possible as they're both going to fight over control of directX I think. At least XNA has a decent 2D API for you to work with. You could make some simple components like a numerical display, bar display etc and possibly a little an application to visually drag them around and arrange them, then spit out an XML file with the configuration. There are some programs that allow you to design some menus/hud/interfaces in flash and then insert them into your game. But is there something similar for making all of this stuff in Silverlight and then mount that content in your XNA game?",3
"The NagiosExchange check_lpq isn't a good solution here. You might be able to get away with using check_grep as a wrapper around lpq, you'll either need an lpq that will connect directly to the printer, or one that can connect via a working CUPS or lpr-ng. lpstat will try to connect to the IPP port, I believe the LPD service on Windows provides just a standard unix lpr on port 515 (see http://technet.microsoft.com/en-us/library/cc728404.aspx ). You could also consider using check_nrpe and nrpe_nt (or similar) with service/process checks on the Windows system to make sure the services are running correctly; or some powershell to query the print queues directly (e.g. via WMI). You are correct, a simple TCP connection isn't going to reliably tell you the system is operational. You'll need a working perl and the Net::LPR module. You may need to hack it a bit,  especially if you need to add logic to detect stuck jobs. I can't say I've used it heavily, but it has worked fine for me with real printers and lpd (lpr-ng), though I have not used it with Windows/LPD.",1
"tl;dr Remake an non-profit game of PS1 from the scratch, can the company that holds the copyrights of the game sue me/fine me? I'm not planning on selling it or adding in on a platform like Steam (for obvious reasons, copyrights etc), just giving it for free. But I wanna know when I will finish the development, can I share the game on public with other people so they can download it? Recently I started remaking an old game on Unity. Basically I want to remake an old RPG that was released for Playstation 1. I'm not ripping anything from the old game, all the assets are being created by me. I'm creating everything from the scratch, music, 3D models, gameplay however I will keep the (copyrighted ?) names, I will try to create the characters and maps/levels as they were in the original version of Playstation 1 but with better graphics. Can the company that made the original game sue me/fine me because of copyrights reasons even though I'm not planning on selling for money? You can't keep the real names nor the same level structure with improved graphics. That will be a partial remake of a game and that is forbidden by law, even if you are not charging money for the game. You can however copy the style and make it very similar without infringing copyright. Take banjo kazooie and it's rebirth from the same developer Yookay Laylee, or the most recent mighty number 9 which is from the same creator from megaman. Both games keep the same gameplay and core features from their originals; however, feature new characters, new levels and completely different assets. For further proof you can see that in both examples I mentioned, the creators of the original game are the ones that created this different ""remakes"" and yet they can't use the same assets they created years ago.",2
"I don't personally make 3D games, so I don't know if this would work in 3D (or if it is even necessary). The scene layout is the challenge.  One good approach is as follows (and FYI I use a 3D camera looking at 2D content positioned at z=0): I am strictly speaking mobile, not desktop, specifically with Unity and I don't care about the UI, I only care about the gameplay canvas. My asset also includes some logic for placing UI, but that is mostly obsolete due to Unity's new UI system. I really would love to know what successful games have used to handle different aspect ratios on iOS and Android without making a zillion different sized assets. @Marcel's answer and code are great and helped me understand what was happening. It's the definitive answer. Just thought someone might also find useful what I ended up doing for my specific case: since I wanted something really really simple, one sprite to be always on screen, I came up with these few lines: Define an area of the screen that must always be visible regardless of aspect ratio/resolution, and use a simple rectangle transform to 'stencil out' this region. This is your ideal screen shape. Using a simple algorithm I then zoom the camera until the region blocked out by the rectangle is as large as possible while still being 100% visible to the camera. I created the Unity Asset 'Resolution Magic 2D' to solve this very problem (ad: you can get it from the Unity Asset Store, or see more details at grogansoft.com). I added this to the camera and dragged my sprite (it's my background) to the script's only property. If you don't want any black bars (horizontal or vertical) you can put a bigger background behind this one... You typically don't need different sizes of assets - imported textures and sprites with automatically-generated mip maps will look nice when rendered at any size less than or equal to the original pixel size of the image. My asset provides this out of the box with minimal setup, and it works really well on all platforms. Then your main game area is always taking up as much screen as possible. And as long as there is sufficient 'extra' content (e.g. your background) outside of the rectangle area you defined before, players whose screen is not the same aspect ratio as your 'ideal' rectangle will see the extra content where black bars would otherwise go. Issues I have in mind is when there are key things that have to be in certain places and cannot fall off the screen.  Using black bars on top or bottom is unacceptable these days. I'm using the following script which adds a targetAspect parameter to the camera and adapts its orthographicSize with respect to the screen ratio (more details in this blog post): If you use this technique (or my asset), just make sure you design your game to have 'optional' space all around to accommodate screens that are wider or taller than your ideal (to avoid black bars). I've gotten a lot of answers to this question, but they are all generic and generally not very useful.  None of the tutorials talk about aspect ratio and dealing with mobile devices and there are a zillion ways to do it, all seem to have gotcha's and flaws.",5
"I would say that for a once-a-day operation like that, scheduled tasks are fine.  I use them for various reasons throughout our network.  Having a bunch of services running, even if they are idle most of the time, still uses some system resources.  Services, IMHO, should be used for things that need to run continuously in order to respond to events in in a timely manner or communicate with other network services as needed. I would agree with others in that scheduled tasks would be appropriate for your needs. I would add though (as a programmer myself) that as a scheduled task, either have some sort of confirmation be sent to you (email?) that the task ran successfully and/or append results to a log file somewhere for potential troubleshooting. Doing some sort of reporting/logging/auditing always comes in handy should you need it. Like so many questions given to solution developers, it depends. There's nothing wrong with using Scheduled tasks and as long as whatever solution you're developing clearly states in the documentation what/how you do something you should be fine. There are advantages for running something as a service such as being able to script a solution for a non-admin to run a service if they don't know how to use the services.msc console. Or even being able to monitor said service via Powershell or a network monitoring program such as What's Up Gold. With Scheduled task you can run your program on a schedule you decide but you can also add arguments (which you can also do with services technically) but you can do so in an easy GUI format. Again, whatever you decide is easiest and most useful for your situation. I am a programmer designing a solution to a problem that we are having. I was thinking about writing an app and using Windows Scheduled Tasks to run it once per day or so as opposed to writing a service that would sleep most of the time and wake up once per day to do it's thing. But my question is do admins typically use Windows scheduled tasks? The only problem with scheduled tasks is that once you get more than a handlful of machines to manage it's really easy to lose track of what's where. At which point you have to shell out for a task management program... The application would be to delete old files that we generate and cannot clean up any other way (for good reasons).",5
"How many rows does this imply are being considered? If selecting ""all"" means ""look at all those 10s of millions of rows"" (or ""look at most of them"") then it might simply be this is fast as your IO subsystem can do the job, even with a skip-scan on an ideal index. And you might have an application design issue rather than a database one: is the ""all"" option actually of any real use to the user at this point? You would need to capture an actual plan to fully evaluate if the change is worthwhile, but it may be worth shifting that column from an included column to a key column. It would be useful to see the actual query plan for one of the larger instances, and perhaps the extra details emitted when you run it with SET STATISTICS IO ON. I suspect it may be scanning the whole table at that point, or this whole index large index, as I've seen similar behaviour in the past with large static IN clauses. This is where ""index skip scanning"" as implemented by Oracle could be useful, but SQL Server does not support that. As you are constructing the SQL prepared statement in code anyway, you could try many UNIONs to emulate the behaviour: Whether this is any better or not massively depends on the number of rows each SELECT returns. On the number fo rows... Looking at the (estimated) query plan, the only thing that sticks out as something that could cause an issue if estimates are incorrect is the residual predicate on c7.",2
"Is really a replication what you need? it is common practive to periodically restore from production environment databases to other non-productive envs. This of course can be done in many ways but is tightly related to you development model in the company (or the testing path you have to use for your applications). If you have currently have the app_v1.0 in production and you receive app_v2.0 you actually restore (you might read duplicate) you productive DB into another environment, upgrade the app, perform all needed tests and than decide to upgrade the prod. Be aware that certain changes can require extra steps (adding a not null column without a default is always a favorite), but that is the basic procedure. Also be aware of how your data might affect your ability to promote changes (key constraints, etc.) which is why tools like Red Gate's SQL Compare and Version control are worth the money. If you have a website it is supposed you will track all DDL/DML performed to upgrade your db schema to support the new website and then deploy them in prod during the release. Data replication is not here the solution so. Probably you want to have a software able to summary and keep schemas changes to later apply them in production but of course this is not done by replication. You might use a data modeling software or other more sophisticated software like the one from RedGate. If not, just create a file and script any ddl changes you make and store them in a file, then when you are ready (and have backed up prod) you can run the script against prod to make your changes.  If you have access to a db version control system (ala Red Gate), you can have it create a deployment script fairly easily.  No I don't work for Red Gate, but I've had to do this job with and without their tools and it is a lot more work without them.",2
"Note that in general, if you want to use ls with a glob pattern, you should specify -d. This is because the shell expands the glob pattern into a list of matching names, passing each one to ls as a separate argument. If you don't use -d, ls will list the contents of any directories whose names it's given. In bash, if the extendedglob option is set (it is by default), you can negate a glob pattern by wrapping it in parentheses and prepending a bang (!). For example, !(*.gz) matches all items whose names don't end with .gz. See the Pathname Expansion subsection in the EXPANSION section in the bash manual page for more information. If the filenames have anything else in common - e.g. length of name, number of periods in name, name ending... you can simply adjust your glob.  In zsh, if the extglob option is set (it is not, by default), you can negate a glob pattern by prepending a caret (^). For example, ^*.gz matches all items whose names don't end with .gz. See the FILENAME GENERATION section in the zshexpn manual page for more information. Usually tail -f /var/logs/myLog*log will work. However, if the end of the filenames is unpredictable, and really the only way is to exclude files ending in .gz, it becomes more complicated. One possibility is this:",3
"Personally, I'd use a Linux-based router with a recent Shorewall firewall package (you do want some security, right?), that can certainly do it.  In fact, I just did that using a used server, with Ubuntu 9.10 and the latest Shorewall.  OpenWRT should let you do it pretty inexpensively. Basically, it would be setup so that whenever someone does a ping from the internet to ..*.1 it would come through the router and be routed to our computer 192.168.1.3. And also for all outgoing traffic from 192.168.1.3 it is marked as going out from ..*.1  say we have 3 computers 192.168.1.2-5 well we want for 192.168.1.3 to have the static public IP of ..*.1 where our gateway has *.6.  In PFsense, you can set up ""Virtual IPs"" on the WAN interface - these can be natted through to the lan via 1:1 NAT or 1:N NAT, depending on what your needs are. That feature is normally called ""1 to 1 NAT"".  Look for that in the documentation of whatever routers you consider.  Domestic routers don't normally have that feature, but any open-source or corporate grade router should be able to do it; all the BSDs and Linux have that feature in their kernels. we have a 5 public IP addresses. What we basically want to do is have a router(one single router) that will translate a static private IP so that it uses a static public IP(as in, NOT the gateway public IP)  As Kyle said, yes, this is possible depending on what you're using for your router. Since you didn't mention a specific model/technology, I'll assume you haven't chosen a router yet. In that case, I'll recommend Pfsense. It can run on a spare PC you have sitting around or altertatively, one of several low-voltage embedded platforms. PFSense is based on FreeBSD, and as such is free (beer) and free (speech).",3
"Can anyone speak precisely and non-urban-legendfully on these UPS and power strip interactions, if there are in fact ones worth thinking about? Granted this is not exactly what you're describing, but I would say based on my experience that nothing should be run ""upstream"" of any quality UPS. Plugging a power strip in ""downstream"" of the UPS may not be up to fire code (per @sysadmin1138 excellent response) but it's probably not going to hurt anything. Strictly speaking, don't hang a surge protector that uses MOVs for protection off a UPS that doesn't put out a real sine wave (most are kinda squarish, line power is a sine wave). We've had cheap power strips blow up when connected directly to a wall (with poorly conditioned power coming from a bad transformer), and we've had the same happen to a UPS.  When our transformer was on the fritz, the bad power would flow through the power strip, but wouldn't flow through the UPS (we had printers die that were connected to a power strip, but nothing die connected to the UPS, other than the UPS itself).  Sometimes I hear that you shouldn't plug (UPS brand X / any UPS) into (power strip brand X / any power strip) because of some interaction leading to poorly conditioned power, reduced battery life, massive explosions spattering the room with battery acid, and so on.  Sometimes I hear that it's the power strip that you shouldn't plug into the UPS.  What I haven't gotten is a clear idea of how reliable these recommendations are or how generally/specifically they apply. The problem might not been the power strips per se. It might be the number of devices you can now hook into one source for electricity. Increasing the number of parallel circuits increases the amount of current being drawn through that power source. The more current, the more heat produced. If you are lucky the breaker will trip before it gets to be too much for the outlet and the devices on the daisy chained strips/UPS. If you are unlucky and/or the wiring is not up to code you will start an electrical fire.  Do not hang a surge protector off your UPS, as it will waste a big % of your power, it has to do with the way the cheap surge protector interacts with the non sine wave power from the UPS. In our specific case, the server had a clock that ran ridiculously fast, as in, it would gain 5+ secs per hour. Removing the ""double UPS"" fixed this. I have seen some very, very bad server interactions when you plug a UPS into another UPS, and run a server off it. You must also remember that the flow of electrons is limited by the size of the pipe/wire it is going through. Even if you have great wiring there is always a chance you will reach the maximum capacity of electrons the wire can carry. This means devices will be fighting to grab the flow of electrons. Many will have their digital electronic components fall into indeterminate power levels on the chip power pins (Digital is on or off - it needs input power with a certain voltage on the power pin, if it falls to less power than tolerance it becomes indeterminate). This means the electronics will begin acting erratically or stop functioning. That said, I'd never plug a UPS directly into a power strip, but probably wouldn't hesitate to plug a power strip into a UPS if the power strip was decent and powering lower wattage items (lights/switches/etc).",5
"In addition to using the Nagios object inheritance as mentioned you should write a script that automatically adds/deletes the host definitions from data extracted from your configuration management system (AD in Windows-land?). That way Nagios won't get out of date and you don't have to do busy work. Well, you can just split up the directives into different files with the cfg extension. Nagios will automatically find them if the are in the config folder. You don't need to put everything in one cfg. You don't even need to link them explicitly. In addition to that, you can use the cfg_dir directive in your top-level config (nagios.cfg) to specify an entire directory of configuration files.  Any file ending in .cfg will be included, which allows for configuration to  be toggled simply be renaming files. If your systems are reletively similiar, you should try and use object inheritance to limit the amount of code you have to modify in the future.   I use hostgroups religiously.  By adding machines to a hostgroup and monitoring that serivce on a per-hostgroup basis, adds, moves, and changes are straightforward.  I wouldn't dream of running Nagios any other way.",4
"For large scale enterprise-level reporting frameworks, I have heard a number of companies turning to Thunderhead Weaknesses - none :). Actually, there are a couple. First off, it's a different approach to design and if you want to design in an IDE using detailed property windows this will be different. Second, if you need to build your report in code with the logic there, we're not a good solution (nor are any of the others listed here so far). If you need this say so and I can give you a list of what to look at. I can't comment on any of them but AlternativeTo.net is often handy when looking for  alternative applications.  The link goes straight to the Crystal Reports page. I would recommend QlikView for your reporting, i can't think of anything it can't do.  I have been using it for several months now and it's extremely powerful and versatile... Strengths - You design the reports in Microsoft Word, Excel, and/or PowerPoint so minimal learning curve and you can easily do things that the other reporting systems, because they're banded, find either difficult or impossible. It is also much faster, you'll find report design & revision about 10 times faster. And you can have non-programmers design their own reports. There is a learning curve involved but there are plenty of free tutorials/videos on their website and a very helpful and active community forum you can post to if you need help with anything. Crystal Reports have actually commissioned a comparison, although it may be slightly biased towards them. They do state, however, There is a free personal edition you can download to get a feel for it.  You can pull in data from any source, manipulate and display it at your desire. I've used SQL Server Reporting Services to some good effect. Although it comes with SQL Server, you can configure it to refer to multiple data sources. I have seen some people mention issues with large-scale deployment, but can't comment on this directly.",4
"I'd love to be able to hibernate one Windows instance, boot into another, then switch back to the first instance and have it resume my hibernated state. I think you can do this if you install both Windows versions on different physical volumes and then use the boot device selection of your BIOS to choose the one or the other. On my computer this can be entered by hitting F11 at system boot. Is there some sort of hack or software product that would enable that, maybe by renaming the hibernation files based on the the instance I'm booting to? Funny thing is you can actually do this when using one instance of Windows and one of Mac Os on a mac: hibernating your windows will basically shut off your computer, then you can boot it up again under Mac Os, and if you turn it off again and boot windows, you will restore your initial windows session as though it had never been switched off. I've done this with Windows and Ubuntu. However you will need to fully shutdown the computer before installing the 2nd OS (If it is not already installed) But after than I could hibernate both OS's and switch between them. So in the end, I guess it does have something to do with the bootloader. It would be interesting to know if you can reproduce the same behavior with some Linux and Grub or Lilo, and with multiple Windows... I can't provide a detailed and simple answer, because I never multibooted more than one windows, but you have to options: I understand the technical issues with why it's not supported - I could make changes to the filesystem while I'm running instance two that would cause my hibernated state in instance one, but I'm a big boy and don't need to be protected from myself. Now I'm not sure if this will work on Windows, but assuming they are set on 2 different partitions, I don't see why it wouldn't work the same way as it did for me.",5
"If you have a Load Balanced site with multiple nodes each individually capable of supporting your full capacity one way to do this is to upgrade a single node at a time. Once you have upgraded the first node you can place it into production and hold for a specified period of time before upgrading other nodes. In the event of a bug or other issue you can simply roll back. This solution is not possible for all applications/and configurations however load balancers can make fail over between nodes much, much easier. change the apache host config file and restart apache maybe? Or is the site smart enough to find out itself that it's broken? where the site root is redirected to the site_files_1 directory, than after uploading new versions of files to the site_files_2 directory, and testing, the site root is now redirected to this direcotry. So if major problems arise which were missed during the testing phase, the site root can be redirected back to the site_files_1 directory. I was wondering what the best way of minimising disruption to a website caused when major changes may introduce bugs.",3
"My computer fails to boot from SSD when optical drive is connected. It has worked perfectly for something like a year without it and now I have the same issue again when trying to connect it with another SATA cable that I ordered. I have disabled the DVD drive from all boot settings and added the SSD as first everywhere but it does not help. There is also a boot override option that allows me to forcibly boot from the SSD but same result. I have also tried fiddling with the fast boot settings and other stuff under the boot menu in bios but nothing has an impact. The optical drive is also empty (no CD/DVD) if you are wondering. Ok i managed to ""fix"" this myself i think. I had to enable UEFI boot only and now everything works like a charm. Could it be that my workstation Mobo with integrated mini-ssd for faster boot is causing trouble? Or could it be the optical drive being too old? Any help is appreciated, haven't been able to find anyone with the exact same problem. Any idea what I could do? I have Hirens boot CD on USB and I will happily use it if needed. The Mobo is a workstation version that I bought ~4 years ago and I believe the optical drive is 6 years old and connects with SATA.",2
"As Johannes Rssel writes, Synergy is the undoubtedly cheapest and perhaps even the best solution to this problem. It's cross platform (Windwos, OS X and Linux), open source and (after a bit of configuration) completely invisible and easy to use. I recommend the dual video output solution, as I am using the same solution. I run 1 tower and dual monitors. Good luck with your setup, if you have any more questions please do ask! Not quite sure how exactly you imagine this to work or look like. I can't quite make sense of it, but that may be the time of day here. I have two desktops each with single vga monitor output. I am interested to have an extended monitor setup, where i can extend the desktop of one of my machines and that way i can also work on the other using remote desktop. Any idea how can i do that? I am running Windows 7 on one and Win Server 2k8 on another. what you are asking cannot be done with (2) separate computers. You would need 1 computer with a dual monitor output video card to run extended monitor support. You can though use 2 monitors next to each other and have a KVM switch so you click back and forth between computer towers.",4
"Please notice that I did not select a religious choice for you in the question about handling the middle initial and that I did NOT attempt to address the general case of having a blank slate designing a model with columns unlikely to need nulls. If your perspective is ""from relational theory"", as you claim in that comment, then the answer is simple : null is not a value, and a thing that contains a null is not a relation, and a database that can contain a null is not a relational database.  From the perspective of relational theory, there just is no ""best"" choice, because there is no choice at all. IMO the decision to make your attribute a new table has more to do with normalisation design than NULLs. 1 is a more traditional and natural representation IMO, however 2 has the benefit that inner joins are guaranteed, and also can ease code which uses the DB as you no longer need to check for nulls (although undoubtedly you will need business logic to check for your 'dummy value' in places). I don't see any 'relation theory' why you would make a table to store an attribute, just because that attribute can be null. Now moving on to a particular implementation case, perhaps involving an entity with more than 254 columns and using an Oracle database, it is likely that overall performance will be improved if the columns are ordered by likelihood of having a non-null value, breaking ties by perhaps the columns of your primary key in order and then it is largely up to you although you might put index component columns leftish just because it is easier to grok when considering a narrow bit of a wide listing. Following on, if many uses of the table don't inquire on the value of the likely to be null columns and the ones beyond column 254 end up in another table with a one-for-one key, you can usually devise a physical structure and access plan that ends up being easy to use, sensible and fast. Next, the question is not clear about whether you have a blank slate for relational design or not, and whether you are targeting a particular RDBMS or not. (Note that this does not mean that at the physical (implementation) level, there could not be constructs such as null.  But these constructs should not be exposed to the database user, because the DBMS is precisely supposed to insulate its users from implementation stuff, leaving him to interact with the DBMS exclusively at the logical level, which cannot involve nulls.) If you have a wide existing table containing lots of frequently null columns, it is likely you can make it more efficient if you re-arrange the physical table so that most of the nulls are contiguous to the right hand side of the table. If the table is over 254 columns in certain RDBMSes, you may profit from breaking the table into two pieces such that the overflow is less often referenced and when you need the whole it is via a view sliciing the two physical tables together one-for-one. As for null values, that is a religious war which side of you are on you can most succinctly answer by reflecting on what your implementation choice is if you want to have an option to store someone's middle initial (and you know that some folks don't have a middle initial).",4
"As a simple thought experiment, if I buy a brand new game of a single density DVD, I could almost fit the entire image within a 4GB GPU memory space (You'd need 4.7GB).  And of course they're not going to do that, but again perspective.  This then begs the question, if they don't need the space, why would they want to use 64-bit addressing on the card (necessary to support more memory)?  Why waste the hardware and penalize the performance for the vast majority of their clientele. It doesn't make good buisness sense. Something I never understood, why do graphics cards usually have small amounts of memory on them?  I know memory isn't everything, but surely it's pretty cheap just to load 4gb of memory on any old graphics card? There isn't a strong market need for large memory graphics cards yet.  The GPU market is driven by gamers.  Even the most high end games don't really need double precision.  They're fine with single precision, the textures load faster 'n such, and what's more, in single precision land, they can cram twice as much in the space it would take to store a double. It's not cheap, per se, but it could be done, yes. It'd raise the price of the card, but it could be done. In addition to RAM price, I suspect the memory controller and cache is designed to us as few bits as possible for addresses. It might prove too slow to use more memory. Because it's usually not your run-of-the-mill RAM, it will be high-speed, multi-port and possibly in a very compact size all of which make it more expensive for the manufacturer. The more important reason, however, is that in a 32-bit OS, VRAM gets first dibs on the 4GB addressable space. If you have 4GB of VRAM, that leaves precisely nothing for anything else, and your machine wouldn't even boot. Bummer! This does nothing for my CUDA programs, and I for one would love a GPU with 48 GB of GDDR5 memory -- I drool at the thought -- but the market isn't there to build such a card because the gamers and the game developers just don't use that much memory. Now with the recent uptick in GPGPU especially within the research, financial, and government sectors, you will start to see niche cards developed which much more memory. But they will be slow to appear and be much more expensive. Aka Tesla Question is, though, why bother? If your card isn't fast enough to actually need 4GB of VRAM, it's wasted money, what's the point. You can have as many textures as you like, but unless you can actually pull that 4GB of data through for every single frame, it's wasted.",5
"For a tcp session to work the source and Target IP addresses and ports need to be consistent per tcp stream. While it is possible to have multiple systems respond on the same IP, you are asking for pain and difficulty when things go wrong. Casting my mind back to the distant past I recall LVS (http://www.linuxvirtualserver.org/how.html) which has 3 different mechanisms for load balancing and would be useful reading. I have not used this for a very long time, and I am not convinced it's better then Apache or equivalent load balancing solutions though. [The Internet has moved on and this document is 30 years old. Egress filtering and security in depth we're not really a thing then, and computers were much, much slower. Also, you can't use it to strip https -> http, very useful today] This depends on the setup in most cases all traffic would go through the load balancer in both directions as this is the ""safest"" and easiest configuration.  There are also advantages from a network security POV as you can better isolate servers from end users. If you want to do this without having the load balancer in the critical path you may want to do this at a DNS level rather then a load balancer level.",1
"Someone seems to have told you that Windows NT runs on top of the BIOS, from which you've deduced that interrupts are fielded by the BIOS which then ""raises to Windows"". As for discovering a way.. There's a community that managed their own open source bios firmwares coreboot. I suggest reading their developer documentation and possibly finding some of their devs on IRC and talking with them. Do you strictly need to do this from within Windows? If you're only interested in BIOS, you may have better luck exploring this using Linux utilities. But that's beyond my present experiences. You can probably accomplish this, after all, there are constant interrupts passing through BIOS, I believe - events like keyboard input, disk io, basically everything. So, I'm going to say ""Yes, there is A way"". That's completely wrong, and an erroneous model of how the operating system works.  Aside from ACPI, system management mode, and (for some display adapters) VESA support, your BIOS firmware is largely uninvolved in anything once Windows is up and running.  Interrupts (SMM aside) aren't fielded by the BIOS.  They are fielded by Windows' Hardware Abstraction Layer, which does all of the low-level jiggery pokery with APICs, IDTs, and interrupt vectors necessary for fielding interrupts and handing them over to the Windows kernel. To call functions in the BIOS firmware API  which are (mainly, but not universally) invoked with the int instruction, and are thus confusingly also ""interrupts sent to the BIOS""  and trace through their execution, you need DOS (or some other 16-bit real mode operating system).  In particular you need a DOS debugging tool.  You can run this tool either on a real DOS, such as OpenDOS or FreeDOS, or in a virtualized DOS machine, such as NTVDM on Windows NT.  Be aware that the latter virtual DOS machine will affect firmware functionality. You cannot invoke BIOS firmware API functions  which expect the processor to (variously) be in 16-bit real mode, 16-bit v8086 protected mode, or 16-bit ring 0 protected mode  in 32-bit ring 3 protected mode.  The machine code simply won't execute correctly, even if you had a way to map it into virtual memory.  Such firmware functionality is simply inaccessible from application mode on operating systems like Windows NT or Linux.  (It's not really useful to applications on such operating systems, anyway.) To trace the handling of interrupt signals in Windows NT, one needs a debug kernel, communicating with a kernel debugger utility.  Then one can do things with debugger commands such as !idt, !apic, and so forth.",2
"They will be located in the VHD, the virtual hard disk that VMware player uses to boot and run the virtual machine. Extracting it would require you to mount the VHD in Windows. It may be easier to simply share the file with your Windows machine instead. You need to think of your virtual machine more as an actual physical PC. If you pick up a hard disk, can you see what OS is installed? If you look at your CPU, can you tell what application are running? Easier than setting up Samba or an FTP server on the Linux VM, why not install VMware Tools? That way you can share a folder between the host OS and guest VM - with the folder actually residing on the host. Save the files you want to access there and you can access them from your host OS. Which Linux are you running? If you're running Ubuntu, install the vm tools is quick and easy. (I'm on slackware, not so quick and easy but do-able.) The directory containing your virtual machine will have files that are: configuration information, virtual disk files, log files, snapshot files and some other misc files. Just like looking at a hard disk doesn't tell you what files are on it, you need to do something to see the files in your VM. While VMware does have some built in file sharing methods, you could also just share files as if your virtual machine was another PC on your network. That is, use Samba on Linux.",3
"Here's some details about partitioning database tables in SQL Server, this might give you some guidance as for MySQL. And here's an interesting article about performance partitioning in MySQL. I don't know if this is doable in MySQL, but table partitionning can be useful, and even bring up faster performances. Let's consider a geographical application where you store the people's adresses for the 48 contiguous lower states. Now, we're facing a 5 000 000 rows data table here. This shouldn't hurt the performance much if INDEXes are suitable for the query needs. You should perhaps look for optimizing the INDEXes first. Afterwards, if some performance issues are still present, consider partitioning your table based on a discriminating value. Careful here, I'm absolutely not talking about creating VIEWs, but partitioning data TABLEs, which is very different. You would have a few tables like member_1, member_2... and a member table that would be a MERGE engine. That said, be very carefull. If not done right, partitions can degrade performances. A good partition could be records archived each year, the key to the partition could be the year of the record. I dont know specifically for MySQL, but a number of DB engines (Oracle for example) can partition tables. This looks a bit like what you are refering to. Partitionning can help performances when you know that you will be working most of the time with only a subset of the data. Or you would search on the MERGE table when you need to do a search on the whole table or the table seperation was not what was needed. For example, where member's surname is Smith. You would then have what we might call a base table which would be partinioned into 48 other tables, one for each state. The only time I've ever found it acceptable to duplicate a table it when testing the results of a complex SQL statement on it. Even then, you're usually doing that on a test database rather than a test table on a production database. You would query the individual tables when you knew that the data you are looking for would be there: for example, if member_2 has members have registered to the site 6 months or ealier and thats the search you want to perform. You have to be careful when using MERGE if you plan to use it for performance, because while it may help some of the time, it might hurt in other cases. Depending on your partition definition, this base table, upon SELECT, ""knows"" what table to query against in order to have the required information data depending on what state is queried. It's like an intelligent interface that you might query, and the query is simply redirected to the right underlying table, without letting the user know about this underlying table.",4
"I'm not sure of the steps off hand, but have you tried creating a new folder toolbar? If you right click the Taskbar, and select Toolbars, by default there is one for the desktop (so you can activate that to see how it would work). I think that there is an option on that popup menu to create a new toolbar for an arbitrary folder. So, you could create a folder that contains all of the shortcuts to all of your games, and then you'd have access to those shortcuts from the taskbar. A medium person has about 50-100 application installed.....each one should be reached in maximum 2 clicks, this can be done best by stacked links near start menu. ""Please note that as opposed to the original QuickLaunch toolbar, the LaunchBar toolbar window cannot be manually resized, only positioned. It will always just be one button wide/high and adjusted to the number of buttons present."" You can probably try building such a program yourself which will have a custom Jump List. You would then just right-click on the icon for that to work. Indeed, both 7stacks and Jumplists are useless, you can do this with LINKS and custom toolbars. In order to have a configuration like: Would certainly be possible that way but involves some coding. Altough the .NET APIs for the new taskbar are pretty nice.",4
"If I use multiple wifi base stations on the same network will that improve the transfer speeds over just having the one.  As Robert Moir has pointed out it the issues with WLAN throughput are constrained by the efficiency of the channel between the WLAN client and the base station it is associated with, adding more WLAN infrastructure isn't going to help single user performance. It might be possible to construct a client with multiple WLAN adapters that connect to separate base stations but the usefulness of such a set up will be pretty minimal - you wont get better throughput unless you are using an application that can push data out multiple interfaces in parallel -standard downloads\file copies and the like aren't going to see any benefit. The best solution to this is to invest in WLAN kit that can support multiple channels (well wider channels really), ideally MIMO 802.11n operating at 5Ghz. While some of these claim 450Mbps speeds the best I've seen in testing was a sustained 76Mbps with typical 802.11g WLAN throughput generally struggling to break through 20Mbps.  EXAMPLE: I often use netflix on my laptop over wifi, but if I try transferring a large file while watching a movie, netflix will time out because my connection has slowed.",2
"One idea to make this scale you can make a bitmap of the same resolution as the user is drawing the polygon in and in each pixel store the ID of the event that was rendered on that pixel (or a list of events if you have collisions). Then simply raster the polygon and find all events inside the polygon. However as you still need to go through all the points of the data set and then raster the polygon the run time is now \$O(n + A)\$ where \$A\$ is the area of the polygon.  Then when the user draws a gate, compute the bounding box of the polygon and then construct a quadtree for the polygon where each node is either ""inside"" or ""outside"" going down to pixel level.  I don't do JS so I can't claim I understand fully what it is you do. However I gather that you are having performance problems with determining which points are in the gate polygon. If I understood correctly you are checking each point against the polygon which gives you \$O(np)\$ run time where \$n\$ is the number of points and \$p\$ is the number of corners on the polygon.  This way of doing it allows your users to scale their datasets massively as the only dependence on \$n\$  is in \$log(n)\$ which grows very slowly. You can consider \$k\$ as a design parameter that is constant so you can remove it from the above expression if you want. Then the circumference of the gate is what dominates the run time (note, not the area but the circumference). Then start at the root of the point quadtree, test each child if it intersects the bounding box of the gate, if not you can skip that child. Next you test the child against the gate quadtree. Is the child contained in a node that is ""outside""? Skip the child and it's children. Is the child contained in a node that is ""inside"" mark all the children as being in the gate and continue without recursing. If the child isn't contained in either an outside or inside node in the gate quadtree, recurse into the children of the child and repeat until you reach a leaf node in the point quadtree. If the leaf is still not strictly contained in an ""inside"" or ""outside"" of node in the gate quadtree, test each of the \$k\$ points against the gate quadtree.  Lets arrange the points in the data set into a quadtree in a pre-processing step when the user uploads, this is \$O(n\log(n))\$ work. Computing the run time is a bit hard but lets try... assume that along each pixel of the circumference of the gate, you have to test all of the \$k\$ points in the leaf that covers that bit of the periphery. This bit is \$O(kc)\$ where \$c\$ is the circumference of the gate (note \$c<A\$  typically). To get down to each of the leafs we need to traverse \$O(log(n))\$ nodes in the point quadtree and each of those needs to be tested against \$O(log(A))\$ nodes of the gate quadtree.",1
"While we're at it, let's look at the basic functions you're going to have to handle and their alternatives.  Virtualisation. Do not forget virtualisation. You can run multiple servers on one machine. It will also help in configuration and set-up. If you are unfamiliar with how one server works, you can create a virtual server and test it to your heart's content before setting up the production one in another virtual server. Also, as someone else has mentioned, you can run Windows virtualised too. It all depends on your needs. I run a mix of Windows and Linux, with each chosen to suit a combination of what it has to do and what hardware available for it (it's a small company). As nearly all the workstations run Windows it makes sense to use Windows on the main servers. i.e. Those servers users interact with directly. For what I consider to be ""sundry services"", things such as firewall, system monitoring, spam filtering, network faxing, etc I use Linux. This enables me to implement readily available and well proven solutions at very close to zero cost, without creating interoperability issues. This last point, previously raised by others, needs to be given careful consideration. That, more than anything else, is where the true long term costs of a mixed environment are likely to be. No.  For a small business, you are likely getting your Windows licenses with the hardware you're buying.  It's basically a no cost venture other than maybe some CALs.  While you might be familiar with Linux, you're likely to run into compatibility issues with the software you've already mentioned requires Windows which alone make it a bad idea.  I would venture to guess if you added up potential lost productivity fighting those issues vs the cost of the Windows licenses you'd actually have to buy apart from hardware, you'd find the license cost is far less than the cost of your, and others time.   Besides Evan's excellent answer above, I'd like to add in that there are other interoperability solutions that aren't covered.  If possible, I'd outsource as much as you can. Email is a great target -- it's usually cost-effective to outsource it because the authentication, security, spam, and network transport issues are non-trivial (i.e. reverse DNS &c) and a pain in the butt to keep running.  As I mentioned in my comment on Evan's answer, interoperability is the key thing here.  There's a lot more to client/server communication than raw IP traffic.  If nothing else Windows servers will give you Active Directory and Group Policies.  Add in gains from centralised management, single sign on, and being on a client/server platform that's designed to work together, and it looks like being a no-brainer. Of course Windows has it's issues, but my experience has been that it is quite stable in a standard configuration and Linux will only introduce problems, especially on the desktop.  Servers are an entirely different question. The cost of the licenses is only a small fraction of the overall cost, and with Windows on the desktop you're just going to get a much better environment by putting Windows on the servers.",5
"As a follow up....  After much research and a little trial & error, for ~$300, I successfully migrated my Samsung Slate 7 (XE700T1A) from it's original 128GB SAMSUNG mSATA (MZMPA128HMFU) SSD to a new Samsung 860 EVO mSATA SSD.  Along with the new 500GB SSD, I also purchased a Sabrent mSATA external enclosure, and the Acronis cloning software.   Since both SSDs were made by Samsung, I tried to use the Samsung cloning software, but it wasn't able to recognize both SSDs.  I have a Samsung Slate 7 (XE700T1A-A06US) with an internal 128GB mSATA (SAMSUNG MZMPA128HMFU) Solid State Drive (SSD), and have run out of room on the drive.  Id like to upgrade the drive to something larger (e.g., 500GB SSD).  Has anyone successfully upgraded one of these SSDs already, and if so what SSD was compatible? Once I installed the 500GB SSD into the Sabrent mSATA external enclosure, I connected it to the Samsung Slate via a USB cable.  Then ran the Acronis software for Windows, ran the cloning app in it's default settings mode and it successfully cloned the 128GB SSD onto the 500GB SSD the first time. In doing so, Acronis successfully migrated the 128GB SSD's hidden system recovery partition, the system / boot & and main partition, and the hidden partition where disk recovery images were stored.",2
"But again, from a management perspective, it's not worth it for us to spend the time to figure it out.  We'd never sell Windows or Office or Acrobat or any other widely-used app or utility, since we use a mix of versions and sometimes have upgrades, and OEM Windows can't be sold w/out the hardware...  For lesser-used utilities, by the time our user wouldn't want to use it, the version would probably be too old. So, ultimately, I'd feel free to do it if the circumstances made sense, but we're never in a situation where it makes sense. If you asked the software company they will tell you that it is illegal and you only have a license and not ownership of a copy.   You really need to read each license first, as most I've seen cover this subject. Beyond that, do as others have already suggested and get proper legal advice. We don't usually have the time to sell surplus equipment, and we've never taken the time to sell surplus licences.  At least with old hardware, it's sitting unused in a corner anyway, it's already been removed from inventory, and we can just post something on Craigslist or sell it internally and it's not a big deal.  For software, we keep things in the inventory for a long time, just in case someone needs a specific version or we are able to get a competitive upgrade.  So the extra effort of verifying that something old isn't used on a lab machine in the corner and taking it out of the database isn't worth it. There was a recent ruling (2) that would seem to indicate you may be an owner of a copy which would allow you to resell your software.  Listen to Episode 32 of TWIL they discuss it. It depends on the license you obtained (Or the agreement) from the Original software company. The most fair answer for your query is ""NO""...  Legally, it would depend on the actual license, but practically, if I did have some surplus s/w (maybe something that was never actually installed), I'd feel quite free to sell it.",4
"then you can scale them up by increasing the number of concurrent connections till you reach close to the expected number of users and watch the response of your service and repeat it for number of times and check the time variation and take the average Once you have your results, graph them: number of visitors versus average request times, including max and min bars.  Basically, load testing of an arbitrary application is only as useful as relevant tests; in this case, for example, if it takes 1 visitor 6s to load a page, then 7s a page for 200 visitors doesn't sound to bad, does it? When running load tests, picking an arbitrary number and hitting your server is generally not a good way to go.  All you've proven is that your server can handle 200 concurrent visitors as long as they don't mind waiting ~7s for their request to load.  What you PROBABLY want to do is: you can start by setting a startup number of requests and number of concurrent requests and check the results as follows :",2
"In your case (and I've worked with a lot of smaller clients) you should definitely go down the route that DKNUCKLES suggests.  This is probably a good case for an outsourced email/storage solution such as the offerings from Google or Microsoft. For a small organisation these can allow you to implement some quite impressive tech (e.g. all the high end exchange features if you wanted) with much less up front cost and support requirements than a traditional self-managed system, which I'd imagine is important for an organisation relying on help from volunteers, where what time you have might be better spent on the more fundamental areas of support. 1) They have three full time staff, each with their own e-mail account which is accessed using Outlook (pop3 - because of limited hosting space).  There are other options, mostly linux based - some of them would be cheaper or even work better, but for a novice to setup, it's hard to find something that beats Windows Small Business Server. If it's a charity that's using volunteer sysadmins then on-site resources and budgets are probably quite limited, I take it? I'm personally more of a fan of Google Apps for Business than Outlook but Microsoft's 365 is a decent product as well.  You can also download the server software from Microsoft's site for a free 180 day trial - install it on an old PC and play around with it for awhile so you get some practice before doing the real thing, and make sure it meets your needs. Windows Small Business Server would be a good choice for them - it includes Exchange already, and makes things fairly simple to setup.  The biggest limitation on it is the number of users, but even the cheapest version supports up to 25 people.  Since you are volunteering I'm assuming they are some kind of non profit - if that's the case, they can probably get Windows SBS quite cheaply. I've had a look at Microsoft Exchange and Microsoft Server. Both seem daunting (probably because I've never used them before) and I'm not sure if they're overkill for what I need. Are there more appropriate solutions available or should I go down the exchange/server route?  1) I've been asked if I can synchronize their e-mail contacts so they don't have to manually update them periodically, or every time a contact is updated/deleted. A solution such as Microsoft's office 365 would give you exchange (and all the shared calendar and contact list goodness that brings, solving problem #1) and the ability to store and share documents online via Skydrive Pro.  You should probably be looking at outsourced or hosted solutions. You can find hosted Exchange for a low monthly cost ($5 per user per month). An alternative to setting up the email to go to the exchange server directly would be to configure the POP3 connector included in SBS.  Either way has pros and cons, so look at both and see which will work better in your case. I'm also assuming that they have their own domain, and the email is currently the ""free email service"" that is included with most hosting providers.  If that's the case, you'll need to: I've just started volunteering at a local organisation and I've been asked to come up with a solution to the following problem. However, networking is hardly my forte so I thought I'd ask here before I went (blindly) down a particular path.  Exchange server will do what you want as far as the e-mail is concerned, however it's an email platform and not meant for file storage. For that you'd want to look at something like Microsoft SkyDrive, Sharepoint or even something free like Box or DropBox.",5
"You can also use UltraMon (non-free, Windows, GUI) to set up a keyboard shortcut to move a window to the next or previous monitor. Win + Alt + Arrow: Move the active window to the monitor whose direction is indicated by the arrow. Note that this may cause your window to move outside the screen if you try to go up from monitor 2 or right from monitor 3 in your setup. I'll update it in the future. I had the same issue with winamp. The only (unsatisfactory) solution i found so far: change the screen resolution to a different one and back In this case, if you can open additional instances of the application, do so.  The first few instances will almost certainly appear in the task bar as yet more phantom windows.  Keep doing this.  Eventually, they will begin to populate the primary view.  Then use the task bar icon to right click and close the off-screen instances.  Once there are NO off-screen instances open, close the ones on the primary screen.  Next time you open that application, it will appear on the primary screen and not ""off camera."" I use a nifty little tool called Shove-it which simply checks whether any window is outside the screen edge and shoves it back onto the screen again. It's ancient software (and the homepage proves it) but works on all Windows versions. In some cases, despite having multiple screens at the remote location, you may not have access to them from your location.  The key commands won't work because you have been locked out of any view that is not on your screen. I've written a tool called Borderline that will automatically move off-screen windows back on-screen when run.  You have to run it when you need it (works best if you assign it a keyboard shortcut or put it in the start menu), but that also means it's not always running in the background.",5
"I have read that there is a difference between OneNote and OneNote 2016 (and 2010, 2013, etc.), and in the lightweight version, it seems I cannot insert equations, customize the layout, etc. Microsoft have further obfuscated the downloading of Office 2016 Onenote.  The only place I could find at Microsoft was here (32-bit) and here (64-bit).  These currently redirect to these download links (though for how long remains to be seen). You can also download these from Softpedia, as well as an Office iso from there (although I've not tried that). https://c2rsetup.officeapps.live.com/c2r/download.aspx?productReleaseID=OneNoteFreeRetail&platform=x86&language=en-us&version=O16GA&Source=O16ONF and These versions of the programs are very keen for you to login to a school/business account - I had to fiddle around to get it working with my personal MS account. Despite MS's promise to provide the same functionality in the MS Store version of OneNote, there remain significant differences e.g. tags are not fully supported - search is expected next month, and it looks as though the facility to save to local filestore will never appear. I have Windows 10 with OneNote pre-installed, but it's a very lightweight version of the real OneNote 2016. Microsoft has hidden the download link somewhat recently, which could mean, that this version will not be there forever. But as for now you still find it here: How can I install OneNote 2016? If I go to www.onenote.com to install the full version, it says that OneNote was already shipped with Windows 10. You can download OneNote 2016 at here, despite what the website says, you are eventually sent to the executable download. https://c2rsetup.officeapps.live.com/c2r/download.aspx?productReleaseID=OneNoteFreeRetail&platform=x64&language=en-us&version=O16GA&Source=O16ONF",4
"However, we have a good number of users that only connect through VPN. We are using a user certificate based VPN. The user has to first logon to the workstation using his cached credentials from the legacy domain, then he can establish a secure connection to the corporate network (authenticated through the user cert against the VPN end point). We are able to migrate the users into the parent domain. As long as the client does not have connectivity to a DC, the user is able to logon with his cached credentials and even start the VPN connection. At this point he would have to log off and then log on with his new (root) domain credentials. However, as soon as he logs off the VPN tunnel gets terminated and the user is unable to authenticate against the DC of the root domain. Without that first authentication there are no cached credentials of the root domain that can be stored on the client and we are back to the beginning. Unfortunately, the VPN client cannot be configured in a way that would allow the user to log off and keep the VPN tunnel established. We also tried to do a ""runas"" or a ""Switch user"" but it would not work. we are looking to migrate 35k users around the globe from 60+ child domains into the parent (root) domain. We are able to migrate the users using ADMT. The users can afterwards logon with their root domain credentials and everything works fine.",1
"I have an older but functional Dell server box that I'm not using right now; however I don't believe it has the same Adaptec RAID controller. (I'm actually not sure, but for the moment I'm assuming not.)  I'm trying to figure out whether I can take a disk out of my failing hardware and bring the Win 2003 server back up on the Dell hardware. I have a Windows 2003 server that has hung twice in the last two weeks - the second time after less than 2 hours up.  There is nothing at all in the Event Viewer to indicate any failures.  For a freeze like that with no disk errors, I suspect some kind of failure in my server hardware. 2) How picky is Windows Server about the hardware?  Will it detect that things have change and boot successfully on different hardware? You can try slapping the disks into the Dell machine, but if your goal is a stable, working computer you're better off installing from scratch on the Dell machine, or ditching the hardward RAID controller on the SuperMicro machine and rebuilding from scratch with a JBOD configuration and using Windows software RAID. I've tried to do that with at least one Adaptec SATA RAID controller and gotten BOOT_DEVICE_INACCESSIBLE STOP messages. I can't remember which model to tell you for sure.  1) If I take a disk that was part of a RAID 1 set, is there a way to boot that on the Dell server assuming the Dell doesn't have RAID? My server is a semi-generic 1U Supermicro with an Adaptec RAID 1 controller.  I'm running two SATA 500gb drives in RAID 1.",2
"What I would like to achieve to to be able to remote desktop onto it (obviously I have turned this option on in the settings) I have had this happen on numerous occasions. Sometimes whilst joining a Windows Domain, something goes awry. What I have done in the past is to delete the computer name from Active Directory, then disjoin the Domain (from the computer: right-click my computer, select properties, select the computer name tab, click change. click the workgroup radio button and type in a workgroup name. Finally restart the computer). After the computer restarts, you can then log in and rejoin the domain (same process as above, only select the domain radio button and type in the name of your domain). This should solve your problem. If it doesn't, Terminal Service isn't running, or is running on a different port.  You could always run something like this to see if its bound to a different port: As far as connecting to this PC via Remote Desktop goes, please verify two things and then run a test for me. If I understand you correctly, you have a WinXP machine that you cannot ping, connect to with RDP, or otherwise route traffic to.  This wouldn't be anything specific to RDP, it would be a more general networking issue.   As far as not being able to ping this computer goes...can you upload the IPCONFIG for both machines?  Are there any hardware firewalls between them?   Check the properties of your NIC in Control Panel, Network Connections.  You need to have both Client for MS Networks and File and Print Sharing enabled. After verifying those two things, test that the Terminal Server is running and bound to a TCP port on your computer.  To do this, type the following at a command prompt: Like another member mentioned, it's possible this computer has another software firewall blocking inbound traffic, aside from the Windows Firewall.  I'd check in the Security Center (which should detail if any are active). The machine is running Windows XP, the firewall is off and it is getting an IP on our network.  I am a bit of a networking n00b so I don't know what other information is relevant. I am having an issue with a machine on my company network where it has joined the domain OK and the machine itself can happily connect with play with other machines on the network but nothing can connect to it (or indeed ping it). Microsoft Support Article 299357 (I'm only allowed to post one URL) describes how the listening port for RDP may have been changed from 3389 to some number.   Try setting the XP machine to a static IP and see if you can communicate with it then.  If you can't ping it, which failure message do you get?",5
"I am working on an AR application using Unity3D and the Vuforia SDK for Android. The way the application works is a when the designated image(a frame marker in our case) is recognized by the camera, a 3D island is rendered at that spot. Currently I am able to detect when/which objects are touched on the model by raycasting. I also am able to successfully detect a swipe using this code: If you want to kill anything you touched during swipe then you should either do a raycast in each frame whci you are swiping and record all hits in a list and then kill them all when swipe is approved or do an sphereCast from start position to end position. something like this Thanks to andeeeee for the logic. But I want to have some interaction in the 3D world based on the swipe on the screen. I.E. If the user swipes over unoccluded enemies, they die. My first thought was to track all the points in the moved TouchPhase, and then if it is a swipe raycast into all those points and kill any enemy that is hit. Is there a better way to do this? What is the best approach? This is possible if you don't have covers which you want to block enemies between cover and your camera from being killed. The swipe code is a little unfriendly as well cause a little stantionary should be ok if you don't want only really fast swipes and also you could do it using Input.GetMouseButtonX calls to make it possible to test it easily in editor and make it cross platform.",2
"If you are clustering then you need a block-level SAN/NAS system (you may also just want the snapshotting features some of these arrays offer too of course) - these are typically Fibre-Channel (which uses a dedicated storage-tuned network, is very fast and performance-consistent - but expensive), iSCSI (which uses standard ethernet networks, is generally slower than FC but is often 'fast enough') and the kinda-hybrid Fibre-Channel-over-Ethernet (which is a nice, but very new, compromise between FC and iSCSI). We had your previous question on this subject yesterday and it's pretty conclusive that even IF you can get SQL to store it's data via NAS protocols such as SMB/CIFS or NFS it's a really bad idea! Although recently it has become feasible to use NAS for database storage (but probably only with iSCSI over 10Gb ethernet) I'd recommend staying away from it and using fibre or SCSI attached storage. I am using Windows Server 2008 Enterprise with SQL Server 2008 Enterprise. I am considering whether using SAN or using NAS is better to store database data files. Are there any industry readings to compare whether NAS or SAN is better for database storage scenario? Besides my database storage scenario, any industry readings to compare SAN with NAS to see which technology is better suitable for which scenario? I wouldn't expect so - they are very different products aimed at different markets. SAN at the top end, NAS at the bottom end.  Generally large organisations who rely on their DBs use FC and would only use iSCSI for less important work; for small organisations iSCSI can ofen provide perfectly-adequate performance for much better value (and less complexity) than FC - 5-10 years from now FC will be dead once FCoE has matured. If there's no need for clustering then most people would agree that suitably well spec'ed local (Direct Attached Storage - DAS) with the right RAID level is the best both interms of performance and value.",3
"You can then use FileTypesMan to edit the extension's icon. Browse to the folder where you saved the icons and choose the appropriate .ico file.  Alternatively you could try to rebuild the icon cache, or reinstall office, but Im not positive these options will work. Some months ago i messed around and used Types to change some file type icons without making a system restore. I changed the Excel, Powerpoint, Onenote, Word, Access and Publisher file type icons. They look ugly and I realized how stupid I was so I wanted to change back.  I'm not using Types now, but FileTypesMan. The problem is I can't find the location the dll containing the icons are. I was not getting the Outlook Icon while attaching a .msg file in word 2016. My issue was resolved after selecting C:\Program Files\Microsoft Office\Office16\1033\MAPISHELLR.DLL  in Change Icon button. Select the icons you want to use, right click and save them to your Desktop or another folder as a .ico extension.  Nirsoft, who developed FileTypesMan, has another program you could try called IconsExtract. I dont have Office 13, but i tried it out on 14 and was able to locate all the office icons and change them with FileTypesMan.  Using IconsExtact, browse to and search the folder C:/Program Files (x86)/Microsoft Office/Office 13/ (or something similar to that).  It may take a minute but a list of all the office icons should show up.  If your still stuck, try downloading Office Ico files from the net and loading them with FileTypesMan. For Office16 I went to the office16 directory (in Program Files x86). There are separate exe (aka Application)  files there for each Office component that end with ICO or ICON (ACCICONS, PPTICO, WORDICON) that have the icons. Couldn't find one with all icons. The inconsistencies (ICONS, ICO, ICON) are not mine btw.",4
"We are currently want to demote Windows Server 2008 R2 Enterprise from being a DC.  Originally, it was suppose to replace our older DC, but we have decided to keep the older one operational.  Since the person who did the server is no longer with us, I am not really sure how to demote the DC. I guess my question is can anyone point me to the right direction on how to do it?  Is this MS guide what I need to do? Having said all this, you really should have at least two DCs at all times. Don't migrate to a configuration with a single DC. You'll want to be sure that your DHCP scopes don't reference the DNS or WINS services provided by the machine you'll be removing the services from. You'll also want to be sure that machines with static IP configurations also don't reference these services. Just be sure that the machine has as its first specified DNS server the IP address of another computer hosting a copy of the AD domain's DNS and run dcpromo.exe, taking the defaults (be sure NOT to specify that this is the last DC in the domain). Removing Active Directory (AD) from a Domain Controller (DC) in a single domain environment where there are other replica DCs is pretty easy. Be sure that the remaining DC is flagged as a Global Catalog (GC) server (visible in ""Active Directory Sites and Services"" in the properties for the ""NTDS Settings"" under the DC object). If it isn't, flag it as such, because you need at least one GC at all times. Microsofts technet has a whole chapter on this. One would assume they know the 'right' way of doing this. See the chapter You can use the netsh dhcp server dump > dump-filename command to dump the DHCP server configuration into a text file (named ""dump-filename"", in this example) which can be imported onto a new DHCP server using netsh -f dump-filename.  Step1: After Installing the 64 bit version installer the Role Active Directory domain controller with the Server Manager  However, if you are not sure of what and how to do this, then consider that you are messing with the core functionality of your network. Tread carefully. Double and triple check.  (Or hire someone to do this for you). When you say ""...we did the AD, WINS, File Server, DNS and DHCP roles. We are going to keep it as a file server and demote everything else."", however, you make me think that you're also looking at moving DHCP, DNS, and WINS off this machine. Removing the functionality those services provide involves some planning. When it was setup, we did the AD, WINS, File Server, DNS and DHCP roles.  We are going to keep it as a file server and demote everything else.",4
"Unless you have a very unusual workload for your computer, it is never a good idea to use FAT32 over NTFS. And by ""very unusual"" I mean something like a constant amount of saturated writing to the disk in random files, etc. Otherwise the SSD will not wear out appreciably slower on FAT32; you'll just end up with less features in your filesystem and more chance of losing data. Also, no filesystem on the planet can protect against hardware failure of your SSD/HDD if the only copy of the file you have is on that disk... not NTFS, not FAT32, not reFS, nothing; so ""the data is only backed up"" if you back it up to a separate machine. Just keep that in mind. (this paragraph is in response to your question ""2) Is the data backed up in case something goes wrong?"" -- the answer is ""only if you're backing up the data to another storage device"").   If you've got things on your Data partition that you don't want your kids/employees/guests to view/edit/delete, or something like that, then you definitely don't want to convert to FAT32, because there's no way to stop them from doing anything they want to your data.",2
"If you're looking for ""tiny"" examples and you like Roguelikes, you should check out the listings for the 1kb roguelikes contest. There are a couple C examples that are rather clever (though they probably aren't good learning examples, just fun to read.) It is something I would reccomend You if You are more advanced programmer, because some of these techniques are quite non-readable.Nevertheless, it is a nice set of techniques. Proper way would not be to make such ""abominations"" but to properly design, document, test (...) Your program and such code would be the result of many compression techniques applied on very good design. Please, this is not how You should program but a demonstration how much a game can be compressed. I've got the source for a simple one we're using for a code war competition at Windward Wrocks. It's in C# but that's close to Objective C. It's written with the idea that all an aspiring game programmer wants to do is get to the point of writing games, so that is what it lets you do, from the very beginning. In the end you will be making games that are pretty much what you describe here. You can find a few open source games written in objective-c on github. Although most are larger than a few hundred lines. I am, very slowly, teaching myself programming (only need to grasp basic game logic) with the free book ""Invent Your Own Computer Games with Python"". Maybe it is a good idea not only look at open source games but to some freely available game systems (editors etc.) where you can find some inspiration for your data structures and what is behind it all. I mean for example GameStylus adventure game editor and engine, where - in the editor - you can clearly see what data structures are in the back of the game and how they are used.",5
"I would suggest to setup an avahi daemon on PI which is essentially bonjour. on the iPhone side it's fairly easy to implement as all Apple's hardware plays nicely with bonjour and api is very well documented. I just want the Pi to somehow simply broadcast on my network - ""Hey, I'm RasberryPi and my IP Address is 192.168.0.5!"".  Set the PI up to periodically broadcast the IP message or the PI to listen for a ""Whats your IP address"" and then reply with the IP message via on UDP which can be sent to the whole network via the broadcast address.  On the non-Pi side, you could instead use a program like ""fing"" from overlook soft to detect which IP address is being used by the raspberry Pi.  Perhaps filter by mac address or description of connected device.  Then provide that info to your app. You could also just configure your router to always provide a static IP to your rPi's known mac address... I set my PIs up with static IP address via the MAC address table in the DHCP settings of the router. I have a RaspberryPi connected to my home network via WiFi with a dynamic ip address. I then have my iPhone connected to the same network also via WiFi. The Pi has a lightweight c++ HTTP server running on it that can execute commands. I now want to write an Objective-C app that can find the Pi on the network, regardless of it's ip address, and send it commands. So, Universal Plug and Play seems like the logical solution - but everything I find on Google is dealing with Media players and streaming audio/video content.  I use fing quite a bit with my android phone, macBookPro, and windows machines.  I haven't played with integrating it with a separate application - it does take line commands, but that could get a little clunky.  Depends on how production-safe you need it to be, versus home grown and works for your own needs... You could install netatalk (Appletalk for 'NIX) on the Pi. This enables the Pi to be found using Bonjour.",5
"Make sure not to add too high of a degree to your polynomial or you will be overfitting!! This means although you characterize your training data perfectly, it will not generalize well to new instances. Thus this will be a useless model. That is why you need to split your training and testing data, that way you can verify if the model you build using your training data can generalize.  Let's convert these to matrices. We will also add a column of 1's to the end of the $X$ matrix. This will be used to train the bias value. Look at the dimensions of our weights vector. It only has 2 values. One value associated with $x$, the first column of our $X$ matrix, and a bias, associated with the 1's column that we added. The equation of this line is described as In this case your feature matrix $X$ has a single dimension. Each point in your graph has a $y$ value that depends only on 1 value of $x$.  Just to add a little, you stop when you see that your loss isn't improving any more as such or is improving at the 5th decimal place..",2
"Multi-column GiST indexes can combine the columns in a very targeted useful way.  However, signature based GiST indexes can also degenerate very badly when they get over-filled, and it is hard to predict when this will happen.  So you have to test them on your real dataset.  You might also want to try different ordering (putting the bools last, for example, or just leaving them out altogether). As the other answer says, this likely won't work well even when it is supported (which starts in v11 for bool).  A four column GIN index is really not much different from 4 single-column GIN indexes.  And the boolean columns are unlikely to be helpful on their own, as returning half the index is probably going to cost more than it helps.  By combining the three scalars into one btree index, that gives you the good shot at maximizing the selectivity and minimizing the overhead (and it works in v10).  If the distribution of bools are lopsided, it might help to create partial indexes where you only index rows which the rarer bool value.",1
"What I need is not even in the C:. it is in the partition next to C: that I tried to shrink. It has some 200 gigs of non backed up music. Is there any way I can get it back? Help! Trying to access the drive I was trying to shrinkfrom ubuntu says I need to run CHKDSK /f on it from windows.But I can access the C: partition from ubuntu fine. I got a windows 7 CD and made a bootable usb flash drive with it. Booted from them. They boot, but when I enter system repair, it gets stuck at 'System Restore options' screen where it detects the partition with the faulty windows but I can't click next or anything. I tried to shrink a partition(not C: drive) from ubuntu using gparted so that I can allocate more space to C:. After the shrinking, Windows 7 won't boot. The grub screen shows up(dual boot win7 and ubuntu) and when I select windows it hangs at 'starting windows' screen. The other options don't work as well.. safe mode/repair etc. I connect this hard disk to another working windows computer, boot from the hard drive that has the working windows, and it gets stuck at 'starting windows' screen. I remove the hard disk with the bad windows installation, it boots fine. But with the bad windows hard drive it just doesn't boot. And it is not a device selection issue as the grub screen doesn't show up and it goes straight to starting windows screen. So the working windows is trying to boot. But it fails.",1
"Linux also supports routes with a SRC attribute which only match packets with a given source address.  SRC only works for IPv6, and was buggy until very recently (3.11, if memory serves); I don't recommend using it unless you know what you are doing. In the simplest case, there is just one kernel routing table and no routes with the SRC attribute.  This table contains a number of routes, which were placed there manually (ip route add), by the DHCP daemon, or by routing daemons.  In this case, the kernel chooses: Note that the kernel metric (displayed by ip route show) is chosen by the routing daemon, and is not necessarily related to the metric of any particular routing protocol.  For example, Quagga uses the same metric for all the routes it installs in the kernel, independently of the protocol's metric. I understand that Linux chooses the most specific route to the destination when it does routing selection. But what about a route's metric? Does it have a higher priority than route's specificity? If you need more flexibility than the above provides, you will need to play with multiple routing tables, and write rules to choose one particular routing table for each packet.  A common technique is to dispatch on source-address in order to simulate source-specific routes.  Another technique is to run each routing daemon in its own routing table, and simulate Cisco's ""administrative distance"".  All of this is described in detail in Chapter 4 of the LARTC. A reference to the details of the routing selection algorithm used by Linux would also be appreciated.",2
"However by the end of the year a new binary shader representation will come out called Spir-V designed for Vulkan and an OpenGL extension for it is expected to follow shortly. This will allow you to write shaders in many languages as long as a translation tool to Spir-V exists (several are in the works including a Python and Haskell tool). If there are other shader languages which I have not mentioned I would be interested to hear the comparison for those too, provided they are not dependent on any particular GPU manufacturer. I want my code to be portable across different graphics cards. The shader language is bound to the APIs/engines that support it (glsl to openGL & WebGL and hlsl to D3D). There are tools to translate from one to the other but they aren't perfect. I don't want vague answers indicating personal preference. I'm looking for specific measurable differences so that I can decide for myself which will suit me best. I don't have a specific task in mind - I'm hoping to discover whether there is one or other that I can learn and then apply to any future tasks, rather than having to learn a new language for each new task. I don't know any shader languages. I've heard of GLSL and HLSL, and I'm interested in learning one or both. Are there significant differences between them that would make one or other better in certain situations? Is it useful to know both or would either cover most needs?",2
"Any links that would help further my understanding here would be massively appreciated. I'm going to continue reading up on this topic anyway but I just wanted to make sure I wasn't wasting my time on something that wouldn't work anyway. I've looked into Virtualization and set up a virtual Windows machine within Fedora but someone recently suggested I look into Sandboxing Linux within Windows instead. In general not much. But if you want to play video games, you have to keep Windows as the main system, the 3D video card is hardly supported by the VM software. You may also have problems with special USB peripherals (at least with VirtualBox). I'm trying to work out the best way to set up my computer so that I can run Linux as my main OS and Windows for gaming and working with Media Files.  I'm also under the impression that I'd simply passthrough the hardware the same way as with a virtual machine, is that correct and therefore the only performance difference would be in Windows hogging resources, which wouldn't make a difference in reality as the windows machine is the one that will need the performance anway? I doubt there are malwares aiming VMs since these are often throw-away systems. But a ransomware can encrypt it its image file as any other file. I've had a quick search around but everything I've found has been in reference to sandboxing Windows within Linux. So my questions are; How would Windows be prevented from reading data from within the sandbox, either active or at rest or inputs being sent to the Linux OS for example? Not that much IMHO. Your Windows can be pwned because it has access to the Internet, whether it's a VM or runs on the metal. Same for Linux. The real question is how much you can trust your backups. It's my understanding so far that the Windows host would have zero control over anything belonging to the Linux OS if sandboxed properly but I find it hard to believe Windows itself wouldn't be able to gain access to anything within the sandbox, the other way around sure but not that way.  It's easier to run a Linux VM under Windows, in particular for licence purposes. But you still have Windows around it when you don't need it, with the memory/CPU usage, the mandatory reboots, etc...",2
"Although less than ideal, cold backups are a perfectly legitimate way to backup a database. The assumption is that the database has been shut down, hence the ""cold"" nomenclature versus a ""hot"" or ""warm""(1) backup where the database is online. (1) Warm backups are kind of special case depending on the platform. Some database platforms support ""quiescing"" of the database while it is online, meaning write activity to the data files is halted. You can still query the database, and at the same time you can effectively do a cold backup of the database files. Why would one follow this approach? Some storage subsystems (like EMC) support snapshot-ing a volume over to another volume, effectively mirroring the first volume onto the second almost instantaneously. You can use this to mirror your production data over to a development environment or to a standby system. You need to put the database in a consistent state for the duration of this snapshot process, which is usually only a few seconds. So a ""hot"" system plus a ""cold"" backup equals a ""warm"" backup. Most folks will copy the MDF and LDF files when they are doing some type of maintenance on their server (a server move, reallocating storage space, or hardware is being serviced). I have also done this as an option to get a replica of production into my test environment. I had the time to copy the files when we had the services stopped for server maintenance. If you have the downtime then stopping SQL Server and copying the mdf and ldf files would be one way to backup your database. Being that SQL Server has stopped it has ""cleanly"" shutdown the database so it is a good state to be reattached. Any active transactions can be rolled back or rolled forward when SQL Server reads the transaction log. You can read numerous articles on better backup solutions. There are benefits in doing SQL backups versus copying the physical files each time. One being full backups will aid in controlling the database size, if the database is in FULL recovery mode. Then the main one being it does not require downtime. The down side of a cold backup is you're copying the entire file structure of the database regardless of how full the files are. So if you have a 1TB database that only has 10MB of data in it, you're going to be copying 1TB worth of data files. The hot backup approach works a different way in that only those pages in the database that actually have data are backed up. There's also some level of compression that hot backups support. Generally speaking, hot backups are faster and smaller than cold backups. If they are ""moving"" the files instead of ""copying"" the files that could be the issue. When SQL Server starts back up it remembers the last location the files were located in, so if they are not there.",2
"You can run some T/SQL to check the sysprocesses table (or DMV if SQL 2005+) to see if a backup is already being performed on that database. If not do the backup, if so bail out gracefully. The large log backup is probably happening because of database maintenance operations which happen between the last log backup and the full backup.  Reducing the interval between log backups may help with this. One more possibility:  if you have mirroring configured and if it is suspended, your transaction log and database backups will grow and continue to grow as long as mirroring is paused. In 2000, log backups could not run at the same time as diff or full backups (because a log backup will clear some transaction log and full/diffs must backup some log to allow the restored copy of the database to be transactionally-consistent). In 2005 this restriction was lifted. They can occur at the same time but a log backup concurrent with a full or diff will not clear the log. The log clearing will be delayed until the full or diff completes - this adds to the myth that full and diff backups clear the log - they do not.",3
"If you're trying to detect actions of a malicious software, chances are it's smart enough not to leave traces. An aware user could also try to hide his activity by disabling and reenabling the feature or overwriting the Last Accessed timestamp with an old date post factum. I just found out that someone went snooping through my computer a few days ago. I want to know if there is a way to see exactly what files (documents/pictures) that they opened. Windows DOES by default keep a log of every time a user logs in, unlocks the computer, etc.  It would be under the Event Viewer's Audit Log. Another possible solution which I tried was using RecentFilesView. This program does what I need but it only lists up to 138 files. As an alternative, SwiftSearch can give you a list of all files on a disk in a matter of seconds, then you sort by clicking on the ""Accessed"" column and scroll to the needed time. Well, if they were just browsing your computer, then you are out of luck. NTFS has a ""last accessed"" attribute, but it is disabled by default on Windows 7.  If they changed any files, they would be located under the Date modified.  Where were these files located? What kind of permissions are available (ACL)?   I tried adding the ""Date visited"" header in windows explorer but that doesn't have any data. The ""Date Accessed"" header only seems to store the time when the file was created.",3
"I'm writing a driver in C for a WiFi module attached to a PIC18 microcontroller. I want to implement some functions that I'm familiar with in computer application level programming like Window's API recv function for WinSock. Is this the correct way to achieve this? Efficiency is very important here (It's an embedded system too). The only thing I see wrong with the algorithm is that if you don't have any data coming in from the WiFi module, you could potentially sit and wait for 200ms (the value of RECEIVE_TIMEOUT) before returning to the caller to say ""no data"". Is there other work that your main program could be doing during that time? If so, it might be worthwhile to split the check for incoming data and the reading of that data into separate functions. The module itself already implements the WiFi communication protocol and the TCP/IP stack, and communicate with the PIC18 microcontroller through UART. One more thing missed in other comments. The UART is capable of detecting errors. They do happen, and must be reported upstairs. You must read and analyze RCSTA before reading RCREG",3
"The only way you will be able to see information related to the data being sent and received, would be by using a protocol analyser such as Wireshark However, if the data is encrypted you'll not get a great deal of information. If you don't wish to download another utility, open a command prompt and type netstat -abno and look through the data for the executable you think may be connecting.  TCP View is a Microsoft utility which will show all connections active on your system, including the application which owns the connection, the port, and the remote endpoint. Listening sockets are listed also. If you simply wish to establish if a connection is being made, then this information should be available in your firewall logs. Alternatively, you could use something like currports which is easy to use, graphical network monitor. It does not show the kind of data is sent, but you can have some idea using ProcessExplorer, also from sysinternals.com, seeing the thread properties (in bottom panel, enabled pressing button with ""engine"" icon). I generally use netstat -abn in a command prompt window.  Using -b requires elevated permissions and is not optional in your case. It is what results in the executable's name being added to the output. I have a program which I think makes a connection through the internet, but I'm not sure. How can I see what it's connecting to and what kind of data it is sending and receiving?",5
"Rasterization (filling out triangles on screen) takes some time. If you do it on the CPU, you essentially take that time away from game logic, especially if it's not optimized well. With software rendering, using the CPU, the renderer will be looping, one by one, over all the pixels on a bitmap and issue orders to show each on the screen. So if you're rendering a 1000x1000 sized image, that 1,000,000 loops for your CPU to go over. They are designed with control in mind after all; lots of if conditions, jumping from one set of instructions to another and a strict direction of flow of control. However, a GPU is designed with knowledge that it'll be doing a lot of similar looping over pixels on the screen. A GPU would take a for loop with a 1000000 iterations and divide the work over its huge number of cores for each to work** in parallel, and independent from one another**. So unlike the CPU, every time a GPU comes across an if-else condition, it'll handle both code branches to two cores of itself AND THEN, at the very end, it'll look at what the condition evaluates to and discards the result of the unneeded branch (that's why lots of if-else conditions in GPU shaders are frowned upon; they're always responsible for a waste). It's not just about speed of execution, but also about simplicity. Although the software rendering used in this example would be a lot slower than using hardware acceleration (i.e. a GPU), drawing a few bitmaps on screen is such a trivial task that you would not notice the performance drop. Engines do much more that just draw a picture to the screen. They handle lighting, shadows, input , collision detection. Even just the rendering part is way more complex than just pushing a buffer onto the screen. For 3d scenes especially you need to do a lot of calculations on far more complex data than a bitmap. Let me give you a analogy with a car: What you are describing as simple is the exhaust of the car. You just make a pipe with the right size and then you push the gas from one end to the other. However this is far from the only thing happening in the mechanism of the car.   However, low-level activity like triangle rasterisation, depth sort and the like are well-understood concepts that the GPU can handle implicitly with a few commands. Re-implementing those in software mode is essentially reinventing the wheel. This is fine if you want to gain a low-level understanding of how rendering is done, I myself wrote a 3D software renderer just to explore it a bit, but for most circumstances it's a waste of time when OpenGL can do it faster out of the box. You're drawing the line of what's ""acceptable"" or not at an arbitrary point in the toolchain. You could just as easily say ""why use C++ when you could do the same thing in assembly?"", or ""why rely on the keyboard drivers when you could just as easily read the voltages coming off its wires and calculate it yourself?"" There aren't enough hours in the day, or years in a lifetime for everyone to do everything themselves.  Somebody created the hardware and the machine languages that run on it. Somebody else creates higher level languages and compilers, drivers and operating systems, graphics libraries and on and on. We each build upon the work of our predecessors. That's not only ""okay"", it's a requirement.  While the answers from others are more correct than any answer I could give, I want to point out the fundamental misunderstanding about how software development works that I think underlies your question. While it's always possible to do things ""by yourself"" without a framework, and there's often great educational benefit from doing so, the reality is that's not how modern software is created.  The example you've given sounds extremely basic, just drawing a single image on the screen, hence the implementation is easy. Once you start layering on complexity though, you'll find it becomes increasingly complicated to get everything rendering correctly. The stuff people had to do back in the Quake days of 3D software rendering was insane, though I appreciate you're not going THAT far (yet). And doesn't matter, how small the image is, he needs to allocate certain amount of memory for it. GPUs have a video memory for this. This doesn't apply just to software development, but to modern life in general. Have you ever heard of the guy that built a toaster himself, from scratch? http://www.thomasthwaites.com/the-toaster-project/. It took a really long time and a whole lot of effort. For a toaster. Try building everything that's required to actualize a video game out of the ether all on your own! So yes, GPUs are built around parallelism. That makes working on pixels for them much faster compared to CPUs. The above answers are excellent, but none really goes over the most important reason as to why OpenGL and such are preferred. The main reason is to make use of dedicated hardware designed especially to work with things like rendering millions of pixels on a screen, the GPU.",5
"You can also use WinDirStat to look at your files to see if there are any really large ones that may not be necessary. Rather than just free up space, I am guessing that you have probably two partitions on the same disk, with D: being much larger, and hopefully, with a lot more space. I would use GPARTED, or some other partition manager, and just make C: larger, and solver the problem permanently, as you are sure to keep running low as time goes on. Download CCleaner from http://www.ccleaner.com, install it, make sure to uncheck possible boxes like ""Install Google Chome"" during the installation, start CCleaner, click the Analyse button, click the Clean button, Done! :) In the short term, I am with the guys, who said use CCleaner, but also you can consider moving your swap file, which often takes up a lot of space, especially on newer systems with more RAM. Try using Disk Space Fan to visualise your hard drive. This can help you see where some of your biggest folders are, and what's in them.",3
"When I said that I hadn't installed any other programs, I had forgotten about a disk burner program named Active@ ISO burner from a company called LSoft. From what I can tell, this program uses a driver named stpd.sys to interface with your optical drives in some manner. I originally downloaded and installed this program to write some Linux ISOs to disk, since I was having trouble with my installation media written in the conventional Windows burner. I installed it about 20 minutes before I had my first crash. After going through all of the troubleshooting steps, I determined that the USB-to-SATA/ATAPI bridge wasn't the problem, but apparently, some sort of issue had occurred with a USB device not properly being installed around the time of the crash as well. The drivers for this device (which I believe to be the bridge) were removed, but the crashes did not stop, continuing at approximately half-hour intervals.  As a part of the troubleshooting steps, I used Microsoft's driver verifier, which more or less looks through all of the selected drivers at boot and throws blue screens if there's any sort of problem with testing the driver. I selected all of the non-Microsoft drivers on the system that were loaded, but the stpd.sys driver was not selected, and blue screens were not being thrown at boot. They continued to be thrown during computer operation. I then looked to add more drivers, looking to see which ones had been loaded or modified right before my first crash.  After removal of the driver, the problem seems to have gone away. The machine has been on for the better part of three hours now without a critical failure. If this changes, I'll come back and fix this post. However, for now, it would seem that under certain circumstances (which I'm currently unable to accurately reproduce), the version of LSoft Active@ ISO burner available off of ntfs.com contains a driver named ""stpd.sys"" that will not work properly with Windows 8.1 Pro x64 RTM. I make no claims as to knowing whether an updated driver would work properly or not. I then went to check out my system's event logs. Surprisingly, the driver in question had suffered an Event 4 occurrence, being explained as ""Driver detected an internal error in its data structures for ."". I'm not sure if this is a coincidence or not, but the errors received in the blue screens dealt with ""CRITICAL_STRUCTURE_CORRUPTION"". Well, from what I can tell, the problem is fixed. Here's to hoping I haven't jinxed it. Explanation follows.",1
"Additionally, serial devices (and thus modems - which is how data was transmitted over long distance initially), had different ways of representing data (for example N81 - No parity, 8 data bits, 1 stop bit, so a ""byte"" of data was 9 bits in this example). Then, of-course, there is compression.   If you were/are sending standard text, you could get a lot more bytes through the line then the stated bit rate. Then along came the Internet and grouped data into packets - with additional overheads for each packet.  Depending on the size of the packet and encapsulation, the packet overhead can be significant. Once you need to deal with congestion, the game changes - Congestion usually means packet loss, which signals the system to slow down.  Protocols which break up into multiple streams will provide better throughput (eg bittorrent, some implementations of HTTP downloads), and, of-course compression. David Schwartz is right that bits/s predate the Internet - and this is part of the answer - old systems did not always use 8 bits to represent data - For example, ASCII is only 7 bits (Extended ASCII is 8 bits).   There is no single correct answer to this question.  If the file can't be compressed, and the channel is not congested and distances are short, FTP is pretty good. That said, there is also tuning which can sometimes make a significant difference which sits below the TCP level at which FTP works.  (This is an expert topic, but includes things like larger MTU's, more packet buffering, QoS tagging etc - and the performance will only be as good as these underlying optimisations will allow.",1
"There are unfortunately over 7000 chunks within your 12 chunk sight range.  However, I would expect few locations to have more than three vertical chunks that actually need to be rendered using this approach which cuts the chunk count to probably no more than 1500. I don't believe you can do this with simply loading certain layers because of the problem of transitions. I would apply the same sort of logic within a chunk--when you load a chunk calculate what junctions are transparent and what junctions are opaque touching opaque--you only need to actually render faces where someone can see them.  (Note that in Minecraft you have three types of block--transparent, opaque and vision-altering--glass, doors, fences etc.  You can only skip transparent-transparent and opaque-opaque.) 2)  For each face of the block is it opaque.  An opaque face means you do not need to consider the next chunk.  (Note, though, that depending on where the chunk is there could be as many as three faces involved--a chunk must be rendered if any of the three are not opaque.  I suspect this is best pre-calculated--render so long either b1 is visible and has a non-opaque face f1 or b2 is visible has a non-opaque face f2 or b3 is visible has a non-opaque face f3.)",1
"I would suspect you need to adjust the values like threadsperchild to a much larger value in order to handle that load.  The value controls the maximum number of requests that can be processed at once. We have a Windows Server 2003 machine running Apache2.2.  Most of the time there is no load on the server, but we have a notification program on 3400 PC's that can request a small web page that plays a 64KB .wav file.  When an event occurs those 3400 PC's all request the web page over the course of 3 minutes.  On a few machine we saw the browser sit in the ""connecting"" state for a little over a minute before the page painted.  What is happening, and how can we speed this up? There are few scenarios that so clearly show advantage of a good light webserver, especially a single-threaded one.  Try either nginx or lighttpd, i'm sure either can comply with a much smaller machine. You have a fixed number of clients you can serve at one point in time. Each slot is used for the period it takes to transfer the file to your client.  If you have 100 slots, and it takes the network 5 seconds to transfer the file (perhaps these computers are behind slow links, dialup), then you're going to need 175 seconds to serve all of those hosts. You'll look at the webserver and everything will look alright - not a lot of CPU load, not a lot of disk load, not a lot of network load. But those slow clients are killing you. Why do the computers need to fetch the audio file from a remote server if the alert is occurring locally? Couldn't they have a local copy distributed with the notification app?",5
"Active Directory is a multi-master database. Unlike Windows NT, the concept of Primary and Backup DCs is no longer valid. All domain controllers can serve all authentication requests at all times. DNS is used to list the valid DCs, and clients pick the DC to talk to from that list. What you really have is 2 Domain Controllers installed - the concept of Primary and Backup Domain Controllers died long ago. They are both acting as Domain Controllers without any intervention from you. In short, you've already done what you intended by running dcpromo on the subsequent Domain Controller. Active Directory uses DNS SRV records to locate domain controllers. Unless you've taken the time to set up a different AD Site where that other DC lives, chances are your clients are already using that other domain controller. When one DC is down, there will be some timeouts as clients attempt to talk to the dead DC but will automatically connect to any other DC in the domain. I have domain controller installed in a network and another server which acts as a backup DC. When the primary DC fails, How does other objects in the network know to connect to backup DC to resolve DNS and use it for authentication purposes. IS there a way to implement automatic failover for primary DC? This article describes how it works in Windows XP - I don't think the process has changed that much with later Operating Systems.",3
"This file is normally static. You would normally edit it by hand or use some GUI tool to do it for you. This is what you are doing when you use the networks connection settings. It is also possible that some other configuration system is rewriting your resolv.conf on boot up. Linuxconf was prone to do this. You might find a comment in /etc/resolv.conf warning you not to edit the file because it will get overwritten if this is the case. This might give you a clue what to change. If the DNS server stops working when you reboot you simply need to sort out why that is (See man chkconfig on some Linux versions). As you have to keep reconfiguring your servers after boot and you don't specify what version of linux you are using, I'll make an assumption that it is DHCP causing the problem. There are a couple of solutions. First, you could change the DHCP server to give out the right DNS servers. This may not be desirable or possible if you don't control the server or you don't want everyone on the network to get those DNS servers. The second solution is to tell the client to ignore the options from the server. You may find a GUI option to do this, but more likely, You're going to have to do this by hand. You can do this by editing dhclient.conf (/etc/dhcp3/dhclient.conf on Debian and Ubuntu) and adding the following line: However, if you have an interface configured for DHCP, the DHCP  server can give out nameservers, so the DHCP client has to update the file. DHCP requests will normally happen at boot time and also when the lease runs out (which is controlled by the server; could be hourly, daily or anywhere in between). Nameservers are specified using the nameserver option and are used in the order they're specified, so this client will use 192.168.0.1 unless it doesn't respond for 5 seconds, in which case it will try 192.168.0.2. The search option is a list of domains suffixes to add to requests if a lookup fails (By default if your query has no dot in, and therefore probably just a hostname, the resolver will not do an initial query and just try adding the suffixes). You haven't said what O/S your client machine is running.  In either event, I'd suggest using DHCP to dole out your DNS settings.",2
"My gut says that implementation 2 would be faster by a hair, depending on how the shifting works. All of this being said, I'm assuming that Apple ends up doing some crazy polymorphic implementation for sorting depending on the data, so implementation 1 might end up being faster. I'm probably overthinking this since the amount of data I'm expecting is so small, but I'm still curious about what might theoretically be faster. Part of its functionality is of course keeping messages in date-sorted order, and each message has a timestamp on it. I came up with two sorting implementations and I was curious as to which one would be more efficient. If both new and old messages are sorted it's basically a merge of two sorted arrays, similar to the merge step in merge sort. It can be done O(n + m) time. I'd suggest you to look into how that's implemented. An app I'm building professionally includes rudimentary text messaging. It allows users to send from the phone, and it polls for new messages (I know, yikes. Push notifications are a future task) with variable frequency for new messages from other users.",2
"You might also be able to get a LAN side ""internal only"" proxy to work in this situation if your service can be put behind one. Going the VPN route - On 192.168.1.44, run an OpenVPN server, creating a virtual ""tun0"" interface.  Setup a client that connects to it on 192.168.1.2 - this will also set up a virtual ""tun0"" on its side.   To address performance concerns, if you aren't concerned about security on your LAN, you don't have to enable encryption - and you can set the authentication to be a password which you include in a boot script (though you do need a server cert at the very least) and just have it be a straight tunnel. I think the way to go about what you want to do purely in iptables is to set up a second NAT between 192.168.1.2 and 192.168.1.44. You'll then put these virtual interfaces on their own network (like 192.168.99.0/24).  ""tun0"" on the server side will default to 192.168.99.1, and the other side will likely default to 192.168.99.6 - but you can use a ""client configuration directive"" to assign it a fixed IP.  Tell any services needing to be visible on the Internet to listen on 192.168.99.6.  A simple REDIRECT target in your PREROUTING table ought to make a specific port redirect to 192.168.99.6. It's probably going to be easier and less of a headache to do something like a proxy or VPN tunnel if possible. So you have no default gateway on 192.168.1.2, and presumably cannot specify one.  So traffic outgoing from here to random hosts on the Internet will not know where to go.",1
"You can try to research that behaviour using Process Monitor. Try to monitor explorer.exe with that tool. Collected logs can give a hint what might be slow there. This is a known issue with varied solutions. This is likely cause by some misbehaving context menu (right-click menu) providers. These context menu providers might listen to folder create/rename events but not respond correctly or fast enough to Windows Explorer causing delays/pauses. Installing new software typically installs its own set of context menu providers which might slow things down. open IE -> Preferences -> Internet Options -> Connections tab -> click LAN settings button and uncheck box next to Automatically detect settings and click OK. run a disk check and ensure integrity of the file system. if it only happens with directories with a lot of files, it might be your antivirus scanning the folders. I think System Restore will come into play in this situation.Restore your pc to the time these functions were working fine.",5
"One last thing.  I tried to also run root-kit revealer and IceSword so I could do a rootkit scan on my machine and neither of them would run and I am pretty sure it is because I am running a 64-bit OS.  I am also a computer professional so I have a pretty good idea when NOT to click OK on a windows dialog that looks rogue.   I was at work and got a help desk call about a rather severe malware infection and it got me thinking about my own computer.   I am running Windows 7 64-bit RC1 on my everyday laptop.  I run ESET NOD32 antivirus which does a good job of keeping itself up to date.  I never turned off UAC. All that to say that I think I am clean but I wanted to be sure so I booted into safe-mode and downloaded and did a quick scan using the well-recommended anti-malware tool MalwareBytes tool.  It only found a strange registry entry which I deleted.  No file or folder problems were detected.  I rebooted to complete the clean as it requested.  I was surprised by this because all it did was clean a registry entry. After re-booting normally, WinPatrol warned about new program MalwareBytes which I expected and allowed.  But to my surprise it also had me confirm the install/setup of userinit (I can't remember if it was dll or exe) but the program info was that this is the file that presents the startup screen to windows.  I allowed it but it caught me off guard.",1
"I've had a look for an add-on to do this but the problem is the terms bookmark, folder and organise are too general. The search box in the bookmarks bar only searches the bookmarks themselves, yet I have arranged the bookmarks into folders and subfolders. Sometimes I can't see the folder that I know I have, so I would like to do a search for it. Even if I find a bookmark that I have, onces it comes up as a result of a bookmark search, there's no way of finding out what folder it is contained within so I can get the other bookmarks - I'd like a similar function to Open Containing Folder. I wonder how that's not a problem to a lot more people. I also use Go to parent folder extensions which helps a lot but doesn't solve the problem. I'm one of those that like to tweak my folder hierarchy, so the lack of a 'folder' search in FireFox is a real problem and troubled me for years.   I also have the same longstanding problem. When the bookmarks wast only a html file, I used to put a link to it as a bookmark, open it in the browser and then search it through there. That way I could easily search the folder names, etc.  I used my Windows XP computer. I used Xmarks to sync my bookmarks between Firefox and IE. In file Explorer, search in the folder C:\Documents and Settings\yourusername\Favorites for your target folder. You will find the folder that you are looking for. I've been missing that feature as well. These two addons, if used together, help a lot - still they don't solve the problem of not being able to search for folder names.",5
"Should you want to try mounting it and assuming you didn't break it with fsck or newfs, you first need to find SPARC hardware, at least if you use Solaris. Linux might be able to read a sun label and mount its big endian ufs although I'm skeptical about it. 42 is more commonly used for ""Windows 2000 dynamic extended partition marker"" than for Secure Filesystem (SFS). So it can be a dummy partition that really contains Windows ""dynamic"" disk, with proprietary LDM (or DDM) structure. It may contain Windows volumes, or parts of volumes. I would simply try it on Windows (XP is compatible with these, not sure about Vista/7). I'm thinking that the drive was part of Solaris Volume Manager or Hardware Raid. Any tips on how to verify this and mount the harddrive under solaris? You tried... what? newfs? I would expect it to overwrite your disk, and you don't seem to want that. Your disk is more likely ufs or vxfs. There is no fdisk partition table on SPARC disks, as they aren't using BIOS. The ID 42 partition is probably just an artifact. The Sun-Blade-1500 is a SPARC machine. A Solaris 11 VM (assuming on x86) won't mount a SPARC disk, unless perhaps if it is an EFI disk with ZFS. I've tried mounting it as UFS/ZFS/newfs, doesn't work. Tried to run fsck to find the correct magic number, still doesn't work. The problem you are facing is that SPARC is Big Endian, while x86 is Little Endian. Back in the days that meant, that Little- and BiG Endian systems save stuff in opposite byte order to disk. With modern filesystems like ZFS that doesn't matter anymore, but what you got is probably UFS (just out of my head). So basically you need another SPARC machine to read that. I have received a HDD. It has single partition with id 42 SFS covering the whole partition. But I am reliably told that it came from Sun Fire XXXX (where XXXX is 15 hundred or thousand or something like that).",4
"Note: I would also check out YouTube/Sachin Sammy on how to do a redirect script from http to https. You sent the request TXT file to your Certificate Authority so they could create the certificate for you. Select Place all certificates in the following store and select Trusted Root Certificate Authorities Is this in IIS 6 or IIS 7? Did you import it in the MMC console or in IIS? Try importing it in the MMC console first, and then selecting it in  IIS. Also, make sure that the file you are importing is a .pfx file that includes the private key. This often occurs if you use IIS to create a Certificate Request from a CA. Your CA does not have your private key (you do) so the file does not contain the private key. He are the exact steps you need to do: A lot of people will get here not realising that they are doing the certificate process wrongly for adding SSL to their public websites. They will have downloaded a certificate from someone like godaddy and not realised they have done it in the wrong order! They wont get any error messages, all that will happen is that the certificate in IIS will disappear as soon as you click to another section. Make sure to check ""Allow this certificate to be exported"" otherwise If you'd receive bindings error later. I was having the same issue this morning where I was able to add cert but as soon as I click refresh it was disappearing. Here's How I solved it-:",5
"I am thinking that the problem is linked to permissions, because the authorized_keys file is owned by developers (-rw------- 1 developers developers 2033 Nov 11 22:55 authorized_keys However, you're doing it wrong anyway. You should create different users for each developer and put them in a common group. If they need to work on the same files, you make them writable by their common group and put them in a directory that has the ""set group ID"" flag set, so whatever they write to it will belong to the group. Unfortunately I am on a production box atm and I cannot test any of the solutions... I am also looking for a 3rd option if possible since due to our team setup I cannot change the home directory permissions... (bad model - I know) I have created a common /home/developers account, and multiple users (developerA,developerB ...). All developers have been delegated the same home folder (/home/developers). You only have one user. So it's not a multiple users problem. Make sure the .ssh directory has mode 700. Does anyone have a solution to this problem, or how to go about multiple-user passwordless ssh without creating a home folder for each of them. From this post you can see that SSH will refuse to use .ssh folder in a home directory that is shared. In my case, looking at /var/log/messages I see: Having many users with one home directory is a Very Bad Idea. Users will hate you and programs will hate you. Don't do that.",4
"Look at http://www.techsoup.org/. They serve non-profits by selling software extremely cheaply. The software is donated by various big companies and a valid non-profit can get things for almost nothing. In this way, you can have a ""big boy"" solution for peanuts. I don't think it's worth implementing a centralised server for your AV if you have 25 clients and are a non-profit with low budgets. Check out Microsoft Forefront client security.  Microsoft has a pretty good non-profit program under the open license charity program.  unlike symantec Forefront is licensed per user, not per client, so as a side benefit your users can use it at home as well. I've tested it it vs symantec in my environemnt and it's far simpler to admin and deploy. I would argue that using ClamAV would be a good solution. I have deployed this on a 100+ clients on a Windows/Linux network with pfSense acting as the edge protection device for network firewall IDS/IPS with snort. Apologies for answering somewhat off-topic. Lately though, I cringe at home and small business folks talking about paying for AV. I really don't think it's justifiable any longer.",4
"The blank could be filled by both hot and cold hence the similarity would be higher. This concept is called Paradigmatic relations. Word2vec does not capture similarity based on antonyms and synonyms. Word2vec would give a higher similarity if the two words have the similar context. Eg  What is the best way to figure out the semantic similarity of words? Word2Vec is okay, but not ideal: So what is the problem? The issue lies in word sense ambiguity. Whenever the word itself has two different meaning in two different context, the word vector will tend to really be away from either context. Python ~ Boa (both snakes) and Python - Java (both programming languages).. Word2vec is a good starting point for most scenarios. It does capture semantics by way of prediction using CBOW method. It allows translations (as most repeated example I can put here again), V(King) - V(Queen) ~~ V(men) - V(women) and so on.  If you are interested to capture relations such as hypernyms, hyponyms, synonyms, antonym you would have to use any wordnet based similarity measure. There are many similarity measures based on wordnet. You may check this link http://ws4jdemo.appspot.com/ For the very specific purpose of ""synonyms"" if you want Wordnet would be ideal place. It captures explicit relationship of two words rather than implicit relation based on usage and occurrences.",3
"I always set up Tomcat environments with Apache HTTPD in front of it, serving as a proxy server for the Tomcat backend.  The contents of the daemon.sh and startup.sh look very similar (at least for the env variables, and stuff like that). Daemon.sh calls jsvc in the end. Catalina.sh calls java. What is the (practical) difference between using the two of these when setting up tomcat as a service? The first one is to copy the daemon.sh from $CATALINA_HOME/bin to /etc/init.d, and the other one I have seen is to create a simple init script that class $CATALINA_HOME/bin/startup.sh, etc. Startup.sh calls catalina.sh. I have never deployed a setup that uses jsvc. I have written a simple init.d script that starts Tomcat on a ""service account"", and then I let mod_proxy(_ajp) talk to it from Apache. That way, you can make your tomcat listen locally on a non-privileged port, and let Apache do what it's good at.  jsvc will enable you to run the Tomcat itself as a non-privileged user and only create the listener as privileged user.  I am installing tomcat on a linux server, and would want it to be available as a service. I have found two different ways to achieve this. Ports below 1024 are privileged ports. Basically, this means that if you run Tomcat directly using catalina/startup/shutdown you will need to run as root, to bind to 80 or 443 (SSL).",2
"I think one issue is that we need to fix the ""scale"" of the problem. For example, the paper I refer to below defines NPO so that the objective function takes only positive integer values. With that definition, 1. is easy: fix say $\varepsilon = 1$, and then you get a $\max\{N_\varepsilon, 2\}$ approximation by definition. With this definition of NPO, 2. also holds. In particular, no APX-Complete problem has an asymptotic PTAS unless the polynomial hierarchy collapses. See the remark on the bottom of page 1761 of this paper, and also the proof of Theorem 4.10. On a high level, the proof shows that if a problem has an asymptotic PTAS, then it can be approximated to any degree with a constant number of queries to an NP oracle (Proposition 4.6). On the other hand, any problem $P$ which can be computed with $k$ queries to an NP oracle can be reduced to approximating an instance of an APX-Complete problem $A$ within factor $r$, where $r$ is a constant depending on $k$ (Prop. 4.8). So, if you have an APX-Complete problem which has an asymptotic PTAS, then there is some constant $h$ so that the query hierarchy collapses to level $h$, i.e. any problem solvable with $k$ queries to NP, for any integer $k > 0$, can be solved with $h$ queries to NP, where $h$ is a fixed constant which does not depend on $k$. This implies that the polynomial hierarchy collapses as well (Thm 4.2).",1
"I don't quite understand how this works.  So when I open up PowerShell as administrator, I can right click on the title bar, go to ""Properties,"" then go to ""Colors"" and make changes.  For example, I set the ""Screen Background"" default color to black instead of the default dark blue.  I've noticed that these changes seem to persist even after I restart the computer and open up PowerShell as administrator again.  However, these changes do not seem to apply to whenever I open up PowerShell regularly (not as administrator).  So, for example, I could have admin powershell have a black background and regular powershell have a red background. But here's the point; suppose I change a lot of the colors and I want to back these changes up somehow.  They must be stored somewhere, and clearly it's different for each of the two versions of PowerShell, so where are these properties stored, for both versions?  Is there an easy way to back them up and restore them at a later date if future changes have been made?  If not, is it possible to keep these seetings in my PowerShell profile somehow?",1
"Not sure if there are other things as well, but you have the same server alias site.com on both virtualhosts - you don't need a serveralias directive, so you should drop this from  the mail virtualhost as this is a cause of some of your problems. I am trying to have a subdomain point from mail.site.com to site.com/mail/ all while having this using a SSL based connection.  (I had to use example.com rather then site.com in the rewrite rule above - change as appropriate - This appears to be a limit of this site !) For the mail virtualhost, as you are doing a redirection, you can have a much simpler virtualhost - I'd try something like When I try to connect to mail.site.com everything works fine. Green little lock in browser, shows correct page, etc. However, the issue I am experiencing is that when I try to connect to site.com I get the red lock and an error stating that ""Your connection is not safe!"". When I click past the error it shows the mail/ directory and not the proper page. Additionally, when I look at the SSL certificate, the certificate is for mail.site.com and not site.com like it should be In summary, how do I configure my httpd.conf file to allow for all requests for site.com to use the appropriate certificate and go the right location AND for all requests for mail.site.com to use the appropriate certificate and display the contents of web_root/mail/? FWIW, I have a similar config on one of my setups (but for 2 totally different domains).  Differences in our configs are:",2
"Besides the marquee features (ZFS, Zones, DTrace) people mentioned above, (Open)Solaris also has great observability and management tools. Some things that I like: One thing that has long frustrated me with Solaris is the antiquated packaging/patching system.  This is being fixed in OpensSolaris with a new package management system.  I think there is still some work to do, but it is coming along fairly well. If you are serious into software that you want to run and support for 10+ years, this is the first point you look at - API stability and known lifetime management. Solaris thinks very much on how to implement new solutions so that you're not forced to update or force to change apps when upgrading. Others have already mentioned ZFS/Dtrace. Other benefits are Zones (lightweight virtualization - good enough for use in production), and SMF (init scripts done better - automatically restart a service when it dies if possible and other benefits). For me the first platform requirement is stability. Not long uptime, but APIs that don't change overnight. Solaris loves threads and scales very well.  Prime platform for heavy multi-user/multi-threaded applications on any server, but really shines on bigger servers. Sadly, but understandably, Linux fails this. Every release is different. Stacks change, APIs change, ... No one has mentioned LiveUpgrade yet.  LiveUpgrade gives you a way to install patches, or even upgrade the OS to a new release, but gives you a recovery where all you do is reboot, and you're back to the way you were.  Using ZFS and cloning gives you even more possibilities.  I recently took a Solaris 9 box with an SVM mirror and upgraded it to Solaris 10 with a ZFS mirror with less than 10 minutes of downtime.",5
"Again, add the second formula for if the cell value = 0 and the Stop If True tickbox and that should cover you for the second column. Finally, go to Conditional Formatting -> Manage Rules, select the and next to the Cell Value = 0 rule, click the tickbox Stop If True and click OK to close the box. Okay, this can be done and if you set the top row up correctly, you can copy and paste down the columns, saving you some time. The first thing here is that you'll need to set up 2 rules.  One is to conditionally format with the information you need above, the other is to change the formatting if the cell value changes to 0. Repeat across all columns, amending the second part of the SUM formula to show the last cell that should be checked. In the box that appears, change the default entry to 0 and format how you want it to appear if there is no stock. Now it's time for the second rule:  Go to Conditional Formatting -> Highlights Cells Rules -> Equal To Firstly select the very first cell with a number in it under Lei then on the Home tab, click Conditional Formatting -> New Rule.  In the box that opens, select ""Use a formula to determine which cells for format""",1
"If your virtual machine is contamined by some malware/virus, easiest path to host machine is through network. Usually host machine trusts traffic coming from virtual machine, and for example Windows virus using remotely exploitable (but blockable by firewall) hole, it have much better chances from virtual machine. Majority of things attacking your browser target problems with browser or API of underlying OS, for example image drawing functions. Also, as a security model, there is two way security: from host to virtual machine and from virtual machine to host. Normal viruses do not try to infect virtual machines, but it's possible, as host operating system (in your case Windows) can do whatever it wants to virtual machines and to virtual machine disks. The virtual machine acts as a sandbox so no threats can damage the host machine. Even more secure is your setup, as Ubuntu performs much better than Windows in terms of security. Using virtual machine for web browsing is good idea, because then you can take advantage of snapshots for example: if something goes wrong (and you know why), you can just restore previous snapshot. For example native Windows Recovery is not good enough for that, because many viruses infect old versions too, as those are accessible inside operating system. Virtual machine is sandbox for guest operating system. Breaking it is very difficult, but not impossible. For casual use, you don't have to care about that at all. As already said, majority of threats during normal web browsing target to your browser. That include also some operating system functions, like drawing, image rendering, filesystem handling and so on. In that sense Ubuntu (and Firefox) are much safer than Windows (and Internet Explorer). This is not necessarily because it's better (more secure), but there is much lesser number of viruses for Linux, as it's very marginal operating system. When connecting to your virtual machine, it is possible to exploit bug in Windows TCP/IP stack, but that's highly unlikely. Also, there isn't many (known) bugs in Windows 7 networking. I am thinking of switching to Ubuntu as a way of making web browsing more secure. So, suppose I will go the easy route and run Ubuntu as an app inside Windows and then run Firefox inside of that. What will this do to the security given the current threat environment? E.g. do most online threats nowadays target the browser and flash (which presumably would be safely sandboxed inside easy to wipe Ubuntu environment) or do they target the Windows TCP-IP stack where Ubuntu would give no protection? Well, most likely the above question does not come near to covering all the security implications of this setup :-), so please do discuss whatever other issues that may be relevant here.",3
"Let us assume that I have a large corporate WLAN network with many access points having the same SSID. The WLAN access points are connected to each other by learning Ethernet switches. Now, each client in the WLAN network has its own MAC address. Even if a client is largely only a ""consumer"" of data, there are very few client devices (possibly none on any given network) that completely avoid sending data. Most client devices are actually pretty ""chatty"" when they are connected to a network. Some examples that cause data to be sent from the client:  Normally the client will send a broadcast packet directly after connecting to the network for ARP, DHCP..etc  There are also a few vendors that provide some type of single or virtual cell wireless solution. In these cases, the client may be transitioned from one AP to another, but they don't actually roam. In these cases the WiFi infrastructure may handle the notification to the wired network. Ultimately, if a client is truly silent, the same will happen for it that happens for a truly silent client on the wired network (when the MAC entry ages out - generally as little as 1-3 minutes). Namely the network will flood unicast traffic. APs that receive this flooded traffic will not forward it unless the destination client is actually associated to the AP. When the client has performed an access point change, if there's no uplink data, my understanding is that the switches will direct the downlink TCP data to an incorrect access point until the (MAC address, port) pairs in the switches expire. Most large corporate WLAN installations are using Cisco or Aruba/HPE and typically tunnel all wireless traffic to the controllers. As such, even when a client device roams from one AP to another, it still lands on the network over the same connection (i.e. from the controller) so there is no need to update information on the switches. There is no simple answer to this question as there are quite a few flavors of client ""roaming."" While a bit outdated, this post is still largely relevant to illustrate some of the many types of roaming on wireless. There are also many different ways that different WiFi vendors handle client roaming (possibly different ways in any number of client roaming cases). So I will stick to some generalities. If security of any type is enabled and the client is not fast roaming, the client will send a DHCP request to validate their IP is still valid after reassociation. Even if they are fast roaming they may do so. This will update the switch in these cases. Is there some kind of dummy uplink packet sent after an access point change to inform the switches in the network that the MAC address has moved to a new switch port? My question is, how do the switches in the network learn that the client has moved to another access point if the client is only consuming downlink data? Let us assume that there is a mostly quiescent TCP connection that has data transfer only in the downlink direction at 10-second intervals, with no keepalive. Of course, there will be uplink ACKs but only as a response to the downlink data. Or is there some kind of mechanism where the old access point will redirect the packet to the new access point? When a client moves from the area of one access point to another, it has to perform an access point change because its signal level went down. AFAIK, the MAC address of the client does not change during this procedure.",3
"Think about the security group like a hardware firewall in a normal networking scenario. I guess you wouldn't really have to use both unless you had a special scenario, for example: you have a security group called webservers that controls access to web servers. You want to block an IP from hitting port 80 on one of those servers but not all of them. So what you would want to do is go into iptables on that one server and do the block, as opposed to doing it in the security group which would apply to all the servers in that security group...  In the meantime, I'm contemplating either opening up all ports in my Security Group and then doing all filtering via iptables, or the inverse, disable iptables and use Security Group filtering. Ideally you should use both to complement each other - block all the ports possible with your security group, and use IPTables to police the remaining ports and protect against attacks. I recently stumbled upon a firewall issue with my EC2 instance. The TCP port was made available to everyone via the EC2 Security Group, however there was still instance-side filtering using iptables. I figured if anything Security Groups are just a fancy API for IPTables. It turns out they're running completely exclusively from what I can tell. Is there any reason to use both? One firewall should be plenty and adding another layer of complexity seems to be a headache just waiting to happen. However, security groups are not state-sensitive, you cannot have them respond automatically to an attack for instance. IPTables are well suited to more dynamic rules - either adapting to certain scenarios, or providing finer grained conditional control.  The security groups add no load to your server - they are processed externally, and block traffic to and from your server, independent of your server. This provides an excellence first line of defense that is much more resilient than one residing on your server.",3
"When someone accesses the php file over http through your web server, they will only see blank output. As an exmaple, any wordpress installation has it's config file in the root dir. But as it don't echoes the vars there is no security issue, as long as the files are treated as PHP scripts and not text files. But it is a good idea to block direct URL access to such files through a .htaccess file password protection. It sounds like apache is not configured to execute your php files and is instead serving them as test/html. Under windows the path ""modules/libphp5.so"" will need to be modified to point to the php module, which might be a dll. No, you can't reach those files as a text file using this address (assuming your site is functioning) See  http://php.net/manual/en/install.windows.apache2.php and http://httpd.apache.org/docs/ for details. Alternatively, (and not as good), you can make sure that your config files are treated as php files, then start them with the php open tag: Try not putting your config files in a public directory. If the content on your web server is in here: Is it normal to users can see  www.yourwebsite/php/config.php files ?In my site I can reach thoose files as a text file.And I can see which user name or table or some information about my Mysql connection.All of my files in my Xammp  htdocs folder.Are they will be also visible when I add them to some host?",5
"Once you have your cluster use the (sudo) pg_ctrlcluster 9.1 main start to get the cluster up and running. To finally solve the issue it helps to (re-)install the locales and reset the locale configuration. For me (using a debian 7.8 system) this was done by: Just having a closer look on the output of apt-get install postgresql I noticed the output first states that something is wrong with my locale settings and in the end reports, the cluster could not be created: Without an initialized cluster it really is just an empty directory. The most likely thing is that you don't have a cluster created (see what's in /var/lib/postgresql). dpkg -c /var/cache/apt/archives/[PACKAGE-NAME].deb shows what is in the package and supposed to get installed. Funny. I had this once, when I tried to aptitude install the postgres package, but removing, purging and installing with apt-get did work. I'ts a package issue. Report the bug. Otherwise you may have luck with some ubuntu packages. And in the end it's not hard to build and with checkinstall you can easyly create your own packages and delete the sources. I'm trying to install PostgreSQL 9.1 on a Debian Squeeze server using Backports, but after installation I don't have the /etc/postgresql folder, instead I've got a /etc/postgresql-common folder. Is there something I'm doing wrong? According to the Debian Wiki I should have a /etc/postgresql/9.1/ folder. Then use (sudo)  pg_createcluster 9.1 main to create your DB cluster (called main). Once created you will have a /etc/postgresql/9.1/main directory with the settings for that cluster.",4
"SpeedStep is related to power savings.  When the chip is less busy (but not idle) SpeedStep will slow the clock down so that it's consuming less overall power.  It ramps back up to the default clock speeds when the system becomes busier. It slows down and speeds up in 3 (or more) ""steps"", depending on how busy the system is. I think even the 'default' Windows CPU gadget reflects this, and I know there are other popular ones in the Gadget Gallery that do reflect it for sure (I used one for a long time).  And, since SpeedStep is implemented in Ubuntu it works, and also reflects it. Check your Power Options and ensure it's set to ""Balanced"" or lower, as ""High Performance"" will keep the CPU clock at 'full speed' all the time by disabling SpeedStep. The 'full speed' of CPUs with Turbo Boost is all cores running at the same advertised clock speed (four cores at 1.86GHz in your case). Turbo Boost will shut down sections of the chip in order to boost the clock speed of some of the cores.  This is done when there are a couple/few CPU-intensive threads active, and dynamically.  it's so you can get more power for a short time, with the same overall power consumption (basically). If you click ""Change Plan Settings"" to edit the current power plan, then ""Change advanced power settings"", you can them expand ""Processor power management"" in the tree and set the Minimum and Maximum Processor states.",1
"The first stop to answer such questions should be the datasheet of the part in question. This is where the manufacturer lists the operating conditions of the device. Wear and tear of an electromechanical relay is usually due to switching under load, i.e. with a voltage applied to the terminals, not being permanently on or off. For more on the issue of contact degradation see here https://en.wikipedia.org/wiki/Relay#Arcing (which I won't make part of this answer as it does not address the question). A part of my engineering project involves a L.E.D tubelight staying on for 16 hours continuously. I am connecting the light to a relay module. Is it ok, if the relay stays on for so long? I mean, is it designed to sustain for such a huge period of time?  To add to @Ghanima's excellent answer, you may also wish to consider a latching relay. Briefly, a latching relay has two stable states (i.e. it is a bistable device, similar to a flip-flop). This means that the relay can be latched into an OPEN or CLOSED state, and it will remain in that state until commanded to change by the input. This avoids the necessity of supplying input voltage and current to maintain (for example) a NORMALLY OPEN relay in a CLOSED state for an extended period of time. Latching relays typically find application in situations similar to the one you've described in your question - where they must be in an OPEN or CLOSED state for extended periods of time.  That aside it is usually safe to have a relay active for longer periods of time, assuming you're within the operational range of current and voltage (again, as laid out in the datasheet).",3
"This is a Highly Undesirable Setup. It's a 1970's era legacy construct. It's just bad practice to divide and stack networks like this. I did it out of simplicity and address sanity for openstack development. (dev environments can be messy) A router always forwards according to its routing table (or routing policy when policy-based). There is no might be. According to the connectivity you require, i.e. - if you only want to provide access from the overlapping networks to the services you provide, or if you need also backward connectivity, you might need NAT as well - just map one of the networks out of the overlapping range; stateless NAT might be enough for some cases. Of course from your perspective one of the networks would be accessible under different IPs than actually assigned locally. This isn't possible unless you have another, even more specific (longer) routing table entry. In any case, this problem indicates overlapping subnets which are a general design error. If you do need to provice simultaneous access from overlapping networks (including the special case: two distinct but equal networks), for example to connect two RFC1918 networks, you will need policy based routing with stateful firewall. With stateful fw you mark connections originating from these networks based on interface, which received packets. With PBR you send responses to that interface (to be specific: you create a rule to direct marked packets to the appropriate routing table, as each routing table needs to be unambiguous). With ip-forwarding and proxy-arp enabled, this setup does actually work. The machine acts like a router (ip-forwarding) while bridging the two segments (proxy-arp). Nodes within the smaller segment use 129 as their router. Nodes within the larger segment will ARP for addresses across the entire /24, because the netmask says it's on-the-wire local; .20 will answer those requests with its own MAC and forward traffic into the /28, so long as it has a MAC for the destination. suppose we have two networks net1 = 192.24.0.0/18 and net2 = 192.24.12.0/22 and if we have a packet with destination IP as 192.24.12.8. So, according to longest prefix matching rule, we send this packet to net2. Now I have a doubt that all the packets which match with both networks are sent only to net2, instead, it might be possible that some actually belong to net1. So isn't this incorrect because we are not sending to the actual destination?",4
"I had a similar situation in the past and tracked it down to bad wifi in my router (don't know if it was merely the antenna or card). Replacing the router resolved it for me. It took me a while to realize it was a problem of the router though. Ping tests can help reveal if there is an actual break in the packet flow, but not the cause of it. found that ping dropouts intervals matched this parameters, googled adding parameter name and I found  Unfortunately, if it's not environmental, it may be that either the wifi adapter in one of the computers is going bad, or the router is. In that case, you may be looking at a replacement purchase. If it's the router, try moving it to a different location in the house/room to eliminate possible environmental factors. Also check the logs on the router and see if they reveal the wireless broadcast information (may be a problem in wireless broadcast). If it's determined to be the PC with the USB wireless you can try using the adapter on another computer to see if you get the same effect there. Not clear on what you were pinging. Is it only one or both of the PCs that are dropping out? Try pinging the router ip from each. Do both drop? Are the PCs within a comfortable range? I have had drop outs caused by PCs sitting on edge of range with a weak signal. If you're running a ping test, are you merely having each computer ping the other, or from both computers to the same IP on the internet or your intranet? If the two computers are pinging just each other, then it's possible that the wifi on one of the computers is bad. Try the ping from both machines to either your router's IP, or the same public IP on the internet (4.2.2.2 which is one of Google's public DNS adresses would work).",3
"As you can see in the lower graph in this image, my wifi signal is even lower than the noise level. Hence, the ""quality"" graph in the middle, indicating the SNR, is negative. The upper graph shows the actual data rate is zero. In other words, I have a really, really bad wifi connection at the moment, and my computer knows it. And yet the indicator in the menu bar at the top shows full strength, 5 out of 5 bars!  Here's the thing: Every laptop and wifi device has a signal indicator like this. Users look at it to answer a very specific question: ""How good is my connection going to be right here, right now?""  So why is it that my Mac (and in fact, every other device I know of) shows RSSI, which doesn't answer that question (and isn't really useful to end-users), instead of SNR, which does (and is)? Wouldn't a signal quality indicator be much more useful than a signal strength indicator? It seems that the menu bar indicator is showing the RSSI, i.e. the ""signal strength,"" instead of the SNR, a much better indicator of actual signal quality. As far as I can tell, my phone does the same thing - this would explain why in crowded cities, I can have 4-5 bars and poor reception, while in flat, empty rural areas I can usually connect just fine with 1-2 bars.",1
"Programming skills, or at the very least scripting skills, are needed for the most visible of contribute-upstream roles in these projects. A lot of work goes into making things like startup run faster or more efficiently, and that requires no little bash-scripting to make happen.  To find projects that need this, perhaps you could start by looking at the programs that you use that are open source. Approach them and see if they are interested. You can also do some searching or ask around, I know that the Drupal project might be able to use another testing server.  You can also help some OS projects by throwing in server resources. There are tons of projects that don't have access to a build server, CI server, testing server, etc. You don't necessarily have to become a webhost, just provide even one of those and the people behind the project will love you.  To actually run the server the project needs, find an underutilized VM server and put it there, being sure to provision it so that it doesn't kill the other functions of the VM server.  I also recommend github.com, it's much newer but has some very innovative and active projects on it, and is geared towards connecting people involved with programming open source. Another area is to participate in testing development builds. This will require some hardware or at least VM space, but provides very needed feedback to development about what's working, what's not working, and provides you a lot of troubleshooting experience. That kind of troubleshooting is a great way to get to know your project better. Do it long enough and you'll get your skills honed enough to start contributing patches to fix problems, or maybe even pick up a somewhat rare but very useful (to the community) skill like manual RPM packaging. You can volunteer as a tester and documenter.  You'll need sysadmin skills to set up testing (and you'll probably learn scripting).  A sysadmin viewpoint can be very helpful in writing documentation for system utilities, services, and applications. These are all great perspectives on what type of contribution you can have as a sysadmin. A good first step to finding an open source project to get involved and contribute to is sourceforge.com. Its one of the more popular communities for this.  One area where sysadminly skills do come in handy is in support forums. Get good at these areas and start helping other people. This is contributing to the community, it may not feel like it, but it does make the entire ecosystem nicer to live in.",4
"There is an IMAP extension available for PHP which can get email messages from the server, as well as move them around on the server or delete them completely when you've committed it to the database. While you could poll a POP mailbox its a very inefficient solution. Assunming you can route the mail to the target system, a better solution is to handle the request sychronously. While you could just use the .forward mechanism, I'd recommend using procmail as the MDA - it allows you to script complex behaviours like running programs, forward copies, conditional replies and more. Dependant on your mail server, you could write a PHP script that will poll a mailbox on the server with IMAP and take actions accordingly. The way I've done this frequently in the past is to run fetchmail every X minutes via cron. Fetchmail will  retrieve the mail via IMAP or POP3 and can then pipe the mail through a handler/parser script of some type (usually written in perl, python, etc.). The parser script would pull out any data you need out of the message and then insert that information into the appropriate places in your database.",3
"In this case, the script you're accessing to might be sent with one header (like pdf file), but in all actuality it's a php file. So to deal with HTTP Redirection you just need to loop an HTTP HEAD request until you stop getting responses in the 300's (hopefully getting a 200). Keep in mind it is possible that they will redirect in a loop, which will never end. You can do this with CURL or any HTTP tool. There are two things that commonly happen. Your link doesn't work anymore, so I am not sure the actual scenario in this case, so I will summarize on another link. This is what most download sites use. You click the download link and it takes you to a page with a bunch of ads and says ""Your download will begin shortly"" something similarly. [Example]. With these you can try to parse the actual direct link from the URL, but that would be site specific, and most sites will not include it to prevent you from circumventing it. This is done either via a meta http-equiv=""refresh"" tag in the header, or JavaScript (most common). The JS usually has a header fallback though. in conclusion: you can never know how the server & the scripts are configured, so you can never know the real, actual adress, even if it seems like you know. The site could be scripted, and when it gets a certain command (the URL can pass a command), it might then return a PDF file (or some other file), without redirecting. There it's a server-side thing and depends on how the site is coded. Without actually requesting that link from the server, it's unlikely you could figure out how to get the file. And sometimes even if you knew the direct URL, you might not have permission to access the direct link. Some sites are coded so that direct links won't work. There is probably a JavaScript block that links to the download as well. It may be obfuscated, or linked from another URL. Your mileage may vary trying to parse that out. There may also be a ""direct link"" on the page. You could try a few techniques to find that, but again that could be obfuscated via JavaScript or even missing all together. This is pretty much the ""true"" URL as for those well-protected websites, you have to submit the complete URL for the server to authenticate your request. You may be directed to another URL afterwards, but it will normally be a one-time one. In other words, these file download websites will never give you a leech-able direct link. I'm not really sure, but if you are using CUrl, can you not just obtain the URL contents (get_file_contents(url) in PHP) and then check the MIME type? There is a solution though. If you look at the source on download page you will usually see a <meta http-equiv=""refresh""> tag (usually in a <noscript> tag) with an attribute of URL that points to the actual download. So use CURL (or any other HTTP tool) to download the page, parse it out, and grab that value. A site may exclude this though if they want to be really nasty, thus requiring you to have JavaScript to download files. Server handles the file requests, using some WWW Rewrite (for apache servers, for example), so for example, you could be going to a page www.example.com/13-this-site-has-nice-page-name.html, but in reality you could be accessing some php file, with a parameter, such as: www.example.com/site_handler.php?UID=13 In this particular URL, the parameters, which are protected by a digital signature, clearly list time and IP restrictions of the downloader. For a website with this level of competence, it is unlikely there will be leaked direct links. This is what you see with Bit.ly and other services. What then do is provide an HTTP redirect response. When you visit http://bit.ly/oH3410 it redirects to the actual URL. Sometimes one URL redirects to another. You can see this happening if you plug the URL into http://web-sniffer.net/ or by using [curl][1] -I http://bit.ly/oH3410 you will see returns a 301 pointing to a new Location.  For file downloads with handlers, it might be a tad more trickier, as you can go to a page which is just a download handler, for example: www.example.com?file_downloader.php?param1=7683&param2=jld8ijn, etc...",5
"You don't even need to use joins to achieve all these operations so indexes will help a lot. Also if data will not suite in one machine - you can implement sharding scheme, like storing n_grams started from a-n on one server and o-z on another or other suitable scheme.  This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale. Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale. As for finding ngrams that contain your query sub-n-gram, I would build an index on the observed ngrams, e.g. using a second lucene index, or any other substring index such as a trie or suffix tree. If your data is dynamic, probably lucene is a reasonable choice, using phrase queries to find your n-grams. Inverted indexes will store the n-gram only once, then just the document ids that contain the ngram; they don't store this as highly redundant raw text. I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram. Create indexes on N-gram table/n_gram string and Mapping table/n_gram_id, also primary keys will be indexed by default well. Basically for this task you can efficiently use any SQL database with good support of B+tree based indexes (MySQL will suite you needs just perfect).  Also you can use MongoDB, but I'm not sure how exactly you need to implement indexing scheme. For MongoDB you will get sharding scheme for free as it is already built-in. I haven't done this before but it sounds like a job for a graph database given the functionality you want. Here's a demo in neo4j. I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.",4
"Due to the specificity of these entries, it concerns me that any app running on the internal side of my network can open ports to the outside. Is this behavior normal? Why does it happen? You can probably check to see if UPnP is enabled on your router, and disable it, if you want to have static rules. Yes, it is normal, if you have UPnP enabled on the router. Applications behind the router/firewall can ask it to open and forward specific ports to an internal IP when needed (VoIP, P2P,...). It is done dynamically, and the ports are closed after some time. For fun, I went into the web admin panel of my home router and saw that firewall rules had been automatically added, with app names and sometimes version numbers corresponding to each rule: It is called UPnp It is considered to be okay to let your firewall reconfigure itself according to these requests as it only accepts internal requests from you appliances of software running on your computer. To over simplify the process, the host on the inside sends a request to the UPnP enabled router to have a port automatically forwarded. A lot of times consumer grade equipment does things like this simply and easily, and then you go stick in a more advanced device that requires explicit configuration and people mistake this for things not working ""properly"". UPnP typically will have some kind of timeout where the rules expire after some time or after the host that requested the port forward goes offline the rule expires.",4
"You can make is easier to use by returning a boolean directly, so the caller does not need to make the comparison. I like that the input functions actually check the entered input matches the wanted type. Though in some cases a user might like to be able to abort some action at the prompt, so returning None or some default value for an empty input might be one choice.  However, print_fizz_buzz_output directly prints the result. So I cannot use it if I want to do anything else. Youd be better off either: Then you can rename it to ask_confirm. This allows you to abstract the notion of asking a confirmation with how it is done. You can then consider changing the implementation, and for example support other languages, with localized responses which would not need to be y/n. Id choose the second option since it allows for better reusability and you can still call list on the generator if you trully want a list there: As it stand, your ""real"" FizzBuzz code is contained in print_fizz_buzz_output, the rest being a mean to grab parameters values from the user.",3
"Besides, it'd be better to raise an TypeError, since that's a lot more specific to this case than a general things-just-kinda-broke-RuntimeError. You're also checking the type of the radius argument. You can argue either way if that's a good idea. It just raises a RuntimeError, with no extra explanation of what went wrong. If the check wasn't there, and you called, say, Circle.new(""banana"", ""banana"").area, you'd just get a NoMethodError instead. Not saying that's better, just saying that it's much worse either. Of course the current check happens in the constructor, which might be nice, but is it necessary? For most cases, I lean toward saying ""Garbage in, garbage out"". If you've set the radius to ""banana"", then don't expect the math to make sense. Since Ruby isn't strictly typed and there's no compile step, the exceptions will occur at runtime anyway. So even if you try to add a lot of type-checking, it's debatable how much value it adds. Especially in this case, since raising a RuntimeError with no extra explanation seems less useful than letting, say, a NoMethodError occur, which will at least tell you things like undefined method**' for ""banana""`. I'd still argue that radius can indeed be zero, and that's fine. It's a strange circle to contemplate, but it's not impossible. I would consider it bad style to raise an exception without a message. Most Ruby developers would also use ! and && instead of not and and Regardless, no reason to be dogmatic either way; type-checking definitely has its uses, and I'm not saying to always avoid it. Just make sure it adds some value. Conceptually, a circle with a zero or negative radius might not make a lot of sense, but there's nothing in your code that would break because of it. The area will never be negative since the radius is multiplied by itself. The area might be zero, but again, that is the area of a zero-radius circle, so strictly speaking it's still valid.",2
"You need to read about coordinate/space transformations, in that site is explained well. You can't go far without understanding that. Without using any transformation, you are basically bounded by the OpenGL clip-space which is contained in a cube with opposite vertices (1,1,1) and (-1,-1,-1) , as you stated correctly. Think of it as a SimCity with treasure hunts and lost-in-the-jungle adventures of infinite potentialities. First-time players of Minecraft enter a blank natural landscape of trees. Discovering that the sun can shortly set and darkness is nigh, they need to gather wood and build a shelter or risk being destroyed by the monsters of the night. because the name of the sport suggests, players mine the surroundings for materials then craft things like pickaxes, fishing rods, even chocolate-chip cookies. (When Conan OBrien reviewed Minecraft recently as a part of his series Clueless Gamer, he said: Taking things out of the bottom then building things.  thus its like were in Wales within the nineteenth century and were urgently poor. What a fun game for youths.) How to Draw Tutorials Once that task is perfect, different opportunities beckon: Mine for diamonds, tame cats, stock chests with found objects, produce glass windows by building kilns and gathering sand, create bows and arrows out of spiderwebs (but be carefulvanquish those spiders first!), lay out railroad-like roller coasters, style wonderlands for friends to go to. there's without stopping to the choices. I recently found a great tutorial of drawing Minecraft's graphics by drawing it in RT in Unity 4 but 5 is still compatible with the code. It's a basic tutorial and it can be made into a cube and the top has more- cubes on it but they're at different places like 3 down, 1 up you get the point basically a big cube but with smaller ones at the top. Here's the tutorial have fun (It does not include the sky but comes with pre-made sky assets) the title is misleading and it's quality with simplicity. Hope it helps http://in2gpu.com/2014/07/27/build-minecraft-unity-part-1/ and its all RNG. Although it's not OpenGL but it's the best I could find tutorial wise.",3
"I would assume that the programs are just created that way, as it is the same on my computer. I don't think there is a particular reason for this, but it is interesting. When I open an application button pinned to my taskbar, normally the app just opens, and an underline appears under its button. Yet when I open certain applications, they open under a duplicate button on the taskbar.  About this issue, it's generally because that you could have pinned the shortcut of the file instead of the actual program. Find actual program, then right click it and select Pin to taskbar. In this case, the solution is to Show Hidden Files in Windows Explorer, and navigate to the system path for taskbar shortcuts. This path is : %APPDATA%\Microsoft\Internet Explorer\Quick Launch\User Pinned\TaskBar  there you can attempt deleting the shortcut manually, and re-adding it once again. Sometimes weve run into situations where this doesnt resolve the problem. Theres usually a hidden shortcut that is being referenced by the running program.  The solution is to find and remove the extra shortcut. The most straightforward way to do this is simply to right click and Unpin all the icons for the program in question, launch the program via the Start Menu, and then right click the running program on the taskbar to re-pin it. As you can see, I have Chrome, Slack, Explorer, and Foxxit Reader open, highlighted in green, all with just single, underlined buttons, but with Visual Studio, highlighted in red, the pinned button is not active and there is a duplicate button a little to the right.",3
"I'd be surprised if you couldn't configure the stack with <300 Watts per server (and maybe a lot less) under full load even with your PCI cards at full draw so you should be able to get 24 of them in per Rack with that 8kW power budget.  If it weren't for the sound card requirement Dell M610x's in an M1000e chassis might be an option. You're going to get the aggregate benefits of more effective power\cooling from a (fairly) modern blade chassis and the M610X will give you two full length PCIe x16 slots that can support the power draw you need. They are full height blades though so not really all that dense but your power budget would kill most denser solutions anyway I think. The main drawbacks are that sound card issue, cost of course as these are decent enough dual socket servers, overall density isn't great at 8 servers in 10u. The 610 platform is overkill for your CPU\RAM requirements but you can configure them with a single low end low power Xeon 5600 and 4GB RAM rather than twin X5690's and 192GB and with those components they are pretty skimpy on power consumption.  They do have an internal USB port so it might be possible to meet the sound card requirements with a small USB device. Dimensions are limited to 15.9 mm wide x 57.15 mm long x 7.9 mm and there may be a limitation that this is only for USB storage, I'm not sure.  To be honest though if you are heading into the thousand range then a more bespoke solution might be in order, it's definitely the best way to get a really power efficient solution and one that hits all the requirements without any fudging.  And even though you say you don't need KVM having the centralised chassis management\iDRAC and full plug and play (MAC addresses managed by the chassis) at the server level options for something at this scale is surely a plus.   If you are building quite a few of them you might find designing your own case using something like http://www.protocase.com might be the way to go.  I believe this is the route BackBlaze took: http://blog.backblaze.com/2009/09/01/petabytes-on-a-budget-how-to-build-cheap-cloud-storage/ Rackable/SGI also do customizations (chipset, cpu, etc.), if you're ordering thousands of those then that shouldn't be a problem - i'd talk to them about your specific needs. Find a consumer or low end server motherboard with your needs and wrap a case around it.  If your cards are short enough you might be able to get them into 2U.  (I don't think I've seen right angle adapters for PCI-e). http://en.community.dell.com/dell-blogs/direct2dell/b/direct2dell/archive/2009/05/19/dell-launches-quot-fortuna-quot-via-nano-based-server-for-hyperscale-customers.aspx You might also talk to your vendor, if you are a good customer and you need a few of these thing you might be able to get a custom option setup, especially if the vendor decides other customers will want something similar.  You can certainly get PXE\iSCSI boot from the on board Broadcoms, diskless config is fine too. Since your overall power and IO requirements are minimal you can cut costs down on the chassis by opting for 4 PSU's and just a single Gigabit Pass through module rather than a switch.  The Dell Prevision R5400 is a rackmounted workstation but it uses Xeon processors and ECC RAM, maybe you could make a couple of compromises given the highly specific nature of your requirements you'll struggle to find a product that exactly meets your requirements unless you build them yourself which might be your best solution. Tyan Tank 1u barebones units are well priced and support a wide range of processors.",5
"I can't help but laugh about the whole image of a winch pulling a door open.  I've been in hundreds of data centers around the world and have never seen a setup describe such as this. It was a long time coming but I've just had AC installed in my server room. I've also purchased a MiniGoose II from ITWatchDogs which I can't say enough about. So I'm covered by cooling, and I'll be alerted if something goes wrong but what if I'm a significant distance away? What I would really like to do is have a computer controlled winch which could be activated in the event of a problem. Can anyone suggest any hardware / software I might use for this? My MiniGoose II is capable of sending SNMP traps, emails and has consumable XML services so those could be the methods of sending / retrieving the alert. The server room has a security grate which is always locked off hours and a door which has no latch so if pulled would pop open. If our new AC system broke down it wouldn't take long before the temperature in that room hit 90. I was thinking about some sort of winch that could be activated somehow and would reel in its cable thereby opening the door. Rather than a remote-control winch, I'd say what you need is simply a remote-control power strip (and, ideally, an IP camera to monitor things).  Flip on the outlet for the winch, wait (x) seconds or watch the video to know when, and then flip it back off. You could very easily script this into Nagios, and even include some elementary logic to try to guess whether the door is already open, but unless your server room has a ridiculous delta-T, paging someone to do it by hand is going to be a lot safer. Try to think about this morally and ignore the temptation to create an amazing hack. I know the question is to find a way to make a winch work but there's also a higher issue at stake here. If your system is over-heating then you've a far greater risk of fire and some sort of automatic door-opening machine is a terrible idea. Failure of your thermal protection (ie, aircon) is a ""shut the server down now"" issue. If that server cannot go down, then implement some sort of redundancy but the problem lies with your ability to handle down-time not ""how to stop the server exploding"". The standard practice is to shut your servers off to protect them but, honestly, it should never come to that if this is engineered correctly.",4
"However, why did they stop there? Why did they only change two characters and not design a completely different (like DVORAK) layout for German (assuming English was first)? I am often in the situation where the two keyboard layouts are similar enough that my fingers can type really fast but now catch on the YZ thing fast enough. If the layout was completely different, I might have an easier time to switch between the layouts if needed. So why do they only differ by exactly one permutation of the letters and nothing more (neglecting special characters)? I am pretty confident it has to do whit user adoption. I switched to Neo2 lately and it was a pain at the start (see below). So all the typists that switch from mechanical typewriter to computer keyboard would have to learn a new layout and would loose their ability to be productive on a typewriter in addition their productivity on the computer would be very low at the start. So they stuck with the old typewriter layout. And the same problem persist until today. I have to carry with me a thumbdrive (which I do anyway) that allows me to inject key remaping into windows machines, otherwise I am screwed when using one (Linux supports alternate layouts very well). On a side note: If you wan one that actually does the job right for German and is pretty decent too for other European languaes have a look at he Neo2 layout. I especially like how they handle all the special chareters, really useful for programming. While looking for the history of this, one often finds that the relative frequency of characters is different in German and English. The letters on the typewriter become entangled when used too fast, therefore one needs to space frequently used characters apart. This is okay, I do see this difference in relative frequency of the characters Y and Z. If you look at the actual frequency analysis of German language the QWERTZ layout is still crap . So there must be other reasons to why they did not change the layout too much after the constraints of the mechanics where gone. The US and German character layouts only differ by the Y and Z characters, those are exchanged. There is the subtlety that the US character layout usually comes with an ANSI button layout (slim enter) whereas the German character layout is usually with the ISO button layout (large enter).",2
"It's also worth at least considering writing a little byteWrite function that handles calling bitWrite in a loop for an entire byte's worth of data. I'm going to preface this with the fact that I'm not quite sure what (if any) restrictions the Arduino may impose on the C++ you can use. Since you've tagged it as C++, I'm going to assume you can use C++ as you would on any other platform. While this is one of my first steps into embedded programming, please point out all that you believe is wrong or need improving. I'd rather know what to improve than continue with a false sense of accomplishment :-) This isn't (at least according to the tag), so the (void) in the parameter list to specify that they take no parameters is entirely unnecessary.  I'm fairly new to Arduino and C++ in general, coming from a heavy Python background. The code below is functional, but my lack of C++ knowledge is keeping me from spotting any errors in style, idioms and other good practices. ...should be replaced with something like std::vector<byte> ledStates;. If you can't use std::vector, you should still really do a vector-like class of your own instead of trying to embed its functionality into the parent class. It looks like these should be handled in a member initializer list instead of assignments in the body of the ctor. The one that initially looks like an exception is the call to setlength, but from the looks of things, that should really be as well--but with ledcount and ledstates replaced by a std::vector<byte>, at which point it can be initialized in the initializer list, just like the rest. For the moment, I'll assume you've done this. Not all of the library code is used by the example, as I tried to keep that to the minimum (the functions there are simply more of the same). Based on the usage pattern, I think I'd move latchUp and latchDown into the ctor and dtor of a separate class:",2
"Looking at smallpt (http://www.kevinbeason.com/smallpt/), it doesn't allow for objects to have both, but instead forces materials to be either diffuse or specular. Of course you can improve this via importance sampling, and that is probably where you have problems. For a very specular surface design a random distribution that just draws more directions from the specular reflection direction. Rest will be diffuse. I don't have enough experience with actually bulding path tracers to help you how to design one, but your sources will probably have something here. When you diverge the path and spawn two rays, you have to increase the number $n$ of total samples accordingly because now you have done two samples. Then handle both pathes as if they were separate from the beginning. What is now the correct way to handle a material that has multiple different ways that a ray could travel? There is no one correct way. The only important part is, that your values $n$, $f(x_i)$ and $p(x_i)$ stay correct. Monte Carlo samling, as long as you correct weighting by probability, will always with infinite samples converge against the correct value. This means even if you use a complete stupid way to generate samples, if you do it long enough, that will work. Maybe not in your lifetime or that of your great grandchildren, but it will. If we instead sum $\frac{f(x_i)}{p(x_i)}$ we get $\frac{1}{n}\sum_{i=0}^n \frac{f(x_i)}{p(x_i)} \rightarrow \int f(x) \mathrm{d}x$. I've seen other path tracing code which rolls a random number to choose which way to go, using the luminance of the diffuse and specular reflectance as weightings in the random roll. Define three intervals (one common way is to chose the intervals to be a partition of the numbers between 0 and 1). The relative sizes of these intervals have to be proportional to the material. The tricky part is how do you design this. In my class we defined a number reflectivity $\rho$. Important here is again: Design it so that you know how probable a specific direction is, and then keep your $p(x_i)$ up to it, so you can in the end divide by this and get a correct Monte Carlo estimate. You generate a path of light, that will become one of the samples $\frac{f(x_i)}{p(x_i)}$. In order to do this you model the interaction of the light with the surface at each step along the path. And you always update your current path probability $p(x_i)$. We draw a random number between 0 and 1, if it is lower than $\rho$ the path ends. Else we reflect. The BRDF will take care to weight the specular vs diffuse part if you do enough samples. Now your $f(x)$ is the rendering equation and you got yourself a way to approximate the rendering equation. The Monte Carlo integration says for a sufficiently big $n$: $\frac{1}{n}\sum_{i=0}^n f(x_i) \rightarrow \int f(x)p(x) \mathrm{d}x$ Let's say we are path tracing and that there is an object which has some amount of diffuse reflection, and some amount of specular reflection. I've also seen code which follows both paths and diverges when this happens and sums the results, where each recursive ray was multiplied by it's corresponding reflectance.",2
"I've done this successfully. You need to move your machines from 192.168.1.* into the 192.168.0.* subnet. If they are all on DHCP you can reboot, but it is a lot quicker and easier on Windows to use the connection repair function, or from CMD: I changed the IP address on my bt home hub 5 router from 192.168.1.254 to 192.168.0.254 because I was trying to connect to an IP camera - big mistake.  Now I can connect to the router but there is no internet available and I can't get the web interface to put it back the way it was.  I've tried resetting the router but this doesn't seem to do anything.  Can anyone help? Ta After power cycle you should be able to access its web server with the login and password as they are described in the Manual by default. The procedure is described clearly in your router Manual. It requires you to follow the instruction step by step exactly. Because the process is capricious. User is supposed to be holding button down in some specific time period, and the process may rely on some particular duration (all as it is in instruction). If the interface has been set manually, you will need to change the assigned IP and gateway within the interface's TCP/IP properties. Hardware factory reset of the router is a fundamental procedure and, at the same time, is very easy. It is recommended to be sure it is doable until you stuck with other issues.",3
"If you've copied files from a 2003 computer, you can't use that in order to create a virtual machine. The virtual machine's hardware is different, and so the Microsoft license in the old 2003 machine cannot be reused for the new one. You have no choice get a 2003 installation cd and use that to construct a new virtual machine, and only then you may copy the old files into it. The problem is that a ISOs need to be made bootable, and even then, there needs to be something in the boot sector that can load the actual setup executable (after finding it in the file system). The CD-ROM drive and the BIOS have no knowledge of file systems or anything like that, it's all up to whatever is in the boot sector of the disc. You should check out some guides on how to setup a virtual PC, including probably creating a boot enabled disc image from your Server 2003 dumped files. The BIOS loads the boot loader (found in the boot sector of the disc) into a particular address in memory and jumps to it. If the boot sector does not contain valid executable code then the disc is unbootable.",3
"If you're looking for an easy to use application, try soundconverter for GNOME or soundkonverter for KDE (both available from the repositories). If you prefer CLI applications, you can't do much better than Perl Audio Converter. There's a Debian/Ubuntu package on the Downloads page there.  The quality setting -q:a ranges from 0 to 9, where 0 is best quality and 9 is worst. Here is a more in-depth guide. For most people, -q:a 2 is more than good enough. If you install the package ubuntu-restricted-extra then you can rip to MP3 instead of Ogg Vorbis. Ubuntu doesn't ship with the MP3 encoder by default because of the legal minefield about who owns it. Transcoding from one lossy format (vorbis) to another (MP3) is not ideal, but unfortunately it is necessary sometimes, especially if one has an ageing audio device. The command-line tool avconv can do this well (ffmpeg uses identical syntax). This would be of broadly the same quality as -q:a 2, though the VBR mode should generally be preferred. Sometimes people need to use a constant bit rate, for some really obsolete hardware, or for streaming. If that is the case, Of course, the best answer is to re-rip from the CD to avoid loss of quality. By converting from one lossy format to another, it's like making a photocopy of a photocopy. Quality suffers. will give you a variable bit rate MP3: this means that the encoder will alter the bit rate depending on the needs of the music. Dubstep needs a higher bit rate than whalesong. On average, over several pieces of music, -q:a 2 will get you 190 kbit/s, though most of them will be over or under that bitrate.",3
"4.Click ""Use the following IP Address"" and then go to the start menu and open ""run"". In run type in ""cmd"". Then press OK and cmd should open. Now you need to find you ""Domain Name Server(DNS)"". To do that go to Click Here and it should show your public ip. Open up a new tab and then type in that public ip. The username and password should be username=""admin"" password=""password"". If not you may need to find that out by doing some searches. When you are on this site you will need to do some searching for Primary Dns and Secondary or even Domain Name Server. All of them aren't the same. Once you find those the order they should be in is the Primary or Preferred and then Secondary or Alternate. Just type these in on the Internet Protocol Version 4 from step 3 for what they match. Then click ""OK"" and you may lose internet connection for a bit, but then it will work again unless you did something wrong. 7.Now try restarting you computer but make sure you know what your IP Address or IPv4 Address is before you restart. Then open ""run"" and type ""cmd"" again to see if your ip changed and if it did try again. If it didn't Congrats!. I hope this helped! In cmd, type ""ipconfig"" and a list of stuff should pop up. Find where ""IPv4 Address"", ""Subnet Mask"", and ""Default Gateway"" are all filled up. When you do go back to the ""Internet Protocol Version 4"" from step 3 and for ""IP address"" type in the ""IPv4 Address"" from cmd. The ""Subnet Mask"" should fill up by itself when you click on it if not fill it in from the info from cmd. Fill in the ""Default Gateway from cmd's info.",1
"Issue:  Centralized user management and user permission mangement of user access needs for resources (access to services, home directories, joining to local user groups, file system permissions, etc.) on Linux servers by way of group membership within Active Directory. An added benefit to Method Two above is that you can further extend the users to enable you to use the OpenSSH LDAP Public Key patch, which lets you store SSH keys in LDAP and eliminates the task of synchronizing authorized_keys files around your network. This works with any modern *NIX flavor capable of authenticating against LDAP directories, and may be included by default with Ubuntu and CentOS these days.  It is a bit more robust/modern than the NIS-like hackery in Method One, but less likely to be officially supported by Microsoft.   Both of these techniques require you to extend AD Users and Groups to be POSIX Users & Groups respectively so that there are usable POSIX UIDs and GIDs for your *NIX systems -- Microsoft provides this capability in Active Directory. This works with pretty much any *NIX flavor out there (they all support NIS), and has all the benefit (and drawbacks) of NIS.  It is also officially supported by Microsoft, which means you have someone to call if stuff breaks. Background:  We have a number of Linux servers, some CentOS and others Ubuntu, that are used for development, web hosting, database hosting, PXE serving, etc.  We also have a centralized Active Directory envrionment where all users are added into and provided with group memberships at the time of joining the organization.  Notes:  How does one approach such a task?  Is there a set of utilities available within Linux already that will allow this type of operation?  The access we need to grant to a user is going to be dependant upon the user group memberships that they are a member of from Active Directory.  For example, everyone within the AD Group of Development will need to have SSH access, MySQL access, and a home directory on the Linux versioning server 1 and 2.  Everyone that is within the AD group of systems administrator will need to have SSH access and SU permissions for all of the Linux servers, etc.  I have looked through a number of the existing articles on serverfault and have not found anything that matches up to the needs listed here. Example:  Bob and Alice join the organization, they get added into their appropriate groups within AD, and now they have access to SSH or MySQL on one or more of our Linux servers.  Once Bob leaves, we remove him from the AD group(s) and he no longer has access to the Linux servers for SSH, MySQL, etc. Take a look at Likewise Open (Or now called PowerBroker Identity Services Open Edition). I have used Likewise open in a previous job and worked out great with Samba and our devs. Method One: Microsoft's Identity Management for UNIX - This allows you to expose ActiveDirectory as a NIS server.",3
"You should have geographic redundancy for your dns if possible.  It's ok to run the primary nameserver on your server if you only have one server.  However, you should also set up a backup nameserver somewhere else.  For example, I manage a small company's single Linux server which also serves dns for their domain.  I signed them up for dyndns secondary dns for the backup.  Works fine and not very expensive. It is another story to host the same site in another country with a different domain like domain.ca for redundancy. It's not a good idea to use two dedicated IP addresses on the same server to comply with the requirement of having two DNS servers, if that's really what you meant. You should at the very least assign the two IP addresses to different servers, and preferably use geographically distant servers. So in your case having one server in the USA and another in Canada would be ideal. As a practical matter, do you really want to be managing the server too, or just the DNS records? The latter is easier. Also, who do you really think has more robust servers, you or a major company in that business? I really have to wonder why someone hosting one web site, or even a few, would host their own DNS servers. That is more for companies, who are in that business. This is particularly important if you host your own email.  If your domain is completely down, there won't be any dns info available for your domain, and external mail servers will give up trying to send mail quite quickly.  If your main server is down but your backup dns is still alive, external mail servers will keep trying to resend mail for at least a day or two.  That means the mail is much more likely to eventually get delivered when your server comes back up (although obviously you should work on your mail server redundancy in that case too). Is there any reason you don't just have your DNS with a major domain registrar like godaddy.com? I have a number of companies I consult for there, and we do get two name servers that are on different subnets (I believe this is what you mean by two IP addresses). It is not so important to separate them by country.",3
"(Unless your router is set up in half-bridging mode (which is uncommon and unlikely), the ARP table will only contain addresses of systems ON THE SAME SUBNET that your computer is in, ie if your IP address is 192.168.x.x, the only addresses which should appear in your ARP table will be 192.168.x.x and 127.x.x.x [ 127.x.x.x = loopback ]. Which works, and Wireshark successfully displays ICMP data. But there is no ARP information right before the ICMP data even after I clear the ARP cache. In order to reach Google, your computer has a default route to the router.  Thus in order for it to send packets to Google it only needs to know this address, which it most likely already knew at the time you generated the ICMP traffic, so no lookup was required. The only address in your ARP table related to getting to google would be the MAC address of your router (gateway address).  And some inbound packet, say from windows doing maintainence or DNS could have come back, after your ARP clear, and reset that before you fired up wireshark to watch things. So far I know that if Wireshark does not display ARP frames before ICMP frames, it's usually because there are ARP requests in the cache. Then, I start Wireshark, add a filter: arp or icmp to filter out any traffic except ARP or ICMP data. After that, I try to run a simple ping request:",3
"Flickering can be a form of temporal aliasing. It's a similar phenomenon to spatial aliasing such as jaggies, but it occurs in time instead of space. But I was presented a question about ""A picture flickering in a game room when the user moves"" and the answer was given as being an aliasing problem. Antialiasing strategies that address only spatial aliasing will often produce an image that looks good in a static screenshot, but turns into a flickery mess as soon as things start moving. This is one reason why temporal antialiasing has become popular in games in recent years. For instance, a common cause of image flickering in graphics is when the camera or geometry is in motion, and geometric features fluctuate in pixel size as they move. For example, imagine a railing with thin vertical bars. Depending where a bar appears relative to the pixel grid, it might get rendered as 2 pixels wide, only 1 pixel wide, or it might not appear at all. And in motion, it may rapidly fluctuate between these states, creating a visually objectionable flicker. Also, I know that Anti-aliasing refers to a technic (mainly of blurring) to remove (our camouiflage) aliasing. So in the class I've learned that Aliasing refers to the jagged edges resultant from the discrete nature of computer graphics way of representation. Another common cause of image flickering is specular surfaces with a bumpy normal map and a high specular power. The specular highlights can flicker in motion, due to their alignment with pixels changing from frame to frame.",2
"It sounds like you're using the maildir storage format (one folder on the server for each folder, and within three directories ""cur"", ""new"" and ""tmp"", each containing one file for each email). However, make sure your users restart their email clients after doing this, because mail clients and server tend to get confused if the contents change on-the-fly. Manipulating email within Maildirs is just a matter of copying the files around. You should be able to restore all your old email by copying all the old files to the new directories. We are using qmail as our mail application on a linux server. A few weeks ago our server crashed and we had everything installed from scratch and our users started to send & receive email again. The problem is they have lost their old emails. We have a back up of the whole qmail directory. But I don't know how to restore the old emails without losing the new ones. It's worth mentioning that I don't have any problem with restoring old sent mails. When I copy email files into .sent-mail/cur directory, I have them restored in sent box of users, but restoring files in /cur directory doesn't work for inbox emails and I can't get them restored.",2
"This reduces the entire menu to a single call to operator<< because consecutive strings in C++ (and in C, for that matter) are automatically concatenated into a single string by the compiler. If the user enters the string ""My dog has fleas."", the program will report 4 vowels and 13 consonants, but that's not actually correct.  The 'y' in ""My"" is a vowel in this usage and the spaces and period are not consonants. There are two reasons not to use system(""cls"") or system(""pause"").  The first is that it is not portable to other operating systems which you may or may not care about now.  The second is that it's a security hole, which you absolutely must care about.  Specifically, if some program is defined and named cls or pause, your program will execute that program instead of what you intend, and that other program could be anything.  First, isolate these into a seperate functions cls() and pause() and then modify your code to call those functions instead of system.  Then rewrite the contents of those functions to do what you want using C++.  For example, for pause you might use this: Clearly, you can apply the same idea for counting consonants (or even implement that in terms of vowel counting, i.e., every character that is not a vowel is a consonant, but this requires you to trust the input to be sane). The Vowel_count and Consonant_count functions do not (and should not) alter the passed char *, so that parameter should be passed as const. The difference betweeen std::endl and '\n' is that '\n' just emits a newline character, while std::endl actually flushes the stream.  This can be time-consuming in a program with a lot of I/O and is rarely actually needed.  It's best to only use std::endl when you have some good reason to flush the stream and it's not very often needed for simple programs such as this one.  Avoiding the habit of using std::endl when '\n' will do will pay dividends in the future as you write more complex programs with more I/O and where performance needs to be maximized. Each of those is a separate call to operator<< but they don't need to be.  Another way to write that would be like this: Putting using namespace std at the top of every program is a bad habit that you'd do well to avoid.  Know when to use it and when not to (as when writing include headers).   That is the correct operation, but it's confusing to the reader.  I recommend using the braces always to avoid ambiguity or confusion. I suspect that the if statement in the Vowel_count() routine is not what you intended to write.  It's written like this: The code as posted has inconsistent indentation which makes it hard to read and understand. Pick a style and apply it consistently.",2
"Did you copy/paste this info, or try to type it?  You have ""193"" in your network, except one machine shows 195.  Then you show 192 in  your routing tables. It's a bit strange, to start I would try to run tcpdump on mach01, mach02 and mach03 to see if mach01 and mach02 iaregetting ARP Request from mach03 when you try to ping mach01, if it's replying (for mach03) or not, etc. Also, please can you post the output of 'arp -a' immediately after one of the failed 'arping' attempts? This should show an incomplete entry for the IP address you tried to arping on [eth1], and will confirm that your host routing is configured correctly.  Does 'arping' work from 01/03/04 to 02 or are they updating their arp cache courtesy of incoming broadcast packets from 02? Please can you paste the full host routing table from one of the hosts? It's possible that there is a more specific route for another interface. Did you know if there can be a transparent firewall between hosts ? This could explain what you're seeing. What is the network topology ? is there many switch between hosts or just one ? What kind of switch ? It turns out I discovered an issue with Rackspace Cloud Server's networking. The issue was escalated and has been resolved.",5
"I do not believe they are actually the same, because in your first example you do not deny any hosts explicitly (unsure of what Apache defaults to if anything at all). Just think of it as white/blacklisting. In the 'deny,allow' case with Deny from All you are essentially creating a whitelist of the users you want to be able to access the server, with each Allow from X you add. If you used 'allow,deny' with Allow from All and adding Deny from X you create a blacklist of people you specifically do not want to be able to access the webserver. The second more regular example you showed is correct for allowing only the localhost to access the webserver. This is because the access control directives in Apache are applied in the order used by the Order directive. So with ""deny,allow"" first the deny directives are applied, then any allow directives. In this case all hosts are blocked first then 127.0.0.1 is allowed. Nothing else can get in except localhost. If you were to reverse only the order making it ""allow,deny"" that would mean first 127.0.0.1 is allowed, then all hosts are blocked. Meaning no one can get to the webserver at all (not even localhost)!",1
"This shows her windows userid. If normal Windows authenication is used with IE, this is the user id being presented to Webserver. If you IIS logging, this is the same ID you will see in the log. A user, let's call her JaneDoe, has just gotten married and her login has been changed to JaneJones. We have an application that uses Windows authentication to store the user's login name to a table and then redirects the user to another non-Windows authenticated site with a GUID which points to the table entry we just made. When the user reaches the second site, we read in the login name from the database using the GUID that was passed. Then, we look up the login name in another database where we track application permissions. The problem is that the user is logging into her workstation as JaneJones, but the Windows authenticated site is still receiving a login name of JaneDoe. Is this a domain controller issue? Is it a workstation issue? What's the best way to resolve this? Are you sure Login name was changed and not the display name? You can check her log on name on her pc by If you've confirmed that the username really was changed (see ggonsalv's suggestion), double check to make sure the user didn't check the IE box to save the password.  Follow these steps: Yep, I've seen this before with IIS 6. Turns out there's a token cache external to the W3WP instance, so restarting that (i.e. recycling) doesn't help; I can't remember the details, but either an IISReset or a whole-server restart is required to clear it. Let me start by saying that I am not a server guy - I am a developer. But I develop and manage an ASP.NET application that uses Windows authentication. I've run into the problem I am about to describe before, and I would just like to understand how to remedy it since I am the one who always gets the original support request.",4
"I ran this command and could get it to work with RealVNC Open/Free edition viewer. Ensure that on the client, the color level is set to Full [Options->Colour & Encoding->Colour Level] To answer the other question:  VNC is the Mac answer to RDP, actually.  Even the commercial ""Apple Remote Desktop"" package ultimately uses the VNC protocol. I've also found that the VNC viewer will konk out when connecting to the Mac if the viewer is not running in millions or full color mode. Make sure that its configured for VNC access with a password.  Then try to connect to TCP port 5900 with a VNC client on your PC. From memory you'll need to either use a v3.x viewer or configure the one you have to use the version 3 protocol. How depends on your viewer but it should be covered in its help file or web site. Although performance was clearly lacking. It's using ZRLE compression; perhaps JPEG or something else is superior, and perhaps professional RealVNC is optimized. The free version of RealVNC (v. 6.17.1113, Nov 2017, x64) works smoothly for me. In particular from Windows 7 Professional to OS X Yosemite (10.11) and to OS X High Sierra (10.13 w/ latest updates), the ""OS X Authentication"" that Mac OS builtin VNC requires, with username and password and no tweaks to the VNC server side, posed no problem. These are security types 30 and 35: see https://vncdotool.readthedocs.io/en/0.8.0/rfbproto.html#security",5
"If you can get the serial number for your laptop, the manufacturer may keep records of MAC address tied to system serial number. It's a bit of a long shot, but since MAC addresses are unique, this is definately possible for a company to track. If you connected to your home internet, there is a log in the router that stores all mac adresses and the IP adresses.  I don't mean to be a debby downer, but depending on the model of the network adapter, the MAC address can be changed.  This may may make things a bit more difficult.  They could have changed to something generic like DE:AD:BE:EF:20:12.  On the other hand, this isn't something that is commonly known to be changeable because the stardard for years was that the address was ""burned"" in the chip by the manufacturer. If you computer has been stolen and you need to give them the MAC adress of your laptop you can get it from there. Maybe I'm over thinking this, but MAC spoofing is relatively easy, and the address isn't even unique. Honestly, the only way you're likely to get your laptop back is if the thief accidentally leaves it somewhere, and even then, the police would need to find it, and you would have to somehow prove to them that it's yours. I honestly think that the best course of action would be to change any internet passwords that are remembered by your stolen laptop's browser, buy a new computer, and move on. I can only hope that you have backups of any important data. I know this question is old, but today I had a similiar headache myself after finding out that my router already ""forgot"" my recently stolen Macbook. There is one way not mentioned here, the storage backup. If you have access to any kind of backup from your stolen laptop, it is possible there are some wifi diagnostic files included. Not sure about windows, but from my OS X backup I dug up the whole wireless diagnostics archive (thank God I had this problem with wifi once hehe) and one of the files it contained was wireless_diagnostics-ilTACd.log, which inside among other technical stuff had the MAC address of my MacBook :) However, woliveirajr has a point in comment to your question in that if you have a router at home that you connect to, you stand a good chance of finding a log with all the different MAC addresses associated with their host name that has ever connected to it.  Or if you you connect directly to the Web, your ISP may even be able to tell you the MAC address your modem has connected to it.",5
"I have older .mdf and .ldf files of the particular db with the deleted table in it. How would I properly restore the older version with these files? Can I ""Detach"" and the re-attach the older version that exist in another folder? How does this functionality work? What will happen to the current .mdf/.ldf files and will the old ones I'm attaching be moved to the appropriate folder? I've mistakenly deleted a table in Microsoft Server Management Studio, and the restore function was not properly configured.  First up, move the *.mdf and *.ldf files to the default database file locations. This way, they'll inherit the appropriate file level permissions. Now, you should be able to browse the old database, find the table in question. Right-click it, select ""Script Table as"", and open a ""CREATE TO"" script in the query editor. Edit the script to use the original database instead of the restored one, and you should be able to re-create the table. I would attach the files to a new database, script the table and re-create it in your active database.",2
"The setup is okay, except for when I come to create files on the NFS mount. I end up with files that cannot be read by Apache, or which cannot be modified by other users on my system. There are different ways to fix this, but the cheapest/quickest is to create a group with the same group ID number on both machines - say, group ID 50000 - and set the group bits on the file server while adding the appropriate user to the group on the client; then use the group permissions on the files to control access.  Not a great solution but it will work.  Note that you could have problems with services that explicitly change their group at runtime (aka privledge drop) and you might need to change the setting that controls what group is assumed at runtime to ensure it is the one you have created. With Samba, I can specify force user, force group, create mask and directory mask, and this ensures that all files are created with suitable permissions for my Apache web server.  I can't find a way to do this with NFS.  Is there a way to force permissions and ownership with NFS - am I missing something obvious? I have a number of Samba shares on fileserver so that I can access files from Windows PCs. I am also exporting /data/www-data to the apache server, where I have it mounted as /var/www. You could also use the all_squash option which makes anonymous (user & group) all the exported files & folder, and attach them to a specific GID & UID. In the (distant) past this was co-oridnated with NIS and NIS+, although there are other schemes that have been wedged into this framework (Samba's Winbind being one of them).  However, this requires a central ID server, followed by a lot of hand-fixing permissions. For those files that come inbound via a Windows share (aka Samba) simply force the group to be the same as the one you create.  That way all files automatically get the ""right"" GID. Although I've spent quite a bit of time with Linux, and am weaning myself off Windows, I still haven't quite got to grip with Linux permissions...  If this is not the right way to do things, I am open to alternative suggestions. The problem with that is that all users on the apache server will see your mount point with nobody nobody as user & group, and could write in the mount (but anyway, on the Samba server, the files will be created as <your UID>/<your GID>). The issue you're running into is fairly common.  NFS is passing the UID and GID of the files/directories back and forth between the machines with the assumption that the user and group IDs are mapped identically on both.  This means that you can get a situation where the UID/GID on the server is passed back to a NFS client, but it can't matched in the client's /etc/passwd or /etc/group, which means no access.",3
"I use vi-style editing in bash and vimperator for web browsing with firefox (sadly doesn't have :vsplit). In firefox the fantastic It's All Text! plugin configured to use gvim makes input boxes (likes this one) less crappy. mutt picks up my $EDITOR without problems (tell Karen to set that variable, too). Until somebody writes @Ignacio's SCIM/IBus plugin (which would be splendid with NeoVIM), here's a workaround with xclip. It pastes into a temporary file and edits it with Vim: This isn't exactly what you are looking for, but I also have experienced the desire for vim keybindings everywhere. One solution I've used is Touch Cursor, which I mapped so that my home row keys would allow me to navigated my cursor. The default arrangement is not vim-like, but you can easily change that. It may take some getting used to holding the space bar to achieve chords, but I found it pretty natural. I've not seen anything to allow this, but it sounds like it could be handled by writing a module to plug into an IM such as SCIM or IBus. You'll probably want to assign a keyboard shortcut to this script, so you can do something like CtrlA, CtrlC, SuperC There is also a new chrome extension that adds vim-like keybindings for browsing as well called Vimium. These are some other programs that offer application-specific solutions: I use Viemu, which provides a vim emulation layer in Visual Studio 2005/2008, Outlook and Word.",4
"Look at the temperature field. The Raw/Data column should have a normal temperature value and the Current/Worst columns should have a strange, high value. See several examples below. At least one other person has similar, strange results (see bottom image); though they may also have a defective drive. OCZ tool shows only 114 total ECC and RAISE errors. But what does Data column in HD Tune mean? Why the description says that disk had 89.970.873 read errors? It is new disk. I installed it yesterday and it is passing all diagnostic tests in SeaTools, Performance tests in HD Tune Pro and common check in Chkdsk. The HD Tune data looks quite suspect. For example, I don't believe that you had a temperature peak of 129 degrees celcius. And as you say, the ECC recovered rate is simply ridiculous. Clearly, HD Tune is doing some fancy work and/or pre-processing of the data before formatting and showing it, and it is not working out quite right. In fact, looking at the raw value of the temperature field, HD Tune is interpreting, and maybe even reading the data incorrectly. Just like to add that the errors are correctable, so programs don't get to see them but all the same, I would not expect to see any real read errors from SSDs. I would seek further clarification from the manufacturer. Try another SMART tool (preferably an OCZ one like Harry said). I like SpeedFan. Use that to see what the values are and if the numbers are still weird (eg the temperature field is still showing values similar to 30-129 in the Current/Worst columns and a huge number in the Raw/Data column). If they are, then it could be your drive; either its SMART circuitry is damaged, or it simply requires custom SMART software. However requiring custom SMART software highly unlikely since it is a standard and would be very user-unfriendly. If you are worried about 114 ECC and RAISE errors, the best is to get in touch with OCZ Support, and ask if they advise to use the warranty to exchange the disk. Their answer, if positive, will give you the necessary ammunition when dealing with your vendor. I downloaded trial version of HD Tune Pro and I'm using it to benchmark my hard drives. In Health tab of SSD I found Raw Read Error Rate property. What does this property say about my hard drive?",4
"I know there are other considerations - speed, redundancy, if my link to the main site goes down the users have nothing. But I just am not convinced I need servers at these locations.  I have one main site with several servers an a 2008/2012 environment. I have 4 remote sites that are physically close (a few miles apart) and are all connected to the main site by 20meg fiber on a private network. At each of the remote locations I have a windows server that users log in to and where their files and apps are located.  In my opinion, it is a good idea to have at least one server in every location with more than 15-20 people.  It speeds up logins to their desktop, and it makes managing print jobs easier if they are flowing through a central server. I am really interested in figuring out if I need servers at these locations. $3,000 to $4,000 per server every 3-5 years, licensing, administration... It depends.  There are a number of factors that can influence whether or not you need a server at each of your remote sites, or whether you can get by with alternatives such as a backup VPN tunnel to your primary site or some sort of VDI solution like Citrix, Terminal Server or VMware View (you also don't mention if you have anything like this in place today). There are many considerations to answering this question. But the first thing I am wondering is do I really need a server at each location? Users are just logging in to this server for permissions and a vast majority of my users are only using word, excel and email.",2
"Finally, for the ""other benefits"" part of your question: nginx also handles SSL connections faster than Apache although again, that doesn't make it better for all cases.  For example, if you need detailed authentication using client-side certificates you'll still need Apache to handle the SSL.  If you're just running pages over HTTPS for the encryption then nginx will give you a little speed boost. Nginx uses only a small number of threads (commonly only one thread per cpu) to handle all client connections.  This makes it lighter than Apache and allows it to scale up and handle tens of thousands of connections on a single machine. I've already moved all my static files to s3 (I heard nginx is better at handling static files), so what other benefits does it have? Apache has two models for handling connections from clients: worker and prefork.  Prefork is very resource heavy (requires a full process for every client connection) but still commonly used with PHP.  Worker is much better from a resource point of view, but still requires a dedicated thread per client connection. However, this doesn't make it ""better"" than Apache for all cases.  It generally makes it better than Apache for serving up static files, but not dynamic content generated from a web application (i.e. you don't want hundreds of connections being blocked while one of the requests is doing a database query).  This is why you still need fastcgi, uWSGI, passenger, or even apache+mod_wsgi+passenger to offload web app work into the app server.  These back-end processes are still going to use one thread per request, but at least it's only for the requests that need dynamic content.   Can anyone explain why putting up nginx and reverse proxying Apache is faster than just plain Apache? So the main reasons nginx + apache is faster than straight apache is (A) static files handled much more efficiently, (B) reduced load on (heavy) apache resources.",2
"Mounting to a drive leter could be bad because it is possible for it to change to another mount point. Then you are reading and writing to something you don't expect. If executables from a mount point change, they could contain viruses. 2) Moving your application to a new environment will be tedious. In the event of a disaster recover, moving to a more powerful machine, or if another developer is taking over your application. If the new environment doesn't allow mapped drives or drive letters are mapped differently then someone is spending time rewriting code.  The time saved on the front end will be more than lost fixing it.  Windows (at least  XP) does not support file paths with over 256 characters. Mapping allows someone to add a file where otherwhise wouldn't be possible, by shortening the path. Then you have a program that navigates through all files and folders, and is not aware of the mapping. Without the mapping, the existing file has a path length above 256. The program crashes. At least half a dozen times a user from accounting has called because she gets the same error. It's because she opened program X, which is using a file mapped on network drive Y:, and it's not connected for some unfathomable reason. 1) They take up resources on both the local machine with the mapped drive and the network resources. Local applications can become sluggish because your local computer has to read the contents of the mapped drive when the application is launched or when the system is booted. Try it out. Map a bunch of drives and launch Excel. Unmap the drives and try it again.  We've had serious problems with network drives where I work because sometimes Windows doesn't connect to them, and it seems to not automatically connect a network drive when a program tries to access it.  One problem with the \server\dir syntax is that command windows cannot cd to them. If you have admin privileges and do not want to use a drive letter you can use the mklink command to mount drives to a directory instead of a drive letter. The directory Home should not exist. A number of pieces of software, including various version of Microsoft Visual Studio and CMS Bounceback, will only work with drive letters and not with absolute paths.  Given this restriction then to use any such software requires you do define drive letters - you have no choice.  But Windows does not make this very easy as it seems to ask for a user ID and password, but only one user ID and password is allowed in Windows for all connections to any one network device (e.g. multiple disks and printers). My solution requires admin privileges, so if you are not running with admin rights, it is more secure since another program could not change it on you without admin rights.",5
"Ping from server A and ensure it resolves to the same IP address as your other 3 servers are resolving it to. Do you have any other servers in the same data center/cabinet/rack as Server A where you could test locally? Failing that, I'd try a telnet from Server A on ANY port, for example to a local SMTP server at the hosting provider on 25.  If that works, I'd ask if their support could help you test by setting another of their servers briefly to listen on 1433 - Google ""port listener"" for examples of free software if they don't have their own. What's different about the three servers that work and Server A that doesn't?  Any chance Server B is accepting connections only from certain subnets, for instance? (Keep in mind that your outgoing traffic from Server A is actually only destined for port 1433.  The source port will be something different.) Does server B have more than one external IP address? SQL Server may not be listening on all the IP addresses.  use netstat -n -a to verify that either 0.0.0.0:1433 is there or each external ip address is listed as listening on :1433 Are Server A and Server B are supposed to be on the same network then?  You can verify that there is nothing in the middle with tracert.  If Server A is on a different network than the 3 other servers that you've used to telnet into Server B with then it's not really a valid test of connectivity.  There could still be firewall rules or router ACL's that prevent connections from Server A's network.  Can you ping Server B from Server A or telnet into another port that Server B is listening on? Also, have you tried connecting to Server B using SQL tools on Server A?  SQL Server Management Studio for example? Once that works, and you've checked that Server B sees inbound connections on other ports from Server A, you'd have some evidence to bring to the Server A hosting support staff.  From your description the problem would appear to be Server A. Start at the beginning. Make sure the Telnet program is in fact successfully sending out packets. I'd be inclined to use Wireshark to verify that.",5
"Since all the stuff mentioned is related to sending and clients, but we had similar issues at work, some further searching reveled that one could specify the linger option for listeners as shown in the example over here: http://msdn.microsoft.com/library/system.net.sockets.tcplistener.server.aspx Try throwing a '-b' flag on your netstat command.  It will tell you the name of the executable that is using the port.  Then find that proc in task manager and kill it there.  If that doesn't work post what the executable is that is holding the port open. To avoid endless waits on the socket, your program should use the setsockopt function with the SO_REUSEADDR and SO_RCVTIMEO parameters : In brief: There's an option that tells the socket system to keep a socket open even after it has been closed if unsent data is present. If yes than you can kill it from there, but only after you investigate what it actually is (you can see all the dlls loaded into the process) Some colleagues talked about ports being held by the OS after application termination/closing for about two minutes. Given that, it would be interesting to hear whether the port is still held after a certain amount of time. In your C# app you may specify any related options via Socket.SetSocketOption: http://msdn.microsoft.com/en-us/library/1011kecd.aspx Interesting to note that netstat can sometimes return a pid but not the corresponding executable name !",5
"I'm trying to get replication working between 2 servers running MariaDB 10.1.7 64 bit. The database I'm trying to replicate is huge, 520G and the export/import on the slave with disabled autocommits took 5 days. When I initially started the replication, it seemed that the slave is going to catch up. It actually looked very promising. Sadly, after 30 minutes when I checked the slave again I realized that it is actually falling behind. 1 week after during night time when the master wasn't so busy, it really seemed that it's catching up again but unfortunately now Seconds_Behind_Master is almost 1 week. I set slave_parallel_mode to aggressive and on the slave I've set slave_parallel_threads to 30, while on the master this is set to 10. The innodb_flush_log_at_trx_commit variable is 1 on the slave and 0 on the master. Here is the config for the master: I do know about the Unable to load replication GTID slave state from mysql.gtid_slave_pos: Table 'mysql.gtid_slave_pos' doesn't exist in engine and it should be fixed, but I am not sure if it is the cause for the replication lag. Does anyone have any ideas what I should be tweaking? The slave has a better CPU than the master.",1
"From rsync perspective, what I want to do is this: after the snapshot is created, it becomes my new source tree, $SNAPSHOT. The destination is $DIFF. The base of comparison with the source is $PREV, so the argument --compare-dest=$PREV should take care of it. This, like the previous command, would copy only the files changed between previous snapshot and the current snapshot. Now, for the files to be copied, I want them to be compared to the files in $SNAPSHOT first and then, if existing there, linked against them instead of copied. This is what --link-dest=$SNAPSHOT does, and it also works. Both options work well individually, compare-dest creates a differential copy and link-dest creates a linked tree, but I want to mix both, in the way described above, which rsync does not permit. Any ideas on how to create the $DIFF tree in away that the files on this tree are all hardlinks to $SNAPSHOT ? Or other suggestions on how to achieve my goals with other linux commands? I am trying to create a script that creates both a snapshot backup and a differential backup, but with hard links instead of duplicated files. In other words: This also works fine and is very close to what I want to achieve. The only problem is that all the files that changed between $PREV and $SNAPSHOT are now duplicated on $DIFF, I would like them to be hard links to $SNAPTSHOT instead of plain copies.",1
"SAC compared with partial evaluation: As Rob Simmons says above, dependencies in SAC need not be separated a priori into two categories (static and dynamic); moreover, even in the case that all data has the potential to change, assuming that the changes to this data are incremental (and that the underlying program enjoys ""stability"" across such changes), the SAC approach can still deliver significant time savings. The important thing to note about SAC specializing a computation is that, after a change occurs, the computation itself is updated to reflect this (via a mechanism I mentioned above, called ""change propagation"").  This is in contrast to PE, where the specialization only occurs once, up front.  Hence, SAC is more ""dynamic""/""online"" (like FRP), whereas PE is (usually?) more of a offline process.  (Perhaps someone that knows more about run-time specialization of code can extend this comparison to PE when it occurs at run-time.) Let me try to extend the description of SAC given above: Self-adjusting computation is a paradigm where computational decisions depend on data, which is assumed to change over time.  The computation itself produces more data, which may be consumed by other (self-adjusting) computation.  When any input data is changed, the technique provides a general-purpose mechanism by which the computation is updated to reflect the changes.  (When I say ""computation"", I mean a dynamic structure, like an execution trace, that reflects the operational steps used to produce the output from the input.) By updating the computation, the update mechanism also updates the output of the computation.   [Disclaimer: I have very little knowledge of automatic differentiation. I have some passing knowledge of partial evaluation and FRP, but haven't used or researched either directly. I'm very familiar with self-adjusting computation; I do research in that area.] As with FRP, SAC enjoys nice compositional reasoning: one can compose two functions and get the composition of their (self-adjusting or reactive) behavior.  There may be a subsumption relationship there, where some flavor of SAC perhaps subsumes some flavor of FRP, but that's a topic of future research.  It's certainly not obvious that this relationship exists between current approaches of SAC and FRP. If I had to make them comparable, I might say something like ""PE statically specializes programs based on a fixed part of the input, and SAC dynamically specializes a computation based on the prior input"". This is because SAC is largely a dynamic technique, unlike partial evaluation, which is largely a static (compilation-time) technique.  Because of these phasing differences, I do not think that it's useful to think of one subsuming the other.  They are just different (mostly orthogonal?) approaches. I definitely agree with the decision tree laid out by Rob Simmons above; I also agree with his conclusion: they are all different, and probably not in some subsumption hierarchy.",1
"I personally prefer to do things as is the English way answered, but in the interest of fairness and such I feel obligated to share that I pestered Paul Vixie (as high an authority on the matter as I could think of) about this. His response was that this belongs in root's personal crontab. TL:DR; What is the intended use of the various cron locations? And where should the schedule for a utility that needs to run without human interaction go? With all of them being functionally equal, what is the most correct place for a utility to place its crontab? In an amazing display of pedantry, a coworker and I have been arguing for literal days over the intended use of the various places for a crontab to live. At the heart of the matter is a backup script we use to push archives to our customers of their data. Some want that weekly and others want it exactly on certain day of every month - so our current crontab is pretty lengthy. The utility itself is nothing exotic, just a little node script that gets triggered periodically with different sets of customer-ids passed to it.",1
"The issue isn't the SSD itself.  Rather, it's how the SSD driver, BIOS, and Windows power management compete and are often misconfigured for low-power states like ""sleep"".  It's really inexcusable that the vendors haven't figured this out yet.  It's been known for a decade. If THAT doesn't work, experiment with the BIOS SATA options for the channel your SSD is connected to until you find the right one.  Usually AHCI default settings are fine, but sometimes the link state power management settings need to be adjusted.  You can also try using  IDE mode, but Windows won't likely like that. If your system is a desktop PC, first try disconnecting the power supply for a few minutes like above.  If that doesn't work, remove the CMOS battery from the motherboard for a few minutes, replace, and try to get into the BIOS.  Reset the settings to the BIOS default, and make sure that the SATA controller (assuming your SSD is on a SATA channel) is set to AHCI and that boot options are set for UEFI boot. tl;dr: Your SSD is fine.  It's the idiotic software combination between BIOS & Windows10 that screws up sleep.  As you can see from this search, it's a ridiculously frustrating and common problem: Windows 10 won't boot after sleep As to resolving your problem, you should be able to boot (at least in safe mode) by clearing the current power state.  If your machine is a laptop, remove the battery for a few minutes, put it back, and then try to boot in safe mode (In Win10, the only way to boot to safe mode is from the Troubleshoot screen that you mentioned.  See How to boot in safe mode in Win10)",1
"We found that changing the printer to a different model and then back again seemed to fix the issue.   Changing the size fixes the problem, changing the font fixes the problem. Unfortunately, as our outgoing documents have a standard format, neither was an acceptable solution.  The expected result is supposed to be normally-displayed text. In the picture below, the text that is circled contains approximately 35 characters when rendered correctly. It is squished together to the point of being illegible in the incorrectly-rendered scenario. I found that if, in Word, you select ""Font"", and then in ""Character Spacing"" select ""Normal"", rather than ""Condensed"".TextCondensed Further, the Word install on the users' actual machine worked fine (as opposed to the remote desktop to the server).  Read the Answer to this question to see why this occurred in my case, and what I did to diagnose and resolve it. The problem is related to the graphics rendering subsystem, because disabling hardware accelerated graphics in Word resolves the problem. We also know that the problem is related to the interaction of the rendering changes that occur when transparent objects (such as a ""Draft"" watermark) are placed on the page, and the rendering that occurs for the Times New Roman Bold font. I had this problem with text in a table. No font modifications affected it. I had created the table from a document scanned in as a pdf.  I have found that clicking into the header or footer will fix the squished text, or exiting and going back in.  However, I have not found a way to prevent it from happening. Finally solved it by highlighting the offending text, browsing to home\styles and clicking to select 'normal'. This corrected the squashed text.",5
"So, what I'm thinking is to create a subdomain for each one of this server instances. The only issue here is that I need the subdomain to be available super quick, and I don't know if I can trust the DNS propagation times for this. I did a simple API that modifies the configuration file and it has some good features like requesting SSL Certificates on configuration  Is there any solution to this? Like having my own routing server that have a Wildcard Subdomain and then route the traffic based on that? I think I can't use Apache or those kind of solutions because it's not only HTTP/S traffic that I'm handling. My team and I are creating a service that when user request, it creates a new server instance on the fly, with a different IP for each new instance. The issue is that I'm connecting from a client via WebSockets to that newly created server. But I need it to be SSL certificated. So I can't use just the IP. Probably you know the IP address range in advance? You could create some xxxxxx.dyn.example.com. IN A pattern for the network in advance. That enables you to use *.dyn.example.com wildcard certificate for them all without any issues caused by DNS caching.",2
"I used one of these in the 90s on simulation software.  The manufacturer was one of the hot-shot crypto companies (don't remember the name).  The software would query the dongle once in a while and, if the response was accepted, the software would continue running normally.  If the dongle was missing or didn't respond in time, the software would, IIRC, stop or switch to demo mode (deliberately crippled).  A line printer could be hooked up to the dongle. The only way to figure out what they were was to try to talk to them.  Provide the right key and they would answer.  Our name didn't even appear internally, the only way to figure out whose they were would be the company that made them--they would know the contact information associated with the ID burned into the key. Rainbow Sentinel Pro and KeyLok were Parallel Port Key dongles I worked with in the 80's.  I think there was one called DESLock too but it was a long time ago.  These all required you to communicate to them using some sample code provided by the dongle provider (in my case it was some C code which was compiled to .OBJ and linked into a Clipper application). Well, I know this device, it's called HARD-LOCK,I'm a designer civil engineer and I used this in 90's to unlock a concrete software calculation and design. It was plugged on parallel port of a PC, I think it works sending some information from the software installed in a PC into hard-lock then hard-lock sends back doing a confirmation that the software has permission to run, different way for Autocad which uses KEYS like Windows to unlock the software. I've got something identical to this knocking about somewhere, but it was for, ahem, circumventing copyright protection back in the day. It worked with software called Synchro Express (version 2 point Zero, Copyright Coast To Coast Technology, All Rights Reserved) on the Amiga.  I seriously doubt you could figure out what it's for.  I've written for such things before--we did not manufacture them, we bought them from the companies that do.  There was absolutely nothing about the outside that indicated they were ours.  We were small enough we didn't print special labeling for them. You plugged a second floppy drive into it and it allegedly enabled the user to copy copy-protected games from DF0: to DF1: by duplicating byte-for-byte. I'm not sure it was ever that successful in reality though.",5
"In general, your example sounds like the method is actually doing two distinct things, otherwise there would be no confusion in the first place. But this is code smell anyway: every method should have only one purpose. If that is the case, then the meaning of the return value is always clear, no need to give it a name. Regarding your suggested change with an out parameter, I couldn't do that.  Its not more expressive, and it forces callers to create a variable to hold the result.  In general, I think you should steer away from out / ref paramaters, they should be used pretty rarely. Using out has implications if overloading such a function. The compiler cannot differentiate between ref and out.  Not a good idea IMO. Too much extra work and potential gotchas for something that is easily solved with good code comments. I think the main problem here is not the code itself, its that there's no xml comments.  Seriously, commenting can go a long way.  I'd address that first.  Second, I'd look at the name of the method itself.  Is it doing a check?  Then CanDoSomething, IsSomething, etc would work.  If the method actually does ""do something"" and it's returning success or failure, then the name is fine.  Another alternative though might be an exception, depending on if the call should normally work or if failure is pretty common. I think the key question here is whether the boolean being false indicates an 'exceptional' problem or not.  If it does, then forget the boolean and throw an exception. Youre asking the wrong question. In reality, both methods are bad. If you need the success message, either throw an exception on failure (but only if this is really an exceptional scenario, or return the success and indicate this in the method name. For instance, by calling the method TrySomething. If you would not use output parameter, you had to use any public property or create class as return type. This is easier.",5
"I got around this by intercepting the WM_QUIT (IIRC) message and sending a command to the thread to shut things down. The thread then informed the main window when it is time to actually close, after all cleanup had been completed. And therein lies your problem: your ""message loop thread"" should be your ""render loop thread"". They should be the same thread. Attempting to do rendering outside of the main thread is a recipe for disaster. Especially when it comes to closing the window. I've run into something sort of similar, and the quick answer is that once windows starts processing the quit, you don't have a lot of time. Controls and resources start getting destroyed immediately, and it is entirely possible that by the time your thread gets around to referencing the resource, it's already extinct. The render loop and windows message loop run on separate threads. The way the program exits is that after PostQuitMessage is called in WM_DESTROY the message loop thread signals the render loop thread to exit. As far as I can tell before the render loop thread can even process the signal it tries SwapBuffers and that fails. My question, is there something about how Windows processes WM_DESTROY and WM_QUIT, in maybe DefWindowProc that causes various objects associated with rendering to go away even though I haven't explicitly deleted anything? And that would explain why the rendering thread is making bad calls at exit? When the main thread signals the rendering thread to terminate, you can use a mutex and condition to wait for the rendering thread to exit before allowing the main thread to proceed. You need to make sure this is the first thing you do on shutdown - before you delete anything related to rendering.",4
"My problem was the private keys file id_rsa created and saved by puttygen has a different format than the one created from a ubuntu machine. After I created a pair keys from ubuntu machine, copy these files back to Windows machine under %UserProfile%.ssh folder, then add the new generated public key to Gitlab. No more Permission Denied for me ssh prompts me for a password for any host that happens to have a '-' in its name.  This would seem to be purely a problem with ssh configuration file parsing because adding an alias to ~/.ssh/config (and using that alias in my git remote urls) resolved the problem. If you're using environment variables to pass the key, you should base64 encode them, otherwise they will probably fail with an error asking for your passphrase.  This means that the key is corrupted.  If you see: Replace /home/git with whatever your home directory for the 'git' user is, if it was different in the tutorial. If it's not permissions, then please let comment and we'll see what else might be the issue. Providing that you have loaded your private key on your client, then it sounds like this might be a permissions issue on the 'git' user home directory and .ssh directory. This can happen if the host has a '-' in its name. (Even though this is legal according to RFC 952.)",4
"you can use wuinstall. It has a /search switch to see if there are updates pending. You can execute remotely with psexec. We use WSUS and SSCM to collect and publish the updates, and the majority of the servers are on automatic install on a Sunday morning, mostly development and test server. In 2012 R2 server go to control Panel, click on Windows Updates. On the left panel of the page click on ""Check for updates"" link, you will get to see the list of pending updates (yet to be installed on the server). Click on any of the link you will get to see the entire (both optional and mandatory) list with the brief description for each of them. We have setup our important production servers to only install manually, but occastionally some servers are not rebooted manually for a while (humans forget!) Another option is to parse the %windir%\windowsupdate.log and look for stuff like this: # WARNING: Install call completed, reboot required = Yes, error = 0x00000000 As an admin of the remote box, open an MMC and then add the Event Viewer Snapin.  Select ""Another Computer"" (versus the default of ""Local Computer"" and input the target computer name.  Then filter by event type to see only the ones you want!  It's called ""Windows Update Agent"" and the description will begin with  It would be nice if there was some method (powershell script, WMI query, some magic command) that I could use to count or discover if there where updates pending. Is there a way to find out if a Windows Server (2003, 2008) has downloaded the Microsoft Windows updates and is just waiting for the user to confirm ""Install Updates and Restart the server""?",4
"The fact that we drained the motherboard's capacitors of charge when the mains was off should reset them, so that when the PSU / main's switch is turned on, the PC should not turn on itself automatically; But does in on-off loop. Which leads us to some shorting happening in the PC case.  I have a system with a dual boot, with Windows 10 and Ubuntu. And in both systems when I decide to shutdown the computer, everything works as expected and the system shuts down completely. But SOMETIMES after 2 seconds of the PC being off, it turns itself again, kinda like if the signal that was sent to the PC was not the shutdown but the reboot. If this is something that happens with both OS, then it cannot be a problem with the softwre in itself.  There is an option in BIOS to auto-power on a PC, but even if it is configured to auto-power on, it should never experience the on-off loop. This is very unlikely a BIOS configuration issue. The second thing is to check to make sure all hardware components are not touching each other excessively. Keep the inside of the casing tidy. When I do ""Suspend"" in Ubuntu, it is even worse! The machine goes to suspend state, but then it tries to boot itself up and after 1 second it fails and shuts down again... and tries to reboot itself again! And the loop continues without end! I actually have to take off the power chord for half a minute if I want to be able to reboot my computer, when this happens. The first thing I would do, is to ensure that the motherboard's mounting screws are not too tight. If you have clunky hardware (like big GPUs, CPU heatsinks), it can cause the motherboard to bend ever so slightly so that part of the motherboard touches the casing. That contact between the motherboard and the case can lead to a electric short which can trigger a pc go into that crazy loop. This could be the case of a remaining static charge on the mainboard, or a wrongly connected pin of the power-switch cables to the mainboard. try to find the power-switch connection, disconnect and connect again If your issue is not the same up to this point, then forget about my answer. If not, continue reading. It's possible but highly unlikely to be a power button issue. You didn't provide any details whether when the on-off loop happens, that unplugging and replugging actually successfully stop the loop. If the on-off loop still happens even after unplugging and replugging, you know it's not the power button. And that happened to my own PC. I don't have a particular answer but I'll try my best, if you follow through a particular experience I had, to lead to a likely problem with your mounting of the motherboard, and arrangement of hardware in the PC casing. I don't remember exactly solving the issue, but whenever the auto-turn-on happened, I'll hold down the power button for several seconds, which actually forces the system's power supply to shut down by most BIOS'es (unless specifically configured not to). This sometimes successfully made it stop auto-turning-on; but when it failed I would have to turn off the PSU's switch or the plug's switch, then hold down the power for a several seconds - in order to drain out the capacitors of any remaining charge. Then turn the PSU's / plug's switch on again and that would sometimes fix the problem; but it was when it didn't that actually provides a clue to the cause: Turning the PSU / plug switch back on, the PC turns itself on automatically, going back into the on-off loop.  It's probably not a dual-boot / software issue too. I used to have it on single-boot Windows system. When I did so, the frequency of this on-off loop drastically fell. Although I didn't completely solve the problem, it mostly fixed it for me, for some time before it began to happen again more often. Over time the components in the PC shift slightly from minor knocks or moving of the PC casing. So it would be great to have a good casing that would not be so susceptible to such issues. So on my older system, if I remember correctly, once the PC had shut down, after a few seconds the PC would automatically turn on (i.e. the fans would spin up and the motherboard LEDs, power LEDs will turn on) for about 5 seconds and then the system would completely turn off (fans and LEDs off). Like you said, the turning on and turning off would repeat endlessly.",3
"You should consider checking the Azure Machine Learning platform. It is online and you can use it with a free account. Featuretools is a recently released python library for automated feature engineering.  It's based on an algorithm called Deep Feature Synthesis originally developed in 2015 MIT and tested on public data science competitions on Kaggle. Very interesting question (+1). While I am not aware of any software tools that currently offer comprehensive functionality for feature engineering, there is definitely a wide range of options in that regard. Currently, as far as I know, feature engineering is still largely a laborious and manual process (i.e., see this blog post). Speaking about the feature engineering subject domain, this excellent article by Jason Brownlee provides a rather comprehensive overview of the topic. Note that Trifacta offers a really easy to use tool for data transformation. It has a highly intuitive GUI that allows to set up transformation/ feature engineering maps. There is also a free trial version that can be used for reasonably sized problems.  Finally, on the other side of the spectrum of feature engineering solutions we can find some research projects. The two most notable seem to be Stanford University's Columbus project, described in detail in the corresponding research paper, and Brainwash, described in this paper. Returning to the question, I think that the simplest approach one can apply for automating feature engineering is to use corresponding IDEs. Since you (me, too) are interested in R language as a data science backend, I would suggest to check, in addition to RStudio, another similar open source IDE, called RKWard. One of the advantages of RKWard vs RStudio is that it supports writing plugins for the IDE, thus, enabling data scientists to automate feature engineering and streamline their R-based data analysis. Feature Engineering is at the heart of Machine Learning and is rather laborious and time consuming. There have been various attempts at automating feature engineering in hopes of taking the human out of the loop. One specific implementation that does this for classification problems is auto-sklearn. It uses an optimization procedure called SMAC under the hood to choose the appropriate set of transforms and algorithm (and algorithm parameters).  I took a brief look at some startups that Ben has referenced and a product by Skytree indeed looks quite impressive, especially in regard to the subject of this question. Having said that, some of their claims sound really suspicious to me (i.e., ""Skytree speeds up machine learning methods by up to 150x compared to open source options""). Continuing talking about commercial data science and machine learning offerings, I have to mention solutions by Microsoft, in particular their Azure Machine Learning Studio. This Web-based product is quite powerful and elegant and offers some feature engineering functionality (FEF). For an example of some simple FEF, see this nice video. Ben Lorica, Chief Data Scientist and Director of Content Strategy for Data at O'Reilly Media Inc., has written a very nice article, describing the state-of-art (as of June 2014) approaches, methods, tools and startups in the area of automating (or, as he put it, streamlining) feature engineering. Azure ML provides you with a workflow by using modules in a graphic user interface. Many of them are related with Data Munging and you can easily clean your data. If there is something that you cannot do in the GUI, then you can just add a module which let you run custom R or Python script to manipulate your data. The aim of the library is to not only help experts build better machine learning models faster, but to make the data science process less intimidating to people trying to learn. If you have event driven or relational data, I highly recommend you check it it out! The nice part of it, is that you can easily visualise your data at any time and check simple stats like the dataframe.describe() of the R. As Amazon AWS services have shown a lot of promise and standard, I would definitely count on Amazon ML, with it's prospects and promises for making the workflow of data scientists simpler. But as of now, it's still small.",5
"I'm confused as to what these terms actually mean AND as to what they do/how they contribute to the system as a whole. In particular, back when I was running Ubuntu, there were several keywords like: Xorg is an X server. It implements X11 and provides an interface to keyboards, mice, and video cards. The login screen should appear immediately. Log in again and avoid making the mistake your did last time. Sorry for the long discourse. You can run Compiz in GNOME, KDE, and Unity at least. Just get the CCSM package by entering this into the terminal: Metacity and Compiz are window managers. They decorate X window primitives and support various operations such as moving, resizing, and maximizing of windows. Compiz is a window manager and desktop compositor (a desktop compositor is a program that renders special effects, such as a desktop cube, on your screen). In general, you may also be interested to know that the window manager and desktop environment are what the user interacts with. You don't directly touch (unless you are in a recovery shell) the windowing system. GNOME, KDE, and LXDE are desktop environments. They provide libraries and specifications that applications use and follow in order to ""play nice"" with other applications. (Don't enter the dollar sign and the space after it; these just mean that you are not logged in as root.) In CCSM (Compiz Config Settings Manager), turn on and configure whatever desktop effects you want. Be careful! You may lose the titlebars in GNOME and UNITY. If that happens, press Ctrl-Alt-F1 and then type in: X11 is a network protocol. It encodes things such as graphic primitives, images, pointer motion, and key presses. Note also that these things can be modular to respect with another. For example, you can use any window manager on Xorg, and any desktop environment on top of any window manager. What exactly is the difference between all of these? Which can be changed? Do the same things apply when we're talking about KDE or LXDE?",4
"When inserting into a table without specifying TABLOCK hint the operation is fully logged (every row insert is logged) and the number of logical reads reported by query statistics is roughly equal to the number of inserted rows; so it's the same in case of fragmented/not fragmented heap (1 insert = 1 destination table page access), but what is not counted here is PFS-pages access, and the Profiler does count it as logical reads.  As soon as the searching for free space failes when trying to insert the Nth row, for the remaining rows it allocates new pages directly  When you insert into rebuilt heap, there is no need to try to insert in any existing page because they are all full, but when inserting to a fragmented heap there is free space in the heap and it should be checked for every row inserted (where it's possible to insert it).  IMHO, the fact that there is no more need to search in PFS pages is revealed at the first attempt to find a free space in the existing pages when all pages allocated for this heap are checked without success for a rebuilt heap, and the number of possible accesses the PFS pages grows as the fragmentation grows.",1
"Considering if it is ""harmful"" is subjective, but you could easily compare it to your browser caching pages. You also won't overload your disk (or anything like that) since the controller will determine how many read/write operations it can take. Even though I've just said this is OK, I would also like to point out that this is a bad software design. You should really do this sort of thing in memory then write to the disk only occasionally. IO operations will block other IO operations, writing to a drive this frequently is going to cause other things running on your system to wait for their turn a lot. Hard drives have buffers which 'buffer' this type of activity. Even though your app may be telling the HDD to write data several times a second it is only actually writing as often as it wants to (which also happens to be several times a second but that'll still likely be less than you are writing). No, why should it? Disks are made for reading and writing. It doesn't harm more or less than any other activity on your disk.",3
"The interesting thing here is we may get an increase in performance by making this array mutable, but at least it shortens our method prototype: I spent over 60 minutes to fix all the bugs using the code to solve Leetcode 4 algorithm. Here is the C# code to pass the Leetcode 4 online judge.  What I'm even more interested in is the fact that you wrote this algorithm so clearly that it's easy to follow and read. Personally, I think more people should take that page out of this playbook and write code for clarity first, then deal with whatever issues may arise. And what may be that 'wrong' thing? We're not encapsulating our data. The fact that we have three variables with a suffix of 1 or 2 tells us that we could encapsulate those and save ourselves several parameters, since they're co-dependent. It is Sunday Dec. 17, 2017, 10 months after I posted the question, I had a mock interview and then peer asked me to solve the algorithm Leetcode 4: median of two sorted array. So I shared my solution using the above code, but the peer sent an email to tell me that the code has bugs.  The rest of it looks great. I won't comment on the algorithm, because I'm not an algorithms person, and it looks like it does what it wants. This reminds me of the C/C++ days and when I did DirectX programming. Every single method had the parameters listed out like this because we didn't have the powerful Intellisense then, and it was easy to lay them out in headers like this to allow us to reference easily. But it's 2017, and this is C#, we don't need to lay things out like this, it should be a red flag that we're doing something wrong. Function FindKthSmallestElement_BinarySearch's two arguments start1 and start2 should apply to multiple statements.  The start1 and start2 variable should be applied to the index value of the array in another four places. The comments are added starting from ""bug fix"", the variable's declaration of firstNode1, firstNode2, newStart variables.",2
"In GIMP: Filters -> Map -> Tile. If you change the unit to ""%"" and then enter 300% x 300% for the dimensions, you'll get a 3x3 tiling of your image, which lets you see all four edges easily. After doing it once you can use Ctrl-F (repeat last filter) to re-do the tiling after making modifications, as long as you don't use any other filters of course. Using photoshop or GIMP, you can use the offset command. If you are working for say a 100x100px image, you can offset it 50px in each direction, and the seam where the image tiles together will be displayed in the middle of the image. Krita has a feature that allows you to edit tiling textures and see the changes update live. By pressing the W key, it enables wrap around mode, which makes this possible. A youtube video of this feature in action is available here. (feature is enabled at 0:12) If you want to test multiple images, it may help to have an mini webapp based on Anko's answer. This will let you drag images onto the page to let you test them in quick succession: This page has screenshots demonstrating how this works: http://blogs.adobe.com/jkost/2015/01/how-to-create-a-seamless-pattern-tile-in-photoshop.html",4
"So what you're seeing is perfectly normal, if slightly misleading, behaviour: the time recorded for glDrawArrays is not actually the time taken by glDrawArrays alone, but also includes time for actually committing a whole bunch of previous GL calls. There's a discussion of this for D3D available here but the same basic principle will apply to any modern implementation of either API.  The key point to take away from it is: ""in general, trying to profile your GPU by timing the CPU is going to be confusing and misleading"". Naive implementation of just writing to a command buffer on every gl call makes little sense, when the driver can possibly optimize out state changes. Thus, it makes sense to postpone processing as late as possible, especially on a tiled/binned rendering architecture, which most OpenGL ES implementations are. On some architectures it actually makes sense to do some shader processing as late as on a glDrawArrays call. (And yes, even your ES1.x hardware likely has some form of shaders under the hood). Calling glFinish/glFlush like doug65536 suggests won't help on tiled/binned architectures, as it causes a total pipeline flush which has a very, very, VERY heavy overhead on such - causing the rendering of everything queued so far to every tile and forcing a unresolve on the next render cycle. The way most drivers operate is by using a ""lazy state changes"" model.  What this means is that the vast majority of your gl* calls will actually do nothing much more than recording a state, storing off some parameters, then return immediately.  This works perfectly fine up until a gl* call is made that actually needs to do something with all that state (or that depends on the result of some other previously made call); at that point in time all the previously buffered-up state needs to be gathered together and flushed before the call can be made.",2
"Currently,if I am using the same model for a different window then I get the accuracy decreased by 4-5% from the previous window. From this I believe that window size again is a hyperparameter which when changed requires the model to be tuned again.  Using different window size is almost equivalent to using different features(as in a non time-series modeling). Tuning hyper-parameters is usually done after features were selected(I.E for a given set of features). I am currently working on the time series data classification problem using deep learning. As we all know that in time series, we process the time-series data sequentially for some time steps at a time through the model which is called as a window. We slide the window and the next window is our input again. Since I have just started working in deep learning and time-series domain, my question is if we tune some deep learning model for some window size , let's say 10 then should the same tuned model be used to get the near accuracies for some different window(say 15) on the same data or the model should be tuned again for the latter window.  So, what is the right thing in this case? Should the same model be used or should it be tuned again? I tried searching this online but I couldn't get any help on this. Help is appreciated as I am new to this domain.",2
"I have a script set up to deal with this which sets up a hosts file with aliases on one of my servers here, then scp's it to the various instances. So it uses ec2-describe-instances to list all the machines, the name tag of the instance is also the name that the machine is known as. This is for a mixture of windows and unix ec2-instances. For such case of multinode installation on EC2, I've used this Exapark utility: http://www.exapark.com/product.html Incidentally, a useful fact is that the public DNS name of an instance resolves from within EC2 to its internal IP  even if an elastic IP is in use: I've got a private DNS Server running on Amazon EC2.  I don't need a public IP Address because its only used for private addressing web1.xxx.internal database1.xxx.internal Problem is I had to terminate the instance recently and start a new one.  This meant that the private IP address of the DNS server changed and I had to log in to each of my other 15 server one-by-one and change the DNS address to point to the new DNS server. Use puppet to keep the configuration in sync. Changing the configuration on your  puppet server it will be replicated on all other servers.  It's quite easy to install and quick to run. It takes Name tag of instances and their private IPs and puts togather in hosts file. So you can configure inter-instance connections with persistent names resolved to private IPs. If the server gets as little traffic as a DNS server would, it probably makes sense to use an elastic IP internally, if you have any to spare.",5
"Provides connectivity to IPv6 hosts to IPv4 hosts.  Built into Windows and hence very easy to deploy. The 'easy' answer I can think of for your question is setting up a dual-stack device on your network which translates a dedicated v4 address to the v6 destination and advertising the v4 address on your v4-only network for the service. I am not aware of a Windows approach to this (a windows server version may be able to do this). Linux/BSD can be configured to do this. Your question confuses me a bit, because you mention a IPv6-only service to be reached from IPv4-only clients, and then call it NAT64. But looking up NAT64 on wikipedia gives me the answer I expect: NAT64 is for IPv6-only clients to reach IPv4-only services. You do not specify which protocols you require, if it is only HTTP then a HTTP proxy would be a valid solution. I only know of Linux or BSD based implementations. This is one with source and pre-built binaries for different Linux distributions: http://ecdysis.viagenie.ca/download.html If you go to http://www.sixxs.net you can sign up for their IPv6 tunneling service which allows you to use AICCU to get a routable IPv6 address even if you're behind an IPv4 NAT.",4
"Selecting ""automatic proxy configuration"" and pointing to where your pcap file resides. Don't forget to use file:// extension. If you're using a windows VPN connection, there is a check box on the ipv4 settings advanced properties window (of that connection) that (since the release of windows 7 is checked by default) forces all connections to use the 'default gateway on remote network'. Un-check this box and you should be able to navigate to any address. This may require you to specify a domain when you rdp into a machine in your office (i.e. machine.domain.com). Theoretically proxy auto configuration (PAC) script is meant for this (but this requires some programming). I think what you want to do is configure your home computer to only use the VPN connection when trying to reach machines at work, otherwise forward through your local(home) router.  Presumably, you won't have to deal with any web filtering that way. Practically, I think you can use different browsers - one for sites you access via proxy and second for all other sites. In Firefox you can also set up different profiles - one for using proxy, and second for direct access to internet:",4
"You may have to determine the upgrades. Lets say you want to set this up and run it on this hardware for 5 years, your client is going to pay a lot for this unused space. I say unused space because the overhead for the next 5 years isn't going to be used and isn't going to be cheap let say you would choose a SAN. Although this solution is tempting, (ie: buying a large SAN with 1/5 of disks used.) BUT, in 3 years, you may be able to buy the same hardware but there is less chance you may buy the new and (most of the time bigger, let say 6TB disks).  I'd seriously consider using HP's MDS 600 disk enclosures - you can get 140TB in 5U, they're very fast, reliable, can serve multiple servers or have multiple boxes connected to a single server and can easily be RAID 10'd. The cost/gb is pretty low. I deployed the first shelf on 2006 and never had any problems with CORAID equipment. If on the other hand you plan for 18 months to 30 months upgrades, they may save a lot. Hardware changes fast and SAN or storage technology move fast too. In the last year, most vendor now offer SAN deduplication which by example takes a 512kb into a 128kb encrypted hash. If this storage solution is for disaster recovery, depending on the budget, tapes sill does a good job. They are slower to restore but still works and does not cost much. If the data need to be accessed, either on a daily basis or from times to times, tapes are probably not a great idea. I suggest you call some software vendor like Commvault and Symantec and also Dell(EMC), HP, ... for the hardware solution that they will suggest. Make sure it is clear with your client what kind of restore time they need and are willing to pay, restoring that much of data is a couple of days of downtime. Hope this may help. the obvious hardware will be something like a rack mount server with hot swap trays and dual xeon based type processors. the use of the data is for archives of information, files will be made up of small file sizes. For most situations I highly recommend going with a ""professional"" solution as it will be supported and more likely to continue to be supported 4+ years from now. HP Lefthand storage arrays will do all of what you want. RAID 10, or 6, or 60 (probably the best choice given the data density you'll need). They do off-site replication, data deduplication, and you can have your choice of iSCSI or FC connections. The only drawback is the price, be prepared to pay for all that storage and features. i currently have a client that will be adding replicated data from satellite locations in the number of approximately 80TB per year. with this said in year 2 we will have 160TB and so on year after year. i want to do some sort of raid 10 or raid 6 setup. i want to keep the servers to approximately 4u high and rack mounted. all suggestions welcome on a replication strategy. we will be wanting to have one instance of the data in house and the other to be co-located (any suggestions on co-locate sites too?). If you're willing to roll you're own solution; you might consider a Chanbro case, 4U can hold 48 SFF drives (with 500GB drives thats 20+ TB per 4U), and they have SAS expander boards that allow an empty chassis (full of drives) to be used as an external case. We are using CORAID shelves for some of our stuff. The last shelf we are setting up is a 24 port filled up with 2tb drives http://www.coraid.com/PRODUCTS/SR-Series/SR2421-EtherDrive-Storage-Appliance_2 . We got 4 shelves so far. It takes up 4u, is certified with vmware and has linux & windows drivers available (I use both currently).",5
"One approach in projects I've used in the past is to manage this via a set of different phyics ""states"" which an object could be in. Examples might be: Grounded, Airbourne etc. For example, imagine the case of a player character who appears in midair, above the floor. THe inital state is set to Airbourne and the physics system applies gravity to the character to start them falling. I think you may need to look further than simple collision detection to make this happen. Due to the discrete nature of a simulation such as this, it's hard to achieve a smooth ""flooring"" of one   object onto a surface as it will constantly collide, then be adjusted, then collide, then be adjusted again. Now, the physics system knows the player is grounded, so can use different techniques to track the floor, rather than relying on the hitbox. Depending on your game, you might have a defined ""floor"" plane which you can query the height of at the current location. You might be able to ray-cast downwards (a short distance) to ensure you are still near the ground, and then transition back to 'Airbourne' when not. Eventually, the AABB hitbox detection system detects a collision between the player physics hitbox and the floor hitbox. At this point the physics state transitions to Grounded and the position is adjusted to ensure there is no collision (as you discuss). In this way, you run a different set of phyics rules depending on the state your object is currently in, and use hitbox detection differently in each, and to help transition between the two. There are many approaches to this, but the principal is that simple hitboxes are probably not sufficient for ""smoothly"" adhering one object to another where movement is possible. Use them to detect collisions, and use that information to inform transitions in your physics system to control character movement.",1
"My answer will only make sense, if and only if, you make those changes again and it does not take that long, but if you create a new project and do the conversion again, you have the delay. For then there will be a pattern in behavior. If I am not mistaken, it was when I activated this command for the first time (modifying all the options) that there was the delay, just like yours. Opening the project again, the moment I was modifying each option individually, the option below gave me a delay of 5 seconds, where the message ""compiling shadows"" appeared the moment I released the Q key: First person shooter project, blueprint of character (this was the project I had this same ""problem"" that you, when I was following this tutorial => https://www.youtube.com/watch?v=2sMwmEE3MuM&): I have tested and modified all scalability reference items (https://docs.unrealengine.com/en-us/Engine/Performance/Scalability/ScalabilityReference) separately and together, but I did not get the result I had and what you are getting now. I recommend not to take my answer as definitive, because when modifying only the effects option (in the individual tests), I had a delay of about 5 seconds in which the message ""compiling shadows"" appeared. And even after modifying everything, there was no further delay. That is, Unreal together with the computer must previously save data of these graphic changes, because there are changes, there is always this delay, but only the first time. For example, cache, temporary files. I have almost nothing in my scene--in fact, I reduced my scalability settings from ""High"" to ""Medium"" before even loading my level layout--and doing this has crashed the program multiple times. It didn't crash this time, but it is telling me it is compiling over 5000 shaders (!) and my objects' materials are not showing up in the viewport.",2
"Now save this profile (ie- work-webserver) and when you want to connect to it load it and click connected it should log you into the office web server after it logs into the jumpbox silently. Okay... This is very possible with Putty though not as easy as with OpenSSH on a Linux machine. I would very much recommend setting up an SSH identity key and installing it on the boxes that you are connecting to and use the Putty Agent (pagent.exe) key forwarding agent. That said here is the steps to take. With this setup you now want to setup a second profile for the box behind the work jumpbox. Set this profile up with the correct host name/IP address and port of the webserver in the office. You then want to go to Connection -> Data -> Proxy and set the Proxy Type to local and set the Telnet command or local-proxy-command to plink -load work-jumpbox -nc %host:%port\n (replace work-jumpbox with whatever you called your previous profile for the work linux box.  First, create a Putty connection profile for your work Linux box that you can reach from home. You'll want to be sure under Connection -> Data that you set your Auto-login username. Then under Connection -> SSH -> Auth be sure that Allow agent forwarding is checked. Now save this profile and make note of the name you call it (ie- work-jumpbox).",1
"We run about 2000 View desktops, and love the technology.  We are not at the point yet to trial offline desktops. Remote desktop services seems theoretically attractive, but I'm struggling with a practical aspect of deployment: mobile (laptop) users. Finally, should I just dismiss this type of infrastructure configuration as untenable and opt for a more traditional Active Directory roaming profile deployment? Citrix Xenapp has a feature for exactly this- it's called application streaming. It works just like normal Xenapp/Presentation Server published apps, except after you compile the application into a streaming app package, that package is downloaded to the client's computer through the Citrix Offline Plugin, and the cached application is run on the client, self-contained. My question is: does the ability exist to ""check-out"" or ""cache"" a remote desktop session to a laptop so the user can use the computer offline, but will then resynchronize with the RDS server once the mobile user's internet connection is reestablished? VMWare View has just come to my attention, which appears to do so.  Any experiences with the software? If you have a stand-alone app in Citrix that you want people to use offline then @Izzy, @KJ-SRS and others have good answers here in technology but they don't answer the ""why"". 95% of Citrix deployments are either for apps that won't run on new Windows OS's or because those apps need close proximity to their app/SQL servers to perform well.  If the apps you're talking about require connections to their data to work, then no VMWare/Citrix/Microsoft technology will help you go offline and ""store"" the network packets for later replay.  I've not seen that solution yet. The other 1% of the time, say on an airplane, which is apparently the xen garden of productivity for certain salespeople, they will not have internet access, hence no access to RDS - an unacceptable, non-negotiable situation. Our desktop infrastructure is rapidly shrinking, being replaced by laptops - what makes the most sense in terms of scalability and long-term manageability? I would say about 99% of the time, mobile users would be able to obtain an internet connection - either a hard-line, wifi, 3g, or 4g that would allow them to access RDS outside of the office.  Granted, establishing, maintaining, and providing an an acceptable user experience on any of those connections may, more often than not, require an investment of valuable time, and become quite frustrating to the end user versus ""just having the damn OS installed on the laptop"". What, if any alternatives, especially Microsoft native solutions, exist?  And how have your experiences been with such? Why is the app not installed locally on the laptops?  Why was it originally installed on Citrix? Discussing these issues helps to point you to a different technology to solve the problem for offline access, as the different ones mentioned here are to solve specific problems and have their own limitations.  Some are to support legacy apps (MED-V), others are to support using apps that don't conflict with your locally installed ones (App-V, Citrix app streaming), etc. I probably did a horrible job explaining it as I haven't used it since Citrix training a few years ago.",4
"If you need to check the SQL Server, check the SQL & NT logs, and again check the memory, disc IO, and CPU - task manager will give you an ideal of two of these (not disc IO).  Ideally, SQL Server should be on a dedicated box rather than shared with anything. A quick way is to fire up profiler and look for queries that take over a couple of seconds to run.  Then look at the waits which are being logged to see what the cause of the long running queries are. A website we host has recently been experiencing slow load times. It's been ruled out that the problem lies with the web server and it maybe database related. There is a lot more to performance tuning than this of course - but this will quickly show you what SQL Server is actually doing. And one important fact to remember: the database will try do whatever work you give it, but it can't always guess your intent. If the app is sending bad SQL (e.g. SELECTing many more rows than it needs and filtering them in the app, I have seen this many times) or the schema design is bad (e.g. missing indexes) then the fix for the perceived database problem lies outside the database. The Tuning Advisor can help you diagnose this kind of issue. I used to be a web admin, and my first check would have been the webserver - is memory and CPU ok? Do the OS and web logs show anything? Can the scripts output a log to show what's going on? eg I used to put the database stuff in one section, and write times/durations for each database activity to a log, when activated. What steps or procedures do I take to confirm or rule out that SQL Server is the reason for the slow load times? Also, without a baseline, what may be behaviours/characteristics of SQL Server to look out for that could indicate that there is a problem? Whilst someone is using the website, look at either Activity Monitor or sp_who2 - are you able to see the query dawdling? Is there any blocking? Keep refreshing... if the query is runnable/suspended/running for a while, then you need to find why it is slow. If you can't see a query in those states, it may not be SQL Server. I'd suggest you need a more definitive problem statement - is it all web pages? If it's just some, what are those web pages doing? What is the acceptable load time, and what is the current load time? Is it all users, or just some? First, fire up the Activity Monitor. What do you see in Overview - are lots of sessions waiting? Now look in Resource Waits - is the server struggling for disk I/O, or is it spending a lot of time waiting for locks. You can also see recent expensive SQL statements. For more detail you can look at the Activity section, or fire up SQL Profiler (with the caveat that profiling itself is an expensive activity - don't make a bad situation worse!).",4
"I need to find out if a disk is IDE or SATA (or anything else, maybe). I know that the device is /dev/sda, so I think it's SATA, but I don't know if I can be sure just by the name. I tried looking at dmesg and it always says ""SCSI"", but I'm sure it's not... One pretty reliable method is to use lshw to lookup the model number and then to lookup the model number on Google to see what type of device it is.  This method doesn't work if the drive is in a USB enclosure, some USB controllers hide the HD info from you. MadHatter is right in that the designation difference would be /dev/hdx versus /dev/sdx. But the surest way is to open the case and look at the cable. You don't necessarily need to shut off the machine to do that, depending on the system you're looking at and what kind of tangled nest of cables you have around the case. smartctl -a /dev/sda which will produce a lot of information including manufacturer and model number; cross-referencing that on the manufacturer's website is usually fairly simple. You cannot rely on /dev/hd for PATA drives.  For modern systems all PATA/SATA drives will show up as /dev/sd because the new ATA layer uses the sd prefix.  You will only see /dev/hd if your OS uses the old deprecated ATA drivers. If the system can be rebooted you could use the Ultimate Boot CD to run a hard disk diagnostic, they normally identify the drive type and model. Even the BIOS might tell you at reboot what kind of drive it is.",5
"Note that the number of R2 cmdlets is greater than that of the plain 2008 server. I think this is the problem. The command that sticks out most is that the 2008 server lacks the ""Start-WBBackup"" command, which is needed to execute a backup. Interesting. Looks like I am going back to the WBadmin tool and running this stuff as a batch on my 2008 servers and saving powershell for 2008 R2 I recently put together a PowerShell Script on a server running Windows Server 2008 R2 that created a dated directory then started Windows Server Backup to make a system state backup. The script is run on a nightly basis and works great. Anyone know if there is something different I should be doing on Server 2008 to make this work or is running backups from powershell simply not supported except on R2? Wanting to use this on another server, I copied the script over and, after adding the WSB snapin from the powershell command line, only to find that it would not run. The difference between the two servers is that this new server runs Server 2008 (not R2). Has anyone gotten powershell scripting to work for running backups on Windows Server 2008 with Windows Server Backup? I wonder if the commandlets are different. So far all the docs I find are based around R2. I think this might have something to do with the cmdlets being different in regards to Windows Server Backup on 2008 and 2008 R2. Did a ""get-command -module windows.serverbackup"" and got the following cmdlets on the 2008 server:",1
"Do a grep on your existing blogs and look for any iframes or eval method calls in your WP directory. Also check the DB. Once it's all clean, update your WP version and themes/plugins and keep it updated.  Alex, unless you're a full-time security person, doing forensics on stuff like this is a waste of your time. Running WP 2.5, which is 3 years old, is just asking to be pwned. See FAQ: My site was hacked  WordPress Codex and How to completely clean your hacked wordpress installation and How to find a backdoor in a hacked WordPress and Hardening WordPress  WordPress Codex You could spend days looking at CVEs and exploit code but the reason they got in there (assuming it was through wordpress) was through some bug in the code. This bug was probably found several years ago, widely published, and already fixed. There's probably nothing special about your wordpress install, it was probably exploited through some automated tool looking for old versions of wordpress.  I used to have this problem with a few of my defunct blogs, but keeping them constantly updated fixed it. That's your problem right there. Most of these attacks are carried out by automated scripts that look for known vulnerabilities in older wordpress systems. Since anyone can look at bug reports and changelogs, it's not too difficult to engineer a script to exploit a weakness. Next login to Google webmaster and, if you haven't already, prove ownership and ask for a review of your site. The warning should go away after awhile.  I suggest you either put some effert into your web sites or stop playing at being a webmaster and get someone who knows what they're doing to manage your systems and ensure all reasonable safety measures are implemented, including upgrading as necessary. There's more to a web site than just throwing a prepackaged version of software on it, typing some content and sitting back. Do you keep your plugins up to date? There's exploits against them too, that could be the avenue of attack too.  If you just want to see how someone might exploit some old version of wordpress, just search http://exploit-db.com . Do you reuse passwords? Is your FTP password the same for that forum you maybe registered for 3 years ago that stored their passwords in clear text and got hacked? That's another avenue of attack.  Welcome to reality. Your sites got hacked because you completely failed to take any precautions. Running such old and vulnerable versions of WordPress is simply asking for this to happen. Given the invitation you've created don't be surprised when people come to the party. Do you keep your plugins up to date? There's exploits against them too, that could be the avenue of attack too.  Unless you have the logs from the day it happened, there's probably no way you're going to know how it happened. There's tons of exploits against historical versions of wordpress like 2.5. Here's a few CVEs that might be how they got in:",5
"As opposed to @coderino I like the use of Files.newBufferedWriter. It makes the code clearer. However you should specify the charset, otherwise the file may not be portable between systems as by default it uses the system default charset which depends on locale. This is the modified method based on the suggestion given with the additional charset for user convenience. If this is still modifiable to be more efficient, please be patient, I'm a newbie. To get the Path use File.toPath() instead of getting the absolute path as a string and passing it to the Path constructor. It seems that you go roundabout to create buffered writer, as new BufferedWriter(new FileWriter(file)); would suffice. In case of exception, it would make more sense to write to System.err stream as opposed to System.out. Also, depending on the context, I would rethrow exception, and catch in place where I am calling this method, so e.g. if I call saveFile method inside save method, I can show a message box saying save failed inside catch clause.",3
"Yes, 16TB is the largest you can get with a 4K cluster size, as Greg Askew mentioned. You can change your cluster size, but it takes forever. You can also change your basic disk to a dynamic disk easily via the disk manager. After you convert it, you can expand the dynamic disk group using additional LUNs. The filesystem sits atop the disk group, allocating data to all block device members in the disk group. You can use a linear allocation policy (default), or you may set up a software raid if that is appropriate for this. Linear is probably what you want, since this is coming from a SAN host. Beware, if this LUN or any other LUN in this dynamic disk group is provided via the Windows software iscsi initiator (rather than a specialized HBA), it will fail to start on boot (requiring manual activation on every boot using disk management). It will work after manual activation, though.  The boot time activation problem is due to dynamic disk services starting before software iSCSI initiator services.",1
"Where my_image is the name of the image I want to create; my_vhd_url is the VHD URL of the VM disk that you're baking an image from. The problem is that the image does not show up when executing knife azure image list. When I try creating the server with the image name from the Azure portal, it complains that it does not exist. I'm running Ubuntu, so I tried the Azure cli tools and it doesn't show there either. I installed Azure PS in a Win 8 VM and then it shows up. Feeling encouraged, I installed Chef and knife-azure in the Win 8 VM, but it doesn't show up there either. The way I got a User image to show was not to use the Azure UI to ""Capture"" the image, but rather using the Azure xplat-cli. Something like: I'm pretty sure that the Azure xplat-cli will very soon support seeing images created in the UI, as will knife-azure. I'm trying to create and bootstrap a Windows VM in Azure using knife-azure. I initially tried using a Public Win 2008 r2 image, but quickly found out that winrm needs to be configured before this can work. So, I created a VM from that image, configured winrm as per these instructions and captured the VM.",2
"If you really want a bit of snazzy custom kit, check out the MoPi board. It takes pretty much any power input and converts it for the Pi. 8 to 10 hours on 8 AAs. With a screen and Wi-fi that time will go down, but should still be pretty respectable. What's pretty neat in this case is the ability to hot swap battery packs. Current batteries getting low? Replace the pack on the go! I loved it when my old Dell Inspiron 8100 did this, but with most laptops being much smaller and focussed on reducing weight, it's pretty much a dead feature. Oh well. The MoPi board also tells the Pi when the batteries are low so that it either shuts down nicely or prompts for new batteries. Have you looked at the Atrix Lapdock? It's an add-on for a Motorola phone that turned it from phone to essentially a laptop. It has an internal rechargeable battery, 11"" screen, HDMI, USB, mouse, keyboard. Sounds like exactly what you want. Powers the Pi for at least 6 hours. Several people have used them. They go for around 70 quid on eBay at the moment, but they're cheaper in the US I think.",1
"In your case I can also imagine that you process the message already in the on_message_received(...) method without needing a buffer and the get_message_noblock(...) method. You can stop the notifier thread with  Notifier is a class in can package. It starts a thread that listens for can messages. If a can message is received every Listener (in my case only one Listener, the FilteredBufferedReader) is informed about the message and the method 'on_message_received(self, msg)' is called with the received message. In my example I check, if the msg has a specified arbitration ID, and if so, I put the message into a buffer. With 'msg = fbr.get_message_noblock()' I can take a message out of the buffer for further use.  Use the python-can module with pip3 install python-can, documentation is listed here https://python-can.readthedocs.io/en/stable/installation.html But be careful, this are python relevant methods, if it is necessary for you, to have fully realtime connection to the bus, i think its better to develop the can modules in C/C++. There are a lot of examples on github, like the candump, which is programmed in c You can start a BufferedReader that calls a message everytime you receive a message. Or you adapt the BufferedReader class in a way that it filters for the message(s) you want. you write your own socketcan Client with the python-can package. I prefer this method, because you can send, receive and filter CAN message depending on your Project. Look at the socketcan_native.py and the socketcan_constants.py and the message.py which is located at:",2
"The main one I think you should consider abstracting out of IrcMember, is the idea of a Connection.  Make a separate class for connections, that manages the socket, stores the (list of?) room(s) you're in, etc.  This way, IrcMember can just represent the user, i.e. a real name, ident, nickname (unless that is per-server). The leave_server function also doesn't seem to support QUIT messages, not the end of the world - but easily implemented. Secondly, as I'm sure you're aware (As would be anyone having run the code), a command-line interface for something interactive like IRC isn't the best way, particularly without ncurses, etc. Perhaps not a game-changer (depending on what you want/need to do in your client), but your script has no method to allow for the sending of an IRC MODE command (either for the user or for a channel). If not, I'd suggest adding some logic for room lists (most likely associated with the Connection instance for that server). Some (not all) networks use user-modes to handle name registration, etc - but if your not concerned about that then leave it out as it is. I'll try to ""dig into how [you] design and implement the IRC client as a whole"", particularly with regards to the spec (Which I have actually (somewhat, as much as is possible with things like RFC's) read!). As raw_input blocks, you're unable to check for new messages (or server PING's) - which will eventually (5 - 20 minutes depending on which network) timeout your socket as idle on the server-side.  Of course, if you're chatting actively you should have something to say at least every 5 minutes, so this isn't a a huge deal.  But it does mean your client won't be capable of idling online (not 100% sure of your use-case for it here). Part of me just feels that (as you say) there is just a tad too much repetition in that architecture.  (Perhaps some more abstract encapsulation of a server command and arguments?) Along the same lines as above, the channel functionality is somewhat limited without the NAMES command.  At present, the client can join a room and send it a message - but not know if there's anybody else there or not or who they are! NAMES tells the server to send you a list of what nicknames are registered (joined) on a channel (usually, a channel that you have also joined). Okay, since bounties are worth giving people a run for their money, and I love Python, and I miss my days of making IRC bots... I'll give this a go.. There (mostly) seems to be a one-to-one mapping of a lot of your functions to IRC command verbs.  For example, leave_server -> QUIT, join_channel -> JOIN, send_private_message -> PRIVMSG, etc. IRC supports invitation-only chat rooms, so if you plan on joining/inviting people to them then you'll need the INVITE command.  Otherwise, it can (probably) be ignored. Just to finish up, a bit more on your coding... As you mentioned, ""There is a fair bit of repetition that could probably be pulled out"".  Particularly things like send_channel_message, which just ends up calling send_private_message anyway (I do understand why, because they both use PRIVMSG, and channel messages need some additional checks first).  But it just doesn't feel right reading through the code. This one was a bit more of a 'shocker' to me, to LIST support?  Is your user expected to know the exact name of the channel they desire? (Note that there are also extensions, both standard and non-standard, to allow channel names starting with % and probably other characters as well) Your client at present has no way to WHOIS or WHO anyone else, or to send anyone (person, i.e. a nickname) a message (PRIVMSG or NOTICE), and also cannot send NOTICE's to channels. (None of which are big deals, at all, so I just wrapped them all up here). Firstly, as you mentioned - you are doing too much in one class!  You're dealing with a domain where we have the benefit of some pretty well discretized concepts (read: classes). Was this going to be (one day, I'm sure) some sort of logic around eight the WHO or WHOIS commands to prevent nickname collision? The KICK and TOPIC commands, as well as the channel MODE 'b' (ban) are used for room administration (keeping out the rif-raf, setting the channel topic, etc).  If your client is just for chatting, then again don't worry about them - but they are needed in real life (unfortunately).",1
"Shedding a bit more light on your storage requirements would help you get the most out of Server Fault. Things are changing because many enterprises are putting rarely accessed data (archives) onto SATA drives on SANs, which are protected by using the appropriate RAID. But the critical stuff goes on SCSI/SAS, with the appropriate RAID. So - there's a balance to be had. Depending on your design, you might be able to get away with both: faster, smaller SAS 15K drives in a RAID 0 configuration (or possibly SSD) for your data sets that require the most I/O performance; and slower, larger SATA drives in a RAID 5 configuration for archival, less frequent storage needs. That said I'd ask you to consider whether these 3TB disks make much sense in a server at all, certainly if you're asking about performance and 7.2krpm disks in the same sentence anyway. In my experience you either need capacity or performance, if you want both you have to spend serious money. SATA drives don't have the reliability of SCSI/SAS drives (the Mean Time Before failure - MTB), so generally aren't deemed suitable for most enterprises.  In single-user tests SATA can actually outperform SAS but once things start getting concurrent then yes SAS proves itself. So it depends on load, you don't mention your use case, for busy servers that can't cache enough of the drives content then SAS may make a lot more sense.",3
"Comments should go above the code that they refer to.  Also, in multi-line comments, the asterisks should line up, like in the JavaDoc guide: I think all the comments should be removed. The point of a comment is to explain why you wrote this code, not what the code does. If anyone reads the code, they will be able to understand what it does without reading the comments. When you write comments, always ask yourself : ""Will this comment help me understand why I wrote this code 5 years ago?"" (Or.. 2 weeks if you have a bad memory like mine), if you can't answer, you probably don't need comments! I disagree with @user1021726: Keeping a space after the for keyword is good  it's a keyword, not a function.  It should be kept more like return something than return(something). I feel none of the comments in the code are required. Those just make your code look verbose and do not have any meaning what so ever.  These are so obvious things. In my opinion, code should have comments to justify ""why's"" of the code and not ""what's"" of it.  Other than that I'd personally also put the opening parenthesis of the ""for"" loop right next to the ""for:",4
"What are the downsides to this approach? If I install a default MSSQL instance on a Windows server, can I just run the installer again to create a second (named) instance and use that for the other environment? Finally, would it be possible to put different instances behind separate IP addresses if I allocated both to the same VM?  An even easier approach would be using two databases in the same instance of SQL Server; this is what I would do, unless I had compelling reasons to need two different instances. Having fewer VMs to manage is an excellent incentive to reduce server sprawl!  Using instances of MSSQL to separate development from production environments is quite easy.  It is generally just as easy as you've described it -- run the installer again to create a second (named) instance. You can then use the SQL Server Configuration Manager to bind the named instance to whichever IP address on the server you desire.  It's under SQL Server Network Configuration >> Protocols for <INSTANCE NAME>. However, I'm wondering if a better strategy might be to consolidate the dev/prod database servers onto the same VM(s) but using separate instances? I'm using SQL mirroring for redundancy, so I'd theoretically go from 4 SQL VMs down to 2. Advantages would be fewer VMs to manage, less complex resource allocation and a guarantee that both dev and prod are on the same patch/service pack level. Downsides of doing this include the standard performance issues, although if these are low traffic you almost certainly won't run into any problems. I've got a small virtualised environment which hosts a small internal website with database backend. I've currently separated my webservers and database servers on separate VMs running on the same hardware. Anyway, yes, you can have two instances on the same (physical/virtual) server quite easily; you just have to run the setup again and specify you want to install a named instance.",3
"Needless to say, this response has been greeted with derision by various others in that thread. Now maybe Nick's comment to the question basically highlights Microsoft's rationale, but tbh it's a pain in the neck for those admins that do know what they're doing... Zow-- It looks like they've changed the behaviour from Windows XP! You used to be able to select multiple files and see a security tab, or select multiple folders and see a security tab. Selecting both, in XP, resulted in no security tab being displayed. I'm seeing a behaviour where I can't get a security tab with any combination of multiple files, folders, or both seleted on a Windows 7 machine. Ouch! What a misfeature. A workaround (also described in the thread) is to access the server from a 2k3 or XP client, then permissions can still be modified directly through the GUI. Apparently it was purposely removed, according to this thread in the Server forums: http://social.technet.microsoft.com/Forums/en/winserverfiles/thread/dbfa011b-7c27-4e1d-b1a4-f0f8839b2d46 http://setacl.sourceforge.net/), CACLS (built-in to Windows), or XCACLS (http://www.microsoft.com/downloads/details.aspx?familyid=0ad33a24-0616-473c-b103-c35bc2820bda). On Windows Server 2008, is there an easy way to modify security permissions for multiple files at once (as with Windows Server 2003)? Right-click menu -> properties does not provide a ""Security"" tab if more than one file/directory is selected (i.e. ctrl-click multiple files). re: the ""valid security reason"" - Setting file permissions to what you need is a ""valid"" use of an operating system feature. A server OS w/o configurable file system permissions would be a pretty crappy server OS. (grin)",3
"This way, even if there is a lot of classes, you are only training with 2 labels, 1 or 0, thus, avoiding memory error. One solution here is to use Siamese neural network. This takes in two input at a time. Instead of training directly to learn classes as the output, we are training to learn the similarity between two samples. We first select random pairs. We use the label 1 if two samples belong to the same class; else 0. The problem now is that you are doing a multi-classification task. For instance, sample1 and sample 2 are the same class A but sample1 also belongs to class B. My suggestion, but I'm not totally sure here, is to add multiple instance of that pair. As you already know, the main culprit here is the large number of classes (50k). For every sample data, you have a label of size 50k. Even if a sample only belongs to a single class, the label will still be of size 50k. For example, sample1 has a label of A and B while sample2 has a label of A. Sample3 on the other hand has C for its label and sample4 has A. We can visualize it like this: As for the large number of training samples (1 million), this can be handled by using small batches during training.",1
"Some how if I get server B from a host like vultr, I don't get this error but if I get it from digitalocean I get this error. I did some research and saw I should allowtcpforwarding to be yes but I could not find it in /etc/ssh/ssh_config The config file you need to edit is called sshd_config you wrote ssh_config. What you have listed looks like the contents of the ssh client default configuration, not the config for the ssh server (/etc/ssh/sshd_config) I have two servers, let's say server A and server B. I wish to use server B as an ssh tunnel so on server A I did this Note that yes should be the default, so it is strange it does not work. You can add more verbosity with ssh -v or -vv or -vvv and have a look at your server sshd logfile that should provide more information on what is wrong. You likely need to set AllowTcpForwarding yes in the sshd_config on the server to allow this to happen. It connects and as soon as I put in the server B as a SOCKS5 proxy on my server A, the server B comes up with this erorr:",4
"Be aware, however, that we're talking analog signaling here.  Cable quality is a very significant concern as is the length of the cables and the amount of ambient interference in the environment.  Also the equipment quality plays a big factor. S-Video was even better because of the same principle, separating out the video components onto their own carriers as well. In the analog days of yore, component was always the preferred method of connecting video equipment simply because the video and the two audio signals all had their own separate carrier and thus would not interfere with each other. For the best possible quality, use very short, high quality S-video cables, and do the capture in a room that's as far away from your WiFi, microwaves, cell phones, etc. as possible. From my short experience it's very rare that you get worse signal in RCA, only in cases of bad grounding. When capturing old VHS(or cable) to PC, which jacks to use for the highest quality? RCA(composite) or RF? If the PC card supports it would S-video/component be better? Is there a general signal strength comparison between these different connectors?",3
"This is all quite informal reasoning. It can be made formal by considering the question of the cost of sampling from a distribution $X$ in the so called ""Random Bit Model"" --- essentially how many (unbiased, independent) random bits do you need to exactly generate a sample from $X$? Knuth and Yao gave a construction for which the average number of bits required is provably in $[H(X), H(X) +2]$. By a similar ""scaling up"" argument you can convert their sampler to one which uses $H(X)$ random bits on average, therefore justifying the ""average number of yes/no questions"" intuition. If you want to apply your intuition for ""how many questions to ask"", it only makes sense for integral values of entropy. So instead of your random variable $X$, consider the random variable: In the following I will use that $1/0.46899\approx 2.13$ as motivation to set $N = 213$. You can set $N$ to be a better decimal approximation to this quantity to get a more exact argument. Using your heuristic, we can determine a value of $Y$ using roughly 100 yes/no questions. Each value of $Y$ corresponds to $N$ values of $X$ though, so we can convert this value of $Y$ (which we used roughly 100 ""questions"" on) to $N$ values of $X$, so the ""number of questions"" we use to generate each value of $X$ is $100/N\approx H(X)$.",1
"Their OU name is also their client name, so I would like to be able to find their current OU and then use it for the mapping command. I'm not a Powershell expert, or even an amateur for that matter, but that's probably where I'd start. I eventually figured out how to get the OU from http://www.microsoft.com/technet/scriptcenter/resources/pstips/dec07/pstip1207.mspx and thought I would share the results. Any suggestions on how I can get the OU? I am sure it must be easy, I just haven't figured it out... This answers my original question, but just for interest I should mention that I then decided not to use PowerShell for the logon script after all: It is just too painful to deploy; You can't just put a .ps1 file in the Group Policy, you have to explicitly call Powershell.exe from a cmd file. So, I rewrote my script in vbscript in a few minutes and regretted that I hadn't started with that :) However, I then had a realisation that using the OU would be fraught with difficulty as a user may be a member of multiple OU's. I therefore decided to use the Company field in the User object. From the same code above I can do   I did find information about how to do this with VB script, but as it is a whole new environment I thought it would be nice to use PowerShell instead. To actually return an object of type Microsoft.ActiveDirectory.Management.ADOrganizationalUnit of a User Object, use this: I am setting up a Terminal Server 2008 which will be used by different client organisations, each with multiple individual user accounts.",3
"Assuming no power-hungry peripherals and no heavy computations, your RPi will draw about 0,5-0,7A. Since you want to run it 24/7, you should account for these rare cases where it's raining for several days straight without any useful sunlight. If you want your RPi to survive a 5-days outrage (120h), you'll be fine with a battery of 60Ah. This won't be cheap, but if you can afford such a battery, then problem solved. I am not familiar with battery specific supplies but I have worked with Uninterrupted Power Supplies, and I believe you can get solar ones. There is a variety of cheap solar powered 5V USB smart phone powerbanks offered at Ebay. Some of them hold a capacity of 50Ah or even 100Ah.  Another option you can look into kinda stems off of a project I did back near the end of highschool. Essentially, I disassembled a camping solar clock and a Nintendo DS charging cable and MacGyver'd them together. This proved to be fruitless as the DS received an inconsistent charge, was unable to keep up with the load. This would be another issue you may run into. Those powerbanks include all neccessary functions in one easy installable device. I can't say how long it takes to charge one while under load of a Raspi nor do I know how long the last not beeing charged. But that's easy to measure. Simply power your Raspi by a fully loaded powerbank with a covered solar panel and wait for the Raspy to brown out. You may want to append a timestamp to a text file by cron every minute and afterwards read the file. The only concern then becomes can you find one small enough for a Pi? Most UPS are for desktops or servers, which may be greatly excessive. Whichever you choose, it must be able to supply the Pi whilst under load. I would also like to suggest you find one with a wall plug as well to assure reliability when it isn't generating power from solar.",3
"You always size for the peaks, unless it's the kind of workload that can afford to have high latency when it's pushing a lot of IO. That is part of why wide striping is so popular- you can put together a bunch of workloads and size for the peak of their aggregate usage- different parts will peak at different times, so you're able to use cheaper disks to provide the same capacity. The closest I can come to answering your question, is to note that if you cannot handle the instantaneous iops at any given time, you will simply increase latency.  If latency isnt important, then buying storage based on your projected growth in average iops is not a bad place to start. Unfortunately, there is no easy answer to that question.  First, consider your needs.  How much money are you willing/able to spend?  How much redundancy do you need?  How much total storage do you need? How much latency can you tolerate?  How much growth will you have over the amount of time you want the system to last (both growth in size, and in iops)?  Do you have time to maintain and prune you data to keep size down?   Let's say that I have gathered Disk Transfers per second data for 2x24 hours period, i.e., instantaneous sampling of data every 15 seconds. What statistical analysis can/should I apply to the samples if I want to use the data to, for instance, provision a storage? NOTE: Redundancy is not a backup solution, so plan for backups as well.  Backups can (should) be isolated from your live data by time and space. Should I simply use the peak value (which happens less than 1% of the time)? Should I user mean/average value? Or a formula involving the mean and the deviation? Wide striping assumes that this is on some sort of centralized storage. If it's local, of course you can't aggregate workload that way.",3
"I also alter my prompt. I found a long long time ago that adding the last error code is just useful enough that I like it. And I like the full pathname in the prompt. And the current screen number, too. And it just makes sense to include the current user and hostname. My prompt is PS1='\u@\h $PWD $WINDOW [$?] \$ ' If i've forgotten to authenticate, it lets me do so without having to waste my typing doing ssh-add after the ssh session. I also find I have to include export EDITOR=vim because a number of recent distros default to nano which is most annoying to be thrown into by a utility that needs you to edit something, when I was expecting vi. :-/ I use my bashrc on numerous machines, so i've got this little snippet to make sure LS is colourized. This will fix it on OSX machines, maybe even *BSD if you adjust the uname line. However the system is centrally managed via Puppet, including the password file (which includes the shell setting). I would echo @pjz's comment about knowing things manually rather than setting them up. Especially if you access numerous machines, like I always seem to do. Also, I've got a command to backup a file, useful if your about to change a config file and want to make a quick copy before hand. So one I definitely know is set -o vi because I know the vi-editing commands in bash and I don't know the emacs ones (besides, Ctrl+A interferes with screen). On my own boxes, I put that in .bashrc",4
"For an internal cluster (db server, web servers etc..), is it good idea to use IPv6, which is faster and more secure? And make use of IPv4 just for the internet connection.  I have to say that personally I wouldn't have part of my internal network using IPv4 and part using IPv6 so for me the question would actually be ""Should my whole internal network be using IPv6"". Any speed and security gains you might get from making part of the network IPv6 will be negated by the support costs of a mixed environment and IIRC the need for a IPv6 --> IPv4 gateway in a mixed network will tend to negate any technical benefits. That depends - When people say ""faster and more secure"" to me, that sounds more like marketing blurb than a proper technical judgement, but if you can point to specific reasons why IPv6 will be ""faster and more secure"" for your specific scenario and you can be sure  that these benefits will outweigh any cost of making the change then by all means do it. In other words, does IPv6 fix a problem you actually have right now?",2
"Is it possible for me to use modern equipment (say, an all-in-one) to accept the signal and then re-broadcast it in my apartment- thereby creating another (wireless) Lan just for my family? Also, I can now add some security to the network, which is non-existent, now. I would LOVE to know just what is inside the Belkin (and similar) adapter (I can't find out from the internet) as that would help me to understand requirements, etc, a little more, i.e does it contain a router, modem, switch ... etc) in this very small package. At the end of the day, what I need is to have more than one computer on the internet at the same time from the one (single) source that I have. Could you help, please? I have tried (seemingly) everywhere and I get no answers. Like many people, I live in an apartment building, which provides wireless access for which each computer (not apartment or user) has to pay (monthly) to access the Internet. I use a (wireless) Belkin USB Adapter, to access the service. There is no set-up procedure or any special actions required - you just login to the network using the login details provided every month. I use Windows 7, but in my time here, I have used XP Pro, Vista and even Linux (Ubuntu).",1
"For syncing with two animator call the ax's methods from body's animation's Animation Event. If ax has some proposes in the game like it goes in and attacks all the enemies in the scene then you should do it this way. If it just damages opponent when in attack state then go with single animator ways as you need. Layers are usually used for 3D animations that share common avatar and masking them. You said they are sprite animations. In case of sprite animation, ax/prop animation is done with main animation sprites or separate object animation with separate animator. If it's a component(enable-disable) animation you can easily do it as same above. All child objects and most of their components are accessible from animation. If it's a transformation  animation then you can put the ax as a child object of the body and animate in the animation tab with the same body animation. Ax sprite can easily be synced that way in whichever frame you want. If ax's animation is a sprite animation then you have to create an extra state of animation that has both body and ax animation sprite drawn combined.",1
"So, in general, you should be interested in Statistics, more specifically concerning prediction and inference. That's it, except that doesn't help you decide which books to purchase. Also thanks for your recommendation, I'll take a look at it because I want to jump to finance at some point in my career:) If you want to learn about machine learning algorithms in a relaxed and fun manner, good if to take up if the next books give you headaches. Certainly worth reading. The best advice I can give you with these books is to read them from cover to cover. Don't read too much at once, take breaks and try to explain what you read to yourself. It can often make sense on paper and then not so much when you say it aloud.  These first three books will already ease you quite into the field. However if you decide to become more serious about learning, the following books should definitely be on your reading list:  What do you want to learn in AI and Machine learning? Artificial Intelligence covers many practical applications, so your question might be a bit vague here. I will suggest you books on Machine learning itself, as it is as a part of Artificial Intelligence. Inference: the goal here is to understand the relationship between input variables and output variables. If I change the values of the inputs, how do the output values change? prediction: Here we are not as much interested in how the data changes, but just want to know the value of the output variable. This book and next one in the list are freely available online, but if you want you can still purchase paper versions on amazon. I linked you the free versions. Don't look at the formulae as something to skip. Instead, look at them like lego blocks. Each symbol has a meaning that is defined in the index at the beginning of each book. Try to explain each symbol in the formula; Then explain how the symbols interact. Once you understand the formula, try to think what happens when certain symbols change values. You'll get a very firm grasp of the formula that way. The field of AI and ML has a lot of jargon it can become overwhelming. By really understanding how certain algorithms work you will stop being fooled by the fancy names and start to realize that there is a lot of repetition. This one picks up where ISLR left off. it is more math heavy and explores new concepts. You will find some overlap with the first book which will help solidify the concepts you learned in the first book.  This book is the most approachable one in the list. It requires some understanding of mathematics to understand certain formulas, but the text is still written in a way that will make concepts clear before you dive into the math. Make sure you do the exercises with R. It's a good skill to pick-up and it will make the theory much more tangible.",2
"This is a really general question, so you're probably only going to get overly-general answers until you post a new, more specific question. Alpha-beta pruning is probably the most common search technique for most of the class of board games you're thinking of (at least as far as I know, but I'm admittedly not terribly familiar with AI). There's also Negascout and whatever this is (these last two links aren't that great, if you spend more time sifting through the Google search results you can probably find better since you presumably have a more detailed idea of what you're really looking for). Most likely it will not be a small program. I would suggest to start from preparing a very general idea and a very basic principle how moves will be determined. Most likely it will appear not good enough at first and you will need to add more and more code to compensate bad decisions. At some point you will reach what you want. At least this is how I did it in my TicTacToe website. Try yourself - http://www.xo-play.com. It's also going to differ pretty significantly depending on the style of game. For example, the algorithms used to implement Sudoku solvers differ from those used to solve tic-tac-toe, chess, or Go (game trees / alpha-beta pruning). And I'm not sure what kind of AI is necessary for Snake -- generally it tends to be mostly random. I would suggest trying to write down how you yourself would think step-by-step when considering what move to make in a game. Once you define small-enough steps, all you need to do is put them into algorithm. What you could do is do like the old computers did when solving board game: Have the computer play through the game many times (> a couple thousand, not so hard to do if you remove player input). Each time it gets to a decision, choose randomly, but have it mark that it made that decision. Then have it analyze based on the other games where that same exact decision was made, and compare the results of those to the ones where the opposite choice was made.",3
"So any address xxx.xxx.232.81 ~ xxx.xxx.232.94 are valid addresses for use in your network. Well it seems xxx.xxx.232.81 is your Default Gateway so 82 ~ 94 sadly. You can use these if you are more curious on the technicalities: You have been given 16 IP addresses. However only 14 of them are usable. The Network Address for your subnet would be xxx.xxx.232.80 which identifies the network. The last address xxx.xxx.232.95 is used as your Broadcast Address. For broadcast traffic. And then you lose another one because you need a way off the network.  So there needs to be gateway to which everything is sent that is not destined for your network.  This is the default route, and in your case is x.x.232.81 and will be the address of a router that knows how to get to the internet. I couldn't get the computer on the internet unless I used a default gateway of xxx.xxx..232.81 which left my next ip .82 for my server. What happened to the .80 ip address? Is it still usable? I am on the internet so I hope I have it right. http://www.juniper.net/documentation/en_US/junos13.2/topics/concept/broadcast-qfx-series-understanding.html I'm not sure since I really don't understand what a broadcast ip does for me. I vaguely understand what it does, but does it apply to Windows servers? Can .95 be used without any issues? I've done this before but when the server wouldn't reach the internet without me using the .81 ip address for the default gateway, that confused me. However, you lose three of these.  The first address is the network address and the last being the broadcast address.  The network address needs to be reserved for routing to work, and the broadcast address is needed for.. broadcasts - where something needs to go to all devices on the network (various discovery protocols do this).",3
"One thing I've never used, but have seen at work, is a dashboard-like interface that gives the user access to configure all the VMs.  That is one piece of functionality that I'd love to have and I don't know if MS does anything like that.   I know this is primarily a quick question-and-answer site, but I wanted to get some opinions and I think for what I'm looking for it might be just a simple answer. So before I write a book on what I'd like, what do you guys think?  It's the developer asking the other IT guys...don't flame me too hard  =) I've not used Citrix XenServer, only played with Xen pre-Citrix, so I can't speak from personal experience on that count :) One of the advantages of sticking with the VMware family of products is that the VMs are easily passed up the chain as you grow.  Note that this portability isn't impossible if you jump between families, but it isn't as straight forward as it is with VMware. If you are going to do server-class stuff -- ie running linux computers where you will not need to interact with an X gui regularly, like database stuff or web development -- I recommend VMware Server.  It is somewhat lighter than VMware Workstation, and you can more easily configure it to start up and shut down VMs with the host.  Using this I treat my VMs as if they were remote computers, and I SSH into them from the host or anywhere else on the network.  I even run an XP instance or two like this and just remote desktop into it when I need it, but I wouldn't do anything graphically intense this way. If you are looking to do workstation-class stuff -- ie running instances of XP or Fedora and you will regularly need to interact with the desktops of those computers -- I recommend spending the bucks on VMware Workstation.  It has a nice interface, the integration with the host desktop is really nice (speaking for hosting on XP, Vista, and Win7 computers).  If you don't have the bucks, I hear VirtualBox is nice, but I've never tried it. If you are wanting to do production-class stuff, then you will probably want to start with VMware Server (or I believe there is now a VirtualBox Server offering too).  From there you will want to investigate VMware ESXi as a bare-metal hypervisor, and I'd stick the Xenserver stuff in this level too, but I will admit I've never had more than a passing involvement with Xen at any level. You could also use the free VMWare Player combined with the EasyVMX site http://www.easyvmx.com/ to allow you to create shell VM's that you can install software into in a completely free manner. I'm looking to utilize Virtualization at home, having little experience with both Virtual PC and VMware, about 5 computers that are freqently used for development work, downloading, one server, laptop, gaming, etc.  I understand that Virtual PC and VMWare are still the main players (with VirtualBox?) and while I'd like to stay free and/or open-source, VMware ""pro"" wouldn't be out of the question if it's abilities are THAT much better.  Basically I'd like VMs on all my machines for additional OSs, diff. development configs, and backup. It's a hosted solution, it runs inside another OS (Windows or Linux), and then virtualizes from there.",4
"Keep in mind that AI/effects that happen on a boundary between regions.  You may need to save those updates as a ""resolve all boundary"" update loop. Since you want each region to interact with another you will have to play around with the most appropriate way to propagate effects from one region to another. Each of those calls generates a new object, even if that object is identical to a million others already in the game. Additionally, your code will be a fair bit faster, since iterating over the List will be much quicker as you're no longer suffering indirection overhead and poor cache utilization you get when you allocate 5000x5000 individual objects spread all over memory. Now, for all area outside of the player region will be called non-visible space.  Since the player can't see this area, updates don't need to be instant, but as per the requirements, we do need to keep updating this region.  Since we can't fit all of it into memory, the usual trick is to break it up into chunks and load what you need.  So we break the rest of the 90x90 region into equal sized chunks for convenient loading.  These chunks are then rotated into a single (or multiple if you have mutliple threads) buffer.  The trick here is to now fully/partially simulate this region up to the current game tick so as to keep the region relevant to changing AI/physics. Once the region is simulated, just write (compress as needed) the region back to disk. For simplicity, I assume a 100x100 grid.  Assume the player can reasonably see an area of 10x10.  Since this area easily fits into memory we will fully simulate this region, which I will call the player region. You could also watch the player behavior.  If they are moving close to an edge region, maybe you will alter now often those other regions get updated to avoid having the player wait while the game simulates up to the current tick. This is a difficult problem because you have a large world which you want to update and have AI running around, regardless of where the player is.  Since you need to lower memory while keeping the above true you will have to either raise your memory requirement, or cut some corners.  I will explain how to cut those corners. Use a struct.  It allows you to store any custom fields you want, has no additional object space overhead, and is allocated in-place in the List's memory buffer so it has no additional memory manager overhead. However, this approach is only necessary because you have a very computationally difficult requirement of a large area that always needs to be updated with potentially complex AI/effects.  I would look into how important that requirement is for core-gameplay and reevaluate what a more realistic requirement would be. As this approach opens a lots of cans of worms. Note that the other suggestions you received to spatially subdivide your world and perform logic level-of-detail filtering are also good, and you should do that in addition to converting MapCell to a struct. Classes are allocate on the heap.  Every MapCell hence has overhead associate with memory management (bookkeeping of the allocations, padding, etc.).  There's also the overhead of all the internal data that every object instance has like the ""vtable"" handle (which isn't much but it adds up) and you don't need the featurs those provide. What you can use to alleviate the problem is called the ""Flyweight Pattern."" In this case, your MapCell instances become immutable (similar to, for example, String instances), so you'll have to store the changeable state - like entities - in some other data structure. Similarly, if you want to define some behaviour in your MapCell instances, only the behaviour which is common to all of them can be defined as its class methods. Store the tilemap with byte arrays. If byte is not enough for you, use more byte arrays or short/int arrays. Each object has such a big memory overhead in C# (and Java) that you should use primitive arrays for large sizes. Store the objects in other data structures, not per tile. An example using a static Factory and a private constructor; this makes your MapCell class essentially sealed, aside from hard-coded exceptions. An alternative, which allows for extending the MapCell class with your own and creating new terrain types at runtime (for example from configuration files) would have them register themselves in their constructor. You use them essentially the same way as above, though MapCell.ForTerrain(...) can now return null, which you have to deal with somehow (assuming that tile is empty, creating a new tile terrain type on the fly and so on). This works great so long as the player doesn't move to those regions.  What if he does? A  hierarchical approach may be needed.  Instead, we want to update non-visible regions close to the player more often than those which are further away.  You may have to play with this a bit, but maybe we choose to update a close non-visible region two or three times for every further away non-visible region.  This way if the player moves into a previously non-visible region, little extra work is needed to bring that region up to sync.",4
"Some EULAs are in fact invalid in some countries. I think this is a case for most US EULAs used in EU. US legal system has some resemblence to UK legal system, but EU legal system is in fact a mix of few totally different ideas how law should look like, and is quite different from US. For some parts of the world, piracy is as high as 90% or above.  People don't really care or are unaware of software license at all, not to mention EULA. The only times when an EULA is enforced would be in a corporate environment where a large number of licenses are compromised and catch the attention of the owner (e.g. MSFT). In such case, a team from the law enforcement body will raid an office and... you can imagine. What about the right to do reverse engineering provided as fair use in copyright law, you bought the product you have that right. Then you can deny to accept the EULA, which by no means would take from you the rights acquired when you bought the product to reverse engineer it. So you reverse engineer it to take away the EULA in the installer, and then proceed using it normally without accepting the EULA. Isn't this valid? If you understand the Dutch language then De (on)geldighed van EULAs (gastpost) will try to explain it from an European view. If you can't read Dutch, then Google Translate might still make it readable. Some courts in the U.S. have upheld shrinkwrap license agreements.  See particularly ProCD v. Zeidenberg and more generally, Wikipedia's section on EULA enforceability.  This covers how the DMCA may apply, for example. For some developed countries, people pay more attention and respect to properly licensed software, but still, we have an attitude of ""I pay for the software so I am fine with it.""  Nobody is actually reading the content of an EULA. In the end, though, you'll find that EULAs are sometimes deemed to be enforceable and sometimes not.",5
"""Weighting"" was added late in the game, when they realized that a 2-datacenter setup was too vulnerable.  (3 datacenters is resilient, and can use garbd in one of them.)  The example you quote is resilient to any single server, datacenter, or network outage. The main goal of is to allow a single point of failure -- a single node, the network, a data center.  It would take a really large and complex system to survive two failures.  For example, I think it would require 5 datacenters to survive 2 network failures. However, I agree that the sentence is ambiguous -- It can be read that after the network died, node1 or node 2 died.  This leaves three clumps:  (node1), (node2), (node3,node4), each with a weight of 2.  None should be considered ""primary"" because none has Quorum. As I read the last sentence of the quote, node1 or node2 died but the other three nodes are alive and talking to each other.  That is, there is a Quorum, and the system is still reliable. You should not be changing the configuration while the system is hobbled -- you should be fixing the broken components.",1
"It is very difficult to accurately suggest a solution with no ability to run testing, or see how the DB is indexed etc. But I'm going to try anyway.  If you do find this is a performance hit and you would rather cache the results of the select I'd be inclined to use table variables instead of temp tables, and only store the Holiday ID primary key, and join back to holiday on the fast indexed join of primary key = primary key as follows: You ideally need to find a balance, if your query is likely to return a lot of data and run quickly then I would err towards running the query on the main table twice, if it is likely to take a long time and return a relatively small number of rows then I would stick with the temp table approach. This way you are caching as little data as possible (one column of integers), while still retaining enough data to do a fast indexed search on dbo.Holiday.  It ultimately has to come down to looking at your execution plans, creating proper indexes and testing various approaches to find the one that best suits you. Given the information you have provided in the question it appears as though there is no problem with the speed of the select, in which case I would tend to agree with you, the additional cost of inserting into a temp table, then selecting from the temp table is as much overhead as performing the select query twice. The queries can be simplified as follows:",1
"As far as recovery, I wouldn't trust anything shy of restoring from a known good backup.  Those edits are just the ones you found.  Who knows what else might have been put in there. Make sure you keep Joomla up to date!  Keep on top of security updates and get them installed on all sites they day they are released. My guess is that they got the password for your server with a Trojan. Check your computer asap, specially if you store the server passwords in any program (browser, ftp clients, total commander, etc.) Btw: I'm assuming you're using windows Anyway, this sounds like an automated attack. Do a search to check if other sites (not in your server) had the same code injected to them. You don't give us enough details to be able to help with the how, it was almost certainly an automated attack and trying to track it down will just waste your time. Which seems to be a backdoor and the img src is just used to notify the attackers that the backdoor is there... That statement is your first problem.  If you google for ""joomla hacked"" there are 280,000 results just in the past month alone...   For tracking them down, you might want to start by reading this: http://kb.siteground.com/article/Joomla_hacked.html In a nutshell I'd say your chances are close to nil.  However, they go up a few percentage points if you happen to have deep pockets or government backing. There is no quick way to recover from this. Nuke from orbit and restore from a known good backup is the only way to go. About tracing the hacker, its not going to be easy. First check the access-logs from the time this happened. You'll probably see tons of ftp activity there. Have a look at the IP of those logs. If all of them are different, then he's probably using zombie computers and its very unlikely that you'll get to him. If they're all the same, then you might be a little more lucky.",5
"1, Some conversations mean I could have 20/30 emails in each query conversation, I've been manually moving the older emails (which have automatically moved to my 'query' folder) from each conversation to my archive folder, so I only keep the latest email, meaning my folder is updated. In my work, many emails come in to the inbox with differing subject titles, I have already set up a rule that any email with the word 'query' is moved to my query folder which for me is a great start. I'm new to this website so my apologies for lack of knowledge of this website as well as Outlook in general. Does anyone know if there is a way of automatically adding all of the attachments to the final email to ensure they are all noted in one email? Re Q1: Outlook provides a ""Conversation Clean Up"" feature, which could automatically evaluate the contents of each message in the Conversation. If a message is completely contained within one of the replies, the previous message is deleted. Outlook deletes earlier messages that have duplicate content but will not delete messages with attachments. You can also configure which folder to go for these cleaned-up items. See this article for more details: Regarding Q2 & Q3, I'm afraid Outlook doesn't provide build-in options to achieve these. You may wait and see whether others have any suggestion on this. Maybe it's possible via codes. 2, Also, in the chain of emails, quite often I send attachments, quite often, the customer will hit reply but not add the attachment. As I'm moving the older emails to an archive folder, the attachment either gets lost in there, or, what I have been doing which is time consuming is manually editting the final email in the query chain to add the attachment.  Thank you so much for your patience if you've read all of that and hope it makes sense, and thank you in advance to everyone who may be able to help me save quite a bit of time in my day. Do you know if there is anything I can set up that automatically moves the older emails to my 'closed' folder please? It can then be difficult to sometimes pair the two emails (the final email from the query conversation and the order email).  In an ideal world, I'd like the order email and the query email linking so I have everything there I need... Does anyone know if there's a way to do this rather than trying to play Pairs? 3, Finally, once a query is completed, the customer then often makes an order, the way I receive an order is the subject changes from 'query' to 'order' and a few details on the subject remain the same as the query conversation but not always.",2
"I have considered buying a NAS for home use, but I am not sure this would be optimal. I am not looking for product recommendations, but more what technologies I should rely on to be able to run large executables from an expandable storage medium - the current laptop seems to need replacement. I often end up buying a new laptop every second year, which is a bit of a problem. I run multiple applications that take up a lot of storage. For work, it is software for scientific computing (MATLAB etc.), programming (e.g. Visual Studio, Android SDK, Java), and graphics (Adobe Indesign, Photoshop). For leisure I run multiple pieces of music software and games that usually take up a few gigabytes of storage each. I do have an external hard drive (1 TB), but I would like to avoid depending on a modular system and I am not too sure if running executables from a disk other than C:\ produces expected behavior. I am guessing that this is a fairly common problem for power users and would like to ask what the most common solution is. The goal is to have a durable storage system.",1
"I am trying to synchronize two mailboxes which reside in different servers. This is due to a migration process. The old server is a courier server and needs to be accessed via IMAP, whereas the new server is a dovecot server. I am trying to follow the original Dovecot documentation. Unfortunately it is not specified where the configuration of the source IMAP needs to be set, when the doveadm script is run on the destination server. The documentation provides the settings, but does not mention which dovecot configuration file the settings have to be entered.  I had great problems, because my source IMAP only supports STARTTLS on port 143. -o imapc_ssl=starttls was a life-saver in my case. Note this is for a single user; you may want to have different options if you use a master user/password, or if you require SSL for the connections. You should migrate your mail using the dsync utility from Dovecot.  This will preserve the UIDs and even POP3 UIDLs if necessary. Run dsync using the backup -R option, to 'reverse backup' from the remote IMAP server to the local Dovecot server.  You need to have a special configuration file created, something like this: Of course, this is quite insecure if you have more users on the box that can see your commands (and passwords) with who or by looking into your .bash_history file, so beware. Best would be to place the configuration in /etc/dovecot/conf.d/90-migration.conf (all files in conf.d dir are automatically included).",4
"How can I unlock these files? As this is a FAT file system, taking ownership in Windows 7 doesn't have any effect. I tried creating an image and writing the image but that makes the partition only 2 GB big - and doesn't really fix the problem of me being unable to copy (e.g. backup) the files. Can you run chkdsk on the card, or a similar tool, to see if the operating system is rejecting reads from them because of corruption? Unless you skip those that are denied and then you can pinpoint an actual file as having be one of yours... Since you've tried the .iso (creating an image) approach, why not mount the .iso in Windows and copy it out of the image to the new card? Directions for this process: howtogeek.com/howto/windows-vista/ If you just tried to mass copy everything on it, your phone has likely placed some files on it that are locked for it's use only, and you wouldn't want them anyway. I just got a new phone which is supplied with a 2 GB SD card. I bought a bigger card to replace it and I want to copy the preloaded data to the new card. When I'm trying to copy the data from the default card, I receive an access denied error on some of the files, the system won't allow me to read them.",4
"If there were a solution that was nearly universal, and unobtrusive, and didn't double the amount of space needed, then I think it would be worth it, but it hasn't affected me much, so I don't see the need, even though others might.  Just go with the rule of keeping 'it' in multiple places, disks are cheap, the internet clouds are here, online storage is abundent. I suppose it depends on your environment. I occasionally get an optical disk that can't be read, but rare is the time that I've actually lost data because of it.  I usually use par2 when dealing with somewhat large (>100MB) files. The extra amount of processing time is worth peace of mind and not that noticable. Where possible store on 2 different technologies.  In the case that some strange airbourne fungus wipes out all your DVD's you may still be able to recover data from say a disk or online. These days, reliable transmission (through retransmissions) is the norm, bandwidth is cheap, and latency generally sufficiently low for most ""normal"" apps. When I'm transferring things, I typically do so over the network, and when I don't md5sum tells me whether I got a good copy or not, even before I disconnect the storage.  Redundancy like that is really intended for instances where transmission has failed, and transmission is expensive. If you are sending your archive to mars, please do include redundancy, as retransmitting any or all of it might take a while. In terms of media failure - I would suggest that the amount of redundant data you would need to recover from all but the most trivial failures is too much to be worthwhile. It is fairly unlikely for one or two bits to go wonky at a time, certainly in my experience of disk failure. Basically you'd be storing an extra n-bytes all over the place for a received value of sqrt(fa). As for if it's still worth it?  I make 2 copies of everything I back up, and a 3rd for super important stuff (tax returns, mortage stuff, etc).  One on an NAS for easy access and 1 on DVD for long term storage. The 3rd copies tend to make their way into my fire safe on DVD in a nice case to prevent scratching.",5
"The virtualhosts in Apache are a separate concept from the etc/hosts file. The etc/hosts file is just a method of local DNS resolution. Virtualhosts are a configuration option where Apache will give a different page depending on what the requested URL is. This requires some DNS magic, since the end user needs to be able to resolve the host names. Multiple IPs or ports are two other options. My server isn't currently open to the internet yet, but if it is, what's the best way to resolve DNS for my virtualhost domains if it were to become forward-facing (i.e open to the internet)? I don't think it can be put elsewhere for use with Apache, simply for virtual hosts, and the main HOSTS file for blocking sites etc. I heard about PAC files on Uniform Server's website (http://wiki.uniformserver.com/index.php/Virtual_Hosting:_PAC) but they're browser-specific though, aren't they? I'm competent at the basics of Apache, PHP and virtual hosting but have a question about virtual hosting. In DNS, you'd just add an entry for a different domain, pointing at the same IP. For example, www.site1.com and www.site2.com would both point to 172.32.4.6",2
"You can easily restore your 2000 DB to any higher version of SQL Server. The issue you may face is feature deprecation. If you aren't using any deprecated features, the actual DB migration is a non-event for the most part. I'm trying to budget for how many hours it would take to upgrade a SQL Server 2000 database so it will run on SQL Server 2012. I realize you can't really tell much without actually seeing what we're working with, but all I'm looking for now is a ballpark figure (i.e., is it in the 20-30 hour range? 50 hours? 100 hours?). I work with a city government arts program.  In 2004, we contracted with a local database developer to develop a system that allows us to track student participation in our after school art classes.  It tracks each student's address, demographics, what classes they enroll in, daily attendance, paperwork/documentation, class progress,  etc.   It was built on SQL Server 2000. I don't think there are much issues. Try restoring your old database to SQL Server 2012 using Management Studio. You don't need an extra budget allocation for this. We've upgraded all  our other servers but we still have one server left that's still running SQL server 2000 and Windows 2003 just  because of this one outdated db system.  We'd like to upgrade both SQL Server to 2012 and Windows Server  to 2008R2 on this last server.  Because of issues with the city's technology systems back then we were not able to install it right away.  When they finally got around to supporting us (I think around 2006 or 07), they gave us SQL Server 2005.  We ended up installing an instance of SQL Server 2000 on SQL Server 2005 which itself runs on Windows Server 2003.   As of a few years ago, we are no longer under the city's technology systems and manage our own systems through independent contracted consultants.  Last year, we terminated our contract with the original database developer because he wasn't following through on a number of issues we needed addressed over the years.  Last year, we also went through a major overhaul of all our servers and desktops.   Seeking some advice about an oudated database system we're working with.... hope this is an appropriate place to post this question. You cannot directly upgrade a SQL 2000 database to SQL 2012. You need to first upgrade your databases to SQL 2005/2008/R2 after that you can upgrade to SQL 2012.",4
"Adding more swap files is as simple as creating more files (/swapfile1, /swapfileX), formatting them using mkswap and enabling using swapon. If you want to disable a swapfile, you can use command swapoff /swapfile. If I currently use /dev/hda3 for swap, and I rather would like to use /dev/hda4, which steps should I go through? If you have decent amount of RAM and your applications aren't memory-intensive, you might consider using a separate file as a swap instead of the whole partition. That way you can easily select the amount of swap space you use, either by adding more swap files, or resizing existing ones. As for the performance between disk and file version, it's not that terribly different. You can even use swapfile as hibernation disk in laptops (although I always use separate partition for that anyway). You'll need to format /dev/hda4 as swap, which I think just deletes the file system tables, then just edit /etc/fstab and point swap to /dev/hda4.  Then reboot and you should be good.  It goes without saying that you'll lose any data on /dev/hda4. You can use gparted as a gui for the formatting. Let's say that your swapfile will reside in root directory as /swapfile, and will have size 512 MB. To create it issue commands as root:",3
"Looking into anycast, it is primarily used to route traffic to different sites using BGP, or to different destinations within a large site that has multiple routers.  In my case there is only a single switch connecting everything and routing within the LAN only goes so far as ARP. I am wondering whether it is possible to use IPv4 anycast to have multiple computers in the same IP subnet handle requests from other machines also in the same subnet. I have two DNS servers on my LAN.  Since I have learned that having two DNS servers on different IPs doesn't help redundancy (short reason: hosts will not always switch IPs and will keep using the IP of the offline server) I would like to set up some anycast IPs so that DNS requests will get sent to any DNS server that is up and running. I can't find any information on how one might configure anycast in this situation.  Can you simply assign two or more hosts the same IP address and the switch will figure out which port to send the packets to, dropping the port and clearing the ARP entry if the server goes offline?  Will the hosts also reissue ARP requests if packets suddenly stop being responded to?  Do you have to share a MAC address as well as an IP?  Or is it impossible to use anycast with this setup and a routing protocol is required?",1
"The only outbound port your software can rely on for ""phoning home"" or contacting other servers  is port 80. And not only the port, but the protocol has to be HTTP. Some firewalls allow port 80, but they also inspect the protocol and block it if it is not HTTP.   You can also rely on 443, HTTPS.  Still, I would provide an alternative installation/registration workflow which works if there is no port 80 connection.  If you're just sending email on behalf of an end-user, who originates the email within your application, use authenticated SMTP to port 587. I am building an application that will be acting as a combined MUA/MTA on different networks. However, many of the networks are with ISP's that block port 25 for SMTP. Therefore I would also like to open up a secondary port so that some of the installs can communicate on that if port 25 is closed. That being said, of course you can ship software that sends mail, and that uses port 25. However, that mail should go to an SMTP server which is configured by the user. (Quite often, a relay provided by their ISP, which may require authentication.) For outgoing port 25, you'll need to specify a smtp gateway anyway, especially if the IP is listed in spamhaus's PBL list -- even if your mail makes it out, it will almost certainly be blacklisted by most remote mail servers. You can pick any port you want, as long as you're building a closed private e-mail system and the MTA's all know to use that port to communicate with each other, as long as you stay consistent. If you need this system to communicate to the outside world though, at least one of your MTA's will have to use destination port 25 to relay, or use a trusted relay to get out to the rest of the world. ISP's block outgoing 25 because it's used by spammers to directly contact mail servers from subscriber lines and deliver spam. But the bigger problem is that all the mail software out there will expect to connect to port 25.   The mail exchange (MX) DNS record does not include a port number; it resolves to a host. Your question is not 100% clear to me. Are you looking to have incoming port 25 connections, or are you connecting 25 outbound?   I will split the answer into two to cover both bases. How do I choose a second port? I know some people use port 26 or port 2525. What is the correct way to choose a port that won't interfere with existing software? As long as you pick something in the high port range, and it's something you can remember, this is fine. Those people will need a workaround, ranging from changing their subscription, changing to another ISP, or using some machine in some other network as a port-forwarding proxy which forwards port 25 connections to the real server, using some port that is open. My plug for DNSMadeEasy: Comcast blocked port 25 on me unannounced, killing my mail server. After reviewing the rerouting options, I chose DNS Made Easy. (EasyDNS in Canada, DNSExit raised prices, DYN ridiculously expensive). DNS Made Easy had me back up in under 10 minutes, and thats with manual MX Record entries. They also responded very promptly to a silly question I had because I was rushed and didnt hit the little blue i info button. They have a essentially infinite 1 GB buffer and my mail server is a lot more robust to downtime now that I'm using this rerouting service. You can send mail to some servers via the SMTPS port 465, if the remote MTA is SSL/TLS-enabled and listening on that port, but don't count on it. An application that is going to act as a mail server for a domain will generally not be deployable by users who cannot have incoming port 25 open. For incoming 25, you can use one the many smtp rerouting services, provided because many ISPs block port 25. See this post: http://thenubbyadmin.com/2010/06/14/list-of-inbound-smtp-redirection-services-some-for-free/. If by ""installs communicate on port 25"", you mean that the software installation tries to contact other servers using port 25, that is a bad idea. Do not ship software that calls out on port 25.",5
"As part of a requirement I have been issued, I seek the ability to push apps from the Windows Store (.appx) to remote users. I've seen this guide; http://blogs.technet.com/b/keithmayer/archive/2013/02/25/step-by-step-deploying-windows-8-apps-with-system-center-2012-service-pack-1.aspx, which details doing exactly this, but it is only applicable to machines that are administered by a system running Windows Server 2012 R2 with the 'System Center 2012' bundle installed. The systems I target are considerably more decentralised than this, making this guide inappropriate. The framework surrounding the data transmission is sorted; What I need to know is if there is a script that can be sent out along with a URI or the package proper to facilitate installation on the user-side of the Windows Store app. I have a hunch that Microsoft have deliberately designed the Windows Store to be this way, but I figured I ought to ask around before I resign myself to the requirement. I am well aware that numerous guides exist from Microsoft on pushing out Line-of-Business (LOB) (AKA Enterprise) applications -- that is, apps that have been developed in-house and are not for the consumption of the Windows store. This is inappropriate for my requirement, however; the customer wants their clients to receive apps that appear currently on the Windows Store, and for them to be installed in a silent manner.",1
"Yes it does. No matter what RAID level you're running, you may still be affected by fragmentation (unless you're running SAN solutions like NetApp with their WAFL layout). Also, the RAID controller will use predictive caching (if it has it) based on an understanding of the array layout, so that will work better if you have defrag. The only time you don't need to defrag is if the underlying storage medium is random access, so don't defrag your USB key, and don't defrag an SSD. RAID1 is just a mirror of the fragmentation on both drives. RAID0 is just splitting fragmented files up on two drives. RAID5, 10, etc. Yes, defrag does still make sense for RAID.  While it's true that the layout the OS sees isn't the same as the physical layout, it's monotonic, ie the virtual sectors are in the same order on the disk as they are on the array, it's just they are scattered across disks. It seems to me that since RAID volumes are logical (as opposed to physical), the layout that the OS believes they have might not correspond to the actual phsyical layout.",3
"If you can find a copy of Microsoft's Dart8.0 you can reset the password using the locksmith utility. The newest version of Ultimate Boot CD will allow you to change the password in Windows 8/8.1.  That said, there are many good tools listed in the responses here.  Find the one that works best for you and go with it. http://trinityhome.org/Home/index.php?content=TRINITY_RESCUE_KIT_DOWNLOAD&front_id=12&lang=en&locale=en Use the SS64 Reference. The most common way of achieving a password change is through the net user command.  If you will face problem with mounting windows volume. You can manually extract from Windows your SAM, SYSTEM and SECURITY registry files and unlock them on another computer. This distro uses chntpw program that can reset, blank or set password for your accounts. It can even unlock blocked accounts from your Windows station. All offline, it comes even with offline registry editor. If you want to use USB stick, then you can simply use multiplatform UNetBootin and select ntpasswd it'll download images and create bootable usb flash for you.  Depending on your Manufacturer, there's a key combination or cd that will allow you to access the BIOS during the Booting Time.",5
"Do you have any recommended tools for accomplishing this? I can see something like this being applicable to many IT pros so I'm sure there are some good tips out there. How much of this is available through powershell I don't know because I don't use it. But I have written a very simple vb.net tray application that just monitors network availability and connects to a network share when the network is available. If you're interested in the bare-bones of the code then let me know.  This wouldn't be a simple little VBScript thing, but it wouldn't be that much coding either. Maybe somebody could pick up the idea and run with it. There's even sample code at http://www.microsoft.com/downloads/details.aspx?familyid=ef8a6228-f11d-4ba0-b73e-dd8dc7dd11e8&displaylang=en. Presumably the ""active network"" would be based on IP address/gateway range or WIFI network name, etc. There have been numerous times I've wanted this functionality, and I'd think there are more than a few people who would like to see it. The ""Network Location Awareness"" service (http://msdn.microsoft.com/en-us/library/ms739931(VS.85).aspx) in Windows XP (and 2000? I don't recall...) and up will enable this functionality, but I haven't found where anybody has written an application to take advantage of it. I'd love to code something myself, but I don't have enough spare cycles to even begin to think about it.  Using .net framework version 2 it is possible to use the System.Net.NetworkInformation namespace. From this you can determine: You could create a batch file that reads the results of an ipconfig command, and have it run every 5 seconds or so... This is kind of a sideways and partial answer (and not even documented because I can't seem to find the original article), but I'm fairly sure Windows 7 has a property sheet on the printer object allowing you to change defaults depending on subnet. I'd like to create a script to do various things when the network connection changes. For example, if the machine connects to my office network I might desire it to:",5
"My Linux terminal understands C-Insert as Copy and S-Insert as Paste (kill and yank, respectively).  Emacs also understands those (at least the more recent versions do), or can be easily made to understand them.  In fact, most applications understand them, along with S-Delete for Cut; I usually only have trouble at Web-2.0 sites. You can remap the whole dang keyboard if you like, recent emacs even have a menu and GUI to help you with this. Problem is, C-C in particular is a prefix to a lot of other commands. Those would all end up needing to be attached to new key combinations. While your cut&paste habits may die hard, in the long run you may get more joy if you ""submit"" to Emacs' conventions rather than try to force them into line with Windows keystrokes. I know about cua-mode, but I'm specifically wanting to bind C-C and C-V (the uppercase versions) to be like the terminal in Gnome.  Is there any way to do this?  I tried this, but it gave me all kinds of errors (apparently, it doesn't like me binding something to C-c whether the C is lowercase or not):",3
"To add to @heavyd's answer, the reason for this is the OS does not have the ability to determine which parts of the process are OK to run in parallel and which are not.  If a program is not designed to run on parallel cores, you can have routines within the application that are designed to run sequentially running at the same time.  This can cause all sorts of issues (like if 2 routines use the same memory block but are not meant to run at the same time).   The OS can use multiple cores for multiple processes since it does that anyways, but spreading a single-core application across multiple cores will cause all sorts of unexpected behavior. The OS cannot split an individual single-threaded process across multiple cores (although it may change which core an application is running on, but that's a different question), however it can run multiple processes, each on its own core. So, yes, if you have multiple processor intensive applications running in the background, it is likely you will still have a spare core sitting around doing little or nothing you can use to run other applications.  Because of this, Windows 7 (on compatible processors) has a technology known as ""core parking"" which basically disables unused cores on your computer in order to save electricity. When it comes down to it, it doesn't matter if a processor is at 1% usage or 95% usage (as long as it is stable at 95% and not peaking), programs will be running at the same speed as long as it isn't hitting 100%. Unused CPU cycles are simply wasted. Single threaded applications are automatically put across to alternate cores - I am not sure on the technology behind this, but I know it works quite well.",3
"I need to modify my development environment, a virtual image using Windows 2003 SP2, to install the SMTP service under IIS (6.0).  The virtual image is a corporate build, and I do not have access to the installation disc (and going through the corporate channels, this may take several days).  Is there any way I can add the SMTP service without the install disc?   I've been able to successfully install SNMP, for Windows 2003 Server, which requires the ""second CD"" or ""i386"" folder to complete the installation. Rather than finding and transferring that large amount or finding a way to mount it from a remote location. I've now just packed up my own SNMP requirements folder for when installing SNMP for Windows 2003. The required files, found in the i386 folder, include: I Agreed with Sam, if you don't have the disk or i386 folder, then you'll probably have difficulties. You could try downloading Sp2, and extracting the i386 folder from it (it's a while ago when I tried that, and can't remember if it worked). I have had some good luck with and extracted SP2 i386 folder in the past though, and - if memory serves - it worked fine one time with IIS. IMHO, the easiest way to do it is to pay a visit to your sysadmin team, and bring along cookies. Or beer, beer is good too. If there is alternative way of having an email service working on the virtual image, that also would be useful.  The only one I was not able to find was apver.vbs. I had to track that down online. I zipped it up and shared it to myself for easy access/deployment when needed. It's roughly 500KB compared to the 600MB I would need if finagling the ""required"" CD.",5
"It technically is since both those 100's have the same context.  Similar to me using 32 as a width in several different places.  It depends how nitpicky you want to be.  In this case I might use something like MAX_PERCENT.  It does help with readability as well since when I see that I know the calculations have to do with percentages. Any number that remains unnamed remains a magic number. It's not related common usage as common usage is relative. The problem with it is ""when"" or ""where"" to draw a line... when does a number changes its state to non-magic... If forced to use a symbol, I might call it N100.  That way you can tell what its value is.  A long time ago I saw this done for common constants.  And negative numbers were prefixed with ""M"".   Why did they do this?  Because the computer instruction set couldn't load numbers in line (""immediate""); they had to be fetched from a memory location.  And the assembler used symbols to refer to those locations. Others have already stated why they believe that 100 is an acceptable constant here. This answer shows how you can get Sonar to accept that. So if you want to discuss, go for the relative approach. You will find the discussions never ending. And if you think it is over another number comes along. Under your-sonar-domain/profiles/, you can edit quality profiles, and the rules in them. The ""Magic Number"" rule is customisable, you can specify certain numbers for the rule to ignore. Simply specify ""100"" as a number to ignore. The absolute approach is simple an clear: Every number that is not named is a magic number. Discussion over... for all numbers... Of course this is only a (one) definition. But this definition has inherent properties that will make it possible to evaluate it exactly. The relative approach will lead to a discussion that will evolve a local aggreement. And the result to consider a number to be non-magic, will depend on when and where you start the discussion. Unfortunally this discussion has to be lead for each number separately. Or you go and find a definition that can be used in boolean expressions that will evaluate to true or false. I go the second way with following definition: Every unnamed number is magic. Even if you practically ignore it the statement stays the same.",4
"There are plenty of good free to use IDE and tools. This goes in pair with the Java naming convention/ Java Style recommendation. In most GUI code I've seen, you can create an init method and put all the initialization code in there. It helps to keep the constructor clean, but it's no big deals. It's what I'm mostly used too, but it's been a long time I've done any Swing related code. You should use an IDE or a tools to automatically correct any styling or indentation error. The spacing of your variables declarations are not correct and your indentation of your if-else are not correct either.  You should rather create a separate JFrame object in your main method pass that  as parameter to your classes and use this for configuration: This will go a long way to help you, at first glance, know what the variable is about. For the next person that read your code it will be easier too.  This is a special case from your previous point. Since you hard-coded the action command names, if you change it at one place you have to remember to change it in other places.  Your class gui extends JFrame, but it does not add new behavior to it. You just configure its content, but this can be done from ""outside"". I don't think you need to create the Font object every time, you may use an const Font object to reduce object creation. It's a small thing that not improve really but it could help in other cases. Almost all *Listener interfaces have only few methods to implement, usually only one. They are meant to be implemented as anonymous inner classes. Take ""David"" for example, it does represent the same named font in all your case. You could then have a private static String DAVID = ""David"" to represent the font.  This is a perfect example of the Java Enum class. Take a look at https://docs.oracle.com/javase/tutorial/java/javaOO/enum.html, it's a good tutorial on how to use it.",2
"You are mixing up three orthogonal concepts: data model (star schema), workload characteristics (OLTP vs. OLAP), and physical data organisation (columnar). Mustaccio's answer is good.  In addition,  every star schema  I have ever seen was implemented on top of a relational DBMS, often Oracle.  Also, the data in it was downstream from some kind or operational database, also on a relational DBMS.  The operational database was mostly normalized and used for OLTP. Your data model has no bearing on whether column-organised tables are appropriate for you; however, data organisation must reflect the nature of your workload (i.e. queries). For example, SELECT * FROM... cannot possibly perform well in a columnar database. I have never seen a star schema used for OLTP.  I can't imagine anybody ever doing that.  It's not what star schema is for. My organisation is implementing a new data warehouse using a columnar data store (Redshift) and the performance is currently abysmal.  There are many causes for this but I think the key reason is because of the way we have our data structured, we're implementing a classic star schema design, which surely is optimal for a traditional RDBMS (OLTP) and not necessarily for OLAP? I'm looking for someone with experience on this to briefly explain the pros and cons of structuring large quantities of data in a star schema on amazon redshift.",3
"You can include the --no-trunc option to get a full line. The key piece of data relevant to your question from the above image is in the ADD step where you see file:0eb5ea35741d23fe3. If that checksum changes between two image builds, the cache will bust and you'll start a new set of layers. If you see the cache being busted on a COPY or ADD command, and not the previous step, then you need to look at the hash being generated. All the files need to be identical, same file names, cases, file permissions, file owners, and the contents need to be bit for bit identical. You can look at the resulting checksum for this hash by running a docker image history command, e.g.: I am trying to improve CI/CD performance and part of that is improving the caching behaviour of Docker builds. I find myself trying to debug what exactly invalidates the cache. Are there any ways to query the Docker build process to get more information than ""cached"" and ""not cached"" ? For example, at some point, I add a large directory, which has not changed as far as I can tell, but busts the cache. I would like to find out what is causing that, exactly.",2
"Pseudo Terminal Slave (pts) connections are SSH or telnet connections that means indirect connections to the system. All of these connections can connect to a shell which will allow you to issue commands to the computer. So when you open up a terminal on your system from gui, it opens up a pts with source ip 0.0.0.0. From the information provided by you, it seems that it happening because of script running on this server or scheduled, which is using ssh or telnet service or local pts to throw output in terminal.  This looks absolutely puzzling to me. Either it should use some DNS name or IP address. I checked the last.c file also but I still can't find why it is not showing anything. Probably given some time, I can figure out the part about 0.0.0.0. The Third type of Entry comes when you open a screen session after loggin into through SSH. That will also create a entry there and without any IP address. The second Entry normally comes into picture when you log into a Machine and open up a terminal window in GUI. There will also be a entry even if you open a new Tab in the same terminal window. Perhaps your IP address resolves to a blank string on one of your DNS servers, probably the secondary if it happens only 10 per cent of the time (or just possibly a hosts file if these are distributed from a central repository). That would account for the missing (or white space) entry and be consistent with Soham's reading of the source. The First entry with tty* comes when you login through the terminal or console by pressing CTRL+ALT+F1-6. Its pretty much clear from the terminal it's using. I am pretty much sure that your case comes under in any of the 2 cases, one with the terminal Window in GUI and the other with the screen session.",4
"One solution that accomplishes this task is to run a VNC server on the pc with multiple monitors, then connect to that pc using a VNC Client(that connects to the VNC server running on localhost) on the same pc.  If not, a port replicator style thing as is often used in electronics stores to demonstrate computer monitors should do it. Would it work to install VNC servers (essentially remote desktop) on all of the machines whose monitors you want to view, then run several instances of a VNC client on the monitoring computer? You might want to check out software like VNC Thumbnail Viewer (I've never used it, but it looks nifty) which shows a thumbnail of each machine you have VNC running on, then you can click to enlarge the image. My hacky solution - change monitor A to one with PIP (picture-in-picture) and then get a Y-shaped video lead. No idea if such a thing exists or not. However, wouldn't it be possible to capture display data generated by the OS kernel or data getting passed down to the display adapter driver?  I haven't been able to find much about this, but I imagine it may not currently be possible to access/emulate video output going out of a video port in software.  Depending on how you are running the video (svideo, vga, hdmi) you could simply split the video and run one into its designated monitor and the other into a video switch. You wouldnt get pip but you could swap between feeds fairly quickly. This is actually how we do it at work for a computer/security camera feed/computer/fee computer setup. Does anyone know of any software/hardware configuration that will allow you to preview/view the contents multiple monitors on one monitor? Let's pretend:",4
"If you have it check for updates prior to scanning, and scan at least once a day, that's a sort of automation. Otherwise I think you would have to run a script of some sort to scan the names of available optional updates and install anything with Security Essentials in the name. Microsoft doesn't make a method available to do that. No you can not set the MSE updtaes as required or permanent. Setting the MSE updates as required or important could make it seem that MSE is a required installation for Windows. This would then lead to some legal issues. See this article from Computer World Is there a way I can selectively make some optional Windows updates permanent? In particular I'm looking to make the definition updates for Microsoft Security Essentials permanent. To set a scheduled scan open up MSE and click on the settings tab. Check the box ""Run a scheduled scan"" and set the day, time, and type of scan. Then make sure the box for ""Check for the latest virus & spyware definitions before running a scheduled scan"" is checked.  to check for Microsoft Security Essentials Signature Updates on daily, weekly, or monthly basis, without requiring the scheduling of a scan. http://www.computerworld.com/s/article/9195079/Rival_calls_foul_over_Microsoft_s_delivering_Security_Essentials_via_Windows_Update",4
"That would total $34,512 vs $99. Not to mention the price of the hardware required to run 3TB of RAM. Of course, current operating system may not support this at all, but is there any reason RAM isn't used this way? It actually is done in some scenarios. If you have an operating system/app stack small enough, you can run it entirely in RAM. Of course it has all the disadvantages that the accepted answer has. But it is possible and does happen. NRAM is basically ""mechanical""  switches that are a few atoms wide, It doesn't need current to maintain state so it is energy efficient and doesn't need to be cooled down and because the switches are so small the density is very high and this is good for two reasons, one is the access to memory is very fast and you'll be able to have terabytes of data on small devices like cellphone. if you like to read more see this: http://www.nantero.com/mission.html and this http://en.wikipedia.org/wiki/Nano-RAM Yes this is the premise of many in-memory databases that are coming up on the market. One example is SAP HANA. The idea is that RAM is cheap and from a performance point of view read / write operations on RAM are 1000x faster than disk operations. So most of the data is kept in the RAM and then you define a data ageing strategy using which the old data is recycled back to cold storage (i.e. disk). RAM is cheap, and much faster than SSDs. It's just volatile. So why don't computers have a LOT of RAM, and on power up, load everything to the RAM from the hard drive/SSD and just run everything from there, assuming there's no real need to persist anything outside of memory? Wouldn't computers be much faster? Take a look at Puppy Linux, a popular Linux distribution. Their how it works page talks about running from RAM: http://puppylinux.org/wikka/howPuppyWorks",5
"I have been using iTunes on a Dell running Windows XP to sync about 60 gigs of songs with my old school iPod. Well, the Dell's hard drive died the other day, and the only place that I have my music is on the iPod (I do have the CDs, but really don't want to have to re-rip 60 gigs of music). Has anyone recently gone through this process?  Please provide suggestions for copying songs from an iPod formated to work with Windows onto a MacBook Pro running Snow Leopard. I have used Yamipod in the past with success. It runs on Windows, OS X, and Linux and can even be setup to run directly off the iPod. It allows you to play and manage the songs on your iPod which includes copying songs off of the iPod and onto your computer. Ars Technica has a whole guide on this. They recommend PodWorks for OS X, iRip for Windows and gtkpod for Linux. You could use the program SharePod to pull them off the iPod onto another Windows machine. Then import those songs into your Mac's iTunes. Once you're sure all your music is there, you can let your Mac format and sync your iPod.  So, now I have a shiny new MacBook Pro.  Is there a way to get my songs off of my iPod and onto the MacBook?  I googled, and found Senuti.  But, I'm leery of accidentally formating the iPod and losing my songs, and I can't tell if it is Snow Leopard compatible yet. I have used PodsBlitz in the past, and I have sung its virtues for years now. It is written in Java and I have tested it personally on Linux and Windows, but I know that it also runs under MacOS without a glitch. It is safe, very fast and very simple to install: just dump a .jar file in the root of your iPod. Double-click on its icon, choose the songs you want to copy and a destination folder, and voila: instant backup.",5
"I would be really pushing the increased security in both IE7 and IE8. Pop-up blocking, phishing filters etc... ""It is better to 'keep up' with the upgrades and patches as they come out, than to stagnate and be left with a HUGE upgrade in the future that will be MUCH more expensive (in time, effort and cost)"" ""IE6 is a legacy system, only on extended support from Microsoft.  Microsoft was recently pushed IE8 through as a Critital Patch on Windows Update and will only publish future patches for IE6 for the most serious issues"" Regardless of the numerous security warnings our there and lack of applicability in the greater world wide web, my manager still sees intranet apps written for IE6 as the main reason to stay on it. What can we do to swing the vote in our favour? We're tired of supporting an ailing browser when users call us up constantly complaining that website don't look right, and more and more people asking for browser tabs ""because that's what they've got at home"". ""If we continue to allow IE6 users to connect to the Internet then we are increasing our risk of malware infection."" We've already rolled out Windows XP SP2 (no hope of going to Vista or Windows 7 in the foreseeable future unfortunately) across the enterprise and our latest internal roll-out actually incorporates SP3 as well - but unfortunately IE is explicitly being kept at version 6. Get a range on how much money could be lost by keeping IE6 (even in unlikely scenarios), and how much money can be saved by upgrading. Money and ROI talk. IE6 is one of the biggest entry point for malware onto a Windows system. You can reduce a lot of this by leaving IE6 Hiring an intern to update the intranet apps should be probably less costly than doing extensive cleanup of the network due to a security breach, because of using an insecure and standards-ignorant browser both for intranet and internet.",5
"This exact setup now needs mirroring to a dozen or so other identical embedded devices. They are all the same processor, memory, disk size etc (hence identical). What would be the best way to do this? If I mirrored the disk to a USB stick, how would I get that onto the clean box? I guess it would depend on whether the BIOS could help? This copies practically everything across to a mounted directory (the root of your new disk) but you may need to copy the files in /dev/ separately. A few years ago when using an embedded NSLU2 system (ARM, 32MB internal memory, internal 8MB flash-on-chip disks) I was able to just dd from the internal mmcblk0 to a file for backup purposes. I used the copy method to resize my root filesystem a few times with great success but these were ordinary disks and I was able to have both filesystems (source and target) available from the one running (source) instance. The disks in the devices are not really physically accessible, otherwise my first though would have been to put a second disk in the finished box and dd to mirror it. I've just finished setting up a Debian system running on an embedded device. It has custom partitioning, numerous config file changes, web content, and a couple of applications compiled from source.",2
"If I could get something in the $50/60 range that would be great, I've seen a bunch but I don't know if they're good quality even if the specs seem okay. If spending a little more would be advisable that's alright too. Thanks for any advice! In fact, what outputs does your GeForce 6500 have? And does the monitor also have speakers? (i.e. will it need the audio component of the HDMI connector?) And what monitor is it? I was looking at this one earlier today, again though, I don't really know much about them to know if it's any good. It just seemed to have decent reviews: http://www.ebay.com/itm/DIAMOND-AMD-Radeon-HD-6670-PCIE-2GB-GDDR3-Video-Graphics-Card-/321070374078?pt=PCC_Video_TV_Cards&hash=item4ac1491cbe Any graphics card that supports PCIE 1.0 will do the job. You should be able to find something for $30 range or less really. I'm not looking for anything fancy, just something that allows me to watch videos on my new screen how they're meant to be viewed. I won't be doing any serious 3D gaming etc. So I'm in the process of trying to choose a new graphics card. I'm getting a new monitor that has HDMI input and I'd like my computer to be able to get the most out of it.",2
"Sequence diagrams when the detailed interactions between a set of objects become complex - this is extremely useful in modeling complex render flows where previously computed information is needed at barely related downstream locations Both talks discussed when they created and how they maintained these diagrams, to one extent or another. As for myself, I use diagrams all the time - simple notepad drawings for personal projects, simple UML diagrams at work. This UML diagram is what I'd consider too complex, and one that I'd never make because the cost of producing and maintaining it outweighs its benefit, but of course YMMV. Object diagrams when things like instantiation of objects becomes something bordering on the creation of a Frankenstein monster from disparate parts - especially useful in kitchen sink vertex and pixel shader users, to make sure all of the requisite bits are pushed through the pipe How you use UML should be driven from these considerations. Diagramming for your own sake? Use whatever notation you are most comfortable with. Collaborating with other developers? Try to include the details of API calls, message types, directions of dependencies. Discussing architecture? Black boxes and simple connections will suffice. No-one uses the full set of UML features anyway, plus it is very useful as a set of standardised notation that many people understand - whereas my napkin doodles may be incomprehensible to you and vice versa. (The other answers have done an admirable job of demonstrating why and how design-focused diagrams can be tremendously helpful in planning, building, growing, and maintaining a codebase, so I'll leave that aspect alone, and trust these resources might be useful to anyone visiting the question.) It may be worth pointing out a few design diagram talks from the Game Developers Conference 2013.  These are some very practical and road-tested examples -- and it seems they've been presented at many conferences across the years. Noah Falstein gave a talk called ""The Arcane Art of Puzzle Dependency Diagrams"" (paywalled GDC Vault video).  I can't find any non-paywalled link here, but various folks have discussed or posted their notes on-line. I certainly do -  both structural and behavioral - my rule of thumb is that I make diagrams when the cost of making the diagram is less than trying to remember what the hell I was thinking a month later - or when I need to clearly explain myself to some other developer Diagrams are a great way to communicate, document and aid your design, and design is the most significant part of software development. UML has a lot of features but you are not meant to use them all at the same time, only the ones that are useful. Furthermore, diagrams are great for communicating and documenting your design, either to non-technical people or people who are new to your project - and remember, in 6-months time you are practically new to the project too! When navigating in a new city, do you actually stop and look at a map, rather than just continue and follow signs? That's what design vs coding is about. When things are unfamiliar, when the problem is complex, when you feel lost, that's when thinking about design is most helpful, and it's better to do it earlier than later. It's much easier to change your design before you've implemented anything. Joris Dormans and Ernest Adams discussed the Machinations game design/balance diagramming system.  (Here's a paywalled GDC Vault video from GDC EU 2012; samples from GDC 2013 on Dormans' wiki.) Rather than attempt to paraphrase, here's how the wiki describes the system: Diagrams are a great way to visualise the problem and help your design, especially for visual thinkers (which is most of us on gamedev I'd imagine). A lot of problems become trivial, defects become obvious, when it's clearly mapped on a diagram. Some issues you may find in a diagram:",3
"My first attempt using a different method involved making the host names within IIS different and then editing the HOSTS file appropriately. This enabled me to navigate to multiple websites on the host machine. However, I could not do the same from other machines on the network. As for the multiple IP's. There are ways around having to use multiple IP's. Even if you are using SSL , you can use a multi-domain certificate , some call it a unified messaging ssl. Another option is to use a wildcard certificate, but I don't really care for these. If there's a reason for you NOT to do it the way I'm about to suggest please let me know and I can adjust my advice.  However... On your IIS box set up all your webs using host headers, they can all be the same IP address.  Then in your DNS make sure your internal view resolves all of those domains to that IP.  Use DNS internally, not the hosts file - that should solve your problem. An alternative if you don't plan on needing SSL is to use host headers to host multiple websites off one IP (Called VirtualHosts in Apache speak) This is so that I can set up different domain names within an intranet to point to different web sites. Does anyone know of a way to do this? If you aren't running an internal DNS server, you will have to put the same host file entries into each of the computers that you want to access those sites from. If you are running an internal DNS server, then you can add 'A' records to allow the client machines to lookup the host names.  *Note, if you want your site to load both domain.com and www.domain.com , you will need to add both of these if you are hosting multiple sites using this method In IIS7 - adding host header's has changed. To get to it, under the Sites tree, select the site you want to edit. In the Actions pane, click Bindings (also in the right click menu). You can then click edit to change a pre-existing binding or add a new one. In this next screen you will be able to enter a hostname.  The other answers are great - If you don't have internal DNS, you will need to edit the host file of each. Here is a technet article talking about how to set up host headers in IIS7. I made my instructions follow these a bit.",4
"http://www.prime-expert.com/articles/a05/enabling-multiple-partitions-on-removable-usb-storage-devices.php My system is almost instantaneous. Visual Studio 2010 loads in under 3 seconds while I'm playing a vid on MPC-HC at the same time and cruising the web. Not bad for 9 year old system.  THESE ARE INSTRUCTIONS FOR Win XP ONLY AND SHOULD ONLY BE ATTEMPTED BY PEOPLE WITH INTERMEDIATE TO ADVANCED COMPUTING SKILLS,-*- or people that get a kick out of frying bacon in the nude. ;-) Caveats: NEVER symlink inside a symlinked folder. If you do, you're in for more problems than you can imagine. If you decide to also symlink the desktop itself, do not use any shortcuts that point to program files from the desktop. In fact delete all those shortcuts from the desktop and use Start --> all programs instead. Nested symlinks will wreck havoc with antivirus scanners too sending them into an infinite loop that will more than likely lock up your system.    If you're dead set on doing this,there is a way. Apparently you are aware of the dangers involved. That said, before you do anything, create a backup of the system. Personally I use drive image XML, or 're-do' backup. You'll need an external HDD to Bkp to. Re-Do is burnt to disk ( I burn with ""starburn 9.8 ) and can recover a totally hosed un-bootable system. You will also need a burner prog later on in this process to burn a copy of a linux distro.    BTW - I did this because high capacity PATA/IDE ssd's are expensive, but I needed speed.  All progs recommended here are freeware that I have used myself and like. This process should work on desktops and laptops both. I still have 32GB of space, it just gets used 8x faster than normal - lol. Aaaah the price of speed.  I decided to write a reply because there are really no good tutorials on this subject online. I had to do this with my system, because in order to get the speed out of an SSD running Win XP the sectors have to be aligned, and I aligned mine at 4K sectors, which are blindingly fast, but which reduce my effective SSD size from 32GB to 4GB of usable space.",1
"I am using DokuWiki (wiki without db backend, just text and attached files) for three years, recently tried twiki (this seemed to me to be too slow, but it has wonderful plugins) and Redmine (more programming oriented - features, projects, bugs ... nicely integrates with svn/git repositories, has its own wiki). I've used different tools for maintaining the documents like bitrix, share point etc. But faced one issue in all of them that they provide the full service with the premium package but all have some same and some different features.  Using it from anywhere is a huge plus in my book.  The web client lets me get at something if I'm on a strange and foreign computer. I maintain a list of useful documentation links in my browser bookmarks.  On top of that a Documentation folder on my hard drive for the small number of things not available online.  Most times I just Google for the manual in question when I need it since maintaining a local copy of online manuals is a waste of space and more or less a loosing battle. I use Evernote, and pay for the expanded service (though I used the free version for a very long time).  Being able to email anything, add notes from various browsers, access from the Blackberry, etc. all make it worthwhile.  The paid version holds attachments of all types, which I've found useful over time.  That and it gets rid of the darn annoying ad.  Probably why the ad is there in the first place. I do cleanup every couple weeks to move out anything I forgot to file in the archive once I'm done with it.  I've tried to start a GTD organization with it, but I'm not organized enough for that yet.  But I'm getting there... My issue is not only saving or handle my documents, but i was also managing a team in which I need a document handling or document management system. I've found 3 giants in document management industry Laserfiche, AIIM and Onbase. i am using Laserfiche which is pretty good. You can have look to all of them which is better for you",4
"You may experience the problem that is described in this article if the CcTotalDirtyPages value is close to the CcDirtyPageThreshold value.  In Windows 7, for example, I can use Performance Monitor and use the ""Dirty Pages"" counter (one of the cache counters). This counter does not seem to be available in Server 2003. Also on Windows 7 (and other later systems), I can use Sysinternals RAMMap and effectively see the dirty pages on a file-by-file basis.   A little search on the question brought me to this KB article - http://support.microsoft.com/kb/920739 -    Is there a way to view the number of dirty pages (cached file pages that need to still be written to disk) in Windows Server 2003? May not be your best choice but should solve your problem, where to identify count of dirty pages , you will have to resort to the below steps - Use the !defwrites command in the kernel debugger. This command displays the values of the kernel variables that the cache manager uses, and it displays the values of the threshold and of the actual dirty pages that are in the cache. When you run this command, you may receive output that resembles the following:",2
"i have a win 7 machine with an Atheros L1 Gigabit Ethernet 10/100/1000Base-T Controller with latest drivers, the machine is upto date with all latest fixes etc. I have it connecting to a WRT310Nv2 router. Seemingly random, win 7 disconnects from the ""Home Network"" says its in a ""public network"", then resets the connection to an illegal 169 address. I have tried static ips, dhcp, all with the same results. This seems to have started shortly after i installed Vuze, so I uninstalled it but the problem persists. I know that the router is sound given that I have an XP machine attached with no issues of connectivity at all. I am at a complete loss and have tried everything, pleasse tell me i'm not the only one. It can have relationship with UPnP. Windows 7 use it for discover network. Try use non windows firewall to block upnp, i think it is UDP packet port 80 and not authorized incoming connections. OK I want to make sure I understand this, even if you set a static IP on that interface it will still reset back to DHCP and 169.x.x.x?  I've seen a few cases where the NIC seemingly randonmly ""goes to sleep"" because it decides it's time to save some power. So make sure you have the latest version, and check for new services or startup commands that were installed with Vuze. I haven't seen the behavior that you describe, but you might want to check through the advanced settings for your driver in the Device Manager to make sure you don't have any power-saving settings enabled for the NIC.  I had a similar problem after installing Adobe CS3 on my system. It turned out that the bonjour service that was installed with the software was causing the problem. I disabled the service, reboot the machine, and it started working again. I personally hate Bonjour, but to be fair, it was a 2-3 year old version of it.",5
"Am I missing something here? Or is everybody missing the fact that it seems you are assigning dynamic addresses (with DHCP) to servers? We are renting a Windows root server at Serverloft. Recently, when the server restarted after installing regular Microsoft updates, it restarted properly, but couldn't be accessed anymore, and a Linux server was answering instead! After convincing the hotline that this was not our mistake (which took some time), they found out that some other server in the same subnet somehow (they didn't explain how) ""stole"" the public IP of our server (or rather ""took precedence""). It also helps to ensure that a server doesn't get a new address on restart, seemingly making the server disappear. There is nothing you can do to prevent a system admin from assigning a static IP address to system that may conflict with your own.  And if that machine happens to be in the same vlan as your own box then they will come into conflict (just ask a network guy who has had a user assign their machine the same IP address as the router how much fun that is).   They disconnected the ""thief"", and for a short period of time we could see our server again. Then, without restart, it happened again! After another hour or so our server was back. One possible scenario is when the machines are ""Virtual Machines"" under VMware or Hyper-V. What people usually do is that they create a Reference machine and clone it as and when people request it. So, typically all hardware settings are copied to the 'cloned' machine too.  If we go to the basics, the IP is assigned to a NIC, and the DHCP Server assigns IP's to NIC's.. and NIC's are identified by their MAC addresses.. This sounds like user error on the part of another system admin.  If you are using dynamic assignments (DHCP) then permanent leases or reservations can make this less likely to happen.  Your hosting provider could also implement smaller subnets or private vlans to make this less likely to happen. Question: does this make sense (we are simple developers who don't really know)? And is it possible to prevent such a scenario? Or can anyone in a typical hosting environment simply ""steal"" another IP, provided s/he knows how to do this?",4
"As long as you don't need to push too much data back and forth it would allow you to keep your ""program"" isolated from something they control. I've previously been tasked with sourcing tamper evident seals and have not found anything I could recommend. In all cases the adhesive could be softened, usually with either turpentine or kerosine, the label removed and later reapplied. Done carefully you would never be able to tell the seal had been removed. Make sure you set the BIOS so it can't boot a USB stick or PXE boot or boot from anything BUT the installed HD. If you can't do that, you could have your program check in to a central server over the internet.  If your program can't reach your server it disables itself.  Basically it could check in and respond to a challenge algorithm...   (you want to prevent the server from being spoofed) Physical access is tantamount to simply handing it over on a silver platter. But if you have to, you have to. If you machine has TPM then you can have ultimate protection. You can use trusted GRUB with encrypted partition for your program. This will give you protection even if hard drive is physically removed and connected to another host - the OS cannot be booted from it and encrypted partition cannot be read. You wouldn't need to even lock your chassis. Obviously, user should not be given 'root' access under any circumstances. There have already been some really good ideas posted by others, so Ill just build on the physical security. I suggest you use physical seals instead. I prefer the old fashioned type consisting of wire and a crimped lead seal. Primitive, yet surprisingly effective, which is why they are still being used for many security situations.",5
"Unfortunately, breaking down the data in these tables hasn't helped. For e.g. 100 rows may represent a single entity to the end user similar to a document per say, so each row represents a section of a document, these updates treat the ""Document"" as a single entity which may include actions like merging section information, adding new sections, deleting old sections, updating content, moving sections around etc. I'm working on a SQL Server 2008 R2 DB and I'm experiencing some performance/concurrency issues with probably the main table in the DB. Problems like slow reads, deadlocks, poor performance in general. 60% of users will read from this table while 40% will write to it. The problem I'm faced with is that the writing is actually a collection of big updates to about 50-100 rows at a time, doing stuff with re-parenting, updating FK's, updating chunks of HTML, etc. These writes are a problem when multiple users are doing similar updates while others are trying to read. Even without reading, the updates alone are an issue.",1
"Computers that are configured to automatically register in DNS do so every 24 hours.  Before re-enabling at the server level, you may want to export the list of records (so that you can properly sort by datetime) and spot check for records that are not updating but should be.  One scenario where this can occur is if you require secure updates, but a computer may have been re-imaged at some point.  In that case, the new computer account may not have permission to update the record. http://blogs.technet.com/b/networking/archive/2008/03/19/don-t-be-afraid-of-dns-scavenging-just-be-patient.aspx  Before you enable scavenging on the domain zones, you need to ensure that scavenging is disabled on all DNS servers.  When scavenging is disabled on the zone, the timestamps are not replicated.  After re-enabling scavenging on the zone, you need to allow time for all machines to update their records and have their timestamps replicated.  After two or three weeks, you should be able re-enable scavenging at the server level.   Assuming that your AD DNS infrastructure was implemented with Windows Server 2008 or Windows Server 2008 R2, and not a hold out from Windows Server 2000 or 2003, then the _msdcs.company.local sub-domain that you're referring to is a delegated zone. You shouldn't actually see anything in the sub-domain except for an NS record for the DC/DNS server that the zone has been delegated to. On each DC/DNS server this NS will be the server itself, as each DC/DNS server is authoritative for it's own copy of the AD integrated DNS zones. Can you post a screen shot of your zones? The _msdcs.company.local zone is the one being used for lookups, since it's more specific - the copies in the company.local zone are not being used to answer queries (which you can verify by making a change there and checking the response) and can be safely dumped. Scavenging is not enabled by default on _msdcs.  You may want to think about that before proceeding.",3
"I have an IIS(v6)/Windows 2003/.Net 3.5(app code, libraries etc.) server hosting a website at www.mywebsite.com mapped to E:\Inetpub\wwwroot\mywebsite,  we also have a virtual directory (VirtDir) mapped out to E:\Inetpub\wwwroot\mywebsite\files (although in theory this could be in a different directory or a separate machine) where we store a customer's files(a bunch of .pdf & .xls).  Currently to access a file you can enter into the url something like: www.mywebsite.com/VirtDir/Customer/myFile.pdf and get access to the file.   The problem is the user doesn't have to log into www.mywebsite.com to get access to the file, we would prefer them to log in first.  We would like the user to login via the mywebsite and if valid, let them download files from the virtual directory. The www.mywebsite.com and VirtDir are separate sites on the same farm.  Allow Anon Access, and Integrated Windows Authentication both enabled. I'm more of a developer and less of a Sys Admin, but hopefully I'm in the right spot, any help would be appreciated.",1
"Additionally our programmers are asked to manually save a full backup whenever they make considerable changes to the database.  The full backup is checked in to our source control repository. We'll assume that the programmer can run/tailor a script if the functionality is not in SQL Management Studio. Is it possible to save a full backup that does not affect the maintenance plan?  My research suggests that a database snapshot might be appropriate, but I'm just looking for a backup without spawning a new database on the server. It occurs to me that a manual backup will affect the daily incremental backups done by the maintenance plan.  If I understand correctly, a programmer could take a manual full backup on Wednesday and the automated incremental on Thursday will be dependent on Wednesday's full backup instead of Sunday's.  Not good. We use a centralized SQL Server 2005 server for web development.  The server has a maintenance plan to take a full backup each Sunday and incremental backups on all other nights.  All databases use the Simple Recovery Model, so transaction logs are not a factor.",1
"You are making a session even if the login was invalid. Later you only check if a session exists, not if that session was valid. This allows someone to log in by entering a wrong username, or a right username with a wrong password. Your sessions are based around com_create_guid, which I believe is cryptographically secure. The generated string is also long enough that guessing a session guid is practically impossible. You are making sessions less secure by essentially wanting a session to last forever, increasing the time they could be used by some malicious third party. I do not believe this to be a great risk in this case though. You are making sessions where the $userid is undefined. This means you cannot do anything useful with the session, as you lose information about who is logged in. Move the salt to a config file somewhere. You don't want to have to dig through your code when you have to change the salt. If you do not want to have a config file, at least put it as a static or private variable in your LoginManager. You are also setting the expiration date with $Date, a misspelled variable that is not defined either. I am working on a project where users have to log in then if it is correct it creates a cookie to process requests. I would like to find a way to auto-renew sessions every day without affecting the user or requiring the user to log in again. You are using invalid_username, invalid_login and some more magic names that are directly related to the login procedure. Factor those out and put them as static properties on your LoginManager class, so the actual contents are irrelevant and you can simply check against LoginManager::INVALID_LOGIN to see if that is the state of the session. You specifically asked about sql injection and session hijacking. I believe your code to be reasonably safe on both accounts. If you are using truely prepared statements (not a setting that uses sprintf behind the scenes), you should be safe from sql injection. Your queries may still fail, but you should not be able to execute unintended sql queries. Your LoginManager should also not disqualify usernames or passwords based on length, as this is a thing that should be enforced when setting a password or username. The login will always fail in those cases, so don't worry about it. You want to do the same amount of work for every login attempt to prevent an attacker gathering information about valid usernames/passwords based on the time it takes for a request to return. I would like to also know if I overlooked any security flaws such as SQL injection or session-hijacking and all those, are there any other one that I could prevent or improve? One thing to note with your SQLstatements, you are retrieving more fields than you need to. This results in more memory usage and slower performance. While it may not make a huge difference for you, it's still best practice to only select the fields you need. You are creating a date with gmdate(..) which is a UTC date. Later you are parsing it with new DateTime(..) as a local date and comparing it with a local date. This causes sessions to last longer or shorter based on where the server is. You should always keep in mind that queries may fail. If your username query fails, it will return false, and sizeof(false) === 1. Your code would then error out on $userdata['password'], which is not that great.  You will also want to make sure that the username field is unique, as otherwise you would get back the first password for that username but not necessarily the correct one. You are currently not doing anything with your expired sessions. That means they are not removed from the database after they expired. Creation and renewing of sessions should happen in their own class. Other code doesn't need to know how to make sessions, just that a session exists and who is authenticated by that session.",3
"DCE vs DTE. Ethernet (10base-T and 100base-T) has a TX pair and an RX pair; TX has to go to RX and v.v. When connecting like kind devices -- hub to hub, computer to computer, a crossover cable is necessary because they have the same pinout; a straight cable would connect TX to TX, etc. Note the difference to optical fiber: since this hasn't been used for analog signaling, fiber uses a crossover in every connection - plenum and patch. Usually, you've got an odd number of cable segments, so receiver and transmitter are crossed as required. There are both cross and straight connections. In the past it was important to use specifically one or another - depending if you connect two devices directly or via hub/switch. Today most NICs and switches do auto discovery of the cable type and act accordingly  Originally RJ connectors were designed for telephone standards. One design feature was you could put a smaller plug in a larger socket and it would connect to the first pairs. To achieve this the first pair was on the center pins, the second pair on the pins straddling the center, the third pair straddling those and so-on. Most modern networking gear supports ""auto-mdix"", which means they can logically swap their tx/rx pairs. (old hubs/switches had a switch that would physically swap the pairs.) Gigabit (and beyond) technology uses all four pairs for both TX and RX. 10BASE-T implementations chose to use this layout and chose to use the Second and Third pairs. Presumably the second and third pairs were chosen in an attempt to allow coexistence with basic voice service on the first pair. Unfortunately this was not good for high speed data. The splitting and straddling of pairs was bad for signal integrity and this got worse as the pair count grew. However there was a desire for backwards compatibility with basic phone applications. This resulted in a compromise pair layout. The first pair was in the center, the second pair straddled the center, but the third and fourth pairs did not straddle any other pairs instead being placed with one pair down each side. However, connecting two network devices requires the transmitter on the one side to talk to the receiver on the other side. To avoid additional crossover cables, the specification called for one pinout on the ""host"" side (MDI for PC NICs, routers, ...) and a pinout with an internal signal crossover on the ""network"" or concentrator side (MDI-X for repeater hubs, switches). If you now wanted to connect to PCs (MDI to MDI) or a switch and a hub (MDI-X to MDI-X) you still needed a crossover cable. Later auto-MDIx came along allowing automatic switching, so it did not matter if you used a crossover or straight-through cable. I am astonished why RJ45 8-Pin connection was configured from the beginning as the connection is a little wired while Straight Connection is comparatively easy to understand and connect. Today, nearly all ports support automatic pair-selection (Auto MDI-X) that you can mostly forget about crossover cables. Historically, the first twisted-pair Ethernet, 10BASE-T, was designed to use the already commonly deployed voice-grade cabling (Cat-3) with straight-through pinout. (10BASE-T's predecessor StarLAN wasn't offically Ethernet) Ok, that explains the pin allocations but what about crossovers? Well 10Base-T and 100Base-TX use one pair in each direction. For correct operation the transmitter at one end must be connected to the receiver of the other. That means we need to cross-over transmit and receive somewhere. Most of the time this was done inside the equipment, network cards used ""MDI"" pinout while hubs used MDIx pinout. So they could be connected with a straight cable. From time to time though it was nessacery to connect a MDI port to a MDI port or a MDIx port to a MDIx port, so a crossover cable was needed.",5
"The general concept is that individual queries will be stored in individual includes files and will be reloaded periodically using javascript. I am trying to figure out how to solve the problem preferably without having to learn a completely new language.  I am experienced in PHP, so I have been trying to use that. We are a textiles manufacturing plant with many PLCs on an automated line.  We would like to create a display accessible over the Internet which will display key performance indicators in an easily digestible format. As you can see, each piece of dynamic data is being reloaded as a php page every 30 seconds.  The reason I think this might be inelegant is that the complete code for the performance display has about 58 fields.  That's 58 seperate php files with potentially 58 different connections to our database, if I understand that correctly. I am a novice level web developer/junior level SQL DBA/junior level systems administrator acting as the primary net/sysadmin and DBA for our US site. Basically, I have managed to get the system working, but I want to understand if this is a poor solution and/or if there is a better solution that I should work towards.",1
"We want to make the sub domain have a SSL certificate, and redirect it to Intercom help center. This is the article (video) that we use but the one difference is that we do not have the domain at AWS:  We have our support articles in Intercom Articles and need to attach a SSL certificate to the subdomain support.packaly.com. Intercom does not offer its own certificate so we need to do it through AWS. The complication is that we do not have our domain at AWS but at Google Domains.  https://developers.intercom.com/installing-intercom/docs/set-up-your-custom-domain#section-how-to-configure-ssl-with-aws Try this, https://medium.com/@getsee/make-your-intercom-help-center-https-with-a-custom-domain-2255cba66f50 that worked for me, most resources on the web state you need to set up an alias to point to the cloudfront distribution, but CNAME was the only method worked for me Is there a workaround for connecting a AWS certificate for SSL with a Google Domains subdomain and redirecting it to the help center?",2
"For shares, I can create a batch file that will NET USE the different fileservers we use here but that is a huge security risk as I will type my password as plaintext. A Windows (&Samba potentially) domain will use a single sign on system called Kerberos to authenticate you when you log in.  Once you have been granted a Kerberos ticket you can access any ""kerberised"" service without having to type your username and password again - such as your file server.  For some reason, the IT department at our company does not want to add Windows 7 and Windows Vista machine to the domain controller. You may have some success by ensuring your local Windows account uses the same username and password as your domain account and for instance mapping the network drive.  Also you can frequently save your credentials in the applications - but this obviously a potential security issue and as anybody who has ticked Outlook's save password box can tell you it frequently doesn't work. Is there a way to mimic the behavior of a machine that is added to a domain without actually adding the machine in the domain? I hate to always provide my network credentials everytime I access a shared folder on a machine that is joined to the domain. I also hate to always provide my password when I launch outlook or Visual Studio (Team Explorer).",2
"however,  I think you're over thinking the problem.  if you just start with what is already there I bet you will get most,  if not all,  of your devices.  As an alternative approach, you could use the mac address-table notification feature in conjunction with the relevant show outputs.   One possibility I know is ping sweep, but that might take a long time (longer than 5 minutes of aging time) and I'd like to see if there is any alternative. Unless you've got thousands of hosts a ping sweep should almost certainly complete before the switch MAC table caches out the entries. Have you run a ping sweep to check the results? If I run show mac-address-table dynamic I get a bunch of mac addresses on that table. But due to the aging parameter, I don't see the mac addresses of all possible devices on the network that come through that switch. Is there a way to force an update, perhaps pinging a broadcast IP or any other way, which would include all possible mac adresses?  This feature will keep a history table of all changes to the mac address table - so if you take a copy of the mac address table at time A and then enable this feature and monitor it for a few days you should end up with a pretty comprehensive world view by combining all the changes with the initial list. The switch only learns about MAC addresses when a device sends an Ethernet frame to it. So the only way to get the CAM table populated with all of the devices is to get all of the devices to talk. Have a look on the sticky mac-address feature. Some info may be found here e.g.. In general, sticky mac-address enables your switch to learn all possible MAC addresses that can appear on the specified switch interface and then save them into the running config. You can write those changes then into the startup config and you'll have all MAC addresses from your network saved and persistant. Furthermore, you can have it send SNMP traps so you can collect all the reports easily from an SNMP daemon.",5
"My find_path function particularly seems like it needs work.  Seems like I should be able to calculate a path through an 8x8 grid with a recursion depth of less than 1000. If you are going to return a value from your main function, you should pass it to sys.exit() so that it actually functions as a return code. I'm trying to implement my own A* pathfinder, but the voices in my head scream of better ways to do this.  Probably because I'm exceeding Python's default maximum recursion depth of 1000. Don't put mutable values like list as your default parameters. Python will not create a new list everytime this function is called, rather it will keep reusing the same list which is probably not what you want I suggest a function get_blocked that you pass cell to which check for 'X' and also returns True if the cell is off the side of the map. Also catching IndexError may not be quite what you want because negative numbers are valid indexes, but probably not want you want. I suggest checking in the get_blocked function for inboundedness.",2
"The idea is new enough it isn't in a lot of file-systems yet. Btrfs is still 'experimental' in Linux, and ext4 is recently out of that state. I use ZFS and love it.  I know that BtrFS also supports checksumming.  I've been told that checksumming is available in Windows 2008, but I've been unable to verify this. Windows is somewhat famous for having one and only one filesystem, NTFS. I also have been unable to dig out anything that suggests NTFS has checksumming. The closest I've come is a document describing the checksum protecting the GPT blocks on a dynamic disk. The ""What's New in 2008"" doc doesn't show anything like that for NTFS (link).  might i also suggest that providing ZFS via iSCSI or NFS to a windows server is also a method of gaining the benefits of ZFS for a windows server? whilst not directly attached to the server, there are plenty of file-servers out there that use an iSCSI target out of a ZFS appliance as their primary storage. Some ZFS-like features can also be had with other file systems by introducing a logical volume manager to create virtual devices. Logical volume managment on windows is referred to as dynamic disks and volumes. RAID-0, RAID-1 and RAID-5 modes are supported. When you heard that ""checksumming is supported"" on windows, it probably referred to the fault tolerance of RAID-1 and RAID-5. However in reality these modes only protect against easily detectable device failures (disk missing, read errors, file system corrupted...). Checksumming all data is not supported so subtle data corruption can go undetected. The closest thing that windows does provide is SIS (single instance storage) - stores only 1 copy of a file - several steps away from what ZFS actually provides.",5
"Since you probably initialize your weights randomly, you might want to change many of them in the beginning of training but you might want to disturb less and less weights as they become more trained. That's why you decrease the radius. This formula is just a particular policy of change of the radius with each iteration. In this case it is exponential. You can choose another policy or keep the radius constant or decrease it slightly (not exponentially) which may be reasonable if you use some domain knowledge when initializing the weights. Also, you can keep the learning rate constant if you like. The radius of the neighborhood and the learning rate are hyper-parameters. It is up to you how to choose them. The radius r_ij is the radius of the neighborhood of the winner. Only those neurons that are closer than r_ij to the winner are allowed to update.  The formulas that you show here say that this radius is not constant but decreases with each iteration. Initially you take a large radius, and then make it smaller and smaller with each iteration.",1
"... and on a totally different note, my RPi will shortly be running from a 6v (plus 5v regulator) motorcycle battery charged by a solar panel and wind turbine. Application details-A python script running to collect the energy meter via Modbus and sent to AWS cloud. A 40-Watt solar panel in 10% use (typical European climate) and a 50-Watt turbine will deliver 200% of the RPi's requirenents. 50A/H of storage will keep the RPi going for about 4 days (rain and no wind :-). I didn't find this mentioned elsewhere, but as with anything you're expecting to run 24/7, critical or not, you should have backups. The affordability of the Pi would allow you have a spare standing by, or a networked slave receiving regular backups.  How you all are running Raspberry Pi continously for 24x7?. When i did , i faced multiple issues such as when running python application script, it gets hanged and needs to be restarted. Not exactly headless but a number of us run Raspberry Pi's as media servers for months on end without powering down. Any issues I experienced were due to power outages and most it recovers from just fine. The reliability of other machines is limited by the higher powers they eat. 3W (24/7) for the RPi is easy to generate. Yet another reason for choosing the RPi for a reliable solution.",4
"I don't have first-hand experience of the engine itself, but it definitely sounds like it's worth some experimentation. Personally, I would recommend Unity. In comparison to Torque I've been able to get basic proofs-of-concepts running far faster than before. (Of course, there is a bit of bias as I was far younger when experimenting with Torque) You might want to take a look at this tread on the Unity forums.  There's also someone working on an RTS ""starter kit"" but I can't remember who. But in general, it's been a far more pleasing experience to work with. Also in comparison to what I remember from the community at Garage Games, the Unity community is far more available to help with issues that my arise. My Digital Media friends are currently using Unity for their game development. They seem to like it because it makes it relatively simple to add and use custom 3D objects and the script is rather easy to handle( none of them like programming, but they seem to be doing an OK job at learning the scripting in a semester's time ). I would add one disclaimer before you rush in though. Unity has an awful GUI system. Everyone I know who is using Unity in a professional capacity has had to buy a GUI system (I believe NGUI is a popular choice). RTS games often have a lot of UI elements and precious little of the stuff that Unity make easy (things like Physics, particles etc.).",4
"e.g. I published my Scroll Away UserScript on GitHub that changes the style to position:absolute (keeps the elements visible but they will scroll with rest of the page): Right click on the offending bit to start a Lizard session. Then you can use key commands to blast away the bits you don't want. I'm working on house projects and finding inspiration images on BHG.com. Their pages have an annoying header that takes up 25% of the page height. Once I have more data about the typical class names/div ids for fixed headers, I'll look into making a domain-independent stylish style that makes these classes/ids absolute by default. Fixed (unscrolling) headers are getting less popular as of January 2016, but if anyone still wanted to manipulate the fixed elements programmatically, they are easy to retrieve in plain JavaScript: After months of dealing with this problem through either Firefox's ""inspect element"" or really painful editing of my Firefox userChrome/userContent files, I've found that Stylish is easily the best solution for this problem.  It's still not automated, but it's far faster than anything else I've found and it persists across visits to websites. javascript:(function(){var%20found=true;while(found){var%20elems=document.body.getElementsByTagName(""*"");var%20len=elems.length;for(var%20i=0;i<len;i++){found=false;if(window.getComputedStyle(elems[i],null).getPropertyValue('position')=='fixed'){var%20el=elems[i];el.parentNode.removeChild(el);found=true;break;}}}})() This is an improved version of ccpizza's code. It fixes a bug where only one top level element is removed instead of all of them. You could use a plugin like Greasemonkey. Then just add a bit of custom javascript to edit the css for just that element on that site. It should be there again next time you visit the site.",5
"Plaintiff: Well, Bob was late to work that day and tried to punch the clock while I was using the bandsaw. There are none, at least with support for recent technologies (except for very basic CSS, JavaScript etc...).  Defendant: Sure, everyone knows that. We'd just wait for the person to click the Clock In or Clock Out button and then the saw would come right back on! Prosecutor to defendant (Bob): Did you know that the bandsaw always stops while the time clock web site is opening? I would consider running screen sharing software.  After doing some research, it sounds like VNC was never ported to Windows 3.1, but you could try running pcAnywhere 9.2, which I believe is the last version that supports Windows 3.1.  I think the most important thing is to only run stable software on this machine, as there is no memory protection, no bug fixes in many years, and it's controlling an expensive and dangerous piece of equipment.  You can find another thread which discusses browser alternatives for Windows 3.1 here.  They seem to think that Opera is a bit unstable, and other browsers probably don't support all of the modern CSS that you need. It's obviously a lot more destructive then just installing a browser... but it is both capable of running Windows 3.1 software in addition to more modern software like Firefox 3.6. Plantiff: I didn't know Bob was late! The saw stops when a part gets stuck. How was I supposed to know that it would suddenly start back up while I was looking for the part in there..? Chances are that your low-cost programmer already has Windows 3.1 as his primary OS on his 486 DX33. Err... Do you really want to give a second task to a computer from the stone age that is controlling a $150,000 bandsaw..? If your web-based timeclock software has only 2 or 3 menus with 4-5 options, I guess that asking a cheap programmer to develop a Windows 3.1 application that fetch data on the website and sends the appropriate data would be the simplest way to go.",5
"Even if your problem is due to some other issue running the wireless diagnostics utility that comes with OS X may identify the source of the problem or at least give you some useful information to aid in isolating the problem. You can also identify this problem by running the command /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -s - look in the ""CC"" column in the output. If you see any differing country codes that is likely the source of your problem and your Wi-Fi connectivity will periodically drop due to the conflicting country codes. I've found that I can turn Wi-Fi off and on to immediately restore WiFi connectivity, but unless you can track down the offending wireless router and get its owner to replace it, you may not be able to permanently resolve the problem with the iMac if that is the cause of the problem. Or you might be experiencing a problem I've encountered with a MacBook Pro laptop due to some neighbor's router using a different country code. Some routers will advertise a country code in the information they provide to nearby wireless devices. Windows and Linux devices don't seem to have a problem if a router is advertising a country code that conflicts with a country code advertised by another nearby router, but Apple OS X systems may intermittently drop Wi-Fi connections in such situations. You can check for such a problem by running Apple's wireless diagnostic utility. To do so, click on the WiFi icon at the top of the screen while holding down the option key then select Open Wireless Diagnostics. If that is the source of the problem, you will see ""Conflicting Country Codes"" listed in the results summary. If you click on the ""i"" to see further information, you will be informed: On an OS X system, you can view details on a WiFi connection from a Terminal window using the command /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -I. You will see something like the following: The agrCtlRSSI value is a measure of the signal strength - the ""RSSI"" stands for received signal strength indication. The closer the number is to zero the stronger the signal; weaker signals are indicated by numbers closer to negative 100. The agrCtlNoise value provides an indicator of how noisy the signal is; less negative values are better, so in the above example the -95 is not good, though the signal strength level is ok. The numbers may give you an indication of whether the iMac is experiencing some issue related to the Wi-Fi signal.",1
"Booting off a CD and connecting to the old laptop is easy for long time network nerds like me, but can be daunting for the beginner. If you run into problems doing it post here and I'll try and help. Use snapshot (www.drivesnapshot.de). Assuming you have enough disk space on the old laptop you can snapshot the old C: drive to a file on the drive (i.e. snapshot the disk to itself; sounds odd but it works). Now boot the new laptop of a WinPE or BartPE CD and map a drive to the C: disk on the old laptop. Run snapshot and you can restore the snapshot onto the new laptop. It's not particularly hard, but things can go wrong especially in the partition resizing department. I suggest you make a trial run (in a non important disk/machine) if you have the time an resources to do so. (If you're uncomfortable with dd you may also use partimage or Ghost for Linux. Real Men use ""cat'.) CloneZilla(i link to wiki as the site is down at the moment), or NortonGhost may be of use here, although there will be an element of risk involved. You may be better getting this done professionally in order to avoid data loss unless you are entirely comfortable with what you are doing. When Snapshot restores the image it can grow it to use whatever size disk you have in the new laptop. Best of all there is a free eval version that will be fine for the one occasion you need to use it. Boot with a Knoppix CD/DVD, connect the new drive through USB and use cfdisk to partition the new disk with a bootable partition that corresponds to the size of the old drive -- you'll later format the rest as ""D:"". Then do something like Re jacobsse's comment, see http://www.ratsauce.co.uk/notablog/WinPECD.asp for my walkthrough of making a WinPE boot CD.",4
"If you get an empty cluster, it has no center of mass. You can simply ignore this cluster (set k=k-1 for next iteration), or repeat the k-means run from a new initialization. You can also choose to place a random data point into that cluster and carry on with the algorithm if you must have this specific number of K clusters. Three random cluster centers are initialized. At the end of first iteration points 3, 1, 2, and 7 will be in one cluster. 4 and 5 will be in another cluster. And 6 will be in the last cluster. Note here that the distance between 3 and 4 is larger than the distance between 4 and 5 and so 4 is assigned to the cluster represented by 5. Before we begin the second iteration we update the cluster centers and the following picture shows the centers and the clusters at the end of first step. Now, the cluster center for the red cluster moved closer to point 4 due to 1, 2, and 7. And the cluster center for the blue cluster moved away from 5 due to point 4. In the next iteration point 4 will decide that it is closer to the red cluster and point 5 will decide that it is closer to the green cluster. This will cause blue cluster to be empty as shown below.",1
"However, I'm going to assume since you're a company not a home user you do want something which will last a few years. This is an excellent demo of the Hyper-V server product (free) setup using Netapp NFS as shared storage. During the video Matt gives a lot of excellent advice and background on how to get a cheap HA virtualization environment working. Well worth your time! In the ""Microsoft Services for NFS"" window mark all entries, (""Client for NFS"" can be skipped) and press """" There's really only 1 NAS storage company totally buying into virtualisation, and that's Iomega, a division of EMC who owns a large chunk of VMware. They make the Iomega StorCenter ix4-200d NAS server which runs iSCSI and NFS, and is certified by VMware to run shared storage for their systems. Price is around $1000 for 1TB of usable storage. Some of the ""cheaper"" entry level SAN type kit from Dell would be the MD3000i & the Equallogic PS4000 series but you're generally looking at the wrong side of $10k or more for those. Alternatively there are some pretty good cheap NAS boxes that are definitely good enough for small scale Branch office VMware type setups that could be a viable alternative but remember that you are planning to depend on this stuff so opting for a real entry level NAS would be a very bad idea. I would not personally build my own whitebox SAN although you probably can build a fairly decent high performance storage server these days for a couple of grand. The problem is that you will be the only person who will be willing to support it and that's really not worth it in the long term. LeftHand Networks Virtual SAN Appliance is one route you can take. If I recall correctly they support up to 2TB of storage. Basically you set up local volumes, present them to the VSA and it will give you a virtualized iSCSI SAN. It's a bit of a hack but it does work.  Download and unpack ""Microsoft iSCSI Software Target and Management.iso"" (WinRAR freely available from rarlab.com can be used) Click on ""Management and Monitoring Tools"" -> ""Details"" -> ""Storage Manager for SANs"" and Install. Disk 2 will be asked for. Whether it will give enough performance and reliability for your sites, I can't really say without knowing quite a bit more about your requirements. You should be back at the previous window, where you have to click on the user and the group in the top list, and click on Allow -> ""Modify"" for both. The vid is at http://edge.technet.com/Media/Hyper-V-Server-2008-R2-Bare-Metal-to-Live-Migration-In-about-an-hour/ The cheapest of the cheap ways would be to take an old PC or server and run FreeNAS or Openfiler on it, and present the internal storage as an iSCSI or NFS share. In the ""Manage Server Connections"" you can enter the names of the hosts allowed to connect to the target Here you can either pick ALL MACHINES, or use the ""Add"" button to enter IP addresses of the allowed hosts. Give the hosts (or all machines) Read-Write permissions and click ""OK"" We've got VMware as our enterprise VM solution using a SAN but we're looking into the possibility of using virtualisation in-branch.  This might mean ESX or Hyper-V.  As I understand it, to be able to do failover with either host, you need shared storage.  What are our options, given a pair of host servers?  Particularly, at the cheaper end of the scale. In the ""Other Network File and Print Services"" click on ""Microsoft Services for NFS"" and press the ""Details"" button In the ""Windows Components Wizard"" window click on ""Other Network File and Print Services"" and press the ""Details"" button ""Microsoft iSCSI Software Target and Management.iso"" (downloadable from http://microsoft.download-ss.com/, after free registration)",5
"One of my fav games is Guild Wars, you can only get so powerful and that's the end; but what makes it really fun is the skills system - you can only take 10 skills with you into battle - so picking out the best combination is very entertaining and testing skills builds is very fun as well. So even though there is a level cap you still have tuns to explore and new skills to unlock. The Player is never over powering and the only grind comes finding those special skills or looking for collectables and appearance oriented content. The mid-game often features the player exploring the environment for gameplay options. Now, maybe they're setting up strategic forts, or collecting equipment with special effects that have situational uses, but still simply trying to survive to accumulate more stuff. An important thing to realize here is that the player's perceived difficulty is going to be a function of both their power level, and that of the enemies. If you give the player a huge power boost but give the enemies an even greater boost, the game will get harder in spite of the player's power increase. So it's a matter of what you want the difficulty curve to look like in your game. Do you want it to get progressively harder, or easier? Do you want to time the hardest points in the game to coincide with the most dramatic parts of your story arc? You can do it any which way and there's no ""right"" or ""wrong"" answer here, it comes down to your design goals and then finding the best difficulty progression to give the player the play experience you're after. In the early-game the goal of the player is to simply survive, progress, increase in level, get the next equipment with a +1 bonus, etc. Generally everything the player finds is better than what they currently have - whether that's a new fishing spot in Civ, or a new sword in Nethack. In a game like Oblivion, the difficulty is that the player has to make choices in the strategy building in the early- and mid-game by choosing how to level up that can have drastic consequences late-game because there's no option to change that strategy later. It's how you've built your character. The same is true in Diablo 2, and many other games that allow you to select skills for your character without the ability to change them later on. A skill-based character advancement thus takes a bit more fore-thought into how individual skills can be used to build a cohesive strategy. In Diablo 2 you can see how the developers struggled with it by looking at how often they tweaked the skills over the lifetime of the game. It's easy for an inexperienced player to reach a point where their permanent strategy choices are no longer effective against the opponents they're facing. That doesn't necessarily make the game unbalanced - it simply turns the skill progression choices into part of the player's strategy, and may require several playthroughs to develop an optimal strategy. One thing you might think about here is looking at more strategy-oriented games like Civ, Starcraft and roguelikes such as Nethack and Crawl. In these games, game difficulty and player progression are not simple single-variable functions. The gameplay evolves over the course of the game. I think i'd set up a system to track player metrics; for instance, in an overhead shmup, what weapon the player favors, the types of ships he's best at killing, how much health he's lost lately, etc. and mix it up so that the players that are doing well get more tough enemies and less powerups, while players that are struggling get enemies that they can fight and better tools to do so.  The more traditional way around this is simply to divide enemies into areas, where moving to the next area means harder enemies, and it's up to the player whether to move forward now, or grind for a bit in the previous area to ""level up"" to make the next area easier. You can see the same progression in Final Fantasy games - in the early game, most enemies can be killed with a simple attack, and maybe a couple healing spells as needed. Mid-game enemies begin to appear that are more difficult, but with obvious elemental or status weaknesses to exploit. Late in the game, the enemies might be practically unbeatable except for clear well-thought-out strategies based on less obvious weaknesses or AI script. A lot of players enjoy feeling like they're progressing and getting stronger, so removing progression from your game is not an automatic solution. For games in genres built on progression mechanics (like RPGs), removing that core would make for a relatively unique game... but one that you'd have to be careful about since you'd be treading in unfamiliar territory. I concur with Tetrad; On the one hand I am a huge fan of JRPGs and I can be a total stat whole - collecting all the best equipment making sure my uber spells are totally uber etc.. but that is a grind and the grind has to be fun. FF series does a good job at making grinding and leveling fun because they pace the level progression with the storyline very well. I wouldn't throw the player into an arms race by any means, but if it's a campaign type game (like a stealth shooter or something) then just sitting around while you have things to do should definitely ratchet up the pressure. I'd start with reading this series on balance by Sirlin, so we know what we're talking about when we say that a game is ""balanced"". Go ahead, i'll wait! The advantage of a dynamic difficulty system is that all a player has to do to get the ""lower-level"" areas back is that they just need to play poorly; the game will think they're having trouble and give them an easier time. The late-game forces the player into a showdown, usually with the toughest opponents, but also with the most options at their disposal. More experienced at playing the game, now not only must the player survive, but also exploit the effective weaknesses of their opponents with the tools they've discovered and refined from mid-game forward. I'm designing such a shmup (shameless plug: the blog for it), and i plan to do that. In short, the player will have to hunt enemy ships and peril the dangers of space to gain enough energy to fight the bosses, as well as look for upgrades for their ship. The thing to remember is that players will naturally want to progress through your game, so you can use that to drive your game forward. Having enemies scale (as with Oblivion) is one way to prevent the game from getting too easy; the danger here is that sometimes players can feel as if they can never get ahead: the enemies level up with you, so it doesn't feel like progression. It doesn't HAVE to be this way, but I've seen the pattern in many games from console RPGs to PC strategy games, and even handheld tower defense games. CCed here for those that TL,DRed still. While that says multiplayer games, it largely applies to single-player ones as well. However some other game types really do make it a grind and it becomes extra boring. Then there is the problem which happens constantly in FPS shooters is that unless you give the bots auto-aim headshots the game is a breeze. The player is usually way over powered - has much too much ammo - and a gigantic assortment of heavy weaponry.",4
"It's a bit crude but effective. And practical in many cases (probably in all but the most mission critical ""can't ever be down"" systems). The ""&&"" in the command makes sure that if a previous command fails it will not continue. However if the mysql database itself is not that large and the system's I/O speed reasonably high you could easily do this withy minimal down time, just by chaining a few commands together and running it at a time usage is low. I've successfully done this a number of times for critical mysql databases. Perhaps announce the maintenance and do a few test runs on a test system to find out how long it takes. Setup a second server as a slave. You probably want a master/master setup so either host can take writes. Move the IP over to this second machine. Now you can do your maint on the first machine. This will minimize the downtime for the MySQL service.  I'm sorry to say that the answer is no, there's not. If you start moving the filesystem while mysql is using it, you'll get your database entirely hosed.",3
"The KACE appliances aren't perfect, but it's by far the best option we found and it seems like they are working hard on improvements. I'm not a system admin (I'm a programmer) and managing the 70 PCs in our company was becoming more than a side job for me.  We bought the Dell KACE 1000 and 2000 appliances and I've been very impressed with them.  It takes a lot of the complexity and ugliness out of systems management.  It has a simple, intuitive UI and makes a lot of tasks trivial.  There is a learning curve at the start, but they have a kick start program where you get 4 hours of training up front to help you set up each appliance. After you plug the K1000 box in and install the KACE agent software on each PC, the K1000 starts to collect data on all the machines.  Within a few hours you can see every piece of installed software, outdated drivers and missing software patches.  You can then use the K1000 to push out missing software, install missing drivers and patch outdated software.  All our PCs now have the latest BIOS, drivers and software.  We also applied all the patches, so Windows 7, Adobe Reader, Flash, Firefox, etc. are fully up to date. I'm using (Microsoft) System Center Configuration Manager to do this + operating system installation (images). there are quite a few commercial offerings in this space like KACE from Dell (I believe there is a Microsoft specific offering in the field as well)  With the size of your network it may be worth deploying one of these solutions. Secunia integrates with SCCM and or WSUS to identify, prepare packages and deploy updates for any publicly available software. Maybe Windows Package Manager would be useful for you. There is a default repository under the Apache license with more than 100 applications.",5
"I currently run 10 EC2 instances 24/7 and bandwidth and storage are by far the lowest costs to factor in. If you take my Linode VPS instance with 1024MB RAM I pay just around $40/month for it to run 24/7. Compare that to even an Amazon Small (m1.small) EC2 instance which has 1.7GB RAM running 24/7 for 30 days you're talking 720 hours at $0.12/hr you're looking at $86.40 just in hourly run time costs. You can quickly see how that would add up if you ran multiple instances. If you ran a new Micro EC2 instance that only provides 613MB RAM you're looking at around $21.60 which would save you from the VPS but you've got less RAM which could handle a small static website but if you're using a database that's usually not enough to handle things. I'm thinking about using an Amazon EC2 instance for hosting a few websites of mine. A personal homepage, a blog, etc. Is the cost low or should i grab a hosting package from HostGator or similar?  If you're wanting a root-account level system to host your personal sites to then you will find a VPS solution to be a much more cost advantage; however, if you're running a site that needs near instant scalability and redundancy where cost isn't an issue then EC2 instances are an ideal choice. Given the pricing model bandwidth is not that much of cost issue, however when you're talking running an instance 24 hours a day it becomes another story. I highly recommend to rent a ""normal"" VPS if you are familiar with Linux system administration. A VPS is cheaper and often has useful features, such as a VNC access when the system won't boot up. No typically amazon ec2 is more expensive for regular web hosting. Especially when you only need web hosting and wouldn't need shell access (unless you want to do some special stuff on your personal website).",4
"If those are deal breakers for you, the ultimate solution is to create JS script file like the one below, that will run your command in hidden window, wait for your command to terminate and return its error code: Check the ""Run whether user is logged on or not"" check box and executable will run in the background. I've got a daemon process that I run on my machine every hour, and even though I've checked the Hidden box, it doesn't work. Every hour, the task runs, but it shows the black command window, in which my .NET Console app is running.  This stays visible until the task completes, and then disappears.  This is very annoying, because it pops up and interrupts whatever I'm doing: will prevent the black command window, but be aware of the drawbacks : you won't be able the work with some GUI automation tools like 'AutoIt' or SendKeys in PowerShell, as Windows launch the tasks in a headless style environment.   I leave the default settings and write short custom programs that spawn the execution without a window showing.  For example, to run a batch file without popping up a command prompt window, use the code below.  Set the task to spawn ""RunBatchFile.exe (path_to_batch_file)"".",5
"They are different things - The Command Prompt is Not MS-DOS - but as far as the user is concerned they could be the same thing as they do the same things. (Unless your definition of equality does not extend past It is a text interface and I can run programs from it.) Things were different in Windows 95, 98 and ME where command.com would be run in a MS-DOS VM with Windows acting as the hypervisor (yes, they had that sort of thing at the time already). There you had an entire virtual machine running DOS. But on Windows NT, 2000, XP, Vista and 7  no. DOS only lives on there in ntvdm.exe which is the NT Virtual DOS Machine which is just a thin emulation layer capturing calls that the CPU cannot execute directly (which is why it works faster but worse than DOSBox). So it depends on your point of view. From a technical point of view your friend it correct, but from a user perspective you are correct (sort of as there are differences that an expert would spot). What is run when you click Command Prompt in the Start Menu is the Windows Command Processor, a.k.a. cmd.exe. Its built-in commands and scripting syntax (including many quirks) are based on the ancient command.com from CP/M and later MS-DOS, but apart from that they are completely separate things. Also, command.com is a 16-bit program while cmd.exe is a native Windows console application. Inside, I actually cringe each time I see people referring to a window with gray-on-black text as MS-DOS. In the vast majority of cases they don't actually know what they're referring to.",2
"However you could test this case (It's properly not the problem) by setting a different DNS server (See here to see how to do it with Windows) to one that will mostly give you the correct IP (Google DNS as an example 8.8.8.8 and 8.8.4.4). No, DNS is not streaming any contents to you. All it does is translating a domain (http://xyz.com) into an IP-address (111.222.121.212). The IP-Address is then used by your computer to connect to the real server, which provides the content then. The only thing that could happen is that the DNS resolves your Domain directly to a server instant to a load balancer. When the the server allows the connection and many other people use the same DNS server and get to the same server the speed of the server itself could a problem. I am trying to find out the real problem. My friend said the DNS service ( unlocator.com) can be the culprit. But I thought DNS service only takes care of the geographic checks and nothing else, and hence it shouldn't impact the video streaming quality. They do this by having custom DNS entries for certain sites (like say, Netflix.com) so that when you request those domain names they are redirected to a transparent proxy which makes you appear to Netflix like you're coming from wherever that proxy is located. I am using a DNS service ( like unlocator.com) in order for me to watch Netflix.com because I am located outside US. Recently, I found that Netflix ( and also Amazon Prime) videos are very slow and keep on buffering. However, whenever I do a speed test, the speed is always good ( ~5Mbps). Usually with this kind of speed, I can stream netflix videos well, no problem. But not recently. DNS services like Unlocator.com are not just DNS services, they are for making you appear like you're in a different country so you can watch country-specific content, while not being in that country. I was having issues with streaming quality for NHL.tv, even on a connected app built into an Xbox One S. Quality was horrible. This was highly unusual given the Xbox was connected directly to a powerful DD-WRT router and speedtest was showing 60mbps down and 6mbps up. Troubleshooting at the NHL.com website suggested changing back to ISP's DNS servers, which I did. I was using OpenDNS servers. This solved the problem, I'm now back to high quality streaming. Since all the traffic is being sent through this additional proxy, this proxy can become congested, causing slowdowns in traffic between you and Netflix. You are right that DNS is just used to look up addresses, and I can see why you'd be confused as to why someone would suggest that different DNS services can slow down your actual transfer speeds.",4
"I tried xcopy first as the user suggested but then I used robocopy which is included in Windows 7 and this one succeeded: In the past, I've used TeraCopy. Another option would be a block-level copy--for example, using dd or a disk cloning utility. Personally, I would boot from a live Linux CD and use dd_rescue/ddrescue if the data is important. The fastest way would likely to be to use xcopy through a Command Prompt instance, in a context similar to: The /C flag forces xcopy to ignore any issues with copying; the /E flag orders xcopy to copy folders (even empty ones), and the /Q flag makes it a quiet operation (otherwise, you'll get an entry for each thing copied). Might require closing paths in quotation marks '' if there are special signs in names, like whitespace. In order to run, open Powershell console (available in every Windows since XP), change -path and -directory parameters to your start and destination folder, copy the command to PS window and press enter. If you can't copy, enable quick edit mode in console properties - this maps paste option under right-mouse button in Win7. I came across ""Roadkil's Unstoppable Copier"". It has as graphic interface and was designed specially for those cases. This run in Powershell should copy all files and subirectories to specified directory. Will continue copying despite some error with specified file.  For every broken file that command can't copy, you will receive a lot of red output in console. If you don't want to see these, change erroraction to silentlycontinue.",5
"I can think of a couple ways to break the explicit dependency between these two classes, but I think this one is the least complex and still achieves good decoupling: I agree, it does seem a little gross for each state to hold a reference to the Game just so the state can request a state change. But, in a way it makes sense: the Game class is acting as the state manager -- it keeps track of the current state and it knows how to change states. Bonus is that if you later change the state implementation, an example would be to add hierarchical states, then all the changes are internal to the state classes and not spread all over outside files. I'm creating a game engine which utilizes a state machine following this GameDevGeek tutorial.  My concern is the use of circular dependency which I've heard is bad.  The game engine has the game state but the game state needs access to the game engine for a state to be able to transition to a new state.  Is this a proper way of doing it?  Namely my implementation, includes, and forward declarations.  On another note, why does syntax highlighting randomly turn off after two files have #included each other?  Here's my code: Any GameState::changeState calls that used to pass an instance of Game will still work, since Game would be a sub-class of IStateChanger; so why is this any better if we're still passing the Game instance around? Add static members and methods to the class GameState to handle the responsibilities that you've forced into the main Game class. Down in the GameState.cpp source file you are invoking functions of Game, so you need the signature there. Simply add #include ""Game.h"" heere and you should be good to go. Basically, this won't change your current code that much, except instead of the GameState referring to an object of type Game, it would refer to an interface/abstract class type that declares a function for changing state. So something like this: Each instance of GameState will then have access to one, shared state environment like they do now, and Game doesn't have to carry around the baggage. Your issue can be solved by moving some includes about. The inclusion of file containing the declaration of another class is only needed when specific details are required (which functions are publicly available, how much space should I reserve for a field of that type, etc.).  Since your GameState header only ever uses pointers (references might be better suited here so others don't mistake it for an array and whatnot) to a Game object, you can get away with a forward declaration there. This is because the size of a pointer and reference is independent of the size of the data they point to; the compiler only needs to know about the existance of the class/struct at this moment.",4
"3) Nod32 v4 Anti-Virus on Windows 7 slowed down all my MySQL ODBC queries! It was fine on Vista and XP. I uninstalled Nod32 and the apps that use MySQL are fast again! As others have stated, there are lots of kinds of slow-down, so you may need to be more specific. But I have three guesses: I wouldn't expect it to be slower.  Could you have anything else in play like a database or web service call?  I would recommend testing a simple static page with a for loop (i.e. 1M iterations)  Compare the time.  Then start to bring more of your site back into the picture to see what is causing the differences.  IIS7 should perform as well or better in virtually very case.  Any chance you've moved to 64-bit?   As others have indicated, if correctly configured, Win7 on the same computer is unlikely to be noticeably slower. On a faster computer, there ought to be improved performance. How much slower are you talking about? It should be expected for your site to run a little slower, since you are running a newer operating system. XP was extremely stable and streamlined, while Windows 7 is just out and most likely unstable and slower. Of course if there is a significant slowdown something else is wrong. 1) There is one very common kind of web app slow-down that users going from XP to 7 will experience. Windows 7 (and Windows Vista) install IPv6 by default. Firefox and Chrome each operate very slowly over IPv6 in certain situations on Windows. If you are experiencing slowdown with Firefox (pages take forever to load, and images pop in really slowly), but not with IE, look into disabling Firefox's IPv6 support. This sped up my web apps in Firefox GREATLY! 2) I noticed that when I moved from XP to Windows 7 that my web applications started taking a LONG time to start (as long as 20 seconds, while those same web apps started in 4 or fewer seconds on the same hardware with Windows XP x64 and IIS 6)! They were very responsive after startup, however. There could be a million reasons for this, and I never figured it out. I still have the problem. But, I worked around it by using Visual Studio 2008's built-in web server, Cassini. Now I use Cassini to develop and only use IIS when I have to. W7 uses much more resources than XP. Running a local web server requires more power one should think, since You got several apps playing ping pong while sharing the same resources.",5
"You could use Windows Home Server -- either by buying a HP MediaSmart etc or by getting a copy of the OS.  I believe that it is include in the TechNet subscriptions and available retail.  The P3 you mentioned above would easily run it. WHS has a drive mirroring technology -- not really RAID but similar.  This mirroring technology allow you to decide which data needs to be stored redundantly. It has a connector that is installed on Windows client machines to automatically backup the machines. Freenas is great.  I would look at the rsync options in freenas to get the redundancy you want/need if you don't want to spend the money for the sata card It may not be quite as cheap as reusing an existing box, but it's likely to be quieter (I replaced the fan on mine - easy and cheap), smaller, and take a lot less effort. Personally I have a Thecus N2100 which I've been very pleased with. Recently one of the drives died, and when I'd worked out which one it was, I simply ordered a replacement, followed the instructions to rebuild the RAID array, and all was fine again. In the meantime, I still had read/write access. I backup my most sensitive and critical files (financial, personal records) to matched set of thumb drives that stay in a fireproof safe.  Other stuff (pictures, home videos) go to DVDs. These can easily be stored off-site.",4
"I agree with the other answers on checking hardware/environments/configurations match first, but if nothing obvious comes up, start looking at query execution plans, running SQL Profiler etc. This has happened to me on a few occasions. Each time it turned out to be environment: someone else was hammering away and starving the db of IOPS on one server. Once you've ruled that out, it's time to dig deeper into the hardware: is the work being spread out across all the CPUs, has a network port renegotiated itself to 100Mb. Run vmstat and/or iostat on both machine and compare the differences. The other answers are good, but I would add that you should consider the amount of data in Environment B, and any contention with other queries. Run a top(1) on the slower server and see if the CPU is experiencing high amounts of wait states, or cpu stealing if you're in a virtual environment.  Some SQL queries show no performance problems in isolation (e.g. 1000 rows in table, no other queries running) , but can be horror-show with 10,000,000 rows in table (e.g. parameter sniffing issues), and/or other queries potentially writing to, updating or locking the tables involved. If the datasets are identical, does the same query generate the same execution plan on both? Do tables contain the same numbers of rows? Are the index definitions identical? Do the tables have similar levels of fragmentation? Similar numbers of active connections? In short, you need to isolate whether the db itself is slow compared to the other, or its environment is slower. Rule out the easiest things first. This will also help point to missing indexes that cause execution plans to perform full table scans instead of index scans (but that's easy to spot with slow query logging). This will also show up in ps as procs in a D state.",2
"The web interface may not give you many configuration options, so may not be of much use (I haven't used it before). The real question though is why is the switch not allowing a flat network by default?  It may be that the connection between the two switches is not working.  Is it copper or fibre?  Are you getting a link light on both ports.  if not, check the cable.  If the connection is fibre have you tried crossing over the fibre? When switch reloads it will have a default config and should allow a flat network out of the box.  You may also be able to wipe the config using the web interface. Once you are connected to the CLI, enter the following to enable the web interface (replacing the IP address with something on your network, and the interface number with one of the free interfaces) To enable the web interface, you will first need to connect to the switch using a serial cable, connect to the port marked Console on the back of the switch.  Use a terminal emulator (e.g. Putty if you are using Windows) and open up the COM port the serial cable is connected to.  If you are using a USB-Serial cable, you may need to install a driver first to virtualise the COM port. Now connect a cable to the port in the above and connect to https://192.168.1.80 (or whichever IP you have used) Is the switch new?  If someone else has used it before and it contains their config, you could try the following to wipe the config.  From the CLI, using the serial cable as mentioned earlier:",1
"I am trying to use the case 2 which is common, but not sure what is the strong reasons why common two IP addresses for two nameservers in one server? (2) Now I have I am using Rackspace Cloud Server and I maintain a cPanel. Because of limited IP v4 availability, I cannot buy an IP except for HTTPS and load balancing. So currently, I only have one IP assigned to my cPanel server. The most common reason for a server to have multiple external IP addresses is in case the internet connection goes down.  If the Internet Service Provider the server is connected to goes down or gets unusably slow, the server can switch over to a different connection. This can also help if a cable gets cut or unplugged. Changes to your primary DNS in cPanel will be passed out to your secondary DNS servers through dns-axfr zone transfers, so you will have full control from within cPanel. Case 2: However Rackspace has DNS server virtually located just outside from the cloud. If a domain pointed to this DNS, I can manage the records from Rackspace client area and A record can be set to my cPanel server IP address to serve the hosted web site. By this way, I have two IP addresses for their respective nameservers provided which is like the common case, but I don't have the ability to edit the DNS records in cPanel UI and CNAME records must be defined manually each time new domain registered. Multiple servers can share a single IP address through clustering.  That way, multiple servers can be treated as one.  If one goes down, the others take the job of the failed server.  Also, servers sharing an IP address (through network address translation or clustering) can sometimes see what name was used to connect, so requests to name1 at IP1 can go to serverA, and requests to name2 also at IP2 can go to serverB.  This is usually done when a company doesn't have or doesn't want to use enough external IP addresses to give each server an IP. Case 1: cPanel has built-in DNS. cPanel user with the domain pointed to this server directly are able to manage the DNS records in the cPanel UI, it will autogenerate CNAME for cpanel., webmail., ftp. subdomain access, but remember since I only have one IP address, my 1st and 2nd nameserver defined in the cPanel WHM pointed to the same IP: What I know the two IP addresses used like if one unreachable, the other will use as a secondary choice and user still can access the same server. (1) I am not really understood how two IP addresses (can be more than two) is useful and making sure network up time 99% because both IPs pointed to the same server (I think both may have same physical network path or am I wrong?) Anyone can explain? I am not quite following your English here, but what I think you are asking is how you should best go about having 2 name servers when you only have 1 IP address and 1 server. The idea is that the two IP addresses for the nameservers are in completely different subnets.  That way if a router goes down somewhere or for some other reason a subnet becomes unreachable your server still has a way to perform DNS resolution. Well, the best thing I can recommend is that you run your primary DNS server om your cPanel server and assign that to ns1.yorudomain.com, and then use a third party to act as a secondary name server and assign them as ns2.yourdomain.com (or ns0 and ns1 whatever numbering scheme you like). Also, individual servers can be given different IP addresses for different reasons.  Different IP addresses can map to different services the server offers, or to provide load balancing on their network to make sure one part of the network isn't too stressed.  This is useful if they need more bandwidth (sometimes incorrectly referred to as speed) than they can get from one internet connection, because they can send some data through different internet connections.",4
"If your DNS servers fall into PCI DSS scope, you may be forced to run AV on them (even though it's downright silly in most cases).  We use ClamAV. This could've been a knee-jerk reaction to the shellshock bash vuln, it was suggested online that bind could be affected. There are a lot of DNS security vulnerabilities and issues that should be addressed in an audit.  They will never get to the real issues if they are distracted by bright shiny objects like ""antivirus on a DNS server"" checkbox.   The first thing you need to understand about auditors is they may not know anything about how the technology in scope is used in the real world.     I would speak to the auditors and ask them to clarify the difference in their requirements between PCI and PCI supporting, just to make sure that this requirement didn't accidentally sneak in. We did need to make sure that our DNS servers met hardening guidelines similar to the PCI environments, but anti-virus was not one of the requirements we faced. The important thing to recognize is that while DNS servers do not handle sensitive data, they support your environments which do. As such, auditors are starting to flag these devices as ""PCI supporting"", similar to NTP servers. Auditors typically apply a different set of requirements to PCI supporting environments than they do the PCI environments themselves.",4
"You need a DNS server and a DHCP server to accomplish that. Windows from the XP try DNS based resolving first, and NetBIOS after. So you can easily setup a DNS server (on CentOS BIND is preferable) to resove these names. Install named (this is the name of BIND in the Red Hat world) with the yum install named command. Set DHCP to send DNS suffix to clients .local for example.  The problem is that I can only reach the Linux box by IP address, not hostname. When we installed CentOS on that box we picked the option to report the hostname to the DHCP server. But it doesn't work; I can't ping the box by name. This is odd, because the Linux-based NAS box (a DLink DNS-323) shows up in Windows Explorer just fine, and I can ping it. named.conf.local will load this zone with your servers, and when you try to access server1, DNS append the suffix, and resolve the server1.local perfectly.  Do you know anything about DNS? That's the Domain Name Service, it translates names into IP addresses. If you have a DNS server currently internal on your network, you need to ensure that your clients are using it, and that the Linux server has an entry in it.  I've got a small business network. We have a few Windows machines, a Linux-based NAS box, and a Linux box on the network. Everything is connected to the Internet through a DSL router (Netopia 3000). You need to setup a local DNS server setup to hold records for your computers, DHCP doesn't do that... Assuming these computers need to use the internet too, you need to setup root hints for domains that you don't control so your server knows where else to ask for IPs. If you can't, you're going to have to update the HOSTS file on every machine in your network, and then update it every time you add or change a hostname.",4
"I think the reason they aren't is because of the potential to cause confusion.  If you have a character that looks like a ""?"" but has a different unicode code - how will you tell the difference? The reason is probably because it would be too hard to deal with files with those characters in them in the old command.com shell, e.g. ? also means any one character, * also means any file, etc. You can use those characters now yourself. But I'm assuming you mean to have the operating system automatically transliterate between an ASCII question mark, for example, and a lookalike such as  (SMALL QUESTION MARK - UFE56). I really don't think that's satisfactory, especially since Linux and others, where the only invalid characters for filenames are slash (/) and null (ASCII 0), accept those characters readily. How would you explain it to someone?  ""You can't have a question-mark in your filename, but you can have a thing-that-looks-like-a-question-mark-but-isn't, and to type it you only need to use this 5-key combination.""? There wouldn't be Unicode equivalents for those characters, and if there was an equivalent, it still wouldn't solve that problem: the existing ASCII ? and * would still have to work as wildcards, otherwise everyone would have to re-write their scripts. Rather than teach everyone how to handle those special cases, they disallowed it, making it easier to script.",3
"In a single player, or even a multi-player with under 12 players, the designers can control how much content needs to have drawcalls made on them at any given time.  Additionally they can control fully how many textures any scene, or area if you will, has.  Since they can fully control these assets they are able to turn the model detail far up the spectrum with High poly model and richly detailed textures. In an MMO, by contrast, a zone can be devoid of any other players other than yourself, or there could be hundreds, and each and every character is often wearing different texture and shader combinations.  MMO's, however, can look very nice if the creater get very clever and efficient with their use of well designed meshes that are applied to models.  And this can compensate a lot on the graphics difference between mmo's and stand alone games.  But in the end the graphics detail level of an MMO will never be the detail level of a stand alone game with equal system requirements.  Resource management just doesn't permit it, at least not if you want an MMO framerate above 5. It is also the case that MMOs often have many things on the screen at once; this can differ based on the game, but some games allow many people to be standing in one area (for example if there is an important event or party). That is a lot of polygons; so each character must be low detailed in order to be able to draw them all on the screen at once. Similarly, even in the fighting areas there can be a generous number of NPCs/mobs on the screen at once. Level-of-detail algorithms can be used to tune the detail levels based on how much is on screen, but then you are introducing even more content that needs to be made (each thing would then need to be made several times -- or an algorithm can try to do it automatically). Having done A LOT of research on MMO creation the single biggest issue on graphics is that a designer really has no way of knowing how much content will be present in an area at any given time.   Take a game like Skyrim, single player game, lush graphics, high detail.  But if you pay close attention, there are seldom more than 5 or 6 moving ""characters"" at any given time on screen, and when there are more if you carefully look around you'll often notice that they've cut back a little on the amount of material present in the scene.  Additionally games like Skyrim know's exactly what every character will be wearing in each scene. MMOs often have much more content than traditional games. If you think about it, your traditional one-player game has a mostly linear storyline, so the levels are very straightforward, linear, small and restricted. They can be scrutinized and made to a high level of detail. A MMO, however, has an entire world's worth of content, so a lot more art and content is required to match that same level of detail. Your point is valid. But keep in mind that a MMORPG has more models, textures and animations than a normal game; making those assets in a very high resolution would need a lot of disk space, and they wouldn't be easy to stream to system/graphics RAM. So they are kept low-res to easily load a higher part of the world, and to make the development of the game shorter. The problem in an MMO is you have little control over how many ""assets"" are going to be in a given area at any given time since players can wear what they want, where they want, when they want.",3
"The 84 million rows are all in the same table.  That table alone accounts for 90% of the whole database. I have about 84 millions rows.  Of those all of them needs to be transferred to a separate database on the same server, then I delete to delete about 60 millions rows from the source database. Even minimally logged, those are big transactions, and you could be spend a lot of time dealing with the ramifications of abnormal log growth (VLFs, truncating, right-sizing, etc.). Dropping partitions out of a table is a really fast and resource-efficient way of removing large chunks of data from a table.  Were this table partitioned in a manner that supports your source / destination split the answer would be to restore a copy, drop the redundant tables and redundant partition(s) from destination and drop the complementary partitions from source. I would add that, however you decide to approach this, you'll need to batch these transactions. I've had very good luck with the linked article lately, and I appreciate the way it takes advantage of indexes as opposed to most batched solutions I see.",3
"Note that the ""power surge"" was most likely on the network cable, not through the power lines.  If you have a cable modem the point of entry would have been the cable connection.  The place to install surge protection is on the cable before it reaches the modem. It's doubtful that the the damage would ""spread"", save for the possibility that the adapter may have been damaged somehow by the surge. In my case, though, it wasn't an external event; at least, I don't think.  Whatever caused the caps (all of 'em) on the sound card to pop just ranged through everything it could get to electrically.   I'm pretty sure that it will not. A similar thing happened to me but both the card and the slot were dead. The rest of the system worked until it was replaced some years later. As always, each case is a little different.  I once had a sound card that bricked itself; in the process of it expiring, every other item on the PCI bus lost its magic smoke, too.  A couple of other PCI cards, as well as the on-board devices that were part of the bus.  CPU and RAM survived, the drives themselves survived, but the MoBo was a total write-off.",3
"Now that I typed everything out, it seems clear to me that approach two is the better option. However, are there other suggestions or other possible improvements here? I am building a system where visitors can buy modules from my website. For now I only have 3 available, but of course this number will keep growing.  Furthermore, the second option is more powerful because it lets you easily add other information about the purchase of a module by a particular user.  For example you could add purchase_date to your user/module table.  You could also add the actual_purchase_price to that same table.  This would allow you to keep track of things like changes to the price you charge for modules or whether you sold a particular module to a particular user at a promotional price. Converting repeating columns into rows in another table is perfectly scalable and will help keep your source code more stable because you won't be adding columns to your (option 1) table when you create new modules. One table called ""user_modules"" where a column represents each module, and a boolean (1 or 0) corresponds to either they have the module or not. I'd go with the second approach. That way you can add more modules in the future more easily as you just have to add a new entry in the module table. Whereas, if you go with the first approach and you want to add a new module, you'd have to create a new table. The second approach conforms to Third Normal Form (3NF) which is always a better place to start than something which is un-normalized (0NF) like your first option. In order to facilitate this in my MySQL database I am doubting between one of these two architectures, mainly concerning about performance and scalability.",3
"Which is quite a barrier to entry especially for Internet Explorer users as the process to import a root certificate can be long and confusing for non technical people. This works great with one exception: when a user first connects to our WiFi, we have to ask them to install our CA root certificate to be able to MITM their HTTPS traffic to filter out bad content or they get a message like this when trying to go to any site over HTTPS: We also have a number of on-site machines that need the root certificate, but we can roll it out to those with an Active Directory GPO so that's not a problem. Am I missing something? Is there another approach to solving this problem that is easier for the end-user / doesn't require them to do anything? Currently we're running an instance of Untangle with the Web Filter, HTTPS Inspector and Captive Portal add-ons. This problem also affects our captive portal, as we need to redirect users to the captive portal to log in - which is impossible over HTTPS without MITM the request / response.",1
"The previous ones to this question are all worthy answers, but they are all colored by large-scale thinking. If you have only a single host, providing web sites whose failure will not immediately kill kittens, you may want to think somewhat smaller. I would suggest the following: I would like to add: cleanups of old users, old cron tasks, etc. Passwords renewals. Evolution of system configuration (looking at monitoring history) (and this includes evolution of monitoring and backups) with the life changes of the server. Security audits. Custom reports (i.e. detailed usage of services, shared resources, security reports). And... You ask for scripts. Maybe there are generic pre-packaged scripts, but it is more about DIY (Do It Yourself). So first you learn or read how your system works (this includes distro specific REAMEs, man pages, policies and compilation options), then yo think how your system should behave, and then you write custom scripts (cron, custom commands, etc) to manage the combination of O.S. + desired behavior/results/reports/change-management/rollbacks/reactions/etc. However, the essence of systems maintenance remains the same: kill each issue that occurs dead so that it can never occur again. This is an important point, because it means that there should be no ""regular"" maintenance. And as well as the backups, the monitoring system + customizations, should be validated where possible, simulating failures, attacks, etc. Just wondering what are a regular maintenance you need to perform on your Linux server? Are there any scripts that let you do the maintenance? and a lot of others .. of course it is supposed that system is already hardened (at least with default SElinux targeted policy), Maintenance is done by monitoring. If you have an machine exposed to outside access you have to have some checking tools:  I've seen firewalls survive 500 days of uptime (yes, that is a BAD maintenace, because there are about 10/15 kernel upgrades a year, so big uptimes, use to be equal to vulnerable kernel) without any human maintenance. It's just a good config on partitions, syslog and no major configuration changes. Also, available generic tools (monit, nagios, cacti, zabbix, etc) can be extended with your scripting skills and available third party plugins. Custom scripts are better, when you can validate them and clean bugs, on a pre-production server(s), environment, directory, database, etc. I have a server that host few websites, I haven't done any maintenance for about 2 month other than looking at a couple logs and compressing them.  In example: do you need only to know ""if partition is full"", or do you need too to control disk temperature, free inodes, files and folders rights, checksums, S.M.A.R.T. status, bad sectors, filesystem checks, and estimated life cycle of physical hardware? Use logwatch or similar system that aggregates your logs and mails you a summary. Read the summary at least every other day. Use a tool that crunches your weblogs (e.g. analog) into a readable summary. If you want to be a bit more ambitious, use a cloud service to monitor your sites. Restore backups randomly (without needing to do it). Also always include a copy of the  MBR (Master Boot Record) if you do not use an automated deployment system to restore the whole server. Among the first issues you have when you scale up is that something breaks and you don't know it. This must never occur again. Then you get a full-scale monitoring solution. Also, having seen the issue occur on one host, you want to prevent it occurring on any other host. That's when you get a configuration management system. Patch your OS to get the latest bugfixes and security updates.  This is the single most important step you can take.  Beyond that, you need to have monitoring in place to tell you the state of your system over time.  I have to disagree with the point above that monitoring is not maintenance.  You need to get a baseline understanding of your system and how it runs to see when things are not working or need maintenance in the first place.  The only way to really do this is to graph everything.  We use a combination of Nagios, cacti, and ganglia for this. would be great to do performance monitoring with nagios,zabbix and others .. (to keep on eye on out of ordinary events and status of the machine)",5
"As reference for people stumbling upon this with the same problem, this answer to a different, but similar question gives steps for doing the above.  Best option it seems, given stated constraints, is to draw the compressed texture to a quad then read it back out. (Same as suggested by ""Simon F"" in the question comment thread above.) Desktop OpenGL requires implementations to convert pixel data between the internal format of the image and the format you specify in the pixel transfer command. OpenGL ES does not. Indeed, it is so serious about not allowing this that it actually changes the meaning of the parameters to functions like glTexImage*d. In ES, you cannot use sized internal formats (well, outside of texture storage calls). Instead, the internal format is kinda there. What really defines the internal format is the pixel format and type parameters. That is, when you tell OpenGL ES that the pixel data you're providing looks like X, it requires the implementation to create a texture whose real internal format exactly matches that. And if you're using glTexSubImage*d or glGetTexImage, you must provide pixel transfer parameters that match those you provided to the glTexImage*d call. The commentary in this thread is correct. OpenGL ES does not have built in support for reading textures directly.",2
"Note: Google most often pays just gimmick-attention to users' feedback, never really read their bug reports or suggestions, often closes or even deletes the forum threads involved (thus following the same path as Microsoft, and eventually the same fate). In this case, Google has closed the Issue 345698 to anyone who would have an improvement, whence my inability to post there, and my posting here instead. Well, there must be some sort of flaw. According to other sources, this overlay can not be deactivated. In a nutshell, the problem lies in 3rd party apps, not chrome itself: you should find which application blocks mouse hover and disable it. According to https://code.google.com/p/chromium/issues/detail?id=345698 the problem starts when using a drop down menu... Whence an effective workaround: in the affected window, select all the tabs minus at least one (see Select and Move Multiple Tabs to a New Window in Chrome), and drag them out (thus creating a new window) or into another existing window: all the tabs at once recover their temporary partial status bar (for unknown reason, dragging ALL the tabs in a window doesn't work). People mention some mouse-behaviour altering application and I found my problem in using 4t-min http://www.4t-niagara.com/tray.html that does something simmilarily unusual. I got it back after disabling two extensions (from the cv-pls club over in room 11) and then re-starting Chrome. This brought it back. The bug (hovering an URL FAILS to trigger, in the bottom left corner of the tab, a temporary partial Status bar showing the target of that URL) has been narrowed with precision by ""liam.bro..."" on Sun 23 Feb 2014 17:10:41: I leave this here in form of an answer for further reference and will keep an eye on it if it happens again. So I now have it back and the answer would be: Restart Chrome (probably with some extensions disabled).",4
"Better late than never! You can also use Cocoon to view Netflix from overseas - the proxy IP is U.S.-based and it is pretty fast too :) The traffic is not diverted through a tunlr server so you can even enjoy HD content if your internet connection is fast enough. For more details on how it works visit the tunlr FAQ Do you want to stream video or audio from U.S.-based on-demand Internet streaming media providers but can't get in on the fun because you're living outside the U.S.? Fear not, you have come to the right place. Tunlr lets you stream content from sites like Netflix, Hulu, MTV, CBS, ABC, Pandora and more to your Mac or PC. Want to watch Netflix or HuluPlus on your iPad, AppleTV or XBox 360 even though you're not in the U.S.? Tunlr lets you do this. Mediahint and Tunlr are good free options. However Mediahint is browser only and Tunlr requires that your ISP has been added to the whitelist in order to use the service. Commercial alternatives like Unlocator works with more services and are free to try. I know this is already solved, but there's a browser add-on for this purpose as well called ""Mediahint"". Uses a proxy when the site tries to determine your location and then uses your own bandwidth from then on, allowing you to get the best quality your connection allows.",4
"Call me naive if you like, but if you don't mind me asking - why are you doing this? Is it really that much of a problem to move new user/computer objects manually to their desired OU? It's only a one off process when a new person starts or a new computer is joined to the domain. The one problem you'll run into is older software that assumes the presence of 'cn=computers' in your tree as the default location for stuff. This is happily becoming increasingly rare. We've done it. We made the Computers context an OU instead of a CN, which allowed us to hang GPOs on it. Soon we will be moving it again and calling it 'unclaimed'. Our techs are supposed to pre-create objects before importing, but they don't always do it. Putting the not-pre-created computers into an OU named 'unclaimed' makes it pretty clear who is responsible for what computers... no one.  Has anyone done this in the past and experienced any gotchas that we should be aware of? Much obliged for the advice, tips and/or forewarnings.",3
"Also, since you presumably have a Windows Server 2012 Standard license, you are entitled to running two virtualized instances of Windows Server without purchasing a further license. So you might choose to set up an additional VM running your AD DC, effectively enabling you to use VDI at the cost of 2 GB of RAM and 50 GB of storage (even less with de-duplication enabled) on your system. we (a small 5 employee company) ordered a hosted server to use it as a use-from-everywhere database frontend. Our database basically is a SQL Server 2008 with an MS Access frontend, which I intend to install on the server, so every employee can login from everywhere via RDP. You can install the Remote Desktop Session Host role without having an AD domain, though - simply use the ""Role-based or feature-based installation"" and tick the required roles instead. I believe what you are trying to do is install VDI. VDI required an AD domain with the Server 2008 schema extensions and in Server 2008 functional level even back at the time of its introduction in Server 2008. The requirement to have an Active Directory still stands with Server 2012. The configuration wizard now gives me the option to configure the server as a) ""Role based or feature based installation"" or b) ""Remote Desktop Services installation"" Since I thought it'd be best to make full use of the new virtualisation features of Server 2012 and use b) I chose this, but the wizard asks me to add the server to the Domain first, which of course does not exist, since this is only a single server setup. I also can't create a domain, because I would need to chose a) in the first place.  Since I am completely new to Windows Server 2012 and not really an admin (I am more like a IT generalist) I am kinda stuck here. Any help would be greatly appreciated!",2
"This is variously called an Amphenol, a Telco 50-way connector, and sometimes (rarely) a Centronics.  In the kinds of places where you find DSLAMs like this unit -- communication service providers-- the term ""Amphenol"" is what I've heard most.  Basically it is a very convenient form factor for aggregating connections.  In this case, the DSLAM is going to be aggregating multiple connections from end user premises (houses, apartments etc) and so instead of plugging in dozens of individual lines they are aggregated into the Amphenol and connected.   This page has the more accurate picture I can find. That is how we often use them, pack to a patch panel to break out into RJ11 ports (for testing in the LAB) or back to krone blocks in a live PoP. Amphenol was the standard connector used in telcos for aggregating DS1s.  At higher densities other connectors are used, such as coax for DS3.  These days fiber is also used a lot, for even higher densities. You have to look for a cable for 25 pairs male with centronics / amphenol connector like this or this: I can't find a good authoritative source, but as per your picture, we also use in our ADSL and VDSL DSLAMs/MSANs.",3
"Unfortunately - this is like asking which car is best suited to people who drive... there is and never will be a definitive answer.. The real list of available languages/methodologies and their purposes could go on for ever - so your best bet is to start looking at the sort of tasks you want to accomplish and find a language to suit (you want to work on 2012R2 IIS - PowerShell, you want to script browser-side webpages - Javascript etc..) For older Windows systems, VBScript and batch files were generally the way to go.  Batch files because they are an effective and simple way to perform most tasks and have their roots right back to the Pre-Windows days.  VBScript because it gave a lot of extra functionality that somewhat bridged the gap between compiled VB programming languages and batch file based scripting. Web - Javascript/PHP.  Both cross-platform languages - heavily rooted and used the world over.  Javascript is more predominantly Browser-side and has heaps of libraries that make it very powerfull (JQuery for example).  PHP has been around for around 20 years now as an open source multi-platform (Linux and Windows) server-side language which drives a lot of popular content managers (Joomla, WordPress etc) Linux - bash/shell scripts.  It has been a staple of the Linux world for a long time (although PowerShell is slowly making its way in the Linux world for Enterprise) There are also a lot of other good cross-platform scripting languages available.  One that I personally have come to love is Python.  I can run the same script from my XP, 8.1, 2012R2, Ubuntu and Kali boxes as long as I have the matching Python version installed on all the machines. For administering pretty much anything Windows 7/2008R2 and newer - PowerShell is a very good choice.  The reason for this is that it leverages a lot of the power of the .net framework, has modules specifically designed for windows fuctionality (SQL modules, Active directory modules etc) and is very object-oriented.  It is also starting to break ground in the Linux world as Microsoft is striving to bring out a unified platform approach to IT.",1
"If you're on Vista, Window 7 or Windows 8 Pro / Enterprise you can use Bitlocker to encrypt your whole drive; With protecting data on a removed drive, there are many software programs to choose from. Just be sure to read reviews before you decide which suit your needs best. This is far better way of protecting data on your hard drive as it will prevent all but the most determined attacker.   Pre windows 8, all windows passwords are very easy to get past to the point where I say they are almost not worth having except to prevent other people in the household getting access. Windows 8/8.1 machines can be set up with either Local accounts or Microsoft accounts. Local account passwords are still easy to ""break"", those set up with Microsoft accounts are much much harder. It's not quite so easy in XP to do full disk encryption, there are a ton of 3rd party apps and a useful technet article here; Without knowing which version of Windows you're on, there are a number of options to secure your laptop.  As @Kwaio mentions, an OS password is a deterrent only and if some one wants in, it's pretty easy to do so;",2
"There is no solution from SQL Server that allows for 'transparent' data access. Querying across many servers requires Distributed Queries (ie. linked servers) and having a solution deployed with hard coded queries against linked servers across many instances will quickly hit the ground, for reasons of availability and maintainability. Is surprising how mobile databases really are: server change names, failover happens, databases are moved for reasons of load balancing. Given the tight coupled nature of DQ this doesn't really work. Here is a blog with an example: http://www.mssqltips.com/sqlservertip/1767/execute-sql-server-query-on-multiple-servers-at-the-same-time/ Other options I'm weighing are using replication, SSIS packages, service broker, or physically attaching more databases to one of our larger machines. Linked servers are the obvious choice, but I'm worried about performance and tight-coupling of a query to a particular server.  Am I too worried?  Do you have experiences to support this solution?  I want to be able to create a sort of ""single logical database"" from many different server instances. Other big deployments approached this by going SOA and relying on messaging, see MySpace Uses SQL Server Service Broker to Protect Integrity of 1 Petabyte of Data. This implies a serious paradigm shift in programming and how one approaches the solution, existing apps that use T-SQL queries will not magically transparently work on a 'farm' of servers. We have multiple MSSQL 2005 servers that contain silos of information.  We want our database developers to be able to join across these silos in the most transparent and performant way possible.  The databases vary in size, but average in the tens of millions of rows, and are updated frequently.  The servers are physically co-located.",3
"If you need to re-run the check, you'll need to make sure the clock is at least 24 hours ahead of when it thinks it last ran, e.g. if you advanced the clock to tomorrow for your first attempt, you'll need to advance the clock another 24 hours from there for your second attempt. *The actual time may vary - for best results, open Control Panel\Administrative Tools\Task Scheduler, expand Task Scheduler Library\Microsoft\Windows\Application Experience and check the status of the Microsoft Compatibility Appraiser task. From my own experience, it seems that the Microsoft Compatibility Appraiser scheduled task does nothing if it has already been run in the last 24 hours. To force the  task to actually perform the check again, you have to convince it that more than 24 hours have passed by changing the windows date & time settings to a future date (e.g. tomorrow). If you then run the task again, wait roughly 15 minutes* and then open the upgrade dialog from the system tray, you should see that the data has now been updated. Don't forget to set the clock back to the correct date and time afterwards.",1
"High-quality laptops come with Wi-Fi/Bluetooth combo modules that handle coexistence (sharing the 2.4GHz frequency band between Wi-Fi and Bluetooth) well. Bluetooth can use features like Adaptive Frequency Hopping (AFH), and the combo radio modules can employ things like Time Division Duplexing (TDD; a fancy term for having the two devices take turns instead of both trying to transmit at once) to share the band effectively. I have no idea if your particular HP laptop has a good implementation or a bad one. I just bought Genius HS-920BT BT headset. I've paired it to my PC on Windows 7. However, when I turned on the ""Forward Voip calls to Handsfree"" service, in order to use it as my Skype communications device, my wifi internet died.  My PC is HP Envy 3270nr with all the drivers updated (which means little being that HP only published one version of those, with that showing how much they appreciate their premium model owners). When I attached the headset as ""Listen to Music"" service, my internet kept running, so did my mouse, but sound became choppy on the headset in cases where I move my mouse and there's an active Internet transfer. In past, I also owned LG BT speaker system. When the speaker system was paired with PC on ""Listen to music"" mode, and my BT mouse was moving, the sound on the speaker system would also become choppy. Is this a driver issue? Is there a way to make all three BT devices (Wifi internet, BT mouse, BT sound system) work simultaneously?",2
"If you wanted to retain the ability to connect from anywhere without maintaining a geo-location blacklist/whitelist, you could implement port-knocking. It would stop most automated attempts while allowing you to still connect from any address. Note: Don't put the port to knock adjacent to the port to open, otherwise a sequential port scan will activate your rule. On the off-chance that you have a BGP-enabled router or two in your stack AND have some kind of idea what the hell it is you're doing/work with someone who knows what the hell it is they are doing, or are maybe behind a DDoS prevention provider cool enough to assist in the implementation of this, there's a relatively fresh method for restricting traffic to geographical regions called selective blackholing which I think is worth a look. Like most servers (I assume), we have people trying to brute force our services 24/7. I have cpHulk blacklist their IP's, but it seems like it'd be better if they didn't get that far in the first place. Myself and my host are the only ones who connect to the server on ports other than 80, so I'd like to block connections from all countries outside the US, except for port 80. I contacted my host to set this up, but they were hesitant because they said it would create an exceptionally high server load. It's a dedicated Xeon 1230 server with 32GB RAM running CentOS 6.6 and iptables. First, any reason not to do this? Second, is what my host told me correct? Third, is there any way to accomplish this without a high performance impact?",3
"If you enable Outlook Anywhere before you install Terminal Services Gateway, users cannot connect to their Exchange mailboxes by using RPC over HTTP.  There is an issue when you enabled NTLM authentication in Outlook Anywhere when Exchange is installed on Windows 2008. When you have the Outlook Anywhere feature configured on a Windows Server 2008-based computer that is running Terminal Services Gateway, you may experience the following symptoms: If you enable Outlook Anywhere after you install Terminal Services Gateway, Outlook Anywhere users can connect to Exchange by using RPC over HTTP. However, after you open the TS Gateway Manager snap-in, Outlook Anywhere users can no longer connect to Exchange by using RPC over HTTP  If that doesn't resolve the issue, I'd suggest running the Exchange Best Practices Analyzer and seeing what, if anything, it find amiss. There are a few different root causes that can make Outlook do what you're seeing. If you're not current on your updates to Exchange 2007 go ahead and apply the pending updates. It's, more than likely, being caused by a problem introduced in ""Update Rollup 8 for Exchange Server 2007 Service Pack 1"" as detailed in this TechNet blog entry. (That, at least, has been the main cause for the problem in my experience.) By default, Kernel Mode Authentication is enabled in IIS 7.0 on the Client Access server (CAS). To resolve this issue, disable Kernel Mode Authentication for Client Access servers that are running Windows Server 2008",2
"RaspberryPi uses ARM CPU so it's not possible to directly use NST on it. You would have to recompile everything and probably do some other changes in order to run this distribution.  You could also consider setting your device up to be monitored by a cloud security monitoring service like siemless (which operates a freemium model - so free for home users). They have an interesting blog piece on setting this up on a Raspberry Pi:  https://siemless.com/blog/raspberry/ Their threat use cases appear quite basic at present, although their contextual rules look to be interesting.  It works very well for things like scanning (nmap), limited traffic sniffing that doesn't saturate the interface (tcpdump, tshark), firewall testing (portsentry). snort should work within bandwidth limits, although I haven't tried it yet on the RPi myself. I am planning on putting either the NST (http://www.networksecuritytoolkit.org/nst/index.html) on my Pi or just downloading a lot of similar tools and then using my Pi as a security device to monitor all the traffic on my network.  Has anyone done anything like this and does anyone know if you can boot this version of Fedora onto the Pi? The NST page mentions including many of the sectools.org tools, and many of those are available on Raspbian. I have installed many of those on RPis and used them during network configuration and security testing work. They work well, so long as you are cognizant of the limitations of the RPi in terms of network throughput.  You are not going to be able to sniff traffic on a busy network that saturates the RPi interface reliably, especially at 1 Gbps+ speeds. It really depends on how busy your home network is. If you're willing to live with possibly missing some traffic, it's probably more than adequate for such use. Most of the tools should be available in Raspbian (or other RaspberryPi's distributions so that would be easier way to use the tools.  You can also take a look at PwnPi distribution which claims to have 200+ network security tools preinstalled.",4
"I'd like to use the TPM to secure my boot process for my linux laptop. Any manual, howto or tutorial I have found about this topic mentions I have to use the TrustedGrub bootloader to keep up the trust chain. TrustedGrub, however, is not in the repositories of any major distribution, it is based on Grub1 so the number of file systems it can boot from is quite limited, and worst of all, it cannot boot from UEFI, so I simply cannot use it. The question on my mind is: do I really absolutely have to use TrustedGrub to use the TPM, or can I use another boot loader like Grub2, Shim, Gummiboot or whatelse is there? In case of both, secure booting and trusted booting you need to get trusted grub. Trusted grub will measure the boot process. I am not aware of anything which is not using trusted grub.  I experimented with the tools on my system and they worked without TrustedGRUB being installed, including sealing and unsealing files.  You'd probably seal a file containing the key and then hook into an existing bootscript (or add a bootscript) to unseal the key and use it to run a cryptsetup luksOpen ... --key-file {your unsealed key on a tmpfs} or similar.",3
"1) I got a new SSL certificate reissued to the new encryption standard and installed it for Exchange and IIS. The former makes more sense in the context of uninstalling the spam filter. For either command, type verbatim or copy paste the name of the appropriate agent that you see in the list generated by Get-TransportAgent. Then try restarting the transport service. You should get a list of the transport agents running and hopefully the spam filter transport agent will be listed with a name that makes it clear that's what it is. From what I can gather, Exchange is trying to still load the transport module for SF, how do I remove that to get my Exchange Transport Service back online? To become PCI compliant after the new regulations regarding POODLE attack vulnerability, I did two things. So here's where I'm at now. I removed SPAMfighter to hopes that reinstalling would correct the problem, it did not... but what it did do is kill my Transport Service. Now the transport service will no run, it will start and die. The event viewer shows this... 1) I disabled SSLv3, made sure TLS1.0 was disabled, made sure TLS1.1 and 1.2 were both enabled. After disabling SSLv3, none of my Outlook clients could connect and OWA didn't work either. So I enabled SSLv3, Outlook clients and OWA began working. So I didn't think much else about, I would just get a second internet IP address and move my Exchange server traffic to that IP. The next day, I was informed that we were being inundated with spam email. So I opened up my SPAMfighter console, so my surprise none of the images were displayed. When I tried to log in to the console, I got all kinds of ASP .Net errors.  I'm running Windows Server 2012 R2 with Exchange 2013 w/ SP1. This computer is a member server of a AD2012R2 level domain. This server was also running SPAMfighter for Exchange.",2
"Depending on your OS you might have the '/srv/configs/apache/sites_enabled/default' file active. Remove (rename) it, restart the server and you should be fine. Like Dom mentioned check the folder. I tend to not delete files. but rename it like default.c__onf. This excludes it from the loading process. I had to convert my httpd.conf to Unix line endings (\n) even on Windows (ordinarily \r\n); otherwise I would receive this error. Apart from that you load every module available to your distribution. You should remove those not required. Rename, or remove index.html or index.php from /srv/www  - Directory lists will only show in the absence of an index file - which is present and showing the 'It works!' message. Are you browsing to the server ip/hostname or the virtual host hostname? (Can use a hosts file if it isn't in DNS). Virtual hosts will only work when the specified hostname (in this case projects.com) is called. Check your apache log to see what is being called. Please note that you are  getting the "" It works"" page because the apache is showing its default page.  You need to give the correct permission for the /srv/www/ folder  so that apache can  show the contents.",5
"If you're bridging IP traffic over to your own router, then the previous modem/router(called a home gateway) just hands all traffic through to yours, so its attack surface is essentially nil. It will only talk to local addresses as private IP space wont be routed by your ISP. As long as the new router for the network has firewalling in place you're just as safe as any other user, possibly more if the original is ISP supplied. I have a router that I want to plug into a  modem/router too (the latter is a poor router) and the advice I'm getting is to set the modem / router to full bridge mode, like so: https://whirlpool.net.au/wiki/adsl_modem_router_bridge_mode With the detailed advice I've been given for my model, this involves turning off the firewall on the modem/router (I think essentially it's just acting as a modem). Or, is it unimportant (because working as a modem, it only looks at layer 2 header while the firewall process looks at layer 3 header - apologies if I'm way out with this)?",2
"Saying that, what you want to do should be fine. The Linksys router, acting as the DHCP server and gateway could be on 192.168.1.1, and then the Belkin could be set on a static IP of 192.168.1.2 (bearning in mind you'll have to change the DHCP server to start handing out addresses from 1.3 rather than 1.2 to avoid problems. Typically, when two wireless networks with the same SSID are in range it should act as a fail-over (or so I understand it anyway) but I don't know if something so basic would support ""roaming"" access, that is, automatically switch between the strongest signals rather than loose access. You will have to set the Belkin to act as an access point - it's important to disable the DHCP server on the belkin because if there are 2 DHCP servers operating on the same network, you will have conflicts. also important to set the belkin ap with an ip that is in the range of the other router/modem and configure the linksys to reserve an IP for the belkin - this can be done by going to DHCP reservations and adding the MAC address of the belkin. In this way you can control all the IP pool on the network from the DHCP server on the Linksys I'll just trot out the line from the FaQ: Server Fault is for system administrators and desktop support professionals, people who manage or maintain computers in a professional capacity. If your question is not about <snip> Networking outside the professional workplace then you're in the right place. So, if this is a home network please say so where it can be migrated to SuperUser when home networking topics are more suited. If this is a workplace network I'd think about something a bit more robost than SOHO routers and basic equpiment (budget pending, naturally).",2
"Then try to swap monitors (change their order of 1 and 2 internally in Windows only) and see, if the second one is actually responding. Another thing to try if whether your second monitor has an ""input"" or ""source"" button.  If your monitor is displaying from DVI in but you've hooked up its VGA cable, then you would not get any picture. Sometimes my second monitor isn't available too and flickrs (?) all around. Then i just turn it off and on again and then it is recognized by Windows. I was about to try and reset to factory defaults as I read elsewhere on the web might work, but now... I don't need to. But there might also be a wrong setting that prevents outputting video information on the second DVI-out. You might double check your setting within nVidia Control Panel and the (usually) built-in Intel Graphics Control panel or whatever. While a DVI-I to VGA connector is also passive (no re-encoding), use of the analog pins in the DVI-I connector signal to the video source to generate VGA (analog) signalling instead of DVI, so that the monitor receives a real VGA signal as it expects. Check if the current screen settings for your second monitor are actually supported by it (screen resolution, refresh rate). I just encountered the same problem. Found the solution which is to right click on the 2nd monitor -> Properties -> Under the Adapter tab -> List All Modes and change it to 16 bit. There exist ""video range extender"" solutions which connect signals directly to a different cable style, without changing the signal format.  This is useful if, for example, you have long VGA cables permanently installed inside the walls of your room, because you can use the existing cable to carry DVI or HDMI data.  But you must use the corresponding adapter to change the signal back to its original connector type, that matches the signal encoding.  You can't expect to connect directly to a device supporting the new connector format, since it also expects a different signal encoding. If that doesn't help, change their connections on your PC - maybe the second DVI-out out is somehow ... ""broken"". Wow, all I did was press the monitor menu buttons on the physical monitor and it magically starting displaying and no longer blank.",4
"You could investigate if there are differences between websites inheriting defaultLanguage and websites overriding it via web.config- 2) All your sites did override the unchanged defaultLanguage set at IIS root in their own web.config and this configuration was modified in every web.config. 1) The defaultLanguage was changed at the IIS root level. Since VB is the preset defaultLangauge it would mean that your defaultLanguage was set to C# at the root level (and changed at/around the date you mentioned). It would be relatively easy to reproduce an update reverting your selected defaultLanguage C# back to VB. We have developed a module which enters Windows Scheduled tasks in on our server that are calling KeepAlive pages on different servers. Normally everything went fine, but since 21-07-2011 on 17:00 all our KeepAlive pages generate errors. Those errors are because on IIS level the defaultLanguage of the different websites that run there are set to VB instead of c#. Has there be any update or whatsoever that can mess up a setting like this? How can it be that suddenly the standard language on IIS level has been changed to VB instead of C#? Hope anyone has a bright idea on this issue. I'm guessing perhaps some update has been run (our machine installs updates automatically). Can anyone track this? Thanks in advance! The default defaultLanguage is set at the IIS root level (ASP.NET -> .NET Compliation) and inherited by all websites unless redefined there.",2
"The observed behavior (""smooth"" SSH immediately after adapter reboot and laggy after some time) suggested to look at power-saving options. Finally, the problem was solved disabling ""Allow the computer to turn off this device to save power"" option in Device Manager > Network adapters > adapter properties  > Power Management. The problem/solution was successfully reproduced after enabling/disabling this option (the result is seen after some minutes without adapter reboot).  The described problem was very annoying too. I tried approach with disabling Large Send Offload and Jumbo Packet as was suggested above (as well as other options in network adapter properties > advanced tab). Unfortunately, the positive result was only ""short-time"": after adapter reboot, which is necessary to activate chosen adapter options, everything was fine, but then after some time (several minutes) ssh session became laggy/with freezes. The approach was tested on the following software setups: SSH Secure shell on Windows 7 Professional 64-bit and Putty on Windows 7 Ultimate 64-bit both connected to different Linux servers.",1
"When launching EC2 instances, it seems most community AMI's come with a 8gb EBS volume attached as the root drive. We will definitely need larger than 8GB as our database size grows. What is the advisable way to design our system? The options I see are: (There is a 4th option, but it really isn't practical for something like a database - you can mount S3 as a fuse filesystem - and since you don't have to pre-allocate space on S3, you would have storage that grows with your needs. It is however, quite slow compared to EBS or ephemeral storage, and its reliability is questionable at best.) Unless you're using an EBS-backed EC2 instance the AMI image is extracted to a 10GB drive image that is recreated when the instance is started. I use instance-stored EC2 instances instead of EBS-backed ones for all my server instances. I then simply make the EBS volumes whatever side I need and mount them as secondary drives.  Put your important data on (at least one) external EBS volume.  For anything that you actually need performance for, use Linux MD RAID-10 across a lot of EBS volumes. With the EBS volumes I found that using the xfs filesystem and simply using the entire EBS volume without any partitioning was the best course of action. To increase the volume when it was necessary I would perform a snapshot of the existing EBS volume and then create a new larger volume built from the snapshot. You then simply detach the existing EBS volume and attach the new one. Once the new volume is mounted it will show as the current volume size until you run the xfs expand utility command that has to be ran while the filesystem is live. Checking the capacity after that is done will show the new larger size. Now if you're using an Ubuntu AMI you can install the ebsmount package and create a hidden directory on the EBS volume and configure the system to actually automount using udev when the EBS volume is attached to the EC2 instance.",4
"Additionally (depending on what your rules are) you should consider doing these calculations as a check before the tower is placed. This will disallow the player from placing towers that block all paths. Or as some tower defense games work, if the play blocks all paths, the units just ignore towers when path finding. You can likely save calculations by grouping units in the field and calculate a common path for them all. For example if you have a group of units in tile (4,7), they can all use the same path, so you just have to calculate it once. A* should be plenty fast enough. Each time a tower is placed you should calculate a new path for each spawn point, and assign that path to each unit that is spawned there. You should also calculate a new path for the units ""in the field"". Units in the field can have their paths calculated as the shortest path to get back on track, as in a path to the new path. Or the units can have their path calculated from their current position to the destination.",1
"I have been searching for a solution to this problem for over one year, including using the older XP drivers from Creative, using the KXProject drivers, and even modifying drivers from other sound cards (e.g. the Audigy series). I know this is not the answer you want to hear, but after searching for a driver for this particular sound card from the Windows 7 Beta, I still have found nothing, and decided to purchase something new.  As a warning to yourself, if you do decide to continue with Creative's products, ensure that the new card is from the X-Fi series (or newer).  I found compatibility issues with the Audigy series under Windows 7, and was forced to upgrade again (after which I decided that I would not be getting another Creative sound card). The two main problems are that there are no drivers compatible with the sound card, Windows 7, and over 2 GB of RAM.  The only compatible drivers for the sound card and Windows 7 work with 2 GB of RAM or less, and even then, are still unstable.  The second problem is the lack of official driver support from Creative.",1
"Your password hash is typically left when you logon or RDP.  So if you are using your admin account on your workstation, your password hash is there.  Similarly, when you RDP to a server (except for newer OS), your hash is left there.  This problem is mitigated in the Windows 10 OS codebase (Server 2016).  You should have an admin server, and not log on with domain elevated rights to your workstation. You don't want to be using email or browsing the web using elevated rights or from a workstation where you routinely elevate or log on with elevated permissions.   You can also consider using LAPS to put a random system maintained password on your Windows workstations.  LAPS is free from Microsoft.  This prevents the problem of a single workstation compromise taking down all workstations, and if you connect with the local administrator password for the machine, any compromise would be limited.  The downside of using the local administrator password is auditing and that you may have to enable that via a GPO. You can RDP to the admin server, and administer from there.  If you are using PowerShell to connect to the remote systems, you aren't leaving as hash.  If you are using a smartcard, you can flip the smartcardrequired bit twice to invalidate your current password hash.",1
"In the usage message, what does input mean?  Is it a filename?  I think that string would be clearer.  Similarly, maxwidth would be clearer than [n].  Note that by convention, the square brackets mean that it's an optional parameter, which it isn't. You have a //print remaining characters epilogue loop, which is not only ugly and repetitive, but also makes it hard to handle degenerate cases correctly (such as when the input is an empty string). You can parse argv[2] as an integer using atoi().  COL_BOUND is not a compile-time constant, so it should be named with lowercase.  Note that if COL_BOUND is 0, then you'll get an infinite loop, so some validation would be nice. I'd define a nextline() function to find the start and end of the next line.  Its design is vaguely inspired by the strsep(3) function in BSD's C library.  Then, main() would just be responsible for handling the command-line parameters and printing the output. Your length() function is just a reimplementation of the standard strlen() function.  You call it every time you go through the main while loop, which is very inefficient.  Ideally, you should perform this task without measuring the length at all  it's possible to analyze the string in one single forward pass, stopping when you reach the NUL terminator. What exactly is the specification of this program?  Here are three sample runs (with space () and NUL () characters made visible): Don't contaminate stdout, which should be solely for the string-processing result.  Error messages and status reports should go to stderr. Other than the useless length() function, all of the code is in main(), which makes it one very complex function.  One of the causes of your buggy behavior is that you try to do whitespace analysis (with printed) while printing, when it's clearly too late. By convention, main() should be defined after the helper functions, so that you don't have to write forward declarations.",1
"We have applications connected to SQL using windows authentication. While having connection with Application user can also access to Database instance on the same time as well. We need to limit the access of user outside application. How we can limit user to access DB from application but not outside application? Does your application need to directly access the data tables? If the application only uses Stored procedures, then I'd restrict the user to those - that way direct table access is avoided.  The general principle of giving users the minimum necessary permission to do their job applies here - don't give the Application more than it needs, either. However, if you can change the application, I would suggest you investigate Application Roles - These can solve exactly the problem you have, that the application should have more privileges than the user alone. The user has little more than connect access, but when the application connects it runs a stored procedure with a password to gain the additional permissions it needs in the database.",2
"You're connecting to the SQL 2012 instance.  Your 2005 instance probably has a different instance name.  You can tell what it is if you don't remember by looking in Services and seeing what SQL instances are named there (2012 is the default, or MSSQLSERVER).  When you find the name (for example, if it's 2005), ask SSMS to connect to .\2005 instead of what you're using. Management Studio is just a client. It connects to database servers. It is not the database itself. It is possible to install, operate, and maintain Sql Server without ever installing or using Management Studio. What you see when selecting @@version is the version of the database server. It has nothing to do with the version of Management Studio you are using. There is never a reason to need an older version of Management Studio, though you may sometimes have applications that only work with older versions of Sql Server. In many (but not all) cases, these applications can be appeased by setting the Compatibility Level of the database.",2
"When using root, a request to mysite.com/files/ will look in the local directory /home/myfiles/Downloads/files/ for an index file, and if not found will automatically generate a directory listing (since you have specified the autoindex option).  Note how nginx appends /files/ to the root directory you specified. According to the nginx documentation: ""The request only reaches the ngx_http_autoindex_module when the ngx_http_index_module did not find an index file."" I am trying to set up nginx so that a certain url produces the directory index of a certain directory on my server. Currently this is what my default.conf looks like. Since for your case you want /files/ to be a synonym for your download directory, you need to use alias /home/myfiles/Downloads/ in the location block.  Then, any request to mysite.com/files/ will be translated to the directory /home/myfiles/Downloads/ (e.g. mysite.com/files/bigfile.gz will become /home/myfiles/Downloads/bigfile.gz).  Note that the trailing / on the alias is necessary. Remove the index.html from /home/myfiles/Downloads/files/ and it should display the files in a directory listing I don't want it to search for files/index.html, I just want the directory index of Downloads. What do I need to change to make things work this way?",3
"If you want different sites for various domains, like site2.example.com you'll have to use virtual hosts, one for each different site. example.com is specifically reserved for cases where you don't want to expose your real domain name. This site is not for home users, but since your question is related to Apache, I'll point you in the right direction. I have another folder where I keep my files at home I can get to it from my local network by going to LocalIP/~user but when i got to ilovecarssometimes.com/~user it doesn't seem to work. Mapping your home's public_html (or similar folder) to exmaple.com/~user is a function of the userdir directive in your httpd.conf file (or similar since I don't know what OS your running). Apache has tons of documentation, and although it's quite technical, if you read through it you'll find it contains many simple examples. System Administration takes some learning and effort, it's not all easy. What am I doing wrong? Maybe getting it to point to bmw.ilovecarssometimes.com would be a better solution? Domain names can't be pointed to ports; just IP addresses (with common setups). So your domain example.com must currently be pointed to some server IP 1.2.3.4. It sounds like you have a virtual host setup for it, especially if you get different results from entering example.com vs 1.2.3.4 into your address bar.",2
"Note that not every residental FTTP/FTTH service is asymmetrical. Here in the US, Verizon Fios is symmetrical on all plans. The idea is that most consumers need much more downstream bandwidth than upstream bandwidth, and an asymmetrical connection would therefore better utilize system capacity. While content creators (e.g. YouTube channels that upload lots of videos) can use lots of upstream bandwidth, most consumers will use far more downstream bandwidth than upstream bandwidth. As a result, reserving some one-half of system capacity for the uplink is going to waste a lot of resources that could go to the downlink instead. The more efficiently the available bandwidth is used, the more customers it can serve, which means more revenue and fewer resources wasted. While upstream bandwidth cost (as mentioned in current comments) may be a small part of the puzzle. The bigger picture is simply they don't give you the bandwidth required to run a mainstream server.  Otherwise no company would pay for a business class or data center class connection.  Be thankful the isp uses this method to limit the connection.  If they chose to block ports instead it would severely limit what you could do with your connection.  They could also instead charge incrementally for the bandwidth (like your cell phone providers charges for data used) which is unpopular in the consumer broadband market. Asymmetric connection is necessary for users. Imagine that when you are browsing the web, you can enjoy the faster download speeds than upload speeds. Whit this type of connection, cost effective solutions and quick installation are also ensured. Certainly, this asymmetric connection has its weak points. Whether it's symmetric or asymmetric connection, just choose the right Internet connection type for your owner or business use.",3
"I've encountered an interesting case in my own research of small differences in alphabet size making dramatic differences in the resulting theory.  A rough description of the problem of learning probabilistic circuits is the following: a learner can override gates of a hidden circuit and observe the resulting output, and the goal is to produce a ""functionally equivalent"" circuit.  For boolean circuits, whenever a gate has ""influence"" on the output, one can isolate an influential path from that gate to the output of the circuit.  For circuits of alphabet size $\ge 3$ this becomes no longer the case -- namely, there are circuits who have gates with large influence on the output value, but no influence along any one path to the output!  We found this result quite surprising. More generally, if we have alphabets $\Sigma_1$ and $\Sigma_2$ with $|\Sigma_1|=n$ and $|\Sigma_2|=k$, then there exists a conversion program from $O_{\Sigma_1}$ to $O_{\Sigma_2}$ if and only if all the primes appearing in the prime factorisation of $n$ also appear in the prime factorisation of $k$ (so the exponents of the primes in the factorisation doesn't matter). Let $\Sigma_1 = \{ 0, 1 \}$ and $\Sigma = \{ 0, 1, 2, 3 \}$. Converting $O_{\Sigma_1}$ into an oracle $O_{\Sigma_2}$ is easy: we query $O_{\Sigma_1}$ twice, converting the results as follows: $00 \rightarrow 0$, $01 \rightarrow 1$, $10 \rightarrow 2$, $11 \rightarrow 3$. Clearly, this program runs in $O(1)$ time. Usually, as long as $\Sigma$ contains more than 1 element, the exact number of elements in $\Sigma$ doesn't matter: at best we end up with a different constant somewhere. In other words, it doesn't really matter if we use the binary alphabet, the numbers, the Latin alphabet or Unicode. A consequence of this is that if we have a random number generator generating a binary string of length $l$, we can't use that random number generator to generate a number in $\{0, 1, 2\}$ with exactly equal probability. Now let $\Sigma_1 = \{ 0, 1 \}$ and $\Sigma = \{ 0, 1, 2 \}$. For these two languages, all conversion programs run in $O(\infty)$ time, ie there are no conversion programs from $O_{\Sigma_1}$ to $O_{\Sigma_2}$ that run in $O(1)$ time. In error correcting codes, it is possible that there is a fundamental difference between binary codes and codes over larger alphabets in that the Gilbert Varshamov examples for codes which correct a fraction of errors (which are essentially greedy or random examples) are believed by some to be tight in the binary case and are known to be not tight over a large alphabet via algebraic-geometry codes. This led some to speculate that the standard definition of error correcting codes for a large alphabet is not the right analog of binary error correcting codes.   For any alphabet $\Sigma$ we define the random oracle $O_{\Sigma}$ to be an oracle that returns random elements from $\Sigma$, such that every element has an equal chance of being returned (so the chance for every element is $\frac{1}{|\Sigma|}$). I thought up the above problem when standing in the supermarket, pondering what to have for dinner. I wondered if I could use coin tosses to decide between choice A, B and C. As it turns out, that is impossible. $C$ may make less than $d$ queries in certain execution paths. We can easily construct a conversion program $C'$ that executes $C$, keeping track of how many times an oracle query was made. Let $k$ be the number of oracle queries. $C'$ then makes $d-k$ additional oracle queries, discarding the results, returning what $C$ would have returned. Specifically, the overall structure of the proof is an iterative application of a graph powering technique a logarithm in the graph size number of times. On each iteration, the graph is pre-processed into a regular expanding graph, amplified by a power (which blows up the alphabet size), and then a PCP composition is applied (turning each constraint over a large alphabet into a system of constraints over a small alphabet). For some alphabets $\Sigma_1$ and $\Sigma_2$ - possibly of different sizes - consider the class of oracle machines with access to $O_{\Sigma_1}$. We're interested in the oracle machines in this class that behave the same as $O_{\Sigma_2}$. In other words, we want to convert an oracle $O_{\Sigma_1}$ into an oracle $O_{\Sigma_2}$ using a Turing machine. We will call such a Turing machine a conversion program. This way, there are exactly $|\Sigma_1|^d = 2^d$ execution paths for $C'$. Exactly $\frac{1}{|\Sigma_2|} = \frac{1}{3}$ of these execution paths will result in $C'$ returning $0$. However, $\frac{2^d}{3}$ is not an integer number, so we have a contradiction. Hence, no such program exists. The result is somewhat technical, but if you're interested, you can contrast Lemma 8 with Section 4.1 for relevant the theorem statements. I am not an expert on this but one nice example were the size of the alphabet does matter are coding and succinct data structures. Imagine you want to represent a message over the alphabet $\{0,1,2\}$ in the alphabet $\{0,1\}$ (e.g. to store it in your binary computer). You want to minimize the space required but at the same time, you want to be able to read and write individual characters of the message fast (let's say in $O(1)$). Problems like this have been studied for quite some time now. The recent paper by Dodis, Patrascu, and Thorup on it, and the references therein, should be a good point to start. Let $\Sigma$ be an alphabet, ie a nonempty finite set. A string is any finite sequence of elements (characters) from $\Sigma$. As an example, $ \{0, 1\}$ is the binary alphabet and $0110$ is a string for this alphabet. The requirements of your example are quite strict. If you relax it to only require that the conversion works in $O(1)$ in expectation. It is possible to sample uniformly from $\{0,1,2\}$ using, in expectation, a constant number of coin tosses. This can be proven by contradiction: suppose there exists a conversion program $C$ from $O_{\Sigma_1}$ to $O_{\Sigma_2}$ running in $O(1)$ time. This means there is a $d \in \mathbb{N}$ such that $C$ makes at most $d$ queries to $\Sigma_1$. The implicit goal of the process is to find a way to re-use the amplification step until the UNSAT value becomes a constant fraction (proving the PCP theorem). The key point is that unless the alphabet size is pulled back each time, the resulting graph is not what is needed for the final reduction.",5
"I downloaded eclipse and unzipped the file into a folder and also added some plugins, changed some settings. As such, it doesn't matter as much if you use it on another computer, but more if you use it on another workspace. This means it can work on any machine that doesn't have Java installed without having to run special command line or path settings. Just run eclipse.exe and it finds the Java runtime in jre for you and carries on happily.  Neato. I just download the Eclipse zip file, extracted it to a directory on the desktop and do the following steps. After that when Eclipse asks for a workspace I simply enter .\Workspace so that the workspace directory is created within the eclipse directory, and it seems that all the workspace details are kept under that directory in a ""relative directory"" fashion so it doesn't matter if the drive letter changes.  Plugins like Pydev keep their settings in the workspace folder (in a folder named "".metadata"") so once you've set it up they'll get remembered between places too. Eclipse does store all of its plugins and configuration in its own directory structure, so making it portable it surprisingly easy and intuitive.  I've used Eclipse in a portable fashion with no problems at all.  All I can recommend is a couple of extra steps to make your life just that bit easier and this is what I do: Copy the Java runtime from a computer that has it installed into your Eclipse directory.  The folder with java in it should be named ""jre"" so you end up with the following: If I copied this folder to a pendrive and opened it in another PC, will all my seetings and plugins work out of box? It should. I have eclipse installation on a flash drive and it works fine. Sometimes I have to select which JDK I'm using though.",4
"I would definitely accept solutions involving browser extensions, as long as they integrate with the existing console or at least do not require a completely new implementation of the developer tools console tab. I am currently in the process of moving my web-dev work from classic IDEs to the web browser console. The console is basically a JavaScript REPL, which is great, but that also means it can only process valid JavaScript expressions. Is there any way of redirecting the input to a custom processing function instead? Right now the best solution I came up with is using a very short function name, e.g. $('log hello') or the slightly shorter template literal syntax $`log hello`. Both solutions require at least three additional characters, even more keystrokes, and the input must be surrounded by quotes/ticks/braces. This would make it possible to integrate a custom DSL directly into the browser console and make it feel more like a native terminal. For example, the custom processor could translate an input of log hello to the valid JavaScript expression console.log(""hello"") and eval it.",1
"Not quite a solution to the actual problem at hand, but it is quirks like this that cause me to always buy my SSL certs from Thawte.   Looking around the web, it seems that others have experienced this issue as well (http://blog.boxedice.com/2009/05/11/godaddy-ssl-certificates-and-cannot-verify-identity-on-macsafari/) But no solution seems to fix the issue. Verify that the correct intermediate certificates are being given out by the server at http://www.sslshopper.com/ssl-checker.html That gd_bundle.crt chain has a ""Go Daddy Class 2 Certification Authority"" that verifies up to a Valicert root. I don't think this is valid anymore - GoDaddy seems to issue certs that are signed by ""Go Daddy Secure Certification Authority"" that is in turn signed by a different, self-signed ""Go Daddy Class 2 Certification Authority"" - not the Valicert-issued one in your chain, so it has nothing to  do with your actual certificate. We just got a new SSL certificate from GoDaddy.  And, while all browsers are fine with the certificate, Safari gives the following error: You may be using the wrong cert chain. I assume your ""gd_bundle2.crt"" is the same as ""gd_bundle.crt"" on this page: https://certs.godaddy.com/anonymous/repository.seam Go to the page referenced above, download ""gd-class2-root.crt"" then download ""gd_intermediate.crt"". Concatenate the two files (they're just plain text files) into ""mybundle.crt"" and specify this new file in SSLCertificateChainFile. See if that makes a difference. For some reason Safari doesn't stay up to date with the latest trusted root certification authorities.  You can contact customer service and ask them to reissue you a certificate with a different trusted root certificate. Does anyone know why this would be caused, or have experience with this happening, and how to fix it?",5
"Try mlxtend. Here's an example of multi-class case: http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/#example-2-multi-class-classification Look at sed_eval library. It is developed for evaluating event detection in audio which is a multi-label problem (as in each audio, multiple events exist). They have many evaluation options, which might fit to your needs.  There are many different parameters which can evaluate the performance of your method by comparing the real and predicted labels. I suggest PyCM module which can give a vast variety these parameters which are suitable for multi-class classification. You can get the true-positive rate, ... and from there computing the confusion matrix is not that hard. Also take a look at scikit-multilearn. It is a very good library that extends sklearn for multi-label learning. However, I'm not sure how the confusion matrix works for multi-label problems... Scikit-learn does support multi-label confusion matrix. See the links below for documentation and user guide:",5
"$PYTHONPATH is used to specify the location of extra python libraries (very similar to adding the path to python's sys.path) and not the python interpreter itself. I'm guessing your system is picking up the version of python, which is found first in $PATH. How is Bazaar even finding 2.6.7 and how can I point it to the Homebrew version? I thought that's what PYTHONPATH was for. The problem is that I've installed Python modules via Homebrew and pip and Bazaar can't find them. Ultimately, what I'm trying to do is convert a Bazaar shared repo to Git via fastimport, but that's another question... (It's also possible you installed it via, say, easy_install-2.6, or some indirect equivalent of the same thing. But that seems like a silly thing to do.) Since Bazaar is pure Python code, the way it's finding 2.6 is simple: The first line of /usr/local/bin/bzr is something like this: I'm on OS X 10.8. My system Python in /usr/bin is version 2.7.2. I made the mistake (?) of installing another version of Python, 2.7.3, via Homebrew. That one's linked from /usr/local/bin. My PYTHONPATH points to the latter; specifically PYTHONPATH=/usr/local/bin:. Is that right? I'm guessing you installed a binary package, and it was specifically packaged to rely on /usr/bin/python2.6 because that version is there in all OS X 10.5+ versions, or because that's what they tested on, orwhatever. If you want Bazaar to use packages you've installed via Homebrew, you're probably going to want to use either Homebrew itself, or the pip from Homebrew's Python, to install Bazaar. I build the official Mac OS X installer packages for Bazaar. However, I am not employed by Canonical. You could hack that up to, say, #!/usr/local/bin/python2.7. But that's a very bad idea. You've got something that installed and configured itself against one Python, you don't want to try to run it against another. (Since it's pure Python code, it will mostly work, but sometimes fail in mysterious ways, which is probably worse than code that uses C extensions or embedding which will probably just fail immediately.) If you're using the Bazaar installer package downloaded from the Canonical website, then it is built specifically against Python 2.6 and the bzr script uses /usr/bin/python2.6 in its header to ensure that it is invoked with Python 2.6. There are some native Python extensions which are compiled against Python 2.6 when I build the installer, so running it with another major version of Python may not work (I've never tried). From a comment, you said you vaguely remember running the installer from Canonical. As their Mac OS X Downloads and Installation page makes clear, what you downloaded is for ""Snow Leopard (10.6 - Python 2.6)"". (Also, you downloaded the ""Test"" version instead of the stable, given that you have 2.6b1.) It even says: So, this is all documented pretty clearly. Their installer uses Apple's system Python, and specifically uses 2.6 for OS X 10.6+.",4
"If you update Ff and decide to run it as if it is an older version, then it will have some unxepected behaviour. You can disable automatic updates, keep using the older version and whenever it is you feel like upgrading, do so. Once you have upgraded Ff it needs to be restarted. Currently, when choosing between updating at Firefox's schedule or not updating at all, I choose the latter. Obviously, this is not optimal in terms of features or even plain security... Unfortunately at some point this behavior changed. Now after update Firefox refuses to open new pages, is informing that it was updated and should be restarted, while promising to make the restart painless (which is a lie: text I wrote in text fields is gone, page scrolling is forgotten, youtube videos don't remember where I was, etc), and then at some point actually closes without asking. Previously (at most 1 year ago) updating Firefox was easy. Firefox was automatically or manually updated from the official repository and I could still use my currently running Firefox instance. I would get that new installed version of Firefox only after I manually chose to restart old-Firefox. This ""we are forcing you to restart your software when we want and not when it's convenient for you"" approach is very Microsoftish. I hate it!",2
"If you go with EFI/UEFI-mode booting, you must create an EFI System Partition (ESP). This partition must be FAT-formatted and must be significantly larger than a BIOS Boot Partition, so you'll have to resize something to create it. I recommend a size of 550MiB, although a tenth of that might work in a pinch. Alternatively I could just go back to grub-legacy and not worry about that (is there really an advantage to upgrading to grub2 if it causes this much trouble?). EFI/UEFI-mode booting is less of a hack; but EFI implementations vary greatly in quality and the overall level of expertise and support on the Web for EFI is lower than that for BIOS. Windows ties use of GPT to EFI-mode booting, so if you expect to ever install Windows on your computer, EFI is definitely the way to go. Older computers are BIOS-only. EFI began to take off in the marketplace in mid-2011, so if your computer is older than that, you might not be able to use EFI. You should first determine whether you want to use a BIOS/CSM/legacy-mode boot or an EFI/UEFI-mode boot. The former is the way that PCs have been booting since the 1980s, but it's an ugly and hackish system that will be going the way of the dodo before too long. Windows ties BIOS-mode booting to the MBR partition table, which you're not using (but could; your disk is nowhere near big enough to require GPT). Linux, FreeBSD, and most other modern OSes are more flexible, and support BIOS-mode booting from GPT; but there are sometimes firmware-created complications, and of course if you later decide to install Windows in a dual-boot setup you'll need to make changes or compromises. If you want to use GPT partitions with old-style BIOS boot mode, then you need to create a BIOS partition of a few megabytes (16MiB should be fine and future proof) on the device where you want to install grub with grub-install. If you go with BIOS-mode booting, you should create a BIOS Boot Partition on the disk. There's enough room at the start of your disk for this partition, but you'll need to set your sector alignment value to 1 (from the usual 2048) for this to work. I don't know offhand if this can be done with parted, but you can do it with gdisk. (Note that the space at the start of your disk is slightly under the recommended 1MiB size for a BIOS Boot Partition, but just by a few sectors. It will probably work fine, but might conceivably fail at some point in the future.) Alternatively, you can shrink any of your partitions by 1-2MiB to make room for the BIOS Boot Partition. This partition does not need to be the first partition on the disk, although that's the conventional location. To install GRUB, you should first be sure that you've installed the correct GRUB package. I'm not sure of naming in all distributions, but in Ubuntu, it would be grub-pc for BIOS/CSM/legacy mode and grub-efi-amd64 for EFI/UEFI mode. An EFI-mode installation will also require booting whatever you're using to install GRUB (a live CD/USB, presumably) in EFI mode. Doing this may require using your computer's built-in boot manager, which is typically accessed via a function key, but the details vary from one computer to another. Is there a simple way that I can get around this problem?  I understand that I need to create a tiny partition at the beginning of the disk as a ""BIOS Boot Partition"".  I suppose one option would be to move the swap to /dev/sda4 as a logical volume, and use /dev/sda2 as /boot.",3
"The downside to the CREATE TABLE AS approach is that a simple UPDATE query becomes a monster multi-statement transaction. The latter code is not only cumbersome but also fragile: the table schema must be repeated for every update that uses this approach. Imagine a data warehouse pipeline with dozens of such updates. Any change to the table schema must be hard-coded into every single update operation.  This is not trying to be a real answer, but at least a reference comparison point, and some hints with an extremely simplified setup on a non-RDS machine: As an alternative approach, I suggest you first try stripping all indexes not involved in the update (joins or where clause), then use a regular UPDATE statement and rebuild the indexes. Depending on the dimensions of you table, its indexes, the complexity of the update and the number of rows involved, an update-in-place may nearly as fast as the ""CREATE TABLE AS"" method, and your code will be simpler and more stable. An effective solution to this problem is to create a new, unindexed table using the CREATE TABLE AS method described here. You will need to rebuild indexes, keys and constraints on the new table, but this is still much faster than updating-in-place. See also the related answer here. I would need to know more about the table being updated to know if WAL is the culprit. Is a large proportion of records (say 30% or more) affected by the update? You stated that the value column is not indexed, but are there indexes on other columns? If the answer to both questions is yes, then I would strongly suspect WAL. This test has been performed on ""PostgreSQL 9.6.3 on x86_64-apple-darwin14.5.0, compiled by Apple LLVM version 7.0.0 (clang-700.1.76), 64-bit"", on a MacBook Air with i7 processor @1,7 GHz, 512 GB SSD, and 8 GB RAM, and macOS Sierra 10.12.5. The settings for PostgreSQL are ""out-of-the-box"" (installed via Postgress.app), with no further optimization. This is a common problem for UPDATEs, INSERTs and ALTER TABLE operations on very large tables. According to my understanding, for any row affected by an update--even if that update is only on a single column--Postgres archives and replaces the entire row and updates all indexes on all indexes columns. While Postgres's implementation of WAL is extremely effective for maintaining data integrity in a transactional setting, it can seriously degrade performance for operations on large tables--particularly for data warehouses, where bulk updates involving many records are common. Have you considered the possibility that WAL (Write-Ahead Logging) is slowing down your update? Also see this more detail explanation.",2
"After getting the correct repo enabled, use yum distro-sync to replace all the wrong versions of packages with correct versions of packages. You'll also need to remove some packages you got from SCL or other repos, such as those with names starting with php70-. I'm tried to understand how it is setup, but I'm missing something, so I'll reproduce what I've and what I've got so far: You've mixed a whole bunch of manually downloaded RPMs (some built for the wrong versions of PHP, Imagick or other things) and conflicting repositories. Stop now. Don't manually download RPMs. When I'm trying to run this: sudo /usr/bin/pecl install imagick, it's giving me the following error: So, I'm trying to install Imagick in order to be able to render/make an image off the first page of a PDF. You've done well to use Remi's PHP repositories, but you've enabled his repo for PHP 5.6, while you say you want PHP 7.0. You need to disable the remi-php56 repo and enable the remi-php71 or remi-php72 repo (7.0 is EOL, don't use it).",2
"The only thing you really have much control over at the client is the TCP Window scaling - which can result in fast connections with (relatively) high latency giving poor throughput. Note that all nodes on the connection must be capable of supporting rfc 1323. Is there some kind of IIS setting that is limiting the speed per connection, maybe something that is meant to save bandwidth for other users? The server is hosted with Peer1 with a 100MBps connection so it is supposed to be pretty fast. There is no other traffic on the server while I am running this test. There's all sorts of reasons why this might be the case. You might have 100 MBps at both ends - but what's going on the middle? And that's before you consider any policy restrictions at either end. I am running Windows 2003 Server. If I download a large file from the server in FireFox the connection maxes out at 100kB per second. If I download the same file from Internet Explorer at the same time, it also maxes out at 100kB per second. Therefore I am transferring 200kB per second from the one server on my internet connection to the same computer. If I also download in another browser like Google Chrome I get a similar result.",2
"I was wondering if any other NAS units provide the ability to mix and match drive size and still have one growing pool size. Unlike the drobo however it would be nice that if the device fails I could shove the drives in a replacement to get the data back. I've been looking at Drobo units lately, and the ability to just add in a drive of any size and have the useable size of the array grow seems really nice to me. What doesn't seem nice to me is that if the Drobo unit itself dies you lose access to all of the data on the drives and have to send it back to Drobo to get the data off apparently. That is a non-starter for me since it is a single point of failure, just moved from the drives to the Drobo, which should fail less often than drives but it is a piece of electronics so it will fail. Also are there custom solutions which give me what I'm looking for? I've looked into unRAID and that seems to offer what I am looking for but I want to know what my other options are since I'm sure there are a few things that I'm missing.",1
"One thing to do would probably be to find a better way to implement your software, using tools more suited to performance computing, or even a language/framework that is specific to your problem domain.  Many tools are out there, i.e. Mathematica, SAS, R, and on and on.  In addition, you should parallelize it as much as possible. You are using the 25% of the CPU because the COM object is single threaded and does not use the other cores of your CPU. You can not increase the performance of your calculation without modifying the COM object code. Also, avoid using the disk as much as possible.  If you are going to use disk, beyond RAM disks you should consider using a high-performance raid array like a RAID 10, or even better a RAID 10 composed of SSDs. You should profile your process while it runs and see how much is system time and how much is user time.  I'm willing to bet that much of the execution time is taking up in system calls to perform all that disk I/O.  Get rid of it. You can then click twice (not double-click) on a column to have it sorted in descending order of values, so as to check for excessive usage.",3
"At a high level, a player's turn could be the start of a new state, followed by all the possible actions that are allowed during that turn. Where appropriate, these functions return status messages and/or trigger events so that the GUI can show what happened during the turn or let the player know a computer was destroyed, etc... My player end turn function will perform functions specific to that player that isn't affected by the order it takes place between players: I have recently built a turn based strategy game similar to Master of Orion (it's only single player right now, multi-player would be a bit more complex, but a similar idea), here is what my endTurn function logic looks like (in my main Game Controller class): A turn based game is going to be governed by a state machine.  Basically, you would lay out a series of states that can occur in a logical order.   Obviously this will balloon quite quickly, as I've only sketched out an extremely limited plan.  Having a good grasp on possible states early on will mean that you should be in a good position to implement.  I'd highly stress sketching out exactly how you want the game to run....a good turn-based game requires a lot of planning IMO.",2
"BTW, this method is being obsoleted by newer, dependency based startup systems in newer Linux distros. The best answer I found to this on another website is that the K and the S are ignored on run levels 0 and 6 as the system will simply executed all of those scripts with the stop command in alpha-numeric order anyway... So S or K doesn't even matter. The K examples have stop commands within those scripts. The scripts also specify default run levels when the start and stop commands will run like it shows default start on run levels 2,3,4,5 and default stop on 0,1,6 Yes, the traditional system V init style (what that is) makes symlinks that start with S, or K. those with S means ""start"", and they are run with the ""start"" parameter when that runlevel is entered. Those with K means ""kill"", those services are run with the ""stop"" parameter when that runlevel is entered. This makes the different run levels have different sets of services running. If you cange one from S to K the server won't be started, it will be stopped then. That may or may not be a problem depending on whether or not it was a critical service.",2
"I had this issue after I installed Oracle VM and a Lubuntu image. Right after the Oracle installation everything was fine and I also managed to install Lubuntu. I've done everything through RDC. Suddenly, the screen went black. I haven't restarted the computer or done anything worth mentioning at all, it just happened. This all happened in one single session. In the end, I restored my computer to the state before I installed Oracle VM with a restore point that was generated automatically by my computer. Now RDC works again and I'm able to see the screen. I then tried restarting the computer manually, I tried all of the above settings which helped other people and I also uninstalled Oracle VM. Nothing helped. I had this issue since I was having different DPI. On my laptop I had DPI set to 125% while a computer I was connecting had 100%. Setting DPI to 100% on laptop solved the problem. I've had the same problem - switch from 32bit to 24bit colours and it will let you connect as normal. All other settings can be left on their defaults. When I have this issue I move the RDP screen to my default screen and maximize the screen.  The RDP screen should no longer be black.  I then move the RDP to my secondary screen.  Not the best work around, but it works.",4
"into cron to be run every 5 minutes or so and after a crash have a look what was eating your CPUs just before server crashed.  As for automated reboots when the server becomes unresponsive, what you'll want to look into is called watchdog (ubuntu man page). For monitoring you may try to use monit -- it should be able to restart a runaway server, if you put it under its control. my server has been going down over the last 24-48 hours, CPU spikes from 15% to 100% and server becomes unusable and all my sites go down as a consequence of it. You should turn on Linux Process Accounting if you want a more detailed historical view of what was using CPU and other resources at the process level and user level than /var/log/messages et al. normally provides. than you can try to setup something like virtualbox for test purposes, and run yours main servers in it. this will decrease productivity but add some stability and you could access it.  Any suggestions on software that could help me prevent the CPU to max out indefinitely, and maybe force an automatic reboot of the box? you should try to investigate what the problem was. check for /var/log/messages before you reboot it and other logs by time.",4
"IMO certs are semi-worthless because they're always behind the times.  You think there's a cert for running large cloud infrastructure?  There might be one for running vmware farms by now, but what about xen and virtualbox and kvm? I just wrote my LPI-101 2 hours ago and I totally agree with negu. I've been using linux since late 90s and I had to learn commands and options that I never knew existed for the LPIC-101 exam, even though I would never probably use them in real life. My skills in the real world, (DNS, virtualization, apache, security, samba4 AD DC etc) on linux go beyond LPIC-101 (more like LPIC-102) but taking an LPIC-101 course was helpful as it helped me to better understand some commands and techniques that I never knew existed and therefore did increase my knowledge overall.  Once you have a few years experience in your chosen field (in this case Linux system administration), it has practically zero value. I see jobs looking for product specific certification (RHEL, Cisco certs etc.) but these are usually only valuable coupled with experience and most companies worth working for will rely on far more than a certification to prove your worth... I guess it would be ok for a novice sysadmin position. But with intermediate/seniors, I'd be more interested in their work experience and mindset. Being in IT for 15+ years now has taught me that I would never hire someone for my business based on certs alone, BUT someone that did have some certs and knowledge with real-life experience is what truly matters. Would I hire someone that had zero certs but seemed smart? No. Would I hire someone that had several certs but not much practical experience? Probably not. Would I hire a person that had a combination of both? Absolutely. A good interview with the candidate should sort this out though. I always thought that training courses, diplomas, and certifications where for people who didn't do the job, but only read about it. I never had time to do any in any of my jobs - the training schedules never matched when I needed the particular skills anyway.  It shows you have a minimum set of skills at a point in time, and if you are entering the job market I think it does have some value (assuming you are applying for junior or trainee sysadmin positions) Potential employers are likely to be impressed with a senior LPI qualification, particularly if you work in the contract world.  However, in reality I don't think it's truly a badge that demonstrates your professional capabilities, for anything but a junior admin.   There are always exceptions of course, as some people are gifted with technology that could actually fail an exam. Many hackers know of concepts at the network or binary level yet could not answer a simple question such as ""What is the path of the package cache on a debian based system?"" One significant value in doing these sorts of certifications is that it forces one to look at and experiment with every aspect of Linux administration.  This is especially beneficial if professionally your current role only exposes you to a certain subset of the discipline. However, for your own development, discipline, and interest, I'd say they're well worth doing.  Especially if you can get your employer to pay for the exams, or you do them for free at a conference! :) Certs like that only really matter to large companies, because they're an easy checkbox for the HR person to see what you've got.  I'd never hire someone solely because they have a cert and I'd never discount anyone solely because they don't.  If a cert is the only difference between two people, I'm still going to want to interview them both, because it's the skills and your facility with them that matter. The more senior exams are really quite well thought out, and go into considerable detail.  I would say it would be difficult to pass LPI2 and certainly LPI3 with a purely academic knowledge.  I would, however, state that in my experience the best certification courses are those with a heavy bias towards practical testing, such as the RHCE. The early LPI exams are easy, but well-designed, comprehensive, and kept up-to-date.  They are also vendor-neutral, which is ideal for junior sysadmins wanting to get up to speed. Nothing beats real-world experience but a certification program does help hone skills as it forces the individual to study and take the time to learn. All knowledge is good, and some kind of proof of it is good just like a driver's licence, pilot's license etc. You could be an excellent driver yet not have a drivers license, however, a good driver with a proof of license is even better. At least then you can base your decision off some kind of foundation. It's very popular in the Linux / Open Source world to bash certification programs.  There is every truth that there is no substitute for hard-earned experience, but my experience, both as a Linux and Solaris certified engineer, and as a hiring manager, is that there is value in them.",5
"I've been seeing a lot of junk in /var/log/messages on one of my servers, lately. There don't appear to be any associated I/O errors and the server is still performing its intended tasks without any appearance of a problem, but I was wondering if this is indicative of some kind of other failure that I could be concerned about. Try to track down where that stuff is coming from.  If it's being logged by something (and thus has the normal header that syslog adds), well, maybe that program has a problem.  If on the other hand syslog is writing that stuff out instead of what you expect, I'd get worried.  Any errors in dmesg?  Segfaults?  Have you done a memtest recently? Is this all that's getting into syslog, or is it scattered among ""normal"" entries? I'd try shutting down the log daemon, move the syslog file, start it up again and see if it persists. If so, schedule a fsck and restart. Check if your partitions overlap, then run fsck on your /var or /var/log filesystem (you do have a /var and/or /var/log filesystem, don't you?)",4
"-NAT: if want/need your VM to effectively share your windows machine's IP and only receive incoming connections from your windows machine, this is the way to.  The really nice thing about this option is that as you move from place to place (common if it's a laptop) the IP of your VM  will stay constant even as you change networks and ip ranges on your main machine so it'll always to a snap to connect to.  If you need other machines on the network to talk to your VM, though (running a webserver?) this isn't what you want. -host-only:  you probably don't want this.. this will your VM talk ONLY to windows.  No ubuntu updates, package downloads, no contact with the outside world from your VM Ss for the Bridge problem:  Just below ""bridged adapter"" there is ""Name"". You need to choose which adapter you're bridging to, so open that dropdown and choose your main adapter.  (if you do an ipconfig /all in a cmd prompt you can find the name of your mail interface by whichever one has your operable IP address -Bridged:  this effectively makes your VM an 'independent' member of your network.  It'll have its own MAC address, its own ip (assigned or it can do DHCP on the network).  Imagine plugging you windows machine and your VM into a network hub and plugging that hub into the wall -- same idea but virtual.  If you're using wifi, you won't need to connect to wifi in your VM, you'll just connect through whatever wireless network windows connects to with the above benefits/detriments.",1
"That Microsoft moved the Users folder to the root directory for Vista and 7 probably is a good indication of how many people prefer the root directory (default ""c:"") in practice. However, I'd bet that many programmers were lazy and just decided to plop their program into C:\ by default, just out of convenience. Some programs malfunction when they exist in a path with spaces (i.e. ""Program Files""), which is why they are installed to the root of C:. As Tonny suggested, this is partly to simplify tech support, but it's also for your own convenience.  When you extract the driver files to a location that's easy to find and accessible to all users, it's also easy for the system to locate the files again if you need to reinstall the drivers.  Unfortunately, most vendors do not clean up old files, so you can end up with gigabytes of old driver packages that will never be used again.  In some cases it may be more appropriate to extract them to %TEMP% or to the ""All Users"" account, and some vendors do exactly that. You state that this concerns Windows only, but the software may be ported to/from other operating systems.  Also, there may be differences between Windows versions, e.g. some translated versions of Windows also have this folder name translated, and the user can also change the location (e.g. instead of ""C:\Program Files"" I used ""P:\"" for a while).  This can all be detected by the software, but it's easier to use a fixed location.  Also  in the case of drivers it could be necessary to know where the files are at a time that this information is not available from the OS. Although 'most' software and configuration files can handle long directory paths, mixed capitalization, spaces and 'nonstandard' characters, they don't always and tripping up on this in the middle of your work-flow can be a minor hassle..  Some developers don't follow standard conventions because they're either unaware or too lazy to do things ""right.""  Many amateur developers will also hard-code paths into their software and/or installers rather than querying the environment (e.g., %TEMP%, %APPDATA%, %PROGRAMFILES%).",4
"Now, after a reload the router waits 5 minutes before configuring NTP, which seems to work OK so far ... It's a bit of a hack though - there's probably a better way ! This setup works fine when issued in Global Configuration Mode when the Dialer0 interface (ATM0.1) is up.  The configuration fails at startup though: Is there a way to delay the NTP configuration until afte the Dialer0 interface is fully initialised?  Can the NTP commands be triggered by the Line Protocol on the Dialer0 interface transitioning to the up state?  Alternatively, can the NTP commands be delayed for 5 minutes after the router has finished initialising? Obviously the DNS lookup for the server(s) fails because the DNS servers cannot be accessed because the external interface is not yet online. OK, I think I might have solved this - though my head hurts now!  Posting for others who might encounter this ... I have a Cisco 877W which I'm using for my home ADSL connection (and as a refresher in Cisco IOS).  I've got a working config in-place with my PPPoA connection coming online correctly, and VLANs and other settings configured as I want them, but I can't crack the NTP configuration.",1
"I just pressed left and then enter. It worked for me. This is the fix that helped me but I believe it should be fixed permanently.  I killed the installer and created the folder manually and ran the installer again. it worked fine after that. I've struggled the same problem and I think the problem comes from gnome failing to pop up the confirmation dialog. That's why there is nothing in the error logs. If you don't know how to install vnc server on your CentOS 7 or Fedora box here is a guide: https://www.howtoforge.com/vnc-server-installation-on-centos-7 If you prefer to install oracle via vnc then skip the last two lines and change the /home/vnc-user/.vnc/xstartup ( vnc-user is oracle at my box ) like this: The same issue I have just came across on CentOS 7. The installer form is disabled and does not allow any input. But clearly it is waiting for some input. In my case it is a GUI glitch. I was not able to find and resize any dialog. However, pressing Enter was enough to return focus to the main window. Though I can't really say what I confirmed or cancelled. :)  The installer is displaying the directory where it suggests to install the inventory. If the directory doesn't exist, it seems that it will ask you either to create it or not (can't tell exactly as the message box was so tiny) Also check /etc/oraInst.loc to check where the installer thinks your orainventory location is. If you got to step 6 of the installation, then some log will have been created. I suspect you are looking in the wrong place. See: It would probably work with another manager or there may be a fix for gnome but here is how to install xfce:",5
"Is this easily done using JS or PHP; if so, could you give me some pointers, and if not, what could I use to have this kind of game running from a browser rather than an executeable? If you can nail down your game processing logic, then animating it should be a simple step forward.  But, debugging AJAX requests can certainly be a headache if you're not careful. I'd like to make a simple text-based game using either JavaScript or PHP. Ideally I'd like to be able to render ASCII text in a kind of blackscreen ""console"" style (think console roguelikes such as Crawl or Nethack, or old Infocom text adventure games). The game would be simple, mostly text entry and menu choices.  Yep, should be pretty easy to do in HTML/JavaScript. Take a pre block and style it as you see fit. For input you should probably ditch text input fields and just use the key events. You could build the entire game in JavaScript for sure. Be aware that the source-code will be visible to the user. So if you're concerned about cheating (probably a non-issue if it's just a single-player game), then you should have your game logic on the server. Before you get into animating, make sure your PHP is solid.  One good way of testing is to have several buttons on the page for your various actions, and use them with a simple POST call.",4
"Unfortunately I don't know of any way to ""undo"" it, but you can probably leave system files as owned by root and restore all the files in your $HOME to be owned by you (and do the same for all users of the system).  At that point you can fix permissions and/or owner on each file not in your $HOME directory that needs it as it comes up.  Yes this is a pain, but I don't think there's an easy fix.  That is what I would do anyway. ""Not quite"" in the sense -- are you sure you were root when you did it and did the command go through to the end? If you've cancelled it, as soon as you saw it, then you might be lucky and the repair may be low. If you weren't root, this command shouldn't have been able to do it, unless you did something like sudo .... It is also important not to restart the system before checking all the applications running and the user launching them on boot. If you do, some of them may not start properly due to permissions problems. ""Very"" in the sense that if the command has indeed gone through, your security is screwed. You now do not know which paths have what owners and who should be allowed to do what. If your using OSX apple provides a restore feature within Disk Utilities to fix this very problem. If your using a linux distro, I'm fairly certain you'll have to redo all permissions manually. In either case, smack your hands and don't do it again There is no single remedy to this. If you have a backup, you can restore to it. You might need to check the ownerships in the backup and apply them. If you have been using a rootkit (say rkhunter) checker, it might have a list of the most basic ownerships and possibly be able to fix it. (Not quite likely).",4
"If you absolutely must have acme.local go to your website, you could edit the default.htm in your DC/DNS Server's Default Website in IIS and put in a meta refresh tag or do a permanent redirect (301) as such in IIS 6 (I think IIS 7 is the same): Is the domain in question your Active Directory domain (i.e. acme.local)?  If yes, there's a reason why that resolves to your DC/DNS server and should stay that way.   About the problem, x.y is your ACTUAL DOMAIN, so any query asks for x.y domain, always {should} go(es) to your AD's DNS, and because of the AD behaviour, it will resolve to the address of DC and you'll get that DC's IP adress ;) ( this will be more and more annoying when you have multiple DCs, because there's a very good chance to get different IPs {which will be that DCs ;) } ) And there it comes the 3rd mistake: Entering a host record ""www"" to DC's DNS that faces to outside... When a www.x.y query comes, DC's DNS will resolve that corretly, but, IT IS your DC! Your hearth, your center point! ( this is connected to second mistake i mentioned above ) And of course, semanticly, you'll want to query x.y to get your web server but no, what will return to you is your DC... Second mistake is using AD's DNS as your outgoing DNS... Always use another configured DNS server which faces the internet, and to achieve that you should name your domains differently ;) And configure that outsider DNS to relay internal queries to AD's DNS... ( redirect x.y.local to IP(a.b.c.d) ) if i try the website with ""www"" it will work and go to the iis server xxx.xxx.xxx.174 but without the ""www"" it will go to the xxx.xxx.xxx.175 (the active directory and dns of the network) and becuase that there is no iis website set on it it will show iis under construction error. About the solution, you'll have to configure the IIS on that DC and redirect your default site to your actual web server ( www.x.y ) ( like ""gravyface"" explained ) unless you dont want to deploy a second DNS server to your network to achieve this Front-end Back-end acrhitecture... The easy solution is to put a redirecting site in the IIS root that redirects all traffic to ""www.domain.co.il"".  First mistake is naming your outgoing domain same as internal domain. You can simply make them seperate ;) Like outgoing: something.com and internal: something.pvt or something.com.local. Just differ your name ;) Having said that, you can certainly create as many A records as you want internally in DNS on your domain: i.e. if it's an intranet there's no reason why you can't create intranet A xx.xx.xx.174 and have www.acme.local as a CNAME to 'intranet'.",4
"What that means is: one of the clients is also the server (they are the ""host"" and act like a server for all of the connected clients). In my project I was making a Tower Defense game and used Game Center to let each copy of the game that was running know about where different towers had been placed by the other people playing and give a damage, rate of fire, special effect, or range boost to towers placed in the same spot. So a poison tower placed by Player 1 would mean that Player 2's bullet tower in the same position got a small DOT effect. If Player 3 also placed a poison tower, then their tower and P1's tower would do slightly better poison, and P2's bullet tower's small poison effect would increase (the bullet tower also applied a small buff to the poison towers, but I forget what it was). You can do multiplayer this way and for some games it works really well (things like Space Team, Spyfall, or Codenames, where 'cheating' isn't important because the game is short and fast paced and switching to a different person as host if someone's cracked their copy is easy. I used it once a number of years ago using Prime31's Game Center plugin. What it does is facilitate two (or more) copies of the game communicating with each other in a serverless multiplayer context.",1
"Time to make a cache right? Let's forget for a moment any complication related to how you could make it memory efficient, and analyze this particular scenario: Though appealing, this system is inefficient unless you keep in memory the chunks surrounding the chunk the player is in (if an enemy needs to spawn on a chunk which is currently not in memory, you have no way of knowing wether you can actually place it in a specific tile or not, since it may be occluded by a wall) which is why I'm currently spawning enemies as soon as chunks are loaded, but there's a problem.. I'm currently working on a 2D open world game loaded in chunks, initially I thought It would be sufficient to spawn entities randomly in a specific ""radius"" close to the player, and periodically remove distant enemies. A massive complication arises when enemies are spawned with specific criteria. Assume a little dungeon spawns inside a chunk, it needs 2 enemies and a chest with loot in-between them. When you're done with it, it shouldn't respawn unless an hour passed. You can't just spawn those enemies inside the structure as soon as it's close enough to the player, otherwise you could go back 'n fort, kill the enemies and keep looting the chest's content.  Is there a better solution? How do modern games handle enemies spawn? How do they get around these difficulties? I'm particularly interested in knowing how online games do it considering the crazy amount of memory (which should be shared between servers) such systems may require Doesn't look too bad, but if a player moves in a straight line in my game for an hour I would need to cache about 200 chunks before cache entries start expiring! I thought about caching all enemies that where present inside a chunk when it gets out of view and restoring them as soon as the chunk is visible again, yet again this requires caching every entity inside a chunk and keep the entry available for an hour, and periodically check if in that particular chunk an hour is passed and we can spawn again enemies",1
"Since you didn't specify what exact type of a ""hack"" it was, I think I can assume you got compromised through SSH brute force. Here are some general tips: ...we switched from Windows to Linux recently because it was supposed to be more stable and secure. Go figure. The hacking of your site likely had very little to do with the underlying operating system, and more to do with the code running on your site. All it takes is a single SQL injection, and you're history. Others have already covered most of the important points.  But to add to what they have said security is only as strong as the weakest link in the chain.  This is almost always the human element involved. On the other hand, if it was a vulnerability in your operating system, you may want to try switching distributions to something intended for web serving, like FreeBSD, CentOS or RHEL, assuming you aren't already using one. You may want to consider beefing up SELinux, or adding an Intrusion Detection/Prevention System of some kind aswell. These rules will allow 3 ssh connections from an ip address on eth0 in 60 seconds and then block that ip address for 60 seconds.  This is normally enough to cause the automated attack to move on to another host. In many places, it is a legal requirement to notify your customers of security breaches if personal information was potentially compromised, you may want to look into that. As for how to tell your clients, if any of their data was exposed or even POTENTIALLY exposed, AT ALL, call them ASAP. No email, no hope it goes away. Use your phone that's sitting on your desk. I've posted some comments on the various bits of specific advice that has been offered to try to avoid future attacks. I found myself in a similar situation just over a year ago, and certainly analysing the attack to improve your setup is important, but imo right now you should be worried about the servers you have running. How do you know you can trust anything on them? If you can't, then you need to wipe them. Since you were blacklisted by Google, I'll assume someone managed to set up a malicious script on your server, in which case you might try something like mod_security, it's not a piece of cake to configure, but it's worth a try. It is, however, of paramount importance to ensure your code is free of these kinds of vulnerabilities. Besides legal repercussions, you were providing a service to them that they were undoubtedly using to provide services for others in some way. You owe it to them to disclose any potential breach of data so they can adjust their workflow accordingly and notify anyone else downstream from them that it may affect. You will need to make sure that these iptable rules are restored when you reboot.  Different distributions have different ways of doing that so google that or let us know what distro you're running and someone can provide you with that information. Assuming that this was a brute force ssh attack if it only took them 20 tries then your passwords were pretty much non-existent.  These attacks are almost always automated attacks and are pretty easy to block with the following iptable rules. Linux is more flexible than Windows in many scenarios, and thus can be made more secure when those certain situations arise. Switching from Windows to Linux for no real reason, especially when you're unfamiliar with the environment, is a bad call. If you run an internet-facing server and don't understand how the services on that server work, whether it be Windows, Solaris, RHEL, or BSD, you're asking for trouble.",5
"This is how ""psql"" works.  When it is waiting for the keyboard, that is the only think it is waiting for.  Can you imagine how annoying it would be if messages were received and displayed while you were in the middle of typing? No, but you need to implement some sort of event loop in your application to detect the notification and handle it. But neither the detection nor the handling need to send any query to the database server. It is only in PSQL when I run any statement do I get my notification. How might I be able to see notification are made without doing anything? Is this possible? If I have to run a statement like SELECT 1; what  benefit might I get from listening in the application layer? Would I have to be running SELECT 1; in a constant loop every second or something to get notified? An application doesn't have to poll the database to receive notifications, but it has to poll the status of its socket (network connection) to know that there is a notification to collect, otherwise there's no way to know about it. psql doesn't do that when waiting on user input. It blocks on user input, as most command-line oriented applications. Use some other tool, which is not principally designed for interactive work.  It is fairly easy to make a perl script, for example, which displays notices as fast as they arrive.  I think the latest JDBC makes it easier than it used to be to do this in JAVA as well.  I'd be surprised if Python doesn't have a solution, although I don't recall seeing/testing one.",3
"It's fairly unlikely Google Bot will run any Javascript on your site that will track it arriving, so the best hope is to look at the site logs themselves - either in webalizer or the hundreds of other log file parsers, or just open them up in a texteditor and run a regex to extract the Google Bot user agent. Just check the webserver logs if the visitor's User-Agent request header matches/contains Googlebot. There are lot of webserver log analyzer tools, either free or payware. Most of them are also able to categorize bots. From them all I've had the best experience with Google Analytics. You should see then how many hits it makes in a space of time. If it's too much you might want to look at using an XML sitemap AWstats is a good, free utility to use for log file analysis. It'll give you a lot of interesting statistics in addition to the Google crawl info. I would recommend Google analytics, it covers all visits to your site and you can work out whether a visitor was a bot or not.  It is also excellent for SEO on your site helping to increase visitors. Signup for Google Webmaster Central, aka Webmasters Tools, verify your site, and wait for the stats to appear.",5
"Modifying this setting requires the user to log out and back in before taking effect, but we find that the hardware profiles for tablets are automatically setting it to 150%, which causes issues with some legacy applications we use. We also have a few users that keep their scaling at 150% for visibility reasons.  In Windows 7 and 10 (possibly in earlier versions as well), you can change how the DPI scaling for your desktop, essentially ""magnifying"" items on the screen. In Windows 7, this option can be found under Control Panel > Display like so: I'm currently tasked with either A) finding a way to make this change more convenient for the user, or B) completely redesigning the way the applications handle scaling. For obvious reasons, I'd much rather do option A if it's available. If I can change the scaling programmatically for the app's purposes, then back without user intervention, that would be ideal as well. Unable, I've been unable to find out how this feature actually works, and/or why it requires a logout.",1
"I know one way to find out would be to try it, but there is a possibility, that I would just pick the wrong restoring software. Moreover, I'm more interested in theorethical explanation why it would/wouldn't be possible. Thanks Of course, without the correct passphrase, the disk/container will look like noise to any program, so you can't do the recovery without actually opening the disk with the respective decryption program. There are ways to restore files, deleted from the system by default, I'm not sure about the way they work but I guess thet read content, that has not been overwritten. On the other hand, there are programs (e.g. TrueCrypt), that encrypt disks, claiming that it wouldn't be possible to tell apart random data and file contents on such a disk without a password. Therefore I think that files, deleted from such disks can't be restored. Is that correct? As far as TrueCrypt is concerned, when the correct passphrase is entered, it seems to show the encrypted partition as a standard partition, formatted with whatever filesystem you used there - so while the partition is decrypted, you should be able to use the usual file recovery methods on it. If the file deleted was on a FAT/FAT32 partition (encrypted through TC), it should be recoverable; other FS types are more tricky.",2
"I can see the CPU and Memory spike, but what I don't know how to do is quickly identify which site is getting hammered. Usually I find myself running tail -f on each site access log for a few seconds until I find the log that is being written to more often than expected. I have a couple of clients that run multiple, low traffic websites on a given Ubuntu server. When I set these things up, I always configure each Nginx virtual host to write to its own access|error log file. I'm wondering whether there's some kind of one-liner script that could help me quickly identify which of these access log files is growing fastest or getting written to the most or whatever. Just anything that gets me closer to identifying the target faster than I'm currently getting there with my laborious approach. Assuming you keep your log files under /var/log/nginx with names that include access, this can do the trick: Notice that this is a ""more or less"" indication, if you're under a Syn flood attack or such you should check other stuff. That approach works great for almost everything, but the one scenario where it bites me from time to time is when one site is getting hammered by some kind of brute force attack or even weird outlier events that aren't malicious, but generate similarly unnecessary and unwanted traffic.",2
"On Synergy, on the other hand, I believe you have clipboard and screensaver synchronization, but aside of that, it is just a KVM switch. I've doing some testing and it seems Maxivista in remote control mode is better than Synergy because you can keep keyboard focus irrespectively of what screen your mouse is. Edit to add: Unless you want to have a dual monitor setup for one of them and don't need both desktops visible all the time. Then I'd recommend putting both onto a single machine and RDP (Remote Desktop) into the other. (Yes, remote in from two feet away!) I haven't used Maxivista, but if its a choice between Synergy and a hardware KVM when you already have a monitor for each, Synergy will win every time. Maxivista's remote control mode is a little more than that. For example you can use Windows' screen keyboard to type on the other computer! magic!! I do just what you're describing all day long every day. Two machines each with their own display and one keyboard and mouse on the ""server."" Sometimes the machines are both Windows, sometimes one Windows and one Linux. Synergy + is so easy to use and get used to that I don't really even think about it any more. One thing I dislike about Synergy is the fact I still must have keyboard and mouse connected to client pc in order to run synergy in case it fails or if I reboot it. I don't know about kvm.",4
"Restoring though is going to be trickier if you are using POP3. Using IMAP is easy, delete the profile and start again. Though that does loose your filters, saved searches and so on so export what you want before deleting. With IMAP, everything is kept on the server and Thunderbird just keeps a local copy if you ask it to. If you are using POP3 rather than IMAP, I think you should see similar things though with a slightly different layout under a POPMail folder - though I'm a little rusty on POP3 mail in Thunderbird as I haven't used it for a very long time. It looks like there is not just the <containing-folder>.sbd directory, but also a <containing-folder> file. After having restored that from a backup, I could access the subfolders again. If the <containing-folder> did not contain important information, but the subfolders do, one also can reactivate the subfolders by creating an empty <containing-folder> file. In that folder, you will find a bunch of stuff including sub-folders for each server you connect to. An in there are a sub folder for the top level of the mail server and in there are files that contain the actual mail. Let me know if you are using POP3 and find a file for the folder you want to restore and I'll try to remember how to restore it (if it is possible). My situation was that the mail server was wiped and I only had Thunderbird local copies. The only way I was able to make Thunderbird see those folders was to copy the IMAP mailboxes to the Local Account, see the images below. Make sure Thunderbird is shutdown, also there was some corruption but it was better then nothing, I don't think the .msf files are needed as they are re-generated.",3
"Currently I keep audio recordings on a PC, with a backup on an external HDD. I keep a subset of these on a couple of USB audio player and some more in a mirrored directory between all the PCs I use. Video material is either on a PVR or on DVDs. I do use Magnatune, but just for downloads, no subscription. This is, in my opinion, a massive problem and one that the companies selling digital media will need to address at some point. How are we going to be able to have a fully digital media collection? How can this work when a typical HD 45 minute TV show is at least a gig in size?  I use Windows Home Server.  It can be expanded to have any number of hard drives, can be backed up without much fuss, and can stream the media to any DLNA device in the house. But this is not practical really. It doesn't connect to a network in order to steam the media to many devices and space will eventually become a problem again which would require more external hard drives and it all becomes a mess!  Option 4 combined with option 2 and all DRM-free media probably gives the most versatility. Option 2 would be my second choice, but that probably requires DRM-free and a licence that allows backup copies (e.g. in the long term there is little point having backup copies of the downloaded media if they can only be played on one particular hardware player). I would quite like to be able to subscribe to programs, ala iTunes Season Pass, and download high definition copies. All my music is digital now and I tend to prefer a digital format for my videos as well. I currently have a 1tb hard drive onto which I burn my DVDs so that on one device I have my entire collection. I see two solutions: Home media servers or, when connections are fast enough, our media stored in the cloud. Maybe one subscription opens up a world on demand media.",3
"I believe a work around using DirectoryIndex instead of FallbackResource might also work. The following will match /a/script/ to the file /some/path/to/a/script/index.py then  /some/path/to/a/script/index.html then /some/path/to/subfolder/index.py and  finally /some/path/index.py. This relies upon DirectoryIndex honouring absolute paths. Repeated calls to DirectoryIndex are appended to previous calls. In my case FallbackResource was not working and i stumbled upon this as a solution. The above is for CGI/Python files but I believe it will work in your scenario as well. In your case you would need a top level .htaccess file with FallBackResource /index.html, then FallBackResource /subfoldername/index.html in each relevant sub folder. The sites about which I originally asked this question are no longer live, but here's what i ended up doing. I can use it the way it is if that's the answer, but it would make it a lot easier to manage the sites that I have if this can be done.  The sites use the same basic code with different themes and database connections, and the way it works now I have to modify the htaccess file every time we deploy a new version of the code (because it is checked into Git).  If we can make it that someone doesn't have to remember to make this modification every time, that would be really great. As you could probably guess, the / at the beginning means that the path is relative to the site/vhost, and here the path needs to be relative to the directory. I would have thought that the FallbackResource path is relative to the .htaccess file in which it is configured, but it seems that it is actually relative to the requested URL. I manage a large number of PHP applications using front controllers and the following htaccess file: However, when the site's rewrites contain slashes, for example, if the application is /store/ and the path within it is products/1234, then Apache looks for /store/products/index.php instead of /store/index.php and returns a 500 with the following message in the log: I ended up moving these sites to a server running Nginx, where all configuration is done in the config files of the server using a try_files directive. It was much easier to manage that way. I don't think this is possible to do easily. I've tried various combinations of DirectoryMatch and LocationMatch in the main configuration files using the newish variable interpolation available in these two directives. Although something like FallBackResource index.html is entirely valid configuration, I share Michael Hampton's concern that any use of relative URLs can cause problems. In the case of FallBackResource index.html for a particular directory tree, index.html must always be present otherwise you will get the recursion loop you describe for requests where index.html cannot be found. (If I were using mod_rewrite for this instead of mod_dir, I would have to add a RewriteBase to each subdirectory as needed, in a similar fashion.)",3
"Linux from Scratch will teach you everything you wanted to know about Linux, and the stuff you were too afraid to ask about. Technical knowledge is needed, but soft skills, organizational and operational context and processes place it in perspective. As well as linux nitty gritty,I think getting meta about system administration is not just useful, but needed. There are good books on specific topics, and good books on basic stuff (in both cases, check out O'Reilly books), but there's nothing that I'm aware of that is a comprehensive middle ground. One of the best learning tools that you'll find is a good virtual machine manager, such as VMWare or VirtualBox. You can set up a complete virtual learning environment right on your pc, and experiment without the worry of making a mistake in a production environment. I learned a lot from the Debian Reference when I was starting out long ago. While some sections are Debian-specific and it of course uses Debian file paths and conventions, a lot of the information is very general. It covers everything from authentication, to network services, to backups, etc.",5
"The environment: *Client/Server system, both running on the same machine. The client is actually a service doing some database manipulation at specific times. * The cnonection comes from C# going through OleDb to an EasySoft JDBC driver to a custom written JDBC server that then hosts logic in C++. Yeah, compelx - but the third party supplier decided to expose the extension mechanisms for their server through a JDBC interface. Not a lot can be done here ;) The Symptom: At (ir)regular intervals we get a ""Address already in use: connect"" told from the JDBC driver. They seem to come from one particular service we run. Now, I did read all the stuff about port exhaustion. This is why we have a little tool running now that counts ports and their states every minute. Last time this happened, we had an astonishing 370 ports in use, with the count rising to about 900 AFTER the error. We aleady patched the registry (it is a windows machine) to allow more than the 5000 client ports standard, but even then, we are far far from that limit to start with. It is a Windows 2003 Server machine, 64 bit. The only other thing I can see that may cause it (but this functionality is supposedly disabled) is Symantec Endpoint Protection that is installed on the server - and being capable of actinc as a firewall, it could possibly intercept network traffic. I dont want to open a can of worms by pointing to Symantec prematurely (if pointing to Symantec can ever be seen as such). So, anyone an idea what else may be the cause? stupid problem. I get those from a client connecting to a server. Sadly, the setup is complicated making debugging complex - and we run out of options.",1
"Ok, after some searching and testing I figured out the answer.  Asuming you can delete all of the reverse lookup zones in the sub domains do the following:   I have 3 subnets, lets call them 10.10.10.1/24, 10.10.10.2/24, and 10.10.10.3/24.  I also have an Active Directory Forest (all DCs/FFL/DFL @ Server 2008 R2) with 2 subdomains, we will call them contoso.com, a.contoso.com, and b.contoso.com.   Reverse lookup zones are becoming an issue.  For instance, the reverse lookup zone for 10.10.10.1 is on the DNS servers in contoso.com, but some servers in the a.contoso.com and b.contoso.com domains are also on the 10.10.10.1 subnet.  As a result, when a server on the 10.10.1 subnet is a member of the a.contoso.com subdomain, there is no reverse lookup zone for the 10.10.10.1 subnet.   AFAIK, if I create the reverse lookup zone for 10.10.10.1 in the b.contoso.com domain, it will create the recors, but DNS servers in the contoso.com domain will be unaware of it.  What is the proper procesure to get correct reverse lookup information across the board automatically?   Organizing forward lookup zones is easy, contoso.com zon on the contoso.com domain controllers, a.contoso.com zone on the a.contoso.com domain controller, etc.",1
"Zenoss will let you watch the server running tomcat, the tomcat process and monitor it via JMX as well.  There's a fair amount of documentation and it's open source. You didn't mention what monitoring system you're using; you'll probably have to set that up to be able to talk JMX as well. Zabbix has Zapcat, Here's some OpenNMS instructions, and here's a Nagios plugin. Take a look at Tcat Server from MuleSoft, it can provide you tons of information on memory utilization. You can use it with your existing Tomcat installation. Unfortunately, I don't think there is any way to break down RAM usage by individual web application, even using JMX.  However, just for this experiment, you could try running each web application in its own Tomcat instance.  Then measure the memory usage of each separate JVM. Try using JMX. Enabling this should be as simple as adding the following line to your startup script: As you can see, there are options for encryption and authentication to satiate your local Manager of Paranoia.",4
"Yes in this case its totally secure. ""Full format"" is only necessary when you want to erase your data entirely, so it won't get recovered by someone. Like when you sell your HDD. (But in that case you ""Secure erase"" it, IE. write random bytes on it.) PS! ""Full format"" nowadays means ""check the disk for inconcistencies"", not ""clean it completelly"". It in no way prevents anyone from recovering the files, it simply makes sure that all areas on the drive are readable. It's simply how drives now work. Is it virus-safe to do quick format of hard drive? I want to format disk that was infected and install windows 7 on it, but I am not sure if Quick Format is secure enough. I am aware that it does not delete data but pointers to it, so I wonder if it is possible that virus activates from that data? I've never seen a virus/worm survive a quick format unless it has installed itself into the BIOS.  I would pose the question, however, does it matter?  A full format doesn't hurt and only costs time on that one machine. To be precise: there is no real difference, as both are insecure. There are viruses which infect the MBR, which is an area not touched by formatting, be it quick or full. While Windows 95 basically killed off this type of viruses by doing some simple checks for them, running in protected mode and, generally speaking, preventing programs from writing directly to disk, they have made a comeback. Some time ago there was a run of a rootkit that infected the MBR under Windows. If you've got that then you have to boot into a clean environment, ie start from a live-CD or attach the harddrive to another computer, and clean the MBR before formatting the partitions. Overwriting the drive (whole drive, not individual partitions) is another way to accomplice the same.",4
"All in all, groupware in OS X Server is a collection of loosely integrated pieces. It doesn't offer very much, but what is offers is mostly easy to use and administer, and might be just enough for what Apple appears to consider it's typical server customers: Small creative agencies and the like. If you want more, every major groupware suite (Exchange, Zimbra, OX etc) will put Apples solution to shame.  There are more but most of them have already been mentioned. Remember that hosting websites and being email servers is not the only thing servers do. I don't understand why people think OS X server is more expensive than Windows.  You have to compare feature for feature what you're getting to get a fair matchup.  Windows does one thing or the other out of the box and requires additional licensing for extra stuff; Mac OS does it all. I have two Mac OS X file servers that have ~40TB capacity combined.  I use these in the Golden Triangle approach in our Active Directory environment, and it's definitely my server of choice.  With ZFS coming soon, it will be a no-brainer to use OS X server as the platform of choice. This is still a very new development, and honestly something I really don't understand where Apple is heading with.  Other major advantages of using Mac OS X Server (at least for Snow Leopard Server) are the following: First, I have to admit that I don't yet really use MacOS iCal server, and also have very limited experience with any other groupware solution, both as user and admin. Nevertheless, my impression from toying around:  My former employer bought one to support some Macs in infrastructure. In the end, it was a miserable failure, after a year of below-bar support, broken features and instability we scrapped the whole mix-in-some-macs project and sold all the Apple hardware we had. The primary reason I chose OS X over Linux is AFP, which is faster than SMB.  Yeah, I can use netatalk on Linux, but it makes a mess on the file system. We have one where I work, installed before I started there. It's used only as a file server for the graphic artists, who of course use Macs. While it's a magnificent machine it's also a complete waste of resources as there is no reason the files couldn't be stored on the main Windows file server (I've been overruled). I'm sure there are those who use a Mac server to advantage but I'm sorry to say we don't.",5
"If that's indeed what's happening, the simplest way to move the LOB data is by recreating the table. You can move the data either with bcp out/in, or by inserting directly into a new table and renaming afterward (or by any other preferred method of moving the data you have). Make sure you specify the correct filegroups for row and text data when creating the new table. Also as @RobertGannon said you can right click on the name of your database, then go to tasks, shrink, files Try using sys.allocation_units instead of sys.indexes.  BOL says it's for internal use only so I wouldn't write any long term code on it but for this purpose it should be ok.  It points to the first IAM page of each allocation unit.  I believe it will work better for what you are trying to do. The error is generic and misleading.  You will also get this error when a PARTITION SCHEME has been assigned to the FileGroup you are trying to remove. You can then select the ""Empty file by migrating the data to other files in the same filegroup"" to get all of the data off of a given file.  This won't help you get rid of a filegroup itself, but it will let you condense it down to a single file. You may have some LOB data (text/image/varchar(max)/nvarchar(max)) sitting in the filegroup still. I got caught up by this briefly not too long ago. Rebuilding a table/index on a different filegroup/partition does not move any of the LOB data.",3
"After playing around with it a few times, I found out that if I turn off the power supply for a few seconds before turning on the computer, it will boot successfully every time. This problem also occur when my computer goes to hibernation or sleep mode.   Aside from this problem, once the PC is on, everything runs smoothly, I did a few stress test and benchmark, the performance match other similar PC, and the temperature are okay.  I tried re flashing the bio, updating all the firmware for my MB, nothing seems to fix it. Any help or tips would be appreciated.  Occasionally, when I turn on the PC, it will light up for a few seconds, then lose power, and turn back on again, and with the same problem as above.  It is a new build, when I turn on the computer, the CPU fans, case fans, LED, power button all turn on normally. However, there are no bio screen, monitor does not ""wake up"", the keyboard won't light up when I hit caps/num lock. Even the reset button won't work. Sometime after a few restart by holding the power button, it will boot to bios, and it will tell me that ""boot failure detected"".",1
"The difficult part is probably going to be figuring our how to logically partition the database so you don't quickly run into storage constraints in the future.  Realistically, if your DB is growing at this rate, I would probably be looking for a large SAN or other multi-TB storage system, to prevent having to upgrade for a long time. I wouldn't want to be manipulating that amount of data very often. Something in the 30 - 50Tb range would last a long time at your growth rate. Does mysql support multiple file systems for a single database with most of the tables being on MyISAM?  With MySQL 5.1+ you can partition across multiple disks. As long as the new storage appears as a logical drive, you should be able to use it for a partition. One workaround I've done is placing the innodb datafiles on one device, and the datadir on another.  That has the effect of being a poor mans way of splitting your innodb and myisam tables across different storage.  But it does not help you in a one-giant-table situation. Instead of adding storage in the ad-hoc manner you describe you should backtrack a bit and start with an actual plan. Work out how much storage space this DB will require and then set it up accordingly, using the RAID method most appropriate to the way the DB is used. That's a rather confused question. MySQL doesn't support file systems, that's the job of the operating system. In short, if the OS supports a particular file system and is itself supported by MySQL then MySQL will happily run on that file system. The DB engine used (MyISAM, Innodb, etc.) don't affect this at all. i would shard that scary thing. otherwise you'll have really hard time taking backup of it and especially restoring or maybe adding an index/column. Similarly 5.1's native partitioning feature only breaks the table up into multiple files, but they all still live in the datadir. If you expect this growth to continue indefinitely there's no way around it, you need to design an abstraction layer across multiple mysql servers.   If you think it will taper you can probably get away with a large DAS or entry level SAN. Context: we have a 1.5TB mysql database, which is increasing at the rate of 200GB per month. The storage is directly attached, whose slots are almost full. I can add another DAS, and increase the file system. But resizing volume, resizing file system, etc are getting messy. Is there a concept of ""tablespace, datafile"" (like in oracle) in MySql world?",5
"That being said, I do doubt that this is why games like this go into slow-motion. In general, it's more for visual flair and dramatic presentation. Those physics systems can generally handle it, performance wise. Slow-motion is the effective equivalent of that. By decreasing the physics timeslice without also increasing the rendering framerate to compensate, the world appears slower. And therefore, you make it much less likely that you get multiple collisions within a timeslice. The problem is when objects collide. Suddenly, now you have to process physics forces within a timeslice, rather than just once at the beginning. If an object collides twice or three-times within a physics frame, then that's more physics computations you have to redo. It is possible that this could be the case. Unless you're doing physics for the collision on the GPU, it means squat for that. But in terms of the physics itself... it's possible. If you're simulating the movement of a number of bodies, they tend to move in a very predictable way. Forces and force fields (ie: gravity) are easily predictable. Where things move is quickly computed. If you have a lot of collisions within one timeslice, you can really kill your framerate. However, the chance of multiple collisions within a timeslice decreases as the size of the timeslice decreases. High-end racing sims like Forza and Gran Turismo run their physics systems at incredible framerates. I think one of them gets up to 300+fps on their physics update. When objects don't collide, you can just move them from the beginning of that 33.3ms to the end. The physics for doing so are simple and has been well-known for centuries. You just determine the acceleration from the net forces, apply that acceleration for the timeslice to the object, and move it at its new velocity (note: this can be more complex if you want greater accuracy). Right up until one thing hits another. See, in physics, you have what is called a timeslice; this is the amount of time that the execution of the physics system covers. If your timeslice covers 1/30th of a second (30fps for the physics update), then each physics update moves objects 33.3 milliseconds into the future.",1
"Google for ""iproute2 policy routing"" and look for ""simple source routing"" or somethign similar - it should get you where you want to go. You need to remove the route for 74.xxxxxx that uses NIC 1 or you need to have it have a less favourable metric than the route via NIC1 and 192.168.3.1 . What's probably happening right now is traffic is coming in on the correct interface, and then, if it's a non-local address, it's going out the default route to another gateway, which is probably a firewall that's dropping the packets because it's not seeing the other half of the session.  The reason it works from other devices on the same public network you are using is because they don't have to go out the default route - they have a more specific interface route to use, which gets the return traffic to where it needs to be correctly. In linux, you should be able to handle this by creating a second routing table that specifies the gateway on the public network side, whatever it is, and then a policy routing entry that says that anything with a source address of  should use that routing table rather than the system default one. What you need to do is have NIC2 be the default route via 192.168.3.1 . That should cause all outgoing connections, except those for 74.xxxxxxx to go via NIC2 and use 192.168.3.1 as the gateway. Assuming there is actually a gateway on each network, and you are just trying to define a specific behaviour (regular host traffic going out the private network, but public facing traffic sending it's return traffic to the gateway it came in on) - you need some kind of policy routing based on source address.  Also you can disable route path filtering with net.ipv4.conf.ethX.rp_filter=0 in /etc/sysctl.conf - this also can do the trick. Try to add default route with high metric on NIC1, it allows answering on requests, but OS will still route outgoing connections on default route with lower metric. Assume you have a Linux system with more than one network interface card (NIC)  say eth0 and eth1. By default, administrators can define a single, default route (on eth0). However, if you receive traffic (i.e., ICMP pings) on eth1, the return traffic will go out eth0 by default. You don't apply a gateway to an interface as such - you apply a gateway to a routing instance - generally, unless you've defined multiple routing tables, just one.  If I understand correctly you the NIC 1 IP to listen on 80 and 443 and respond only to incoming reuest on 80/443. You do not want NIC to be able to initiate/start any outgoing connections. You also need to add a route for the 74.xxxxxx network, that uses NIC2 as its interface and 192.168.3.1 as its gateway. That way anypackets that are destined for the 74.xxxxxx network will also go via NIC2 and the 192.168.3.1  gateway. You do not want the two default gateways you have specified now  - that's going to create strange behaviour - drop the public one from the configuration.  It should go in it's own routing table via the iproute2 mechanism. If you have no gateway set for NIC1 then it will only be able to respond to hosts on the same IP network (i.e. 74.xxxxxx) and on the same physical network segment.",4
"It is best to spend some time researching to ensure that you associate the correct person with the idea and mention related ideas by others. You wouldn't want a misnomer to be called a Squark - like Squark says: ""Be LessWrong"". :) By simply explaining the concept and mentioning that you believe it was first developed by a particular person you permit others to write to you, the author, and correct you with earlier references, this comes across as better than expounding falsehoods and detracting from the creditability of your work. There is a concept introduced by other researchers that I use in my work, and IMO it is appropriate to rename it to honor the inventors. Is it considered normal to just go ahead and name it like that in my own paper? Should I ask the inventors for their permission? Are there any other considerations I should be aware of? When the person whom you propose to associate with the idea is available (alive, and replies to email) it is useful and respectful to contact them; they may be planning a retraction or know of another whom is actually the developer, they mearly refined (or copied) the idea.",2
"Take the spider sounds, for instance.  Instead of just having one spider sound which gets louder as they get closer, consider having multiple, which all come in at different rates.  Perhaps have the music start to change when the spiders are 8 feet away.  Around 4 feet away, you start fading in the sound of their scratching legs against the trees.  When you get to 2 feet away (which may be right off screen), you could start hearing the noises from them mashing their mouth parts hungrily (why not... they're evil spiders!).   You don't always have to use powers of 2, though they are obviously natural for game developers.  Maybe the small animals flee when the spider is at 30 feet away, and the squirrel starts looking visibly nervous when they get 10 feet away.  Then, at 3.3 feet, color drains from the background. Many have mentioned music.  Sound is always good for gut feeling effects because we humans use it for wide awareness of our surroundings.  It's the thing which tells us to stop focusing on trying to fix our radio because there's a zombie behind us.  However, visual effects can also be used.  You just need to make the spiders have a presence.  A few ideas: The ""gut feeling"" people get is an amalgamation of many small cues that the brain processes together to try to guestimate an unknown (in this case, the position of the spiders).  Your goal should be to provide many small clues in many different ways, visual and auditory, rather than trying to come up with one big clue which tells you where the spider is. How you present these effects as the spiders close in is more important than anything else.  The gut feeling parts of the brain work almost entirely on logarithmic scales.  Thus we get the same amount of relief from the spider moving from 1 to 2 units away as we get from the spider moving from 2 to 4 or from 10 to 20.  Cues which work to this scale will be more well received than anything else. Note that a more solid visual representation such as the first two can be used in conjunction with the more subtle hints like the second two. Also, keep in mind that when you are trying to test for the overall experience, you should include every part of the experience when you test - which is why other answers mention music. You mention that the user feels ""safe"" if the spiders are all offscreen.  This is probably the most important thing to address.  Fortunately, you want the right kind of thing: a gut feeling that the user has that nothing is ever safe. Of course, for all of these, its best to fade the effects in gradually.  That way, for any spider distance, the player's gut has many smoothly varying signals with which to develop its gut feeling for where the spiders are.  The more small signals the player is paying attention to, the more they will be drawn into the moment.  This is the real key.  A small number of obvious cues will have less effect than a large multitude of small cues which all operate on different scales, and ideally you want to let the user construct a reasonably logarithmic worldview using those cues. sound of enemy(ies) approaching fused with music that hints danger to give the player a ""rush"" and visual effects that can dusturb the player into making a mistake.",3
"During my first checks in general performance and etc, i noticed that the temperatures were unusually high for a brand new laptop computer (idling at around 80C). There seems to be a temperature differente of aroung 20C between the 99% power state and the 100% power state. My first thought was that there could be a problem in the heat sink placement or the thermal paste, but after a few other trials and errors i came to the following conclusion: If the maximum processor power state (advanced power options in the control panel) is set to anywhere from 1% to 99%, all temperatures are what is to be expected (40-50 idle, 60ish on load). but if the maximum processor state is set to 100%, then i get 80ish at idle and up to 95C at load, which is dangerous indeed. It is certainly not normal if the system is idle.  When idle, turbo boost should not be activated.  Instead the cpu frequency should be lowered.  Even if it is at full performance, when idle the cpu has C states to save power when there is no work to be done.  The deepest of those that Intel cpus support is C6, wherein the cpu is entirely powered down.  Thus there should be very little heat when idle.  It sounds like both of these mechanisms are broken on your laptop.  Make sure you have all of the chipset drivers installed from the manufacturer. The CPU is, by design, performance limited by heat. If you want 100% performance, it will crank as much as it can until it can't go any more. 100C is the absolute max, so 95C is the normal limit. It's not dangerous at all. Trying to keep the CPU cooler is a losing proposition. First, it requires more power to spin the fan faster. Second, it means the same the change in power consumption will produce a larger sudden change in temperature (because the cooler you are, the harder it is to get rid of heat) which creates unnecessary physical stress in the CPU.",3
"Another way we reduce energy consumption is to not run old hardware. The servers we bought over 5 years ago consume more than twice the power, and output nearly twice the heat of low energy servers today. Here's a good article on virtualization and going green, and the use of Solaris Zones compared to VMWare (disclaimer: I work for the guy who wrote it). The size of your business and how you do accounting matters too. If you buy & depreciate servers, buying fewer servers + VMWare licenses will result in tax savings and reduced capital costs. But if you lease servers, those savings may be less. If you're a government agency that receives Federal funding, VMWare will save you a fortune because you won't need to run multiple servers dedicated to particular Federal programs or grants. (The Feds don't like it when you co-mingle services on hardware they fund, and expect money back) Virtualization will reduce the size of your physical server footprint, so you will get some power and maintenance savings.  We saw a 62% reduction in physical servers.  You will obviously not necessarily get reduced administrative savings because your just moving server images, not reducing them. I have to admit that I'm not using this feature on the arrays that host the Vmware OS-VMFS volumes, as I dont want them to lag. It's possible to just select head parking and spindown, and always let the disks run. But I think the power savings of simply replacing old physicals with new are highly overrated, unless you're a super dense shop.  For example, IBM claims their new x3650 M2 class box saves $100/yr in electricity. Yawn. I can't get excited about spending $3K+ to save $100/month.  I suppose you could say it pays for itself in 3 years, but don't underestimate the soft costs of doing a migration to new hardware. And, new hardware tends to become more power efficient all the time without our asking for it, so my strategy is to buy new gear and virtualize when it makes sense based on business value, not just to be green. VMWare isn't the only way to save energy and dollars in a data center. You need to think of your compute resources as an single system and tweak/refactor to improve the systems performance and efficiency. That's the difference between engineering and goofing around with computers. Decommission servers fully once they are no longer needed (i.e: remove them from the data center entirely) We have a Nexsan SATABeast as primary storage for our SAN - it can automatically park heads (~20% power savings), spin down the SATA-disks to 4000rpm (40% power savings) and eventually shut the disk downs (yep, 100% savings). The feature is called AutoMAID. There's no magic number that makes a particular scheme for saving energy or other resources right for you. It depends on all sorts of different things. If you're in New York, for example, power rates will make the ROI for each saved watt of power return 3x more than South Carolina. Automating a manual process may save you money, unless some employees are in a union with a 3 year, no-layoff provision. Another thing we've done is to run a heat inverter (instead of A/C) in the server room. It's like Air Conditioning, except that it's two-way airflow. It keeps the server room around 20C, and pushes the hot air down to our basement (where it heats up the basement and keeps our floors in the 1st floor a bit warmer). Look for the BTU ratings of servers before you buy, try and use efficient power supplies and low thermal-power CPU's.",5
"Personally, if I'm looking for a utility or small application for my machine, I will check that it works with 64-bit operating system, but whether the application is 32-bit or 64-bit is not normally high on my priority list. According to Process Explorer, I currently have 25 64-bit processes and 28 32-bit processes running, ignoring Chrome) I recently upgraded to 64-bit OS(Windows 7) from my old 32-bit version. I noticed that while several applications are available in 64-bit(e.g. DC client, Tortoise SVN, Graphics driver), many are still 32-bit applications (e.g. firefox, adobe reader). Some 32-bit 'applications' will not work in 64-bit mode, for example graphics drivers and shell extensions (such as Tortoise SVN). In such a case you need a 64-bit version. Even ignoring memory considerations, you can not say for certain that a 32-bit application running in 64-bit mode will be faster or slower than the 64-bit equivalent, nor whether the application will be faster or slower than when running on a 32-bit operating system. So, unless you really desperately need 64 bit for the application in question, you can often be safer sticking with the 32 bit version. If you happen to have, for example, 16GB of memory and are running SQL server with an extremely large database, then having 64-bit version of SQL is very important. So, how important is it to have 64-bit applications (performance wise)? I use my laptop for many different things like gaming, coding etc. If, however, you have 4Gb of memory (with almost 1gb 'stolen' by the video card), then you may be running low of memory and using 64-bit applications (which may require more memory) may actually slow your system down. It depends entirely on the application.  Most applications are operate exactly the same in 32 bit form as in 64 bit form.  For the applications where it does matter, you'll almost certainly be aware that the 32 bit versions are limited and wouldn't need to ask the question. One area to be careful of is when an application has a 64 bit version that has poor 3rd party support.  I'm thinking of Excel 2010 which is available in 64 bit form.  Excel 64 itself works perfectly well, but few 3rd party extensions and add-ons have been ported yet.  As another example, consider Python on Windows.  Again, a number of important non-core modules are not available or hard to obtain in 64 bit form.",3
"Another question that relates is what software does Digital Ocean use to create the snapshots of their servers that can then be later restored? From the manual, create a listener on port 12345 on one side and make sure it is open in your firewall: Is it possible to DD one server to another? Maybe through SSH? I've looked around and I have seen examples of DD using SSH from local to server and vice versa, but is it possible to do it from server to server?  A bit more efficient then SSH would be netcat nc as you wouldn't have the overhead of encrypting/decrypting your streams.  No idea about Digital Ocean: you'll probably want to ask them, or start another question here if you think someone might know. The above will work well enough to clone disks/partitions/volumes from within the running OS as long as they are not mounted at the time, instead of files use the device entries in /dev/, but you can't use it clone the device that contains your root file system.  and it'll work.  You could tweak the block side on both sides (pass bs=1m or whatever to both dd commands) for a speedup with some testing. In response to your comment to Bill, if you want to create identical machines by cloning the disks, that is typically done from outside the virtual machine, via the hypervisor, and not from within the OS.",3
"Backing up in depth is quite important, simply because you have to ask yourself when did this file get deleted, or damaged?  A week ago?  Two days?  Two hours?  Two months?  If your backup doesn't extend more than a day or two, you might as well not be backing up.  A week?  Or two weeks?  Certainly, but what if the file was damaged a month ago?  With TimeMachine, it's expensive, but typically not that expensive to go further...My Timemachine backup at work goes back to May...  It's almost 3/4ths full...  But it's available as I need it, and I'm storing other data on the drive as well...   A reasonable backup should give you at least 90 days of backup, why?  For the backup in depth, but because with most backup systems, it gets too expensive to build the storage pool further...  Tapes are typically around $30-75 (or higher) for around 60 Gb (compressed).   For any backup solution the bigger the better.  You can purchase 1 TB external drives with Firewire & USB for $125 - 150...  And while that can be considered expensive, consider what the purchase price was 1 or two years ago.",1
"The real question here, I think, is why you would want to create so many partitions in the first place. Storage space is not expensive, SATA is now standard, and there are, IMO, many better alternatives to partitioning a drive 33-odd times. No, Windows is not limited to only drive letters. You can mount partitions to any NTFS folder. I'm not familliar with any limits thereafter. For DOS/windows, it cannot support more than a maximum of 24 partitions because it runs out of drive letters (C: to Z:) (plus any partitions with unsupported filesystem). For Master Partition Table disks the limit is based on the size of the partition table in the Master Boot Record (512 bytes IIRC), which was invented by IBM back in the 80's. Also, certain OSs have their own limits for a variety of reasons. The standard, such as it is, allows the Extended partition to have up to 24 partitions in it. Why the limit? Same as many limits, it seemed large enough to be 'good enough'. Also, in practice at the time the various OS kernels were under development partition counts rarely got that high. This, by the way, is why many OSs implement their own version of partitioning. LVM gets around this on Linux. GPT is the Windows way of handling partitioning without an old fashioned Partition table. NetWare's NSS file-system runs a lot like LVM in that there is a single 'partition' that is then sub-divided in the media manager. While the extended/logical partition scheme theoretically provides an unlimited number of logical partitions, all operating systems sets some limit (as far as I know). For Linux the maximum is limited by the allocation of minor device numbers. For traditional IDE disks there is support for 63 partitions, while SCSI disks are limited to 15 partitions. Four primary/extended partitions. If you an extended partition you can have any number of logical disks within that extended partition. On a standard format disk the partition table only has room for four entries. If you upgrade the disk to GPT I think the limit changes, though I'm not sure what the new limit is.",5
"Other than the addition of 256-QAM [MCS 8], and (optional) 5-8 streams, you are correct. 802.11ac's advantages are small when limited to 20MHz. In the 5GHz band, there's no real reason to limit it to 20MHz. There's plenty of channels and not a lot of overlap. (which is why 160MHz is even possible) I'm wondering if I limit my channel width on a really nice (Meraki WAPs capable of 802.11ac) WAPs, to 20 MHz to stop them from overlapping, or if the Auto Width channel setting is making that change. if the limit of 54 Mbps will be reached.  Or, is there something about the newer 802.11n protocol that allows for faster bitrate, even with that 20 MHz wide channel, and we've disabled 2.4 GHz radios. My reading of it is that by limiting an 802.11ac capable device, to 5 Ghz and to 20 MHz width, you're effectively downgrading 802.11ac to 802.11n 1X1, and the limit is actually about 72.2 Mbps, but I'm not sure I'm deducing that correctly. For reference to the theoretical limits I speak of, this is one resource, but I'm not sure about what happens if you've turned off the 2.4 ghz radio use. https://www.intel.com/content/www/us/en/support/articles/000005725/network-and-i-o/wireless-networking.html",2
"I don't think this is possible. Even in my home pppoe account I have set up a static IP while only being provisioned for dynamic ip. Only problem is ip conflicts. Lacking such hardware, the best you can do is MAC filtering and block traffic coming from an IP which is not associated with the MAC. This is doable using iptables on Linux. Be aware though that faking a MAC address is quite easy. Locally, its a little more tricky. It may be possible to block non DHCP clients, but really you just want to VLAN out guest machines so they cant cause any trouble.  Imagine you got a network to which other users can connect with their PC's, you have set up some rules, for example there is a DHCP server that gives addresses 192.168.2.100 - .254 and first 100 addresses are reserved for people who want static IPv4 address. How is it technically possible to prevent someone from taking some other IPv4 address? How could you restrict someone to use only a specific IP? This won't prevent the computer from 'taking' the IP in that it can still be set statically on the interface, however all packets using that IP will be dropped, so it's not very useful. How do for example ISP enforce people to use only that public IP they got assigned? For example I have a server in server room and I was granted N IPv4 addresses, I received a list of all IPv4 addresses that I am allowed to use (they are all public IPv4 that can be accessed over internet). I didn't try to take any other IP than I am allowed because that would likely violate some rule and they might shut the server off so I am not even going to try that, but just out of curiosity, is this even technically possible to restrict somehow? How? On an ISP level you can give yourself any IP address you like, but only ones actually assigned to you will route, the rest will get no return traffic. They wont ban you, it just wont work. With the appropriate hardware, this is quite easy to do. A managed switch will allow you to assign one or more IPs to a (physical) port, and will block traffic coming from any other IP on that port. You can also use a different network interface per computer on your server, though that may prove quickly more costly.",4
"So, what exactly is TrustedInstaller used for, and does it need to be the owner of these files?  If not the owner, does it require full controll over these files?  One problem I'm having is that I need to make a few registry/dll changes.  Many of the registry entries I need to modify, as well as the entire System32 folder, are owned by the TrustedInstaller.  To make the changes I need to make, I'm having to take ownership of the files/registry entries, and then grant myself additional access. Another problem I'm having is in locking down certain folders, such as the IIS dlls in system32.  I want to minimize the number of accounts that have access to this folder, and I'm noticing again that TrustedInstaller is owner and has full control over these files. Since Windows Vista and Windows Server 2008, Windows Services can be at the ACLs of a folder. And the TrustedInstaller account is the Service Windows Modules Installer which is used for change files when Windows is performing an update. In the recent versions of Windows in the Windows Files like System32 only TrustedInstaller have modify permissions not even the System account. I'm trying to lock down my IIS and SQL servers.  I have a checklist of items to check, but the checklist does not take into account the TrustedInstaller account, which seems to have file permissions to everything.",2
"Constantly shrinking your log and data files can cause performance issues because SQL Server has to regrow these files as needed, which causes waits on your queries that require the additional space. No, this is unrelated. Most likely your SQL Agent history is limited and you have a high-frequency job filling the job history tables. See this link. As per the title, how much is it safe to use BACKUP LOG db_name WITH TRUNCATE_ONLY in last STEP of any SQL-job execution? Randomly found on google that this Code line can be very harmful in use. But how? SQL is backing up the log to nowhere and then trying to shrink the data and log files of your database. All of these are bad ideas. Backing up the log to nowhere means you lose point-in-time recovery of your database because you cannot restore these portions of the log. See this link for more info. Before you can reclaim space in the log, you need to backup the data in the log. Removing this would mean your shrinking wouldn't likely free much space, however, as stated before shrinking the log is a bad idea anyway.",2
"If your RAID controller is fully supported by Linux as a RAID controller (i.e. it sees it as a RAID controller and not a bog-standard SCSI/SATA/PATA controller), the you may find useful information in the /proc filesystem and find tools out there that display/monitor this information for you.  If you know the RAID controllers that are in your machines, you might want to add that detail to your question then people with specific knowledge of that controller might pipe up with more specific information. Well i don't know your system but i think one of the best way of viewing the current RAID setup on a system is through its BIOS. EXAMPLE on HP DL380p gen 8 bios menu, you can press F8 to view the logical disks. I have several servers running Windows 2008 and Red Hat 5 that are capable of hardware RAID.  How do I check that hardware RAID is configured? If you do not have any OS level tools to monitor the hardware, then you will need to reboot the machines and interact with the RAID controller's boot-time configuration code. If the RAID controller officially supports your OS then it will have a set of tools to monitor it. These were probably provided with the hardware, though will also be downloadabed from the manufacturer's website.",3
"To set it is a little tricker though.. the file required a packed binary representation of the hostid. I guess, it should be possible to ""fake"" this value on any system (at least the mac address can be overridden). However, the best way is to ask the software vendor to transfer the license to the new system. I see no good reason why this should not be possible. I used python but you can do whatever.. (pretty sure someone knows an easier means to print packed bytes). Afaik the hostid is either the MAC address of the (primary?) network card, or some kind of mixed identifiers with other values such as ip addresses (this heavily depends of the kind of *nix you run). This will set the hostid in a manner for which gethostid will return the same value as the donating box as the migrating box. The hostid is retrieved using the library call ""gethostid"". Its merely a generic value which, if unset will be based off of the ipv4 address of the host system. You can set it yourself to anything you want by putting the file /etc/hostid in place with the value you want (presumably thats the same as what comes out of the 'hostid' program on your donating box). Assuming here what your referring to is the tradiitional unix hostid. If I wanted to bind software to a system I would use a dongle or some stronger means to identify a system seeing as a hostid is very arbitrary, but nevertheless..",2
"I have found some way to export users from AAD to import to a local DC.  There are some issues with having to re-enter passwords.  My fear is that no security groups or groups in general will be synced.  AD is not my strong suite.  Through testing Im not able to fully test out all possible issues because we have so much stuff going on in Azure AD.  Im worried doing a AAD Connect sync will possibly delete functioning parts of AAD.  Is it really as easy as exporting the users and loading them into a local AD? I work at a small tech start-up that grown.  We have been using Azure AD for years with O365, security groups, enterprise apps, etc.  I setup AD DS a while back and it has been worked great for our vms in Azure.  Up until now weve been strictly cloud based.  We now have the need to build some on-premises labs and other devices.  I was hoping to use AAD Connect to migrate AAD to an on-premises DC.  My goal was to have one DC in Azure and one on-prem.  After researching AAD Connect it seems there is no way to migrate from cloud to on-premises, only the other way around.",1
"If I upgrade the 2005, will I run into any issues with backwards compatibility?  (Probably a dumb question). Like lynnroth, I run multiple instances (05 standard, 08 dev, and a 05 express instance) for testing specific things. What exactly are your concerns with backward compatibility?  Do you have an app that specifically requires 2005?  If this is the case, then stick with 2005 and run multiple instances. Don't forget that running databases against multiple instances is different resource-wise than running all databases against a single instance. If your going to do an in-place upgrade, then run SQL Server upgrade adviser which will warn you about any issues you may run into. If you're worried about backwards comptible SQL you can use various compatibility level values. It's not exactly like running the SQL against a different version of SQL Server, but it makes migrating from version to version easier. I currently have a machine with SQL Server 2005 SP3 installed.  I would like to add a SQL Server 2008 instance, but I'm not sure if I should upgrade the 2005 or try to install a separate instance of 2008.",4
"So, how to write queries to update balance while keeping every record of the transaction and the balance synced? This question points at writing schema. While I already have the schema, I need to write an optimized query(or queries) to populate the tables. 1st and 2nd approach will lock the row and prevent any update. It won't be feasible if there is another column in the users table being updated. 3rd approach appears not feasible to me as there's going to be a million records at some point.  The transactions can be carried out by the user, the parent user, and the system. Therefore, I have to query the balance every few seconds and show it to the user. There will be no delete operations on both tables. Just insert and update on the users table, and only insert on the user_transactions table. Since there's going to be thousands of users and thousands of transactions every day, most probably at the same time, these insert and update queries should be optimized. I am using MySQL version 5.7. That's a minimal DB structure and indicates all the indexes going to be used. There will be more columns in both tables such as who carried out the transaction etc. I have to keep a record of the transaction in the user_transactions table and update the balance in the users table.",1
"If you can afford to get a professional to do this then the reproduction should be much better than you could do on home equipment. I recorded using Audacity to record the audio and also do some post processing.  However I could not get it completely hiss free as they were very old recordings of a relative who had passed away, so there was no way to reproduce it. My Ion TTUSB USB turntable has an aux in which can be used for a cassette deck or other analog input. Many USB/FireWire audio interfaces will also have these inputs, so if you happen to have any of these, they are good alternatives to just plugging into the line in. You'll need a converter cable to get the output from the cassette deck to the 1/8"" input jack.  They have both 1/4"" to 1/8"" cables, and also dual RCA to 1/8"" cables. On some cards, the line-in jack is a light-blue color.  If it doesn't have one, the mic jack can be used, but I'm not sure if that will cause problem. I know very little about Macs, but on the Windows machine, you can use a free tool like Audactiy to record sound from the line-in jack. I used a Roland Edirol UA-25 to interface the cassette deck of my stereo to my computer (Mac), but only because I had one (Musician).  I could have just as easily have done it using the line in on the standard sound card (usually the blue jack).   Obviously, on the Mac, Garageband is a good alternative to Audacity, easier to use and with some more features.  Amadeus Pro is another excellent alternative, with some filters designed to clean up analog audio, removing clicks, pops, hiss. and hum. Fortunately, I work for a very large media company in the UK and their sound department did an amazing job of reading the audio I had recorded and removing more noise than I could possibly using the equipment I had.  Admittedly, the sound studios contain millions of  worth of sound processing hardware and I was very lucky to get this done for free.",3
"Have a slight increase in volume to your thrusting sound, or crossfade to one that is more intense and ""whooshy"". If your collision is split into broad and narrow phase, raise an event when another object gets checked in narrow phase but doesn't actually hit. When this happens, play back a ""whipping by""/Doppler effect sort of sound. You can vary it by shifting the pitch variance range upward as the difference between the velocity of the player and the velocity of the object increases. Also kick your music up a notch, fade in a supplemental rhythm track (layered on top of your main music track). This isn't directly related to perceived speed of the player, but will help increase the emotional intensity, which is what you're after at the end of the day. To add on to Phil's suggestion about sound, take multiple approaches to it. You would combine multiple visual effects to achieve the sensation of speed, so do the same with sound. I kind of assume that you're talking about a 3rd person view here of the spaceship because of how you described your idea.  It might even be useful in a 3rd person setting too as you often need a bigger FOV when your speeds are higher (Like the camera in the top down GTA games). They have a big problem in that everything is to scale.  That make any sense of speed to be almost non-existent right up to the moment when you arrive at the star/planet/moon/. Make everything else except your ship, act like it's in slow-motion. Even if everything moves at the same speed, but just animates slower. This will give the feeling that you have an increased rate, like adrenaline or something. They denote speed by making the ship look like its flying through a thin dust cloud.  As the particles stream by the leave slight speed lines.  When the ship is at rest the cloud is completely transparent. My 2 cents, some of it has already been said but I'd like to make my answer coherent as I believe many of these points together would create a nice effect but just alone they don't do much to create the immersion: Try the ""pilot pushed back in his seat due to accelleration"" look. ie. pulling the camera a bit away from the cockpit when the boost is triggered.",5
"Here are some HTML5 games on Effects Games... there are even projects there you can jump in an get involved. Been killing time between compiles mostly with their Crystal Galaxy Demo This game was painstakingly created with blood, sweat, tears, and HTML5. The core matrix transform library was spun out as Matrix.js. Additionally many of the core language extensions, sprites, sounds, and canvas libraries are working their way into The Pixie Game Platform.  The limitations are browser-based. If you're using something hot like the Webkit nightlies, there's basically nothing that flash can do that the browser can't do natively, such as: Biolab Disaster is a pretty good sidescrolling platformer showing what can be done with html5 and js. http://playbiolab.com/ It still requires some care and feeding to get it up and running (e.g., WebGL still isn't enabled by default in any shipping browser), but we uploaded a video here: http://www.youtube.com/watch?v=XhMN0wlITLk (sorry it's so dark -- gamma issues remain) and you can see our Google I/O talk here: http://code.google.com/intl/fr-FR/events/io/2010/sessions/gwt-html5.html (I included links to all this stuff in my first draft but StackExchange wouldn't let me post them since I'm new; sorry!) AFAIK that's all possible in Google Chrome as well (and if it's not, it will be soon). Sounds like a complete gaming environment to me :) In the interest of exploring just how far you can take the newest crop of browsers, I and some colleagues cross-compiled the open-source Java port of Quake II to Safari/Chrome (it ought to work on Firefox at some point as well, though it didn't initially, largely for performance reasons). The project is here: http://code.google.com/p/quake2-gwt-port/ I don't know how long it's going to be before we can realistically depend on all these new browser features (WebGL, WebSocket, audio/video, local storage, et al), but if it all comes together it could make a huge difference in the ability to ship games directly on the web. But there are still a lot of hurdles that remain (e.g., don't hold your breath for Microsoft to implement WebGL in IE10).",5
"Had the same problem. What it was for me was the centre pin in the charging cable was broken off. This pin charges the battery(11 volts) and the outer part(shell) keeps the laptop running(19 volts) but does not charge the battery.  If the replacement AC adapter from your friend or from Dell also fails to work, it would indicate to me that there is a fault on the motherboard.  Again, Dell should be able to replace this under warranty.  If out of warranty, they should still at least be able to offer the part at a cost to you.  Given the cost of the replacement part from memory, it will probably be cheaper to buy a new laptop or just live with it. So plug in your own adapter's numbers to figure out your maximum wattage. You can generally use an adapter with a higher wattage than you need, but rarely can you use one that is lower. If your laptop is still under warranty with Dell, give them a call.  If you do not have a spare AC adapter that you can borrow from a friend or work colleague, I see no reason why they would not send a replacement out to you.  Batteries are not covered under warranty so great, but AC adapters definitely are. To answer your question about the power supply size, look on the power brick label for ""output"". It should say something like this: The power connector consists of 3 pins, 2 are power and earth, the 3rd is a signal line to identify the PSU as a compatible (Dell) unit. If the Laptop does not get the signal, it does not allow the PSU to charge the battery, it will, however, allow it to power the laptop. The fault is a common one and happens because the plug sits so proud from the side of the laptop that it constantly gets knocked and causes the connector on the power board to fail, most commonly open circuiting the signal line.  Having just paid over 100 to have the the power board inside my daughter's Inspiron replaced and had the explanation confirmed by the Dell engineer, here is what happens:     To reiterate the other answers, you'll need to contact Dell support for assistance with this. without equipment to swap, you'll have to get Dell to fix it for you. That means it provides 19.5 Volts with a maximum allowed Amperage draw of 4.62 Amps. Going back to basic electrical theory, we know that you multiply Amps by Volts to get Wattage. To clarify, the power board on some Inspiron models is a separate unit to the motherboard and has the power socket mounted directly on it. As the message is happening outside Windows, it something going wrong at the BIOS level.  I.e. a hardware failure of some variety.  A loose connection, cable or a faulty chip on the motherboard.  You can go into your BIOS (F2 during POST) and check the status of your AC adapter there too if you are curious.  It will most likely be under the Battery Info subheading, under System.",4
"Alternatively, you can configure sudo to not ask for a password at all. I don't recommend this except in the case of automated actions carried out by service accounts. In that situation you can specify only the service account can execute only a specific command. However, you're more than capable of removing the sudo password requirement by changing the above line to something like: You're really talking about two different things. Configuring SSH to do key based authentication only is part of the sshd config, and sudo (passwordless or no) is handled by pam. What you should really be doing is this: You'll want to pick your sudo group and modify the sudoers file, using the visudo command and add something like this. That will allow everyone in the wheel group to execute any command as any user. Note that they will need to enter a password to do this. By default most distributions have them enter their own password, but sometimes sudo is configured to have them enter the target password.",1
"The instructions are pretty straightforward but there is one unsettling part: search for ""Enter the address of your NAS server."" Basically it seems that to connect one's Mac to the Synology NAS, you have to provide an IP address. Synology provides a utility called SynologyAssistant (Windows and MacOS) that shows you all Syno devices connected to your network and their ip-addresses. You're right as to the fact that it's possible the NAS will grab a new IP address with method 1. But most likely that's not going to happen if you have a small network.   Or you connect to your router and in the router you configure a so-called static DHCP mapping (refer to the manual of your router for how to do that) so the router always assigns the same ip-address to the Synology. Through this utility you can also connect to the web-based management interface of the NAS and (in case the NAS isn't setup yet) push the latest firmware version to the NAS.   First of all, they don't explain how to determine the IP address of your Synology NAS. How do you do that? Second of all, a bigger concern is that if I'm not mistaken, that IP address can change whenever the NAS reconnects to the router (for example, if you restart your router). Doesn't that mean that your Mac's connection to the Synology NAS would break whenever either the router or the NAS is restarted, since it is based on this IP address? I am considering buying the Synology DiskStation DS115j after discussion at my previous question (how can NAS work if it is not connected to Internet?). Before I did that, I wanted to read through instructions about how to set it up with Mac's Time Machine, to make sure it would be easy. I found the instructions here: https://www.synology.com/en-us/knowledgebase/DSM/tutorial/Backup_Restore/How_to_back_up_files_from_Mac_to_Synology_NAS_with_Time_Machine I'd say that best method is to use 2 and remember the address.  Just be sure to give it an address outside of the DHCP scope.",3
"For Internet Explorer you might be able to hack some DLL and replace existing functionality. And you might need to do it for each version of windows you are using and keep it up-to-date after OS updates. hrome and Firefox have their own SSL stacks (NSS) which you would need to change. The situation is similar on Mac OS X where Safari is using one TLS stack and Chrome and Firefox are using their own TLS stacks (again NSS). And on UNIX system you mostly have to deal with Firefox and Chrome, which again use the NSS stack currently.   If you know that you are working in a lab environment or newly-provisioned environment or small business environment, strictly on systems-related matters, there is no benefit in terms of confidentiality or integrity (or awareness of the lack thereof) when you always suppress SSL warnings because you already know that they are going to appear. I suppose you could argue, ""well, what if somebody knows that you are so lax about this and exploits it by specifically targeting those kinds of hosts you are looking to suppress,"" but that argument only holds if a) there is any obvious means of exploiting your testing of IE8 browser compatibility for an Intranet app, b) you have misconfigured your virtual machine networking and actually allowed it to transmit or receive packets through its gateway, and other reasons, but most importantly, c) you have ever done anything differently as a result of that warning while working on a host you know will produce that warning. The efforts needed to hack all these places is probably more than the effort needed to add the custom CA you use for your test sites. That said, if I am using Internet Explorer 8 on a freshly-provisioned test box that has no access to the Internet, it serves no purpose for me to have to click one or two buttons constantly to manage a server at https://my-vm-213.goofy.local or whatever the case may be. And if we're dealing with hosts that are going to go up and down and sideways in large numbers, it makes no sense for me to spend a moment adding root CA's that are stupid to add and won't have a reason to exist within hours. You might be better off adding a transparent proxy that strips SSL, or creates matching certificates on the fly as a MITM style attack. That way, only one private CA is needed. This should be easy when using any provisioning system. If you need security, then use SSL, and use it properly. If you do not need security, then don't use SSL. I understand why SSL warnings are needed and why users, even experienced ones, should be prevented from easily ignoring them. I also understand that in general, the ""white list"" or trusted untrustworthy root CA approach is the best approach when working on your everyday workstation that you also do banking or trading or emailing on.",4
"One final note.  PostgreSQL has its own APT repository, so if you want to use a newer version than 8.4, you can do so with little hassle.  To use it, as root, add the following line to /etc/apt/sources.list: If /var/lib/postgresql/ is empty, something is wrong.  It should have a directory for each version of PostgreSQL that you have installed, assuming you did the installation through the normal dpkg/apt-get processes (which you say you did) and that the package was intended for Debian.  On the other hand, you have the postgres user and the psql command available.  It is certainly possible that you have one of the postgresql-client-* packages installed but not the server package. refer to https://stackoverflow.com/questions/2748607/how-to-thoroughly-purge-and-reinstall-postgresql-on-ubuntu/40545924#40545924 for an indepth knowledge of why and why you shouold take these steps. Similar error occurs, if installation is not fully complete.Try setting default language , it is one of the common problem.  Since you pretty clearly do not have PostgreSQL successfully running, I would suggest a purge and reinstall through aptitude, or (as root): I can't seem to find the folder where all the config files is either. On my CentOS server the config files are stored in /var/lib/pgsql, on my Debian server there is a folder called postgresqlin /var/lib but it's empty. You can then install versions 8.2, 8.3, 8.4, 9.0, 9.1, or 9.2, as of this writing.  You can have multiple versions installed at once if you like.  More information is available in the documentation to the postgresql-common package: I've just set up a server running Debian 6.0 64-bit. I've installed PostgreSQL through aptitude, and now i try to access PostgreSQL without any luck. I am trying to access psql as the user postgres, but get this error message",4
"I have a need to open Linux .desktop web links on a windows machine. Currently I drag the web page's address to a shared folder in Linux, and that is available on the LAN for others to open. This works fine with other Linux machines on the LAN, but Windows machines are not reading the URL. All I did then was go Open With... for a .desktop file and point to the batch file, with the Always use this program ticked. Now, just clicking on a .desktop file will open the url in Chrome (which I will soon switch to point to Vivaldi). Over at https://www.computing.net/answers/programming/batch-file-to-extract-address-from-url-file/27904.html, ""Razor"" posted a batch file that will find a URL in a text file. I had to amend the path to Chrome, which evidently now installs in the standard C:\Program Files folder. The Linux .desktop link is actually a small text file, and the URL is embedded in its last line. But Windows does not recognize the file type, and when I associate it with a browser in Windows (vivaldi, in my case), the browser simply opens the .desktop file as a text file rather than going to the desired URL target.",1
"Because routers dont work with MAC addresses (layer 2 of the OSI model), they work with IP addresses (layer 3). The router knows which IP address the packet is for, and so it forwards the packet to the switch, which looks for the IP address in its ARP table to find the corresponding MAC address and then send out the frame on the appropriate interface/port. Only the WAN-facing interface (ETH1 in this case) would have an (external/public) IP address assigned by the ISP. The other interfaces would have a manually configured (internal/private) IP address (in this particular case). No, facebook.com would see a request from your external IP address (the one on the ETH1 interface). Since private IP addresses are not routable on the public Internet, the private IP address of the PC (192.168.1.x) is translated to the public/external IP address through a process known as Network Address Translation (NAT). The router keeps track of these translations in a so-called NAT table, so it knows where the request came from, and where to send the reply from the server.",1
"You need to be root to do the kind of snooping that tcpflow and tcpdump do. So you either need sudo or a setuid root wrapper. My recommendation for a wrapper is not to use a shell script (I don't know if any of the shells in Debian is suitable for writing setuid scripts  shells tend to use a lot of environment variables), but rather Perl (where this kind of thing is explicitly supported), like this: Replace 1.2.3.4 by the IP address associated with the interface. I don't have a good solution to offer if the IP address isn't fixed: if you read it from ifconfig output or otherwise at the beginning of the wrapper script, it's open to a race condition. I don't think you'll be decently able to allow the user to specify an expression to match packets. Rather than give extra permissions to run tcpflow, I suggest giving extra permissions to run a fixed tcpdump command (writing to stdout, let the user redirect into a file if necessary). Then the user can run tcpflow -r to analyze tcpdump output (even in real time, through a pipe). For other interfaces, I think the following command selects tcp ports 30003999 (but make extensive tests to be sure): You can use sudo or a setuid root wrapper to let user1 run a command like tcpflow -i eth0 tcp and \( src host 1.2.3.4 and src port 3000 or dst host 1.2.3.4 and src port 3000 \). Note that you can't just let user1 run an arbitrary tcpflow command: that would allow user1 to run things like tcpflow -r  -r /path/to/foo and read (at least partially) /path/to/foo with root permissions. Well, technically all you need the CAP_NET_RAW capability. If you install the libcap2-bin package, you can use the setcap command to create a setcap-CAP_NET_RAW wrapper around tcpflow, rather than a setuid wrapper. But that only helps make a security hole in the wrapper be a little less damaging (a user who obtains CAP_NET_RAW access can probably gain root with little effort).",1
"Additionally, most FTP implementations support and use the PASV command to negotiate an additional port to establish a data connection.  This allows for multiple data streams to be transferred using the same FTP session. You can also establish several connections from one computer to another on the same port, as the source port will randomly change. Yes, and yes. Server connection are all based around 'sockets' not merely ports. Each socket is made up of the following info:  Multiple connections from different hosts (or even the same host) can be uniquely identified using the above information. Yes, each connection to your computer is identified by its destination address/port and source address/port. *Suppose there is an FTP server on port 21 of a PC X. Is it possible for 2 different Pc's (Y and Z) to simultaneously have a connection with this FTP server on port 21? Why or why not? Because the source port will typically be randomized and thus different for each connection, the FTP server has enough information to differentiate between two connections from the same PC (and the PC has enough information to differentiate between them too, when the replies come back).  So, yes and yes, even though FTP is a stateful protocol. A server can handle many sockets. If two machines are connecting, then their source IP will be unique, and each connection is given a randomised source port when it connects. Quite how many connections your server can handle varies, limited by memory, network bandwidth, etc. Your FTP server software should give a configuration option for a maximum number of client connections. *Is it possible for pc Y to have multiple simultaneous connections with this FTP server on port 21? Why or why not? When a server is listening on a port, it can have several simultaneous connections on that port. Think about the problem we would have if servers accepted only one connection at a time on port 80 : websites would only allow one page loading at a time.",5
"First off, point at that drop message in the Sonicwall's admin page.  You should see a pop up message, and included in that will be a 'message ID'.  You can cross reference that value with the Drop Codes listed here (for firmware version 5.6.0.x).  If you're using an older firmware, here's version 5.5.0.x or version 4.2.0.x.  The codes are pretty vague, but it might help you get on the right path. Result, you're fucked. This only happens in situations where the firewall cannot fragment by itself the package due a cyphered payload which needs to be recomposed in the source, not in transit. That's why seems to be a lot of issues with SSL, SSH and other crypted connections. So the problem is Sonicwall refuses to negociate MTU size by using the ""first"" and most standard method because is insecure, and Linux refuses to negotiate MTU size by using the ""alternative"" method, which is more secure but less efficient. Is a problem in how Linux manage the MTU negotiation (tcp_mtu_probing). Most linux boxes comes with a mode where Linux doesn't negociate in an alternative way to the standard (which by other side is insecure, and thats why Sonicwall doesn't accept it). You can fix in your linux box by doing this: With OpenSSH you can ""hack"" the way it negotiate the need of resizing the payload, by touching some options in the openSSH client options, but you may found the problem by using other application, so it's better to fix the problem by altering how the kernel manage this situations, if not, you can fix SSH but later found problems with SSL or OpenVPN. Ahh, the joys of figuring out why the bloody Sonicwall drops packets.  Welcome to owning an NSA appliance. The SonicWALL does not honor or pass to the LAN MTU Path Discovery messages because there is no way to authenticate them and they can be used as a form of attack. For example, someone can initiate a denial of service attack against an unprotected server by simply sending a Path MTU Discovery packet to it that directs the server to limit packet size to 5 bytes instead of the normal 1500 bytes. This will slow the server down to a crawl because it has to process the same amount of data into many more packets, creating much overhead. Alternately, you can do a packet dump, and the drop code and module ID will be listed in the packet capture interface when you look at the dropped packet.",2
"Alternatively, you could 'Restart' instead of shutting down Windows and starting Linux instead of Windows when it starts. In case of restart, Windows does this the old way. If you create a partition for Linux and install it as a dual boot configuration, then the answer is no, the performance  will be the same. Windows will not know anything about Linux, but Linux will be able to access the Windows partition. I have Windows 8.1 already installed in my laptop. Now i'm thinking o installing Linux too. Is it a good idea? will it slow down my system?  If you need to access a file saved on the Windows partition, then you'll either have to mount the position as read only in Linux or remove the hiberfile.sys. Removing the hiberfile means that next time Windows would need to boot the old way, excruciatingly slowly. Actually, it will have a slight negative effect if you don't make a partition common to both Linux and Windows for file exchange. These problems can be eliminated by using a shared partition or installing Linux in a virtual machine, however, running a virtual machine requires good amount of system resources. When you click 'Shut Down', Windows 8 doesn't shut down completely. It hibernates all the system processes and reloads them as they were at the time of Shut Down. This gives us blazingly fast boot times. But it causes a few problems, too. If you install Linux to run in a virtual machine inside Windows, then Windows will perform identically, but Linus would run a little slower (not that you would notice it). You would need to benchmark it to see any difference.",3
"If the amount of data to be replicated outpaces the available bandwidth, the replication is not going to be very successful at all. Therefore high bandwidth would be important. If you're doing replication of individual transactions (can be done through different means with different database engines), you may be generating traffic that is bursty small packets. When considering the placement of an offsite slave database, which is more important in a network connection for maintaining up-to-date replication: low latency or high bandwidth? However, low latency would be important if you are making lots of small updates. So, it depends on whats being written to this DB. This might depend on how much activity your database is going to receive, or changes that will be produced on the source db. That depends on the database, the type of replication, and quantity of data being replicated (workload / transactions per unit of time, log updates, etc). Log shipping usually means that your unit of replication is fairly large (multiple KB), and you're probably going to benefit from more bandwidth moreso than lower latency. My general rule of thumb for this is that you want a lower latency connection for an OLTP-type system [online transaction processing] with active mirroring (lots of small transactions) or an actively mirrored ODS [operational datastore], but you will want a higher bandwidth connection for a data-warehouse (or any other kind of setup) where you are mirroring in batches. Your best bet is to mock it up in as close to a real-world implementation as possible and measure. You can talk to people using similiar software and having similar workloads to get a feel for scale. In the end, you're going to probably need to do it to see how it works. If there will be a large amount of data changing on the source db then I would opt for higher bandwidth to keep up with the changes.",5
"According to the following KB, This error occurs because of an issue with the Intel storage controller driver that's included in the released versions of Windows 7 and of Windows Server 2008 R2. This issue is resolved in Windows 7 SP1 and in Windows Server 2008 R2 SP1. Probably a silly suggestion you have already tried, but... are you formatting the partition or installing on top sort of? You say What fixed it for me was essentially bypassing that error, dropping to command line and completing the out of box experience: Other suggestions I agree with is upgrading the BIOS if possible, resetting it and of course making sure that you in fact have a 64-bit CPU. If you format your partition it shouldn't be any state at all. If you have several hard drives I would disconnect those. win8 and windows 10 would both install if I did not format the drive first, but installing windows 7 64 would fail at the ""Windows Setup could not configure Windows to run on this computer's hardware"" Consider resetting the BIOS to its defaults. Also, make sure the configuration in the BIOS is set to use AHCI. Fixed mine on my XW6600. I had changed the SATA mode from SATA I to SATA II whereas apparently my WD Green drive is SATA I. Once I flicked it back to SATA I installation resumed perfectly.",5
"You run the UI under Windows straight after installation, and after selecting the shrink operation, and clicking Apply, it will ask to reboot and shrink the volume while offline. You don't get to delete $LogFile, or indeed any of the other metadata files.  That will prevent the correct operation of NTFS. My system came with win xp. I installed Win7 in a separate partition, volume E.  That was a while ago.  I don't need xp anymore and now I want to shrink the C partition so that I can grow E.  I deleted most files from C but since the boot manager is there I'm not totally deleting the partition.  However I cannot shrink the partition because of a system file that's at the end.  FSUTIL reports that the file name is ""$LogFile::$DATA"".  I can't find it using any method I know of, I don't know what it is, but I want to get rid of it. What is it, and how can I delete it or otherwise shrink the partition? Instead, I moved a stuck file with a free version of EaseUS Partition Master. In minutes I had it installed, rebooted, and my partition shrunk. ::$DATA denotes a data stream of the transaction log file.  (This denotes the default data stream of a file, in fact.)  You cannot shrink your volume because the place where that stream is stored is currently at the end of the volume.  You need to relocate the contents of $LogFile to nearer the beginning of the volume, a task which some (not all) disc defragmentation tools are capable of. The only working solution was to shrink partition with MiniTool Partition Wizard which will move files at boot-time and shrink partition to desired size. If it writes something about filesystem error then run chkdsk /F /R C: in cmd.exe after Windows loads, it will suggest to run this at boot-time, reboot, wait while file system is checked, then again try to shrink partition with Partition Wizard. It's documented in loads of books and WWW sites about NTFS.  This is one of several files  $MFT, $MFTMirr, $Volume, $Bitmap, $Boot, and so forth  that are integral parts of the NTFS on-disc structures.  They have these names by convention, but the names don't appear in any directories seen by application-mode Win32 code  and thus end users.  They have fixed, well-known, node numbers in the Master File Table.  $LogFile is MFT entry #3, and it is used for NTFS transaction logging.",4
"Bash is using the same syntax for if and while as that can be used in /bin/sh scripts on the command line.  Popularity, #bash 430 users. #zsh 123 users. I believe the zsh website has a good comparison of zsh with other shells. zsh has better vi mode support. I still use bash instead of zsh. I rarely come across compelling features that zsh supports that bash does not. Occasional problems with zsh over the years, or it not existing on certain sysetms, have made it not worth making the transition. I can finally use the same shell on every Unix system, it's not worth breaking this for features I'll never use. The features present in zsh that are not present in bash seem to mostly be cute but not ones that would matter on a day-to-day basis. I'm a fan of zsh because of the vi mode support, but I am discovering that it's not very widely used.  I think I read that zsh likes to take popular features from the other shells and combine them (so things specific to bash and thing specific to csh are both available in zsh). I don't know of any bash features that zsh does not support. The design goal of zsh seems to include supporting any features that bash adds. Someone also said that I'm flexing my geek factor by using zsh, but I can't confirm or deny that rumor. Within zsh the syntax is different. Effectively using zsh you have to remember both, if you are using  an if- or while-statement on your command line",4
"Index data is not stored in the dump file, only index definitions. You can have a 14 GB dump file with 14 GB table data and 0 index definitions or hundreds of index definitions, and building those indexes may consume a lot more storage than the base tables, or just a fraction of it. Assuming you are not using bigfile tablespaces and assuming you have no standard for how large datafiles should be or how empty they should be, you should create a tablespace with one 14G datafile and allow it to autoextend to a reasonable percentage of the overall free space you have available.  If you have plenty of free space, you should set the maximum to unlimited (which is actually a finite amount dependent on your block size).  By doing this you can set an amount for the autoextention that reflects your balance between system performance due to frequent autoextending and the space benefits of allocating only as much space as the objects need. Using a combination of MASTER_ONLY and KEEP_MASTER switches, it's possible to get Data Pump import to do the dependency tracking, calculations, and estimations of sizes that it does at the beginning, create the master table that it uses to track everything, and then stop. Remember that when importing data, if importing to a schema/user which has a default tablespace of ""XYZ"", then the data will go into that tablespace no matter what tablespace it came from on the originating database.",3
"I suggest you to stop these services if they are running and put them in manual or disabled (IF your PC is NOT connected to a Windows Server...) From your description, it sounds like one of the drivers is having a problem. Hopefully all that is required to fix it is to update the driver. Another fairly easy solution would be to adjust certain BIOS settings depending on what is causing the problem. Warning: use Autoruns with caution. Don't disable or delete anything unless you really know what's you're doing. I also had the same problem - Windows 8.1 with i7 4700MQ- quad core -> one core - specifically always core 2 gets hot. I found a workaround for the problem - after waking up from sleep - go to each windows explorer window that is open and press Ctrl+N -> this  opens the same folder in a new window.  The processes signatures/checksums are compared to the System Explorer database, There's many useful features but one interesting is the possibility to send any file, program, to VirusTotal... 6) Update Windows and your applications. I suggest you to double check with the online Secunia's Online Software Inspector and set the option Enable thorough system inspection (Java must be installed to run these tests) Hint: some Windows services slow down Personnal Computers when they are not connected. as a workstation, to a Windows Server such as Server, LanmanWorkStation and LanmanServer. Task Manager is limited. Use Process Explorer to get a better insight into exactly what is spinning the CPU. 2) From the same, dowmload and install Autoruns (to know what started with Windows) and Process Monitor (for future detailed system checkup like tracing boot process...) After you have opened the new windows for all -> close the old windows -> Voila! your explorer CPU usage drops to negligible and CPU temp also comes down - don't know why it happens though! I dont think its a process per say, I just think it is a driver that is having trouble, literally, waking up.  You may start the services.msc but the best is to run mmc.exe and add services.msc in a personalized ""console"". (This keep your display setup for the next time). 5) Check if there is defective driver for devices with devmgmt.msc (or add it in the same ""console"" created for services with mmc.exe...). Check if there's yellow triangle, if so update the driver... If not: don't fix what ain't broken...",4
"Is it OK to run RedHat 5.8 off the RedHat 5.9 kernel (version 348) ?  This automatically happened when I updated my OS.  Do I need to change it back to the correct kernel version in my /etc/grub.conf file ? You should be fine. Red hat subreleases are far more flexible than, say, Solaris U# releases. It isn't uncommon to install off a rev like 5.4, run yum update, and then see the redhat-release file suddenly show that you're at RH 5.8 (in fact, it can be kind of troublesome when you need specific file levels to ensure multiple environments are exactly in sync, or if you tell technicians that you are installing 5.4, they run updates, and suddenly see 5.8). In fact, if you check /etc/redhat-release it may show that your OS is now running 5.9 instead of 5.8. About the only thing you may have to worry about is if there were any kernel modules built which relied on the 5.8 kernel. The dmesg output should show you if you have any errors. Other than that, soak the environment for a few days and see if there are any problems. If not, enjoy the new kernel!",2
"The best way to find out is to try it.  I just installed DD-WRT on my router a month ago. It took a few days to figure out how to go about doing it, but once you're prepared, the actual installation is very simple. Of course, my situation is not your situation.  In my case, my router was dropping connections and returning spurious DNS entries.  Installing DD-WRT appears to have fixed it.  But if you have to reset your modem (not the router) to temporarily fix your issue, then it sounds like your modem may be the culprit, not the router.  Did you purchase your modem or do you lease it from the provider?  I know I've taken many modems and DVRs back to my cable company's office and they replace it without question.  You can try that first, then try replacing the router's firmware. Yes, I have found the DD-WRT to be much more stable than the latest Linksys firmware on the WRT160Nv3 and it may be true for the WRT-54G as well.  Make sure you follow the upgrade instructions on the DD-WRT wiki exactly.  Sometimes the reset process of 30/30/30 is essential in making the hardware accept the new firmware and be stable. If I install the open source DD-WRT or Tomato firmware, will this issue go away? Is this a software problem or a hardware problem? I was thinking, well I'll just try it. Then I looked at the instructions for installing DD-WRT -- wow that's a lot of work.  I've got a Linksys WRT-54G wireless router. The connection is flaky. Once or twice a week I come home from work and can't connect to the wireless so I have to reset the modem. It's annoying.",3
"We put $d$ with the outgoind radiance and irradiance because it's a very small part (we are looking at just one instance of that summation/integration) It is helpful if you always look at the units that a certain physical quantity measures. Since you use Real-Time Rendering, I'll also quote from that (3rd edition). Also, for the sake of completeness, I'll go through all quantities and units related. I will however assume you understand solid angles. The time $t$ is measured in seconds $\left[s\right]$ and the solid angles $\omega$ are measured in steradians $\left[sr\right]$. But more commonly and what makes more sense to me, and to the answer of your question, think of irradiance as the integration of radiances over a set of directions. So coming back to your question at last, I hope you might have gotten some intuition as to what ""irradiance in a direction means"".  Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. Mathematically proving it is no hard feat tho. The answer here explains it quite well. I'm just gonna give a brief explanation. We know the rendering equation is given as Yes they do. In fact, steradian is a dimensionless unit, since it is $\left[\frac{m^2}{m^2}\right]$, and therefore it doesn't really add anything. I see how this is confusing. I hope I could clear up why you do this. Again this was just the way I convinced myself and might have some mistakes. Though this is the best I came up with. But still, $E$ only considers the amount of energy per time, not from which direction it comes. Why is this important? Because of the way you usually look at lighting in computer graphics. You calculate how much light is being reflected from a surface to your viewer (/camera), which also means, that you want to know from which light source it originates (seeing as you would like to have the right amount of energy and the correct color). Additionally, you usually use point lights, meaning that you can view the lighting calculation as that of a ray from a single point (the light source) onto a single point on a surface (your pixel/fragment) and then to your viewer. These directions are written in the matter of solid angles, or to make the theory even easier, in differential solid angles. Another point is that your surface may reflect light differently depending on the where it comes from, which also makes important the direction part.  I'd say it is the other way round, $L(v) \leq E(v)$, since $E(v)$ would consider any light source emitting light onto a surface, whereas $L(v)$ only considers light form $v$ (if you consider $v$ as the light source to surface direction. In my writing earlier, this is $\omega$ and in Real Time Rendering specifically, this is $\omega$ as well, or $l$ in chapter 5 as well as in the BRDF Theory chapter). Also, remember that these two physical quantities don't have the same units and should not really be compared this way. Now as I wrote before, if you forget the BRDF for the time being, we are just integrating the radiances over a given set of direction which is the same as irradiance.  ""Radiance"" is what you say associated with a certain direction. To be more formal and according to wikipedia, $L_{outgoing} = L_{emission} + \displaystyle\int_{\Omega} L_{incoming} \;f_{BRDF}(\omega_i, \omega_o)\; \omega_i.n \; d\omega_i$ Currently I am reading the BRDF section from Real Time Rendering and I am having a hard time to visually understand the definition of this function. BRDF is the ratio between the radiance in outgoing direction and irradiance of incoming direction. I can't visually understand what does irradiance of a certain direction means? And what is the difference with radiance of certain direction? They both represents the power of light. What does irradiance of a certain direction means? Doesn't we associate a direction with radiance? Isn't radiance defined as irradiance in a single direction? I feel like i misunderstand something here. Is E(v) <= L(v) where v is direction? To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I hope it is clear, that radiance is irradiance from a certain direction. If not, please try to specify which part still bothers you. This is the difference between irradiance from a certain direction and radiance. Think of it like this. You are holding a paper and there are 2 light bulbs in front of you. You want to measure the irradiance. Normally it would just be the radiant flux received by both bulbs per unit area. But now let's say I limit the direction so I am only concerned with the first bulb. Note that I am still calculating the ""irradiance"". If I move farther away the flux will decrease thus the irradiance even tho I am concerned with a specific direction. However this isn't the case with radiance where moving farther away won't change it since we divide by the solid angle too balancing the change. Now there is one thing to consider: $E$ is measured with regards to a a surface $A$ that is perpendicular to the light direction (in other words, the normal of the surface is parallel to the light direction). Therefore we project $A$ onto a plane that fullfills this requirement. If the angle between the surface normal and the light direciton is $\theta$, then our projected surface $A_{proj}$ is calculated thus: $A_{proj} = \frac{A}{cos\theta}$ Careful. We don't associate direction (other than considerung only surfaces perpendicular to the light direction) with irradiance. We do however with radiance. As we can see here, we have limited the irradiance to a set of directions, the upper hemisphere. This doesn't necessarily change it into radiance which is associated by direction. Instead what this means is, when calculating irradiance we are concerned with the light coming only from these directions, although we haven't incorporated the directional quantity into the formula like with radiance. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's  So if we integrate the radiances from every direction that leads us to the original definition of irradiance where direction isn't of concern. However usually we are concerned with only a subset of all the directions such as the Upper hemisphere or the lower hemisphere. This means for example, The last quantity is differential irradiance. I thought of it as a tiny amount of irradiance from a specific direction. (Again direction gets involved) If you think of irradiance as not assosciated with direction at all, even then when you try to think of differential irradiance you are gonna say it's a tiny amount of irradiance from a small range of direction or maybe a specific direction. That's the reason why it's small. But if you think of irradiance as the sum of all the radiances over a specific set of directions. You'll see that it makes things clearer and you'll naturally arrive at the conclusion that differential irradiance will then refer to an irradiance from a specific direction. $L_{outgoing} = \displaystyle\int_{\Omega} L_{incoming} \;f_{BRDF}(\omega_i, \omega_o)\; \omega_i.n \; d\omega_i$",3
"I would recommend that when you talk to people, you use the right language. Telling an IT tech that you want to go off the domain is a good way to get a long lecture. Saying you have a printer that you want to install locally usually goes down better. If you can get a hold of the technology policy for your company, you can usually figure out how big of a deal a new local printer would be.  Axxmasterr is mostly right when it comes to network printers. You can never be assured the same IP address for a device when you use DHCP. Unless you can get someone to reserve the IP address for the network card in the printer, that number may change at some point. In most cases, when working with DHCP, it's best to use the host name of the printer when possible. Usually, this is something you can control by going to the web management console for the printer. http://""printer's current ip address"" It should be noted that if you don't have admin control over your laptop you are usually stuck when it comes to adding printers. If this printer will only be used by you, companies with loose tech policies will allow the tech to install it for you. If not, then you may have to talk to someone higher up the corporate ladder to get approval.",1
"I use Outlook as a calendar and contact list for my students.  For each meeting that I have with a student I put the class number in the Notes section of the appointment.  If I want to have a review session I can e-mail all of my students who are taking a specific class. My idea to accomplish this task is to link specific contacts with their appointments.  Then, when I search for a particular class, I'm hoping to list the e-mail addresses in the search results.  I can link the contacts, I can see the contact name after the search, but when I add the e-mail address field to the search results the e-mail address field is blank. Currently, to let students know about a review session, I do a search in the calendar for the class name, get a list of the students, then go back to my e-mail client and type them one by one.  This can get rather tedious so I'm looking for better solution. Is there a way to display the e-mail address of contacts in the calendar search?  If not, does someone have another way of performing the task?  Thanks.",1
"You can still have stateful firewalls on routers and other firewall rules can be added too in a simmilar way as ipv4. Many universities (and several large companies) have valid, routable IP's on every single computer.  That does not mean there is not a gateway firewall device.  It doesn't mean you can reach that device from the internet either.  Most of the time, the firewalls are set to block all traffic by default.. It does, however, guarantee that their computer is on a globally unique address. that said, just because you loose NAT does not mean you loose all the benefits (You can still have NAT on ipv6) The only difference is that you may be able to identify the exact computer from within a private network and if thats a problem you can install NAT. However, it doesn't at all mean that you won't still have a central firewall at the network edge - the change is simply that it'll be acting as a pure firewall instead of a firewall/NAT device.  It'll just be up to the people managing those firewalls to make sure to avoid accidentally exposure of services; fire up the deny rules! IPv6 gets rid of NAT, which has certainly been a large part of avoiding accidental exposure of services to the internet from internal hosts.. so in that way, yes, it's a change to how most everyone is doing things. Getting rid of NAT is a big change to network security practices, and there will certainly be times before too long that we hear about some accidental information exposure breaches due to misconfigured firewalls and IPv6.  But NAT has always been a hack, and getting the firewalls out of the business of tracking all of those connections and fake connections for stateless protocols and port translations will be a good thing in the long run - less complexity sounds good to me! If you use NAT, things just plain get nasty.. IE, you want to setup a VPN between you and your customer, but you both have internal networks of 192.168.1.x.. this means, you have to then NAT the natted connections, to make them appear to be a different internal only IP, which makes things just get ugly in a hurry. ( I have to do that with 5 other companies we have VPN's with)",3
"My main script is Python 3.5.2, the database is PostgreSQL 9.5.10 on Ubuntu 16.04 LTS. I am using psycopg2's copy functionality (on the simple account of it being the first that worked). I'm happy to change that, as long as I can call it from python (since I need to do some other processing per file -- organizing files, searching, determining the target stage table, unzip, cleanup, etc. -- since I am not sure how exactly the data transfer works using this method. Per psycopg2's doc the sql-version of copy is used (https://www.postgresql.org/docs/current/static/sql-copy.html), not the command line version. However I suspect the entire file will be buffered through stdin, but again, I'm not sure. I have a number of csv-files that I would like to import into my postgres-database. Since each file is quite large (200m-2bn records/file) I believe it would be better to add each file in smaller batches.  This process works fine, and I'm not even sure there are any drawbacks. However, it seems there might be a better/more robust way? I will use a trigger to export the data from the staging tables into the normalized main tables, surely that would benefit from smaller data blocks? FOr comparison, when using MS-SQL and using bcp, files get automatically split into 1000 record batches. That is exactly what I (think I) want to do with COPY.",1
"Can someone provide me with either a better way to think of / do this, or some concrete code detailing performing the transformations in a shader and constructing and passing the data required for this shader transformation? The transformation matrix at its simplest is a 3x3 matrix representing the homogeneous transformation.  You have your translate (location), rotation, and scale.  The matrix is composed by concatenating (multiplying) the matrices together.  GL prefers column-major column matrices (but you are free to use any other representation, if you wish, but I won't get into how). If you're using raw sprites, instancing can be a lot faster.  In this case, you will use a single draw call to render a number of copies of your quad VBO.  To access the matrices, they must be stored in either a texture or what's called a Texture Buffer Object.  The latter is better.  The idea there is that you upload all your transformation matrices for your objects into a VBO, which is bound to a Texture, which is then bound to the shader.  The vertex shader uses the special variable gl_InstanceID and the texelFetch() command to read the matrices out of the Buffer Texture, and then you apply it to the vertices. 4)Here you will need to about the glDrawArrays command. If you wish to do your transformation in the shader, you will need to learn about glUniformMatrix4fv command or one of it's many cousins as well as how to implment it on the GLSL side. I am new to OpenGL 3 and graphics programming, and want to create some basic 2d graphics.  I have the following scenario of how I might go about drawing a basic (but general) 2d rectangle.  I'm not sure if this is the correct way to think about it, or, if it is, how to implement it. As a final hint, if you're going the instanced route, you might find it a bit quicker and easier to avoid calculating the entire transformation matrix on the CPU, and instead calculate it on the GPU.  You can pass in the rotation, scale, and translation.  If you do the math, you can take those inputs and calculate the matrix directly.  Then instead of needing to eat up 9 floats in memory bandwidth to the GPU and doing a lot of work on the serial CPU, you can pass in 5 floats (4 if you only have uniform scale) and do the work on the parallel GPU. The basics of drawing are really not much different than with 3D.  You need a set of vertices, a transformation matrix, and material properties.  For a sprite-based game, the vertices will always be a simple quad, and the material will usually just be a texture (though there are often other properties, e.g. to make sprites blink different colors when an enemy takes damage, or the like). I recommend looking up a book by Edward Angel, namely his Interactive Computer Graphics book, the latest edition will assist you, though local librarys may only have older versions that are centered on older versions of OpenGL, it will help you build the basics. If your wanting a more free-bee lession approach, the http://openglbook.com/ is decently done. It helps get a simple implimentation going in both 2D and 3D and will help with the basic first steps, though it is not the best implimentation. And once you get going, fool around with the code and shaders, best way to learn. What you'll want to do is create a vertex shader that will pass the VBO directly on it's way down the pipe. This way, any transformations you make to you VBO will be applied. This however, will cause you to have to relearn transformations if you ever switch to 3D as you will want to impliment them in the shader instead. How I would break it up is, using your steps: The end result is that you'll only have a single draw call for every ""material"" (e.g. texture), which with atlasing might well be hundreds or thousands of individual objects. I'm really unclear on how 4 and 5 will work.  The main problem is that all the tutorials I find either: use fixed function pipeline, are for 3d, or are unclear how to do something this ""simple"". The matrix can either be applied to your vertices on the CPU before uploading, if you're pushing all your vertices into a single VBO. With the instancing approach, you also want to pack in the UV coordinates, and you'll need to use a texture atlas for rendering.  I recommend using 2D texture arrays for your atlases, assuming all your sprites have the same width and height.  It has a lot of advantages over a traditional texture atlas.  The traditional kind works just fine in many cases, if that's what you'd prefer. If you have complex shapes, you're going to want to use a batching approach.  Push all your pre-transformed vertices into a single large VBO and render them all in one go.  Doing a separate GL draw call for each object is slow. 5)You will want to learn about perspective matrix, want kinds there are, like orthoginal views and frustum, well those are for 3D, but you'll get the idea. But, working in two dimentsions makes things a little easier as your limits will be -1.0 to 1.0 unless you change them. 2 & 3) Edit each element in your VBO, or if you want to learn to do it in the shader, create a transformation matrix of what you want to do to the rectangle.",3
"We have been given the requirement that certain DLs must be restricted so that only certain internal AD users can send to the DL. Additionally, these DLs must remain visible in the address book. Setting the 'HiddenFromAddressBookEnabled' property to $true is unacceptable. Leadership has stated that ""The only people who should be allowed to see who's in the group are the people that can send to the group. Furthermore, the only people who should even be able to SEE the DL entries in the address book are the people who are allowed to send to the DL."" I don't think that's doable, because: I know this is old but for anyone searching for a way to do this, there is a registry key that you can add that will disable expansions of a distribution list. A GPO is the best means to enforce to all computers in your organization. Please - any additional enlightenment or comments encouraged. I think we have to go back to the business and tell them their requirements are not achievable. (And I have two other nasty requirements that I'll start separate questions for.) If you enable Moderation on the DL, users will not be able to click the ""+"" sign to expand the group. Attempting to do so in Outlook will result in the following message: I am a Unix guy who recently picked up powershell to help my Exchange admin coworkers implement a challenging project in Exchange 2010. (The requirements we've been given are challenging if not impossible to meet.) One way to get rid of the expandability would be to use Dynamic Distribution Groups - they expand based on a query during transport, and thus cannot be expanded in Outlook. Your understanding is dead on.  You could potentially maintain a number of different default address lists based on a user's access level (only letting them have a given group in their list if they're authorized), but that's incredibly ugly and would be nearly impossible to maintain. This prevents access to the curious, but not the determined/knowledgeable - keep in mind that without some nasty permissions changes, a lot of the user and group attributes in question are readable to any domain user with the tools and knowledge needed to view them. Of course, this means someone (or a group of people, if desired) will then have to moderate all messages that are sent to that DL. In our case, we wanted moderation anyway, so this worked well for our needs.",4
"I am sitting in South Africa. Our Head office is in USA. I need to connect to a Server there in order to create/change/update items on that Server. You'd either need to open up port 3389 (RDC) on your USA location's firewall & forward port 3389 requests the the USA server's internal IP address, which isn't very secure, or establish a VPN tunnel into the USA location's subnet & simply allow RDC connections locally. Someething like gotomypc is easy enough to setup and will be secure enough in the interim that you can use it while deploying a better corporate solution such as a VPN or SSL VPN or secure terminal services. If your office is expecting to remote into another office server on your own network, why would you want to go through the internet? If its on the same network you should just be able to use RDP regularly. If you're not on the network check with your sysadmins about a VPN connection. I do know how to connect remotely to another Server running Windows Server 2003 on my network. But what i would like to know is..........can i connect to another Server through the internet? If YES.....how can i do this? For a ""cheapo"" solution, I would actually recommend something like GOTOMYPC.com .  I say this based on the assumption that you are new to doing remote administration.",4
"If you are connected wirelessly, there may be a few issues.  First, certain wireless cards' drivers do not support being put into promiscous mode.  Nothing can be done about that unless you want to write your own.  Second, if your network is encrypted and you are only seeing layer 2 traffic from various sources and not the higher layer protocols expected (which doesn't appear to be the case), then you must enter the WEP key into wireshark so it can handle the decryption. WPA and WPA2 decryption get more complicated, as older versions of wireshark do not support it, and if it is supported, then you must capture the entire handshake taking place between the router and the device (EAPOL packets), as unique keys are generated between the device and router. If you are not connected via ethernet to you home router, most likely that home router than the home router uses a switch for its LAN ports and not a hub, thus each port has its own collision domain, whereas in a hub the collision domain is shared among all the ports and you would see all traffic on every port.",1
"If you have your sword and armour, then changing AD can be done with LDIFDE, or directly with ADSI calls. Is it possible to configure microsoft DNS server (widows server 2008+) to use Microsoft SQL as a data source for DNS data? Is it any guides available? You could use DNSCMD to change the zone somewhat safely without mucking in AD directly. I say ""somewhat"" because there be dragons in this land. DNS Server using a SQL backend will just slow up DNS response time. Are you looking to get statistics of the DNS? If so then you need to look at the Windows Server SDK which will allow you to interrogate DNS, of course you will need experience in programming. There are lots of good DNS server libraries for many languages out there that support database back-ends. Usually they are focused on geolocation, like pgeodns. I agree with everyone else, mucking with AD is not a good idea. If you want your application to interact (add, delete, etc.) with DNS zones\records on a Windows DNS server, use the dnscmd utility.",5
"That last is more Ubuntu-centric than the others, but you might still glean something useful from it even if you're not using Ubuntu. First, be aware that EFI/UEFI isn't simply a new BIOS feature; it's a replacement for the 30-year-old BIOS. EFI also uses a new partition table type (GPT) that's replacing the older MBR. Thus, the huge chest full of BIOS- and MBR-centric tools for managing dual-boot configurations is pretty much useless on EFI-based computers. You should forget everything you ever knew about dual-booting when you deal with EFI. Also, if you run across a product and it doesn't explicitly state the boot method (BIOS vs. EFI) or partition table type (MBR vs. GPT), you should assume it's for the older BIOS and MBR systems. That done, you can begin learning about EFI and GPT. Unfortunately, there's a lot of bad information on the Internet on this topic. Three sources you might try include: As to your problem specifically, if your partition table has been damaged, you may be able to recover it with the help of my GPT fdisk (gdisk) utility. See in particular the GPT fdisk documentation on fixing GPT problems. More specific advice will require more specific information from you. You could run the Boot Info Script from a live CD, post the RESULTS.txt file that it generates to a pastebin site, and post the URL to your document here. That will give us a better idea of what's happening on your system. It's not clear that your partition table is damaged, though; it could be that your boot loader is damaged or missing. In that case, fixing the Windows boot loader will require running a Windows recovery tool, and you can then install any Linux EFI boot loader you like.",1
"Once you have the database populated just run:  locate file.  If you just use the name of the file it will search for that file starting from /.  So you could get /etc/file and /usr/share/file.  If you want to just limit it to your home directory use this: That said, find has the distinct disadvantage of searching the filesystem each time that you call it. The locate utility, in its many variations, on the other hand, uses a regularly-updated database of the files in your system. There are two utilities find and locate.  Since others have suggested find, I will talk about locate. find ./ -name ""filename"" or you can do something like find ./ -name ""*.jar"" to find all the files with the .jar extension.  You can also do find ./ -name ""*.jar"" | xargs grep -n 'main' to find all the .jar files that contain a main in them. find has a number of options that allow it to find files by name, regular expressions or even more complex criteria such as size or ownership. E.g. you say find location -iname means not case sensitive and in the '' is a regular expression if you wish. You install a package named mlocate or slocate on older systems.  After installing run the command updatedb.  locate uses a database to store the path to all of your files, so searching is much faster but it has the disadvantage of going out of date frequently, and populating the database can take a couple of minutes. If you want a faster lookup you can use locate, there is a utility which scanns the disc regulary, like every week or day dependin on how it is setup Pipelining find and grep will work, otherwise find provides itself some options (like -name and -iname) to do this:",5
"For figuring out which replacement string goes with which search string, it might be better to use a HashMap<String, String> instead of a linear search. If you really want to test the actual exception text, then the better solution would be to have separate specific exception types for each problem type, or to reuse different existing exceptions. For example - it is quite OK to throw a NullPointerException if one of the input strings are null when they should not be. An IllegalArgumentException for empty-strings is OK. Or, create the DuplicateSearchStringException if you need it. This is a StringUtils class, not RegexUtils.  Therefore, I would expect all of the searchStrings to be taken literally.  You must quote each of the searchStrings when composing your regex. It's better because it stops streaming when the first problem is found - it does not need to check everything. There are few small things that stick out to me... starting with the least significant, and moving up... Why are your error message text values public? If it is to satisfy the Unit testing, then the common solution is to make them package-private, and put the unit tests in the same package as the code. Most common build tools (ant, maven, etc.) have separate folders for the tests and the source, and you can have the same package structures in each. Having said that, you should also consider your search-patterns - are you looking for exact strings? You should probably be escaping the inputs, if you are. Otherwise, what if someone inputs ""Hi|there"" now what? Despite its name, anyNullOrEmpty returns true if all values are null or empty. As a result, validation will only raise an error when all values are null or empty. If there is one non-null non-empty element, validation would incorrectly pass without raising errors. These test null or empty values in the search strings, or null values in the replacements should throw validation errors.",3
"Sometimes it helps if you serve the images from a different domain name (say, static.yourdomain.com instead of www.yourdomain.com), but rest assured, the browsers are like Terminator 2, they'll be back sooner than you thought. A simple Ctrl+F5 or Ctrl+Shift+R is enough to force your typical browser to refresh EVERYTHING. And if you have a dynamic web page, most likely your browser will still try to load the static images, too, even if the browser would have been told earlier that it does not need to come back for the images. In those cases the browser will try to request for an image, and the server will respond back 304 Not Modified.  But don't worry, that is the only response your browser will get back. No more data is transferred for that particular request, so from the resource side of view that is a very light thing. How web browsers ultimately behave, that's almost up to the web browser. Some browsers will not request already cached images at all, some will. Some will request them only if tuned for private browsing or something similar.",1
"I would suggest installing another instance on your local machine, unless you happen to have access to a test server. Then take note of everything you do to get your job up and running on that instance. If you can repeat it over and over again, then you have your deployment document completed.  Not knowing what all the steps of your job do, we cannot provide you a step-by-step instruction to deploy it. You can script out configured SQL Server agent jobs, which may be a starting point. Deploying the stuff you need to move a job is pretty much just like packaging the stuff you need to move application code assuming that you correcly put all the new items and changes in source control. This includes your SSIS package, any object creation or alteration scripts such as tables, views, UDFs, stored procs, CLRS (Never ever create database objects using the GUI if you want to deploy later), any scripts to populate tables (such as lookup tables). You may need to number the items in the deplyment folder to ensure they are run in the correct order. Usually I write a deployment document as well because some of what we are deploying will go to differnt servers (Our ssis server is is differnt from our database server).  You can also script out the job, but you will need to review and change the script for the new environment. I often find it just as easy to set up the job on the other server manaully (but I have dba rights to all servers, if you do not, you will probaly need to this).",2
"Unable to open the physical file ""c:\"".  Operating system error 2: ""2(The system cannot find the file specified.)"". Fortunately, we did have a backup.  It was located on a drive that had been disconnected after the server reinstall.  Network guy was verbally spanked on that one. The point is: IF you are using FILESTREAM, you have to get the stream data in addition to all of the normal data files otherwise you can't reattach.  Short answer: a database is screwed IF you are missing the filestream data.  I tried pretty much everything under the sun and you cannot attach a database that has a filestream dependency WITHOUT the original filestream.hdr file.   You can get it to the point that it's in recovery mode, but that won't allow you to get to the tables.  All we cared about was structure. Armed with the .hdr file and everything else in that directory we were able to attach the database back to the server with zero issues. I have a sql 2008 database that had a filestream.  Unfortunately, all I have is the .mdf and .ldf files.  I no longer have access to the filestream .hdr file or anything that was in that directory.",1
"I was wondering if I could leave my monitors turned on after Ive turned off my pc, here are some other questions I had: Does the yellow status LED always suggest the monitor is in standby mode? Every time I turn on the monitor without turning in my computer the light is yellow instead of green or blue respectively. Does that mean it automatically turns into stand by? When the power is supplied to the monitor, and it is on but not receiving video input, the status light will be one color or flashing pattern. In your case, these colors seem to be a solid orange and yellow.  i have one 144hz BenQ led lcd monitor which lights up green in use and yellow when my pc is turned off and an old lg lcd with a mercury containing ccfl tube which usually lights up blue I think and if my pc is turned off yellow, not sure If thats important When you turn on your PC, or wake it, your monitor will receive video input and the LED will turn a different color, or stop flashing. In your case this seems to be green and blue.   Yes it is normal. The Monitor operates normally upon operating the PC device itself. Usually LEDs have a living time of 5 years before starting to mis-operate and die !!! The different colors are used to help troubleshoot problems, if something occurs. The one color shows it has power, but is not receiving video input. The other is when it is powered on and receiving input.  The LED color or flashing pattern is unique to each monitor. I have one that has a flashing red when in standby mode, and a solid red light when it receives video input. My other monitor has an orange LED when in standby, and a blue light when it is receiving input.",3
"Q2: Do I need to backup the certificate, master key or symmetric key if I want to restore to another server? Q4: Should I worry about who can open and close the symmetric key? I use SQL Server 2008 for ASP.NET web applications. The DB's are hosted on Windows Server 2008 VPS's. I generally create one DB user of type SysAdmin and supply the connection details from the web.config file. I would be the only person with access to the VPS so I do not need to factor in other developers accessing the DB. However, if the VPS/Windows security was breached and an attacker gained access to the VPS and/or DB. Could they compromise my DB and information?  Q1b: If I create a master key password on Server1 and encrypt a column on a table. When I backup that DB (.bak) will I be able to restore the DB to another Server2 or will I have to create a master key on Server2 with the same password? Q3: When should the symmetric key be opened? Should it be done before each select and then closed? i.e. opened at the start of a stored procedure and closed afterwards.",1
"Something else you might try, since an empty .vimrc works for you, is to do a binary prune of your .vimrc file. That is, put a :finish command as the first line in your .vimrc file, restart vim and verify that copying works, then move the :finish command about halfway down your .vimrc file, restart vim and try again. Repeat, trying to narrow down the region that contains the problematic command(s). Commenting-out lines works instead of the :finish command, too. You can keep your .vimrc file open in one Vim while restarting a different Vim to make the whole process go a little faster. I based my vimrc file on a popular thread here on SO, and although it's great I have a real problem when it comes to copying text from vim to other linux applications.  I suspect the error lies in this vimrc since I can copy when vim is loaded from a blank vimrc file. The issue is that the exclude term of the 'clipboard' option, which is present by default if X is detected, must be the last term. By using +=, you have made unnamed the last term. A better setting would be Press the shift key when selecting text if you have mouse support enabled on vim. The normal selection (without shift) enables VISUAL mode. Using shift passes mouse events to your terminal emulator.",3
"Can you not use per-account resource limiting to limit the number of connection the client can make, and therefore limit the system resources used. In MySQL 5.0, you can limit the following server resources for individual accounts: If customer runs this script from the command line, ""nicing"" it in addition to the above is of course a good idea. For example, if a ""nice"" php script generates a massive INSERT statement in low-priority then sends it to the mysql server, the mysql server will execute that insert in its typical high-priority mode regardless of the ""niceness"" of the PHP script. This is because the Mysql server is a separate process with its own niceness and PRI. If you're picky about RAM usage you can ask customer to ""unset"" unneeded variables just before the sleep statement. But here's the thing: if you ""nice"" anything except the mysql server itself all inserts will be done at Mysql's priority, which is rather high. If the resource in question is processor then try cpulimit. cpulimit is a simple program that attempts to limit the cpu usage of a process. It seems to me the best solution is to ask this customer to set up his PHP script to do inserts in small bits (over a loop) and tell him to put sleep() statements inside the loop. Inserts will be full priority, but they will be manageable in size, and then the sleep statement should let other process run and give the database a chance to catch up. Here is your solution: http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch01.html If you use something like OpenVZ or VServer, you can essentially put a cap on the amount of CPU time consumed by each virtual server. You could create a virtual server that merely runs MySQL and then cap that. Does not change much otherwise. On then other hand, if you ""nice"" the mysql server itself everybody's DB ops will go slower. Not good, as stated in other answers.",5
"In general, the answer to your question ('is there an algorithm available to sort the objects in the correct order?') in arbitrary 3d settings is no - no such algorithm can exist, even when the objects are as simple as triangles.  It's easy to construct three triangles with a cyclic overlap; for instance, imagine looking down the Z axis at three long, thin triangles: As Nathan Reed suggests, the fact that you're in an isometric view means that you would probably be able to avoid this particular issue; the question is whether it's worth avoiding or not, and that's more contingent on your environment and the particular functionality you have available.  Most platforms you could be developing on have at least some form of 3d rendering readily available, though, and my first inclination would be to just piggyback on top of that for your purposes. Then where A and B overlap on-screen (near the XY point (0,0)), B will be on top (have the higher Z value): its Z value will be approximately 0.66 whereas A's will be approximately 0.33. Similarly, where A and C overlap (near the XY point (1, 1)), A will be on top; and where B and C overlap (near the XY point (1, -1)), C will be on top.  There's no way of ordering these three triangles such that drawing them to screen in the specified order will give the correct result.",1
"If you loop through all your servers and grab all services that start with ""MSSQL"" you'll have a rudimentary list to start with.  You may have to manually filter that down though. The Microsoft Assessment and Planning (MAP) Toolkit is a Microsoft product that's specifically designed to perform discovery and help identify licensable servers. If you want to programmatically find this information, you can make a call to the SqlDataSourceEnumerator.GetDataSources Method: Also, look at Generate a SQL Server Instance Inventory. It has all tools available for scanning sql servers on network. Another thing you could consider is to reach out across all of your servers (again, very time consuming) and get all services that contain a known SQL Server string in their name. You can also download Discovery wizard tool from Quest and see if you can use it in trial mode to get the job done.   You cant be 100% sure youve listed all sql servers on the network unless all are configured to work on default 1434 port. Otherwise this would turn out to be a complex task that would require scanning all non-standard ports on all servers. There's a decent webcast available showing some of the canned reports, which look like they might serve your purposes.",4
"Keep your downloads in a non Web accessible directory and then use a PHP script to create unique symbolic links into it. Then you can delete the symlinks after a specified period of time (e.g. after 24 hours). I have used a service call Net 2 FTP, which is a pure html/PHP interface to access FTP accounts online. You can download and install the software relatively easily and it has no overhead since it's such a small website. This solution doesn't prevent someone from sharing the link for the next 24 hours, so it probably isn't appropriate in situations where that matters. But it doesn't require any additional Apache modules etc. You can then use a cronjob to run a ""find /public -type l -mtime +24 -exec rm {} \;"" or something similar to delete expired symlinks. (NB: be very careful when using find to delete files.) This software is extremely useful and easy to use, it sounds like it would be perfect for your usage. Here's an example. Assume that your files are stored in /private and protected by a .htaccess file. You also have a /public directory which is writable by the user Apache runs under. X-Sendfile is made specifically for this type of operation.  You can read about it at http://codeutopia.net/blog/2009/03/06/sending-files-better-apache-mod_xsendfile-and-php/",3
"Disconnect all wireless clients from the router during the upgrade process. Make sure no wireless clients attempt to associate with the router until the upgrade process is complete. So my question is, what does the above statement really mean?  What problems might I have if the machine I am trying to upgrade from is on WiFi, or if another machine has WiFi on at the same time?  What is really necessary for me to upgrade safely in terms of what is practical for me to do? Problem is, the actual web page of the router itself doesn't mention anything about this.  All it says is: I've got a Linksys WRT54G and decided to upgrade to the latest firmware to see if it resolves a problem I am having with the router. Secondly, you should always update router firmware using a direct, wired connection between the PC and the router. WiFi connectivity is more prone to errors in packet transfer and a corrupted firmware package will potentially brick the router. Do NOT use a WiFi connection to update the firmware. So now I'm a bit worried.  First of all, all the headed machines on my network are connected to the router wirelessly.  So, without a lot of extra work I can't disconnect all wireless clients from the router because then I wouldn't be connected to the router and wouldn't be able to perform the upgrade.  Second, I'm also a bit concerned because this seems to imply that I need to go around and find every machine that is or might be connected to the router and forcibly go in and turn off WiFi until the upgrade is complete so that no wireless clients will attempt to associate with the router during the upgrade process.   Considering that I might not easily have access to some of the machines, this might be problematic as well.  (And does this consider people who might be able to see the router, but which aren't even in the building?  I'm assuming the security on the router might be disabled during the upgrade as it is telling me to save my settings because they might revert to default settings during the upgrade process.) I've never done that step myself but it sounds reasonable since you don't want to put the router in an unexpected state while updating the firmware. You can disconnect them forcefully by deleting their IP leases using the router's DHCP settings page and disabling the WiFi network (or change its settings so the computers fail when trying to automatically reconnect).",2
"I had to migrate the logging to RAID 0 disks and to overcome the possible loss of data, implemented Facebook's newly open sourced Scribe technology to move the logs to a centralized logging server.  Now we are now at several hundred million hits per day and we are moving terabytes of logs from our front ends, through scribe, to a central logging server which now makes analyzing those logs, graphing data trends and monitoring much easier.  For your purposes a single scribe server would handle that traffic and moving that data easily.   The logging for that many hits will definitely be having an effect, but how much effect depends on the size and amount of the actual requests - if the files being served are large than the logging will be making little difference overall and it is just the act of needing to read the files to serve them that is causing I/O contention. If the content requests tends to follow the usual pattern (many people requesting the latest few files with a relatively small number of requests for older content) then adding more RAM may help by letting the OS hold more of the content in cache. If the requests don't follow a useful pattern like that (i.e. the content requests over any given time period has no chance of fitting in a realistic amount of RAM) then this isn't going to help, of course. I would say you have answered your own question. Both of your ideas should improve the performance of the webserver. If you add additional drives for the logs they should be RAID 1. If logging activity is significant in your I/O bottleneck, then moving the logs to another device will certainly help. That device could be another drive/array on the same machine or over the network. Unless you are half way saturating the servers NIC at any point in normal activity (which is unlikely - your external bandwidth will be the bottleneck there) then this will not need an extra NIC unless keeping the logging activity away from the interface the public web service is on makes it easier to secure (or otherwise fits in better with your network arrangement). An unsaturated network link is not going to see latency issues in, or due to, Apache logging activity though if there are times when the link is heavily used for long periods Apache processes may end up blocking for extra time (meaning more RAM being used in busy times) while waiting for log writes to complete. mod_log_spread is a patch to Apache's mod_log_config, which provides an interface for spread to multicast access logs. It utilizes the group communication toolkit Spread, Is the fact that Apache is writing its access log to these same disks affecting the speed of reads? Do you typically add a separate spindle for logs? If so, do you add a single spindle or RAID it (you'll obviously lose log data otherwise if the disk fails)? In summary: moving the logs to other drives (in the same machine or over the network) may help, as will moving the main content to RAID0, RAID10 or RAID5 (beef up your backup arrangements if using R0, be careful of write performance issues with R5). More RAM may also help by reducing the need for the actual I/O operations if most requests in a given time period are for a subset of your data that is of could-fit-in-RAM-with-a-chunk-to-spare size. Having a busy webserver logging to a RAID 1 is problematic.  I can't remember at exactly what point we had to change our completely logging/archiving strategy but it was somewhere around a few million hits a day.   Or should we be pushing Apache logs over a network interface to a central logging server? I assume probably a separate network interface than the one that is serving all those HTTP requests? We have a busy webserver (> 5 million hits/day) that serves about 500,000 unique files. We're running Apache on FreeBSD 7.2. According to iostat -x, the bottleneck seems to be the seek speed of the drives (we're running RAID 1 with two spinning disks). If the reading of content is as much (or more) of a problem as the writing of logs, then you might consider using RAID0 for the content. You may need to beef up your backup arrangements to copy with the extra risk of a single drive death taking out the entire array. You could mitigate this with RAID10 but that will mean using four drives not 2. RAID5 may be an option too as for reads it will perform similarly to RAID0 and would only need one extra disk, but this is not recommended if the content changes often because RAID5s write performance issues [each block write becomes at least one extra read (the neighbour block(s) and an extra write (for the parity block)] will kick you. Also you are better not having your log files going to RAID5 for the same reason.",5
"If any of the files on your server have a lower version number or an earlier date than that shown in the KB article, the update might not be installed.  (Note that the date shown on the server files may vary slightly depending on what time zone you are in.) However, if you are uncertain, the definitive test is to look in the Knowledge Base article for the update in question, where it says ""File Information"" and compare the dates and/or version numbers to the corresponding files on your server. In my case, wsus server is never part of ""approval for install"" group. it is in a separated GPO group to receive all patches directly from Microsoft update center. Besides I have no evidence that patch KB3159706 is installed.I don't remember to the post install phase and systeminfo doesn't list it. How can I know if it is effectively installed and post install has been executed ? If you have Monthly quality rollup installed, the previous fixes maybe are included in the roll up package already. (Check for details on Roll up package) sever will only show you KB numbers for roll up you installed. It looks to me as if KB3159706 is included in the regular monthly Windows updates, so unless your machine is several months out of date, it should be present.",3
"first of all many motherboard manufacturer provide easy upgrade utility for windows only and bootable cd/floppy image.(i am saying this with my experience only and please don't argue on that because your specific motherboard manufacturer provide linux upgrading also.) I'm thinking of buying a new motherboard for my desktop. I've chosen a model and I've read some reviews on it. A couple of the reviews talk about BIOS updating/flashing and stuff like that. As a moderate hardware guy (I've built computers, but only know the basics of hardware and how they work together) running Linux, should I worry about this. I've never updated/flashed my current mobo's BIOS (maybe it needs to be -- I have no friggin clue). So are all these reviews specific to Windows users? I've never had much driver issues in Linux (other than ATI graphics cards), nor have I had to update BIOS. I would suggest reading the BIOS release notes to see if there's anything worth updating ... sometimes they fix major issues, sometimes its minor issues, sometimes they make things worse. second it is risky process. if you face a power cut during upgrading process(only some gigabyte motherboard provide dual bios system which can recover from this type of situation) your motherboard will not be usable anymore unless you send it for repair to the company. My experience has been that most of the major motherboard brands have utilities for both Windows and Linux.  Even if they don't, Linux has a strong community where someone else has that same motherboard and probably has found a way to do it. As for Windows vs Linux update issues, you're only concern is if they release only a Windows executable version. The better brands tend to build and package their own bootable CDs so you can easily update your BIOS without needing to go into your OS. Some brands even make it as simple as putting the file on a flash drive and there's a tool in their Boot menu that lets you update from your flash drive. At worse, for brands that only have Windows only packages, I'd suggest avoiding those brands or looking up how to make a DOS bootup flash drive and using that or BartPE if you have time to research that.",4
"In your case you need first option. I also recommend reading this article for more in-depth understanding of requirements and limitations in both modes. I'm using standard Remote Desktop Connection on windows to connect to a Ubuntu instance (through Hyper-V) . Extremely simple setup on the ubuntu -side (http://www.liberiangeek.net/2011/06/connect-to-ubuntu-11-04-from-windows-via-remote-desktop/)  /multimon -- Configures the remote desktop session monitor layout to be identical to the current client-side configuration. First option allows you to view multiple remote monitors on single client monitor, on the other hand, multimon feature allows you to view remote desktop on all local monitors. spanning across multiple monitors, if necessary. To span across monitors, the monitors must be arranged to form a rectangle. However, I'm nog getting a dual monitors setup, even when setting 'Use all my monitors for the remote session' in the windows rdp-client.  MSTSC [<connection file>] [/v:<server[:port]>] [/admin] [/f[ullscreen]] [/w:<width> /h:<height>] [/public] | [/span] [/multimon] [/migrate] [/edit ""connection file""]",2
"Exceptions that I've encountered are situations where you have lots of small writes mixed with large writes. One example that comes to mind is a busy windows file server with users doing stupid things like running active PST files on the server. Another would be a linux pop3 server with mailboxes in Maildir format. It's a bit of a yes, no answer. Useful in certain circumstances but it's less of an issue than it was with FAT or regular HFS. All filesystems will fragment but newer ones are more resistant to fragmenting so badly. HFS+ does fragment, all filesystems do. However, it doesn't appear to suffer from it, at least not to the extent that NTFS / FAT32 do. Fragmenting still happens and you can see performance drop because of it, especially in video editing systems or a workflow that requires the ability to read or write large files quickly to the disk. For your standard user - a near non-issue. A caveat-- I stopped noticing performance drops from file fragmentation about 5 years ago, at least as far as local files were concerned. SATA bandwidth and a 7200RPM HDD make the issue pretty much un-noticeable, IMO. Speaking for Mac OS X specifically HFS+ does a decent enough job of trying to keep things from being fragmented compared to older systems but it still happens just not on the same scale. The OS itself also defrags ""small"" (20MB or smaller) files on the fly since 10.3 (Panther). This is a religious issue. IMO, fragmentation is only an issue for specific workloads, and it hasn't been terribly relevant since NT4.",3
"One small thing to check; from the asterisk CLI (run using asterisk -r), do a ""sip show channels"" while the recording is playing to confirm the codecs you expect are being used on the problem channel. On my systems, I've also discovered duplicate recordings with .WAV and .wav extensions, where each worked with specific codecs. When I've had similar problems, I found that the recordings were created with a different codec than what was currently in use. (ie; recorded from a handset connected via g729, then copied to a system using ulaw.) I have no idea why I can't hear the sound even though it's saying 'Playing'. The file (custom/Sales.wav) is present, the permissions are right, ownership is correct, codecs ulaw and slin are allowed in sip.conf.  I'm setting up an IVR in asterisk. It was working fine but I installed some new hardware and the sound has disappeared. I can see this in the console:  If your SIP phones hit the default context, then any incoming calls will play the ""Hello World"" sample sound. Can anyone give some hints on how to debug this thing? (Btw, I'm using FreePBX for configuring the IVRs). This will at least tell you if the SIP phones are able to register with the server and place SIP calls, and tell you whether your dialplan is being reached at all. If you don't hear any sound, the most basic debugging tool is to setup a [default] context that answers the phone and plays one of the default Asterisk sound files.",3
"In Function waitForArchiveReady() your whole attempt at a Try/Catch/Finally construct is well intentioned but it is not the way errors are handled in VBA and leads to awkward constructs like the inclusion of the unqualified Goto Success and the (I'm surprised this compiles) Resume CleanExit. This would be better handled with something like this (comment block removed for brevity): Your comment is contradictory to the code, if you want to check only 64 bit, then why not #If Win64 Then? Looks to be incomplete. You will always fall through to the Case Else. If that's the intent, then there's no need for the complexity of the Select Case statement. Also, the Resume line will never be executed because of the Resume ExitProc above it. Also you're mixing naming conventions here (upper / lower first letter, order of words), try to harmonize them In Function CreateArchive() you have the ErrorHandler: label, but you don't have an On Error Goto ErrorHandler line to catch errors and send execution there. You'll still end up with the default Excel error window if something goes wrong. It feels good to clean these up like this, but they're actually useless lines of code. VBA will do this for you automatically when a Function (or Sub) ends whether there was an error or not. If there's any error other than 55, your code will loop forever. As a college prof once told me, a supercomputer is one that can execute an infinite loop overnight. I'm not aware of any supercomputers that support Office & VBA, so this will be going for a while... There's something about Create and Unzip that just seem off in the function names. They're not unclear, just off - they're opposing functions, but the names aren't opposite enough. Maybe change UnzipArcive to ExtractArchive? Maybe I'm just being hyper picky.",2
"Someone has suggested that my browsers and curl have a looser requirement for TLS validation than openssl. Is that really the case? I have a hard time believing it. Can anyone help me explain this behavior? I have tried to add in -CAfile intermediate.pem (where I downloaded the intermediate certificate from TrustWave). Even though the intermediate certificate does not appear anywhere in my KeyChain, I have exported my KeyChain System Roots into a single file and tried that via -CAfile also. Nothing is working. The only filesystem location I see certificates is /etc/ssl/cert.pem and when I specify that via -CAfile, it still fails.  BTW, I know this can be resolved by including the intermediate certificate with the endpoint certificate on the TLS endpoint for www.visitflorida.com. Now if we could just find that missing key file! On a Mac, High Sierra 10.13.5, I'm seeing a difference in TLS certification validation. Chrome and Safari are happy with TLS validation when visiting https://www.visitflorida.com. Also, curl has no complaints and I am NOT using '-k'. However, openssl complains that it cannot find the intermediate certificate when trying this openssl s_client -connect www.visitflorida.com:443 < /dev/null | openssl x509 -subject -noout. I have used both the base openssl and brew-installed one.",1
"I recall this being chewed over on alt.sysadmin.recovery back in days of yore, when there was no such thing as /proc, and /dev was just a regular directory containing entries for a bunch of unusual inodes... If you let it go, the OS will pretty much be beyond recovery though you may be able to get some data back easily. On such systems, if you started one up in maintenance mode (so nothing was running but your shell, not even init, and no secondary file systems were mounted) and did exec /bin/rm -rf /, you would be left with a completely empty root file system except that /bin and /bin/rm would survive. One point I didn't see made by anyone else: files that are currently open (e.g. rm itself), even if deleted, won't actually disappear fom the drive until closed. It'll go quite far... if you're using a gui you might have fun noticing things degrade more visibly. (icons on menus stop loading etc.) For having tried it once (on a server that what pissing me off), logged as root, in terminal, you'll lose almost everything. The only thing that'll not be erased will be only the process that was essential for the OS. ... but, on some variants of Unix (my recollection is HP-UX, but that could be totally wrong), you could not remove the last directory entry for a program that was running.  (Shared libraries?  What are those?)",4
"However there is at least one clear offline occurrence that I have spotted over this past year: when one of our DCs restarts, some users are being put offline, despite the two other DCs remaining up and running. Surely this should not happen, even for users who got their DHCP lease through the restarting DC. This makes me thing that something might be misconfigured that could lead me to the cause of the more general offline/synchronisation issue. We run a small wired LAN with 3 DCs (W2k8) and about 25 workstations (most of which are XP SP3, some are 7 SP1.) People got used to live with it (so to say) and synchronise every so often. At least Windows 7 seems to deal better than XP regarding this whole offline/sychronisation issue in the sense that it doesn't bother people so much with popups... etc. I have searched quite a lot for what could be the cause of this with no success so far. At this stage I still don't even have a clue as to this issue is even software related or not. What version of the CSC files are you running?  Given that there are a lot of known issues with offline files functionality, you may want to try updating those files and see if if it resolves the issue.  A recent version is available here:  http://support.microsoft.com/kb/2705233 Users are put offline often, and apparently randomly, with Windows asking for synchronisation despite the users not being offline at all (all pings are fine, and other maybe less connectivity-sensitive services keep running fine.) People use roaming profiles with folder redirection for Desktop & My Documents, Application Data and Start Menu.",2
"if possible, also do a iostat -xCzn and grep for any HW or Transport errors on the disks or controller. However, is there any package already made that does this, or specifications on how I can get some performance metrics from this array? This is all fine, but I'd like to automate a way to check every once in a while to make sure my pool is error-free. this also assumes that 'zpool' will be in the path of the zabbix user, and that this user will be allowed to run zpool. if not, specify full path and use sudo. another catch might be a default shell that doesn't support used syntax, in which case you can either rewrite the userparameter, or force it to use bash. now, in zabbix add an item with key ""zpool.status"", create a trigger against it (using a function like "".last(0)=0"") and you're done - trigger will fire whenever that string is missing from the zpool status output. I have cacti, and zabbix available at my disposal. I suppose I could also write a program that greps that output and if it doesn't find the phrase ""No known data errors"", send me an email.  I set up ZFS on Ubuntu (via fuse) for a storage array at home and it has worked great for almost a year now, despite its 'beta' status. I log in and check the array every once in a while using:",3
"So basically my question is, is there any way I can reset my credential cache without having to reconnect to the network? Or is it IE that is causing the problem perhaps, since I can RDP to servers and use Tridion 2011 instances in other browsers fine? We had a major network issue where our secondary domain controller (responsible for Win2k3 boxes) died and had to be rebuilt (I beleive this is what happened, I am a developer not network admin). Anyway, I am working remotely via VPN at the moment and since this happened, I am getting an authentication box when trying to access certain areas of SDL Tridion via IE (Tridion 2009 SP1 is IE only) it seems like somewhere my credentials are not being passed correctly or the ones cached on my laptop do not match the ones the Domain Controller has.  This only seems to affect Windows 2003 servers. Our IT support thinks that the only way to sort it out is to connect my laptop directly to the network. I am not planned to go to the office for a few weeks at least and this issue means I have to work with Tridion via Remote Desktop. We thought changing the password on my account might work but this didn't help. Usual tricks to 'synchronise' the account is to connect to VPN, then lock and unlock the computer. This will reveal whether your account has expired, locked etc. Changing the password has the same effect. Aside from that there is also an IE security setting that controls how logged-on credentials are passed to websites. It's called ""Automatic Logon with current username and password."" But there are implications for switching this on and you need to speak to your sys admins. If you login over a VPN and the resulting network you are connected to has sight of your AD domain then your computer should synchronise properly. Assuming of course your computer is domain-joined and the VPN can deal with AD traffic. As an aside, there is no such thing (since NT4) as 'secondary' domain controllers. And specific domain controllers are not responsible for specific OS versions. If there is a specific DC that is generally responsible for a specific group of servers it will be due to AD sites, but if that DC fails those servers should still be able to authenticate across the WAN, if your network is setup properly.",2
"The address I get via PPPoE is 188.xx.xx.177/32, which according to our provider is our Default Gateway address. They claim the subnet is correctly routed to us on their side. The provider requires us to connect via PPPoE, and I managed to configure the ASA as a PPPoE client and establish a connection. The ASA is assigned an IP address by PPPoE, and I can ping out from the ASA to the internet, but I should have access to an entire /28 subnet. I can't figure out how to get that subnet configured on the ASA, so that I can route or NAT the available public addresses to various internal hosts. We've just received a fiber uplink, and I'm trying to configure our Cisco ASA 5505 to properly use it. To clarify my config; The ASA is currently configured to default-route to our ADSL uplink on port Ethernet0/0 (interface vlan2, nicknamed Outside). The fiber is connected to port Ethernet0/2 (interface vlan50, nicknamed Fiber) so I can configure and test it before making it the default route. Once I'm clear on how to set it all up, I'll fully replace the Outside interface with Fiber.",1
"Of course, a single scan is not enough. You need to do it regularly in order to get a complete picture. Once you have a reasonably complete picture, you'll have to start questioning which of the identified services are really needed. Check that the configurations are sane, according to best practices for each. Make sure everything is up to date, antivirused if Windows, the works. NetFlow was designed to do exactly what you're looking for. If properly set up you can see all of the conversations on your network and discover each service in use. There are a wide variety of products ranging from cheap to enterprise grade, closed and open source. Some of the fancier products will derive dependent services (e.g. the front-end web server depends on the database, DNS, and LDAP servers.) and alert you when there are outages or performance issues. In Windows you can run ""netstat -a -n -o"" to list all listening ports in numerical form and match those ports to processes. Pull all the above together, ask around if somebody can tell you what is supposed to run on the targets, check with users what they use on the machines. A combination of nmap from the same network (no firewalls in between) and local checks (as the answer by jeoqwerty or lsof/netstat/others on Unix/Linux), check what services are being started, check what is installed. To use it you'll have to set up one or more collection points on your backbone (either active on your switch, router, and/or firewall or passive via a SPAN port or tap). Each collection point will feed NetFlow data to a collector. Another possibility would be to scan SNMP information and check out the currently active TCP/IP connections via SNMP. Of course, for windows and Linux computers, SNMP is not sufficient and maybe other protocols such as SSH or telnet need to be utilized. Once you have all that information, you can determine which server is talking to which client/other server. Look at the ports open on the local firewalls of the targets, check the configuration of ""Internet facing"" firewalls, what traffic do they let go through to your targets/leave from them. Any special configuration there is presumably to allow some specific service.",4
"This alternate (incorrect) implementation of SimpleUserStore would still pass, but only because it is lucky. (your 'not exists' tests would fail though). as you are holding the reference for A the whole time, UpdateUser doesn't do anything useful for you the object in your store and the object in your test are the same object. Your use of userdata.Contains() is potentially/probably problematic. Contains checks against reference. as you are using the same two objects throughout your test, the reference will be the same. what would you expect from this test? Truth be told, I can see two bugs in this implementation. I'll leave it to you to fix your tests and actually find them yourselves. However, its bad enough that if you wrote this in an interview I probably wouldn't hire you. One of the bugs makes it look like you are very confused about the semantics of the language you are using. Maybe its just a careless error (they happen to the best of us), but it would give me grave doubts. Try AddUser_GivenAlreadyExistingUser_ReturnsFalse(). The first part is the function I'm testing followed by the scenario I'm testing then finally the expected output. This way you can build your test backwards (i.e. starting with the assert then moving back to the point where you create the system object). A number of the tests provided are problematic since you are only testing with one entry in your collection: You don't do anything to verify that update actually did the update. You trust the return value. But you shouldn't do that. Let's actually look at the implementation. This isn't directly related to the TDD question, but does illustrate your tests are not picking up on problems in your code: You don't verify that this works. You never pass a different object then the one you originally inserted in the Store. You need to pass another object to see if that works. You verify the return value, but you don't verify that the user has been removed from the store. The object could very well return success without actually removing the object. You should have a call to GetUser() or similiar to verify that the user isn't there anymore. It is common to use the list of tests as a requirements list for the component under test. and so should indicate The method tested, the preconditions, and the expected outcome. I like Songo's suggested formatting. The point is someone else should read the name of the test and know what is being tested without actually reading inside the test. Think if it from the point of Clean Code, function names should tell what is happening while the code itself inside it should tell How it does it.",3
"I expect the more expensive QNAP's to be the same, and also the cheaper ones (e.g. the TS-219P+) as they all run the same custom Linux. You can bung this on any old pc. I personally bought a Celeron machine for around 60 (although I spent another 70 on a 4-bay hot swap hard drive holder!). I have used it from Macs without any problem, it even supports AFS (Apple Filing Protocol), I have never used this, but surely that has to be even better! I guess for HFS, you could always just use a HFS friendly distribution and enable networking on it... However, if you want a good NAS, I would highly recommend Freenas, it uses UFS and it works very well. I recently setup an unRaid server for my Nas and it works great so far.  I suggest you take a loot, it requires some setup but the initial cost can be low and you can use existing drives plus it's super simple to add additional storage down the road and everything is semi-raid so if you loose one drive you can easily recover.  The only thing it doesn't do is allow timemachine backups but that's minor in my opinion. I had an issue with one of the disks once, but their support was very good and got things fixed quickly.",4
"Re-starting Desktop Window Manager Session Manager service restored that option into that list (in checked state). I had Let Windows choose what's best for my computer radio button selected here before and after the problem and before and after the fix. Try this: Go to >Control Panel >Device Manager >Display Adapter. I had a Mirror Driver listed (I've been running UltraVNC). I uninstalled (right click on the mirror driver, left click on uninstall). Just leave there your graphics card > (e.g. mine, ATI Radeon HD 5700 Series). Then re-booted. I got my Aero back! :D   EDIT: I got my hint from here: http://www.tomshardware.com/answers/id-1680343/windows-aero-theme-unavailable-desktop-window-service-missing.html Note that control panel\System and Security\System -> advanced system -> Performance Settings -> visual effects tab had Enable Aero Peek MISSING. It was not just un-checked, but missing entirely from the list. However, The Desktop Window Manager Session Manager service, altho it was Started already, I restarted it, and it fixed my problem instantly. Taskbar icon peek and transparency restored.",2
"2) Something accessible over the network. Instead of having the device directly attached to one specific machine, have it available to all the servers over the network. Our File Server is a 12-disk raid 6 set up.. I was thinking something like that, but with no raid involved, all disks are stand alone so they can be used/installed/removed on an individual basis. Does any such thing exist? 1) Doing HDD instead of tape. Tape is hard to deal with. We have a regular rotation cycle, so they don't need years and years of shelf life, so I'm wondering if something HDD-based would be better. We have a Tandberg T24 tape device to handle all of our long term backups right now. We decided that we're not backing up nearly everything that we would like to and that we still have a lot of vulnerabilities. To get to where we want to be, we're going to have to back up a lot more servers than we're currently doing. All of our internal servers have some sort of directly attached drive (I.e. LaCie Raid box or a simple portable hard drive) doing backups, but what we want to do is get those backups off-site. The current tape drive is directly attached via SCSI to a Windows Server 2008 File Server. So to back up anything to tape, it has to be funneled through the File Server. With the current increase that we have planned, I don't think that funneling everything through the File Server is the right course of action and I'm thinking that maybe a second backup device would be more appropriate. Thanks for your ideas. I'm really interested to hear about some of the solutions you guys are using..",1
"What are the solutions out there to either automate the process or just make it a little easier? Sometimes it's a pain just to find all utils again and read through a lot of old blog-posts on strange installation procedures again. If you mean a complete reinstall - also the complete OS, Windows 7 - there is a tool for your needs. Also, when possible, backup settings, export registry keys, etc. after getting things initially configured.  That way you can reimport them quickly after reinstalling instead of walking through dialog boxes twiddling bits. A little easier? Windows Easy Transfer http://windows.microsoft.com/en-US/windows7/transferring-files-and-settings-frequently-asked-questions . Make it the first thing to run on a fresh Windows 7 set up. When ET is done, it will also tell also you which applications need to be installed. Save copies of that information and data, particularly blog posts that might disappear from the internet.  Toss it in a ""reinstall"" folder on your system. For the programs I have that require no installation, I have a folder ready to copy to a freshly installed system. (I'm not quite sure why you would want to just re-install the System. Like most UNIX systems, OS X doesn't really degrade over time. I guess it might be useful if you were really unlucky and got hit with some MalWare.)  To copy an OS X installation, files and all, you can just do a backup with Time Machine, reboot from the installation DVD using the ""Restore System from Backup"" option, and then wait. This process is completely automatic. I would use vlite to make an unattended install, as you can specify the programs that are installed with it. Works perfect in a server environment too.  If you want to re-install just the System, without touching the Users data, use Time Machine again: restore everything from the oldest copy of the Home disk that you have, that has all the utilities you want. When that's done, use Time Machine to restore the latest Users folder backups. Whenever I setup a new computer or re-installs an existing one I'm using a day or more to get all the software and settings setup, so the environment is like the one, I'm used to.",5
"I have a Cisco RV110W. I ran nmap at it from the outside and nmap reports that the router has tcp port 444 open. Yet there are no port forwarding rules specifying this port. It should as far as I can tell, be closed. There's even a service listening to that port which I can connect to through telnet. I threw some SNPP commands at it but the service doesn't respond to any of them so I don't believe it's SNPP. If all else fails, you can always close it and see what breaks/who screams.  I tend to favor that approach, but I do love the sound of users screaming, so maybe that's way. Does anyone have any idea why this particular router has tcp port 444 open? I haven't been able to find anything in the manual or on Cisco's website. It's purpose is really anyone's guess - as you mentioned, 444 is the standard port for Simple Network Paging Protocol, and that's it.  My next guess would be that it's a secondary https server, given that the standard port for that is 443, and people often get lazy and just increment the port when they need to duplicate a service on the same hast.",2
"Because PostScript is a programming language, the entire contents of the document must be computed step by step.  While PDF supports a limited subset of PostScript for vector graphics, it is designed specifically as a document format and does not require this sort of computation. and see if it is any faster. (gv also uses PostScript for rendering, but doesn't convert to PDF.) Or you use Ghostscript directly Is there a reason PostScript is a lot slower when rendering (tried with evince on Ubuntu) than PDF or  are the reader just a lot less optimized? I always thought that PDFs and PostScript files are quite similar, even though PostScript is a turing complete language. It depends on the PostScript code. One could write a 3D rendering algorithm in PostScript and use embedded AutoCAD files as pictures. It will take much time to render while PDF will contain  prerasterized images. In some cases PostScript will be much faster to render. Unfortunately most popular publishing software emits very inefficient PostScript code. But converters PS->PDF are able to optimize their output for fast rendering. But don't forget one thing: exactly because PostScript is a programming language, you can design PostScript files which are rather short in Bytes, but make the interpreter go through a loop with thousands of iterations (for example to compute + render a fractal) before it displays the file content on screen. I suspect that evince cannot directly render and display PostScript on screen. Maybe it can only do so for PDF. So it may use a trick for PostScript files: behind you back it converts them to a temporary PDF file and display that one instead.",4
"I would suggest using a site-to-site IPSec VPN tunnel in Linux using Openswan, however this requires you have at least two nodes to create the connection, one at each end of the route you are trying to tunnel over. If you go for the Openswan solution your VPN will be transparent to your router, i.e it wont know about it nor care with the caveat that If you are facing problems with NAT, try using SNAT instead of MASQUERADING. Assuming that the router supports DD-WRT or equivalent  (and the TL-WDR3600 which TPLink describes as N600 does), then yes, you pretty much can do this.   Also, some ISP's will filter IPSEC protocols/ports etc so you should be mindful of this when troubleshooting!  You could look into DD-WRT (custom firmware) on your router - from what I can see it's supported on your router. This would allow you to configure a VPN provider directly on the router. Preventing you from having to install / run on individual devices or devices that don't allow you to install / run VPN software directly. This is quite easy to do with DD-WRT and OpenVPN [ which of-course requires the VPN provider to support OpenVPN, but most do ], but any router which can act as a VPN client should do have this routing functionality as standard. Is it possible to have a router automatically use a VPN so that all of its users are on the VPN? The router is a TP-Link N600. Basically, once I connect to my Wi-Fi connction, I'd like everything to go through a VPN. In general terms, a VPN acts as a virtual interface, thus you can route all the traffic through the VPN rather then directly out the LAN Interface.  Note that traffic WITHIN your LAN will, of-course, not be encrypted by the VPN because it does not go through the router. NOTE: I have never installed or used it, just looked into it for my own circumstances. There a plenty of things that can go wrong and according to their website Incorrectly flashing can brick your router!",4
"What is the best combination of permissions, owner settings and/or group settings to allow my script to create and delete directories under ""sample"" while retaining the ability for ""user"" to continue to FTP into the directory? This might already be what your doing but, here is a thought. If the contents of sample are only to be accessed via FTP then you can move that directory outside the web server's document root and use your 0775 with owner=user and group=nobody plan. The php script will be able to write, user will be able to use FTP and the outside world will not be able to get at sample through the web server. To use PHP, you have to run PHP as CGI. Usually, this means disabling mod_php, but eventually there is a way to switch between mod_php and php-cgi with different extensions (e.g. .cgi or .phpcgi). Haven't tried it, so it's just an idea. So I want to have a script be able to create (mkdir) and delete (rmdir) directories under ""sample"". Yet, I don't want to obviously overly expose my server by opening up the permissions (I could easily chmod sample to 0777 and make it world write-able). The best would be to run the script as user ""user"". If you are using Apache, this can be done using the suEXEC module: http://httpd.apache.org/docs/2.2/suexec.html. I am interested in knowing the proper, yet security-conscious settings for a directory. Here's my scenario: In situations like this I've used PHP's FTP client support to do the actual file work.  So basically I let PHP do the uploading normally to a directory that it can write to.  Then from there I use FTP locally to move the file to the appropriate location (after validating the file).  This works best if you aren't exposing FTP to the outside world.",4
"Modelview and projection are conceptually different although mathematically the same (it's all just matrix multiplication). With the programmable pipeline all of this goes away, and you can use as many (or as few) matrices as you like, concatenating any you like in order to get the end result. The use of a concatenated ""modelview"" in OpenGL (which corresponds to D3D's ""view"" and ""world"") is therefore just an artefact of OpenGL's design but need not specifically refer to the way things are actually done. The camera analogy is a lie because there is no camera.  Instead all that happens is a transformation of points in 3D space to points on a 2D screen, and the matrices define how that transformation happens. Since you mention ""modelview"" I'm going to deduce that you're talking about OpenGL and with specific reference to the old fixed pipeline.  The fixed pipeline in other APIs, such as D3D need not have the same concepts.  In fact in the old D3D fixed pipeline there are 3 matrices used: projection, view and world.  In this case ""view"" is what you might call the ""camera"" and ""world"" is what you call ""each model's model-matrix"". Projection defines how those points are then projected onto the 2D display; i.e how perspective, foreshortening, etc happen.",1
"Additionally, if you add the -p parameter, and run it as root, it will show the process that opened the port: A good and reliable way to check for ports opened is using ss (replacement for the deprecated netstat), it's usable in a script without requiring elevated privileges (i.e. sudo). The number after the colon in the Local Address field shows the port in use.  If the state is ""LISTEN"" it means a port that is using for incoming connections.  If the IP address in the Local Address field is 0.0.0.0 it means incoming connections will be accepted on any IP address assigned to an interface - so this means from connections originating outside of your machine. Usage: option -l for listening ports, option -n to bypass DNS resolution, and the filter on source port NN: src :NN (replace NN by the port you want to monitor). For more options, see man ss Anything not shown as being in use is free, however users (unprivileged accounts) can only open ports above 1023. (found on http://www.unix.com/unix-for-dummies-questions-and-answers/8456-how-know-whether-particular-port-number-free-not.html)",3
"This sounds like a job for a Recurrent Neural Network. I must be honest I have yet to fully dive into them but the idea is it has connections pointing backwards which it feeds to itself. It is often used for stock predictions and time series related problems. There is a entire section dedicated to it in O'Reillys ""Hands on Machine Learning with SKlearn and Tensorflow"". I highly recommend it. Great tool to have. Good luck. I would really love to apply machine learning to this dataset. I've looked into the following because they seem to be the best solution to my problem: linear regression models, LSTM neural networks, and time series forecasting, although I am still unsure of how my dataset can fit into these certain models. I would like to forewarn by saying I am by no means an expert in this topic and I apologize if I mix terminology, phrased the question wrong, or any information is incorrect.  I would like to use a machine learning model for the current dataset that I have gathered. The dataset has about 8000 blocks (or entities), and each of these blocks has 7 features. Each feature has around 10 yearly values, which are counts (ex: 2005=1, 2006=2 ... 2014=0, 2015=3). The long stretch goal would be able to predict the 11th, and even 12th, yearly value for each feature.",2
"Once you have permission, work WITH your company's support staff to plan the best way to keep them from having problems because of you. This really isn't the forum for these kinds of questions. Most of the people here are the very people trying to keep you from bypassing security measures. My guess is that the corporate filter is blocking traffic it thinks is disguised, and will look for keywords such as proxy and long urls after generic sites. Example: www.home.com/index.php?id=2309324230842303984320948230982039 where the id is a base_64 encoded version of the requested url or something.  My suggestion is still create a VPN network at home, and connect from the office. At the very least you could then RDP into your home box and browse as if you were on your home computer. ...Go to your boss and convince them you need such access. Make him / her a deal; if you aren't getting your job done to their satisfaction, you'll give it up. If you're smart, you'll plan a way to make it VERY easy on the company's administrators. Also, if you're smart, your boss might want to keep you. If you run it as a secure (https) website, then all they have it the URLs, they can't see any of the other content, and thus can't block it unless you have suspicious paths. Web filtering normally works based on the url. If you've got a file called proxy.php or a folder called /phproxy/ then it's going to catch those. If there are no keywords in any of the URLs, then it could be looking at the page title - it's unlikely that it's trawling the whole page.",4
"Revocation- it's very easy to revoke a cert you've generated, and you can give your certs short validity times Liability- depending on what you want to use these for some C level exec may decide that all that money they spend on VeriSign certs is a waste because they've been using you certs just fine.  If your certs get compromised and it's a VeriSign cert (and I'm simply using VeriSign as a placeholder for any third party CA) well that's easy to show that it's not your fault.  If you own the CA...well it's possible that your next IT experience might be running the computerized cash register at the fast food restaurant of your choice. Backup/high availability- Nothing says bad weekend like having HR tell you that your now dead CA gave them a cert they used to encrypt the payroll files and now can't decrypt the files because the cert can't be validated.  Make sure its backed up often and is highly available If all your servers are in-house and accessed by in-house software, then your own CA is the way to go. You can deploy the CA's root certificate by GPO (if you're on a domain then installing the CA role into a server should do this automatically), and then every machine on your domain will trust it automatically. However, if your software is accessed publically (doesn't sound like it is), then the software at the other end may very well bring up certificate warnings stating that it does not trust your published. In that case, the only option you have is to purchase an SSL certificate from a trusted publisher (there are places much, much cheaper than Verisign if you don't need their insurance policies). An in house CA is only valid inside servers you own and and external cert is good anywhere.  That's the basic answer but there is alot more to consider. While it might seem like there are more cons, many of these are just things to be aware of rather than a true negative indicator. Certificates for encrypting traffic are often interally generated.  If you only allow connections from your CA you don't have to worry that someone will get a certificate from your external CA and join your network.  In this case the lower cost solutions may be more secure. For a bunch of internal servers, you can create certificates as needed and have them in place much quicker than using and external CA.  You'll need a little experience.  Tools like tinyca make if fairly easy to create a CA.  For publicly accessed servers you will want certificates from an external source. What are the pros and cons of spinning up an internal certificate authority (primarily Windows 2003 CA)?  We have the need to encypt server-server traffic on a project that has 20+ certificates.  We could buy certs from Verisign, but I was thinking that an internal CA might be a better long term solution.  So I was looking to the community to provide a pros/cons list of what we might gain (or lose) by hosting our own CA?  Thanks in advance for the help. Access to free certs usually means greater usage- Usually folks will start signing their emails, and admins think about using domain and server isolation in mixed environments Additional security requirements- This machine is possibly more important to secure than a domain controller.  This system needs to be completely hardened.  If you are using these certs for anything at all meaningful you need to consider the implications if those certs get compromised If you host your own CA, it will only be valid in sites/computers that have your CA's root certificate installed. In other words, just because you have your own CA does not mean that your certificates will be trusted by strangers. Inability to use the cert external-  It's not impossible to make your CA publically available, no one is going to trust a cert from your company.  While thats not a big deal it's a little more overhead to maintain 2 separate cert maintenance tasks (expiration times etc.).  You could also consider buying a trusted root cert",4
"Change the domain of the client and rechange it back to the same domain this should reset the VTP counter and now the client will get the VLANS from the VLAN server I think you should check whether you configure the client switch port with mode trunk or not, it update automatically, right after you put the right domain name and password. A vtp advertisement is sent only when their is a change in the revision number. How we can change the revision number? Pretty easy.. create and delete a dummy vlan. We cant force a client to pull the update from the server. Updates to VTP are only sent out when there is an increment to the VTP revision number. The only way to 'force' an update is to create an erroneous Vlan and then delete it. This will accomplish a 'force update' and send the change out to the rest of your clients. While most people get the vtp config setup they sometimes forget to trunk the interface. When all your vtp stuff looks right (but config rev not incrementing/updating) then it's probably a trunking issue on the int itself - try switchport trunk encap dot1q/isl then switchport mode trunk - should do the trick - kerry_13 had this exactly right, that answer should be at the top for all the times i've seen new guys do it.",5
"EDIT: There are different routing schemes. What a router does is takes an incoming packet and passes it along to some other device (other router, switch, computer) based on the routing scheme. You can read an intro to some routing schemes here. For a simple explanation, imagine that each router has a routing table with IPs and subnets. Each entry in the routing table has a key (the packet destination IP), and a value (the immediate outgoing IP or subnet). When deciding where to pass an incoming packet, it takes the IP of the packet and searches in the routing table, which gives the immediate destination. The question of when and how these routing tables are constructed is even broader. IP packet routing is fairly complex, and the key to understanding it is to know that virtually every router does not know exactly where the packet is going. It just knows that that router over there knows better than it does so send the packet over to them. You could use the analogy of following a concentration gradient for a pheromone. First: That's why they are called routers - they route. They take the responsibility of passing along your packets, your packet doesn't know the route, only the destination But seriously, you need to read about networking and TCP/IP and this answer is not scientific and fully correct. (0. MAC addresses are network local and depend on everything being Ethernet anyway. The high-capacity links most certainly aren't Ethernet, but instead use different protocols over optical fiber.)",2
"At the time of writing the latest BlueZ release was v5.50 back in June 2018 whilst v5.43-2 is available in the Stretch repositories. Installing the bluez package should have installed 19 or so man pages along with the usual Debian README.Debian.gz file, you can see the list of files installed by the package on it's page on the Debian Packages site.  You can also view the man pages online at the Debian Manpages site. Unfortunately the btmgmt tool you're interested in does not have a man page but it does offer some built-in help with btmgmt <command> --help. As you've noted, the documentation in the supplied man pages is quite sparse and (to me, anyway) feels like there's a missing ""overview"" document somewhere that should be read first. You may have more luck going straight to the ""source"" and looking through some of the files in the code repository at https://git.kernel.org/pub/scm/bluetooth/bluez.git/tree, the ""doc"" directory contains several text files but from a cursory look they seem to be more about the kernel API than using the userland tools as an end-user.",1
"I just want Windows to use the drive as simple plain old storage without cluttering it up with  its internal stuff and metedata - and as far as I know, removable drive is the way to go. I want to connect my external 2TB USB HDD that has 4 partitions, to be treated as a set of  removable drives on Windows. This is because I do not want Windows to create ""System Volume Information"" and ""$Recycler"" ans stuff on it ( these are not created for removable drives right? ) Every time I try to disconnect the thing, I'm told the device is busy. I end up unplygging the darn thing. Explorer then goes bananas, and eats up system ram until something crashes. I've searched around for a while on the web, and most points to hardware or partition issues. If you want to support Linux boxes, the only really good filesystem that works between the systems is FAT32. Linux can read and write to NTFS, but the Linux NTFS driver runs as a FUSE module, and performance is comparatively poor. exFAT would be the best option, because FAT32 doesn't allow filesizes greater than 4 GB, but AFAIK Linux and friends do not know how to read it (and neither do versions of Windows earlier than Windows 7) (P.S. It worked in WinXP but I haven't tested them on Windows 7/8. Try and tell us if it works or not.)",4
"Then use it to build out the rest of the network in a small office server cabinet. I want to make sure that I can get all the 10gbe ports on the same network though and it appears that port 51 and 52 are only valid as uplink ports.  Does anyone have experience with this switch and can I configure those ports to just be normal 10gbe ports in Ethernet mode? I was looking to link up a NAS and a Server via this switch http://support.netgear.com/app/products/model/a_id/19324 There's nothing inherently special about an uplink port.  You can use those ports to connect to switches or to individual servers (or NAS appliances).  The ports can be configured to support VLAN tagging - or, in your case, can be untagged ports in the same VLAN as the 48 GE ports. Ports 51 and 52 are configurable as either stack ports or uplink (read: normal) ports.  The stacking mode is just a proprietary mechanism to make up to 6 of these switches show up as a single device.  If you buy more of these switches this might be a handy way of growing your network.  If you don't, they're just ports.",2
"I'd rethink things a bit. At any point the state contains (among other things) the PC's target location and the PC's target item. If the player interrupts the action by clicking elsewhere, the old actions for that character would be removed (or you could block the user from performing more actions until the current one has finished). Then your game loop will include a move() which checks whether the PC can move towards the target location and if so does, and an interact() which interacts with objects near the PC. Among the interactions are items checking ""Am I the target item and within a certain distance of the PC?"" If so, they call a pick-me-up routine. In your walkToAndPickUp example, I would enqueue an action object containing the character and the target location. Each frame I would move the character closer to the target location. On the frame that the location is reached, pick up the item and finish the action by removing it from the list. Look at animation systems for inspiration. Don't spawn a thread for every item that needs animating! Consider what happens if the user clicks on a different item while your character is still moving towards another item. Then you'll have two threads simultaneously trying to change the same object. All characters, NPCs, projectiles, basically anything in the game, should use this system to update and animate their actions. To make things simpler, you can compose complex actions out of primitive actions, using an action sequence object. For example, you could make your first example by combining a walkTo action and a pickUp action. Your other example could be combining a wait action followed by an attack action. Then you can easily make new actions, like walkToThenAttack or waitThenWalkTo, without duplicating lots of code. Instead, put actions in a list. Each frame you update each action in the list. When an action is finished or cancelled remove it from the list.",2
"I've already tried all the ""standard"" VirtualBox procedure (PAE yes/no; SATA/IDE; no soundcard; no USB; graphics accel. yes/no). If I choose the Debug mode, I see some messages and a hint to type exit to boot, but still no success I also tried run the Live CD in VESA mode: like this it starts, but I'd still prefer to run it from the HDD. If I start the VM with the first option I get error Failed to find cpu0 device node. I found out that it's just a non-blocking warning and it's unrelated with the real blocking-issue being that ""uvesa (v86d) is broken"" (the issue was related to Android 4.4: is it still valid on 6.0?) I don't know if this helps for your case but for me it helped, that I uninstalled the current Version of VirtualBox, installed the older 5.2.0, then I could install Android and it didn't hung at first startup this time. After it was installed and booted for the first time I could update my VirtualBox (again) to the newest Version and Android booted fine like under 5.2.0. It was only the first startup - with configuration - as it seems where you need the older VirtualBox-Version.",2
"If the answers are ""no"" and ""yes"", the way they are in most organizations, then the answer is clear. No, they shouldn't be responsible for the mail system, unless it ties directly in with the application they work on. But there should be a clear run-book of things to quickly reason about so they can either gather information for the sysadmin, or even solve themselves if simple enough. For development, if possible supply them with a copy of the live system and provide a way to push database changes back to the development copy at will.  You will find that many of your developers will be more productive with access to ""real"" data, and you will feel safe knowing that they don't have access to the live system.   From a DBA standpoint, access to production systems for users should be strictly limited, and this includes developers.  Users should only be able to see the data they need and this would ideally be provided via views, not direct access to tables. Many companies have one sysadmin who is on call 24/7, even for application issues. Some companies are fortunate and have two. It is completely unreasonable to expect that one person do all the day-to-day operations work and respond to system alerts at all hours of the night. This doesn't just protect the person (can't be accused of overstepping his/her bounds), but also protects the organization, should the user's account be compromised. However, many companies have a fair number of developers that are responsible for the application code running on the systems. Many of them may even have experience doing operations. Apply the Principle of Least Privilege. If they need access to do the job, you give them the access. But you only give the access they need. If they don't need access to do the job, you don't give it to them. Developers, likewise, should be able to see the data in production, but not alter any tables.  Any changes should be done in test (or dev) first, then scripted and ran on production by the DBA. I am also of the belief that there is no real reason to store certain sensitive data beyond the transaction.  For example, credit card transactions... grab the data and send it to the merchant processing system... but don't save it in the database, instead just store the last 4 digits of the card and the transaction id from the merchant system.  This way when an exploit is found and a fix generated (I know, like that never happens), you have the confidence that no one could have stolen any of this data from your database.",5
"Since you refer to ""replication"" in a very general manner I'm guessing that you probably haven't fully researched the replication functionality in SQL Server 2005. I'm doing POC on database replication, and I'm stuck with question from my boss. Is database locked when it's in replication process on SQL Server 2005? Exactly what happens during replication depends on what kind of replication you're doing - however the Host database should not be locked.  In a log shipping situation the server being log shipped TO will be locked during the restore, no matter what, however you can set it to be read only and accessible when not restoring.  With mirroring you should be able to run queries (selects) against your mirrored (secondary) server at any time. There are different ""flavors"" of replication technologies in SQL Server (publisher / subscriber, database mirroring, etc). Depending on what you're trying to do you may opt to choose one replication strategy over another (scalability versus availability, for example). Your replication latency requirements, throughput needs, and desired overhead are all going to make differences in what you choose. 'Locking' inside a sql server is a normal part of its operation.  Read this for a crash course http://www.sqlteam.com/article/introduction-to-locking-in-sql-server",4
"The other way would be to 'show formulas' but that would show formulas for every cell, not just a selected range. I know you said non-VBA preferred, but if you end up with no other solutions, the Range object has a HasFormula property. This brought out all the formulas, but obviously would also convert things like if $C$1 contained =if($A1=$B1,""Match"",""Different"") to '=if($A1'=$B1,""Match"",""Different"") Notice the A1'=B1 in the middle of the formula which may be problematic, but still means you get to see the formulas, albeit non-pragmatically. Could use the SUBSTITUTE function to change it back, so $D$1 contains =SUBSTITUTE(C1,""'="",""=""), then just copy out to Notepad and paste back into $E$1 Returns: True if all cells in the range contain formulas; False if none of the cells in the range contains a formula; null otherwise. Then specify the type of cells you want and click OK and only those types of cells will remain selected. Well past the original date, but if it's of use to anyone, I managed to get around this by selecting the range of cells, then using Replace (Ctrl+H), setting it to 'Look in' ""Formulas"" and replace an equals = with an apostrophe-equals '= Use Conditional formatting. Choose the ""Use a formula to determine which cells to format"" option and type "" =NOT(Cellhasformula) "". This will format anything that is hardcoded.",4
"That way all users on the system will benefit from any bash completion scripts installed in the directory ""/opt/local/etc/bash_completion.d/"". For Mountain Lion and Mac-Ports I needed to use the following lines in my .bash_profile to get both __git_ps1 and completions to run normally: While the answer above by grundprinzip from 2011 still works fine, I wanted to add a more recent answer now that there's now a Homebrew package bash-completion (formula, repo) that makes this even easier.  (This includes completion for git as well as other common tools.) All you need to do is place the git-completion.bash file in your user home bin directory and place the following in you .profile or .bash_profile file: So, no MacPort install to then install a the ""completion"" version of GIT (especially irritating if you already have it installed). While the current answer is correct for older MacPorts there is a new issue with the most current MacPorts resulting in the same error. Today macOS ships with Bash 3, but you can brew install bash to get Bash 4.  If you're running Bash 4, then install the newer 2.x branch of bash-completion. The current bash-completion package (2.0 r1) requires at least a bash 4.1 to function properly. As a usual OS X Terminal uses /bin/bash it only gets a 3.x. You have to change you terminal settings to use /opt/local/bin/bash -l as described in the MacPorts documentation here: http://trac.macports.org/wiki/howto/bash-completion What this does is make sure that your local bin is in the PATH and the source command gets things going. Then of course the PS1 change puts the currently checked out branch in the prompt.",5
"Example: Google recommends using just // instead of http:// or https:// as it will default to whatever is being used on the page.  You need to tell the browser what protocol you want it to speak to the server. Most browsers support multiple protocols in the main navigation area, such as https and ftp, which use port 443 and 21/22 respectively. You got that the other way round. If you type http://, then the request goes to port 80 unless overridden (e.g. http://www.yourserver.blah:8080/...). When we make a request a page from a server, the request by default goes to port 80 on the server that is the http server. Why do we then type http before the path of a resource. If the request is going to a http server it must be a http request. But if for example you typed https://..., then the request would go - again, unless overridden - to port 443/tcp and would even ""speak"" a different ""dialect"". The same goes for ftp:// and port 21, if the browser understands the FTP protocol (as most do). Also keep in mind, that the usage of a URL is for the user. When you specify http:// it tells the user to create an HTTP request to the address. Your request of the browser for http://www.example.com is then translated to the HTTP protocol. This article gives a pretty good description: Most browsers do consider the lack of a http:// qualifier in a Web address as meaning ""this goes to HTTP port 80/tcp"", and they automatically add the http:// for you. Otherwise, you have to specify, and the port chosen will follow the specification. It is also useful not to think of the ""server"" as a box, but as ""service"" running on a specific port. When the client makes the request, it doesn't know who will be receiving it. While the standard is 80, there is no reason that cannot be an SSH server. Imagine a scenario where someone is behind a firewall and wants to SSH to their home box, if they run the SSH server on port 80, they may be able to get past basic firewall rules that allow you to open a connection to port 80. There are defaults, but they are not requirements.",5
"All of these are free, and some are more integrated than others. Zenoss has the benefit of tying together a lot of things out of the box, and the option of enterprise level support. Being based on Zope, it's a little more resource intensive to run than the others, but very easy to hack on if you know Python. Zenoss does everything you want out of the box and can work over either SSH or SNMP. I've also previously used Zabbix as a full on monitoring system, and previous to that Cacti for trends and Nagios for alerting.  There are various methods for collecting information from machines being monitored. Whichever monitoring solution you choose, I recommend collecting information from each system directly by SNMP. If you're unfamiliar, it'll take a small bit of learning. In the end however, it's the standard solution that Just Works. What's your suggested tool to monitor multiple Unix (Linux and OSX specifically) based systems at the same time?  I need to monitor the utilization of the CPU, memory, and disks in real time and would prefer a single tool to do so. The key for monitoring multiple linux/unix hosts with nagios is creating a tarball that can sit on all of them.  Spend a little time on the front end, and your life will be a breeze later on.  Just unpack it annd you are good to go. Nagios is free, popular and open source. There are a lot of monitoring plugins availble (for different devices and services). Unless you use a separate GUI, configuration is by text file. It sends alerts notification e-mails, which is how my organisation traps and responds to system problems (alerts into a ticket system). If you're willing to set up SNMP, MRTG is quite good. It's quite complicated, unfortunately so requires a certain amount of work to setup. If you're not looking for trend graphing specifically, nagios is reasonably good, as recommended earlier. It's also got several plugins which allow it to do trend graphing (Nagiosgraph is what we use.). Munin is ok, but its graphs are in my experience more than a little overcomplicated. Bigbrother can do monitoring, but I'd avoid it if at all possible. It's more than a little bit of a train wreck.",5
"CEO/CIO/CFO: ""The email integration with our ERP/CRM system isn't working anymore. We can't communicate with our customers. We're losing money by the second!"" I am just reorganising my companies active directory structure, as it hasn't kept up with changes in the company structure. There are a few items I am not sure if I can move or not. Can anyone tell if it is ok to move the below or not: Cleaning up and maintaining the AD structure is one thing but don't get wrapped up in ""busy work"". Organize the OU's and objects that you/the company has created. Leave all of the Built-in and default containers and objects as they are. While there may not be any issue with moving some of these items the fact of the matter is there's just no good reason to do so (or very, very few good reasons to do so). From what I have heard, the Exchange Security Groups OU can be moved in it's entirety as it has a well known GUID and Distinguished name. I am not so sure about the accounts for system mailboxes, although I can't think of any reason why not. All the builtin accounts I assume are fine to move.",2
"Also note it's the GPU's capability, not the ports on the card, that ultimately decides how many displays you can use. Some cards may have 3 outputs, but you may find that you can only use 2 of them at a time. I currently own a 27"" iMac and 2 24"" monitors. This configuration usually runs in a triple monitor config with the 2 monitors plugged into the iMac's 2 thunderbolt ports. Better check the specs of the GPU on nvidia/amd's website to make sure how many displays are supported and the max resolution for each.  My question is if I buy a new graphics card with 2 DVI ports and 1 display port, will it allow me to run the Ubuntu desktop with all 3 screens? If so, can anyone advise me where I can buy such a graphics card? I'm sure there are good places to buy such things cheaply. Right now I'm going through each card since I don't know a site that searches specs. To confirm, the purpose of the card is purely for the triple screen config, not for gaming, etc. so hopefully I don't need to buy a really recent card. I have another desktop which runs Ubuntu and I want to hook it up to use all 3 monitors. The current graphics card on it is 9800GT which has 2 DVI ports and no display port so I'm able to connect the 2 smaller monitors but not the iMac. As I understand it, the iMac screen can only be used as an external monitor through a DisplayPort output. Display port is sort of a multi-purpose port... you can use an adapter to convert it to a DVI port quite easily.  Keep in mind, as it's a mac, it's more likely that it's a mini-display port... not the full-sized port... but there are still adapters.  It should work fine as long as you get the correct adapter.  Alternatively, HDMI ports are becoming more common as a 3rd port on many video cards... which also have an adapter to convert to DVI.",3
"If the racks are deep enough, perhaps consider ""0 U"" vertical PDUs mounted at the back of the cabinet. I'm building a new server room, and have to decide where I'll be locating the power outlets (120V 30AMP locking connectors) that my UPSs will be powered from. In the past I've put the outlets on the back wall, and just run the cords up and over the ladder racking on to the back of the wall. But for this new server room I can't locate the outlets on the back wall. Instead I have to locate them on the racks themselves. I'd always thought that I'd have the electrician run conduit and put the outlets on the top of the racks, but he suggested putting them on the bottom of the rack at the back right at floor level. I see the advantage of this as not having to run power cables up through my cable managers, and thus interfering with any network cable, but the disadvantage is that I'll have long coiled-up 30A power cables jutting out from the back of the racks. Any thoughts? What have you done, and would you do it differently? In our case, we had a raised floor with the racks in the middle of the room and a 6-foot built-in table right next to the racks, so our electrician ran our lines in raceways along the back of the table. On 2-post racks: I like rack-mounted horizontal power strips staggered throughout the rack. The same is true for the UPSs and electrical receptacles-- they're at the bottom and feed the power strips in the rack. I coil the power cords on one side of the rack in the vertical cable management (and network cables on the other size). Increasing, though, I'm using Neatpatch cable management and have less and less need for vertical cable management. My preference is definitely for vertical but I've also never used anything other than 4 post racks. I also like to get the full length ones. Horizontal outlets always seem to get in the way when you need to make changes to the rack layout, resulting in more work than was planned. Any amount of raised floor helps as well since the electrical conduit can come up under the rack and the PDUs (and slack cord) can be done under a floor tile but it doesn't sound like you have this option. If you're talking about 4-post cabinets: I like vertical PDUs inside the sides of the rack. I place them half-way up the height of the rack and coil excess power cords in the side panels. Normally the UPSs are at the bottom of the rack (where they really should be), plugged into mains outlets that are either under the floor or at floor level. The UPSs feed the PDUs, and the PDUs feed the servers and other equipment. Something like this: http://pc.pcconnection.com/1/1/7992-apc-power-distribution-unit-metered-rm-0u-30a-120v-24-5-20-outlets-ap7832.html If none of that is an option, bottom of the rack would be preferable in my opinion to power at the top of the rack. The way your question sounds, I'd get the electrician to put your outlets right at floor level roughly in the middle of the rack footprint, put the UPSes a bit above that so that you have room to coil the cords for the UPSes on the ground.  Then go with what Evan suggested and mount PDUs in the middle (vertically) of the rack. With enough space in the back of the rack, these don't interfere too much with data cable management.",5
"Will I need to use dd to copy the root partition / and /boot to the disk before copying all the other files manually? BE AWARE: You need to substitute (hd0,0) with the appropiate disk and partition for your kernel images I am really just wanting to berid of the multiple partitions so I can use LVM instead, I have no clue how to transfer it the ""proper"" way though. Also substitute hdX with the drive as declared in your device.map, so if you have your drive declared as (hd1) you need to make it (hd1,X) In order to boot from the new disk as you did from the old one you just need to reinstall grub in the new boot sector (I'm assuming you're using grub here). For RedHat based Linux distros (CentOS, RHEL, Fedora) you can do this by executing as root grub-install /dev/hdX, just telling where your new hard drive is. I have a disk with separate partitions for each folder (/usr, /var, /home etc.) and wish to transfer the whole thing to another disk that is slightly larger, now I do not wish to keep the partition scheme, so should I mount all partitions (so they populate the proper folder locations) and then rsync, or cp -avx the whole partiton (of course under runlevel 1) to the new drive? Once you know that the new drive is in device.map from the OS booted from the old drive execute the following commands Normally I do this kind of transfers by rsync'ing between the two hard drives, so your hunch about either rsync or cp -avx looks more than right to me.",2
"Part of the ""teach OOP"" problem is that OOP (and modularity, and top-down design, and variable-naming discipline, and consistent code layout, and...) is practically useful for large programs with a long life, and in the time allotted you write tiny throwaway programs. On the other hand, if you are looking for a more practical use-case, then you can use lists. There is the Node, which in its simplest form is nothing more than a data encapsulation, and the List which implements the process of accessing the nodes. The List and Node can both be inherited as you expand from array-based lists to linked lists, and into doubly linked lists. Then you can switch to stacks and queues, maybe even moving into binary, and n-ary, trees. With inheritance, we eliminate the need for all these ifs, and allow future programmers to add their own saving method: I'm not sure why you don't like the person/employee/manager hierarchy; it is just an example for a whole class of object hierarchies which represent specializations of data with a common subset and are often stored in databases. Other examples would be non-human inventory (every physical item a company owns has a value and a location etc.), or financial assets (they all have some proof of ownership, a value, a date of acquisition etc.).  BlockDiamondOre checks harvest and fortune, putting a different number of diamond items into the list, even determining how many based on the Random supplied.  Now, if new developers want to add their own way of saving, they can write a new class the implements Saver, and just pass it to the SaveTheGame method. 1Well, except Minecraft Forge, who maintain the official unofficial mod API. But Forge is a special kind of magic that isn't important here The chest let's the player store items in it. The furnace smelts some blocks into other blocks. Grass, when broken, drops a completely different block. Stone doesn't drop anything at all unless dug out with a pickaxe. The same goes for items, some of which (like sticks) don't even have a derived class of their own because they do nothing special. They just exist and do everything any item can. If you haven't, I suggest taking a few minutes to find some introductory ""Lets Play"" videos on YouTube first.  Imagine you were making a super-awesome video game, and you needed to write a function that saves the game. There are roughly 100 different methods in the block class to handle all of the possible functionality. Update ticks (crops), neighbor updates (observer, torches), interaction (doors, chest, furnace, crafting bench), whether or not a block has a tile entity (chest, furnace), and on and on and on. Most of them are blank and do nothing, entirely up to the derived class to add to, if it needs it. I'm not as familiar with Python as I am with other languages, but I'm sure your students have played Minecraft. This is why the are so many mods for Minecraft that add so many new things: no one1 has to implement anything special in order for everything to Just Work.  Perhaps a way around that is reading (good!), hopefully modifying/extending, programs written in OOP style. Open source is a godsend... If the students are still needing real ""objects"" to connect metally with the concept of object you could resort to using transportation as a system. Another user, G. Ann - SonarSource Team, gave a good break down you could follow in answer to another question. What if youre writing code for other developers, and you want to allow them to add their very own ways of saving? All blocks can be broken, picked up, placed, stacked, stored in chests, and crafted together. But not all blocks do just those things.  If you are looking for something which can be implemented and played with in a class I have two suggestions:",5
"What I was planning on doing was having the server/client perform some kind of handshake when the client first attempts to join a game session. The server would store the server-side Player object and it's associated EndPoint(s) in a collection. This way, when Player A does something that affects Player B, I can update the two player objects on the server-side and send both players updated info. I've been building a UDP server and have hit a bit of a stumbling point. Since UDP is not a persisted connection, there really isn't a way for me to track if the Player has left the current session/closed the game etc. What happens if Player A closes the game? I need to clean up my collection of Player/Endpoint(s). I'm not sure how that is typically handled. Should I have a timer that runs every minute or two, pinging clients and removing them if no ping comes back? Since It's UDP, I can't assume that ""no new datagrams"" really means the player isn't playing. It's possible that they've just not needed to send any data for a while (maybe it's paused or something idk). I have a low-overhead async timer API that runs on background threads that I could use, but it feels like a lot of overhead to ping every connected client every couple of minutes.",1
"If you are not using a Volume Licence Key to be legal you will need to enter in a new key for each machine after you load the image. (at the bottom of the 2nd link there is a command line version so you could script it to make the action of loading the image faster.) Sysprep can be found at c:\windows\system32\sysprep\sysprep.exe and you can either choose these options from the GUI or specify them on the command line. OOBE: Will ask for computer name, reset SSIDs, initial admin user creation, and a new license key for activation. If the computers are all a single brand and came with Windows 7 installed already, you will most likely not have to enter a license key because it will be pulled in from the BIOS/SLIC. This is the case atleast with imaging Dell machines that had OEM Windows 7 installed at the factory. Generalize: Will make changes to the HAL to accommodate any driver/hardware differences between the machines. Before you capture your image, run sysprep. I prefer the OOBE, Generalize, and Shutdown options. These will prepare your image to handle some operations on firstboot. I am planning to make a Windows installation in some laptops. I ve planned to make installations, new settings etc in one laptop, take an imageof the disk and mount the image to the other machines. The questions is, what can I do if Windows are already activated? It's a bit illegal I think. Any ideas?",3
"Being Multicore aware means that the DL580s DO see all the appropriate cores.  Further, if you add in hyperthreading for some newer CPUs, you will see, for example, a single 4 core hyperthreaded cpu will appear to the OS as EIGHT processors.  Task manager correctly displays these. Server 2003 and 2008 are BOTH multicore aware.  And 2008 R2 will increase the maximum supported cores to 256.   Keep in mind the access to the CPU is through the kernel - you CANNOT ""use cores behind the back"" of Windows.   As for a specific third party utility... I know of no such utility and would see no point to one so I have strong doubts one would exist or be created.  In any case, such a utility would have to rely on the OS, in which case Task Manager does the trick. If you need a programmatic way, there is the environment variable NUMBER_OF_PROCESSORS or there are the WMI interfaces. But as Multiverse said, you can't use the processor without going through the OS, and Task Manager will tell you the truth. Not sure why you are confused.  Microsoft has clearly stated since the release of multicore processors that they were licensing PER SOCKET, not per core (Oracle, is (was?) licensing per core, for example).",2
"However if that was that case you would probably either know the answer to, or how to find the answer to your own question. Is it possible to know the gateway address of a wireless AP that I'm not connected to through sniffing? Airodump-ng can show MAC addresses of the AP and the clients associated but doesn't show IP addresses. If the network is open (unencrypted) then yes; you can capture the full packets of connected clients and see the addresses in IP and ARP packets. Despite the fact I've only just joined the site yesterday - I have to say coming on here and asking this without putting anything in doesn't look impressive - if you had a heap of questions/answers relating to network security then I could believe you with to increase your knowledge to help protect your own network by attempting penetration testing. If the network is encrypted, then no; all of the layer 3 communication is encapsulated in encrypted packets at all times, and the MAC addresses on the transmissions is all you can get without breaking the encryption.",3
"The AJAX console is launched from a link on the cloud provider's website which requires a login.  Exactly how is this AJAX console considered to be out-of-band when it obviously is not a form of physical access to the server? Out-of-band is probably used in the wrong context here, but nevertheless I'm sure is intended to imply the console is similar to or emulates a typical local console. Most virtualization hypervisors (Xen, KVM, etc.) offer this by way of connecting to an IP that is attached to dom0, thus allowing you to connect to your virtualized node ""out-of-band"" -- meaning even if the network interface on your instance is down, you can still get access. Obviously this isn't true out-of-band by the traditional definition, but still fits the description of ""system console access provided, even in the event of primary network subsystem (hard and/or software) failure"" when speaking in a virtualized context.  The first time I access my server with a new installation of Filezilla or Putty, I will get prompted that I should continue only if the RSA key shown to me is correct.  The cloud provider has advice on their website that I ought to use their AJAX console to get a key out-of-band with which to compare to the one shown by Filezilla.",2
"From my experience, I believe most dedicated and VPS providers will not set up special firewall rules just for your server.  But nowadays, you have a few options. This is a solution I developed which applies to non-web servers that cannot hide behind a CDN, such as WebSocket, media content/streaming servers.  CloudFlare supports WebSocket but only for enterprise at the moment. When you are under DDoS attack your ISP it can help you most, but if they don't have DDoS protection it's very likely that you will be out of service until the attack stop. Usually they will see the attacked ip address and null the network on  their upstream router. If you don't have a lot of traffic there are many online services for DDoS protection where your traffic is rerouted, filtered and sent back to your server. Next, do you have a firewall in place? If yes, did you renew the subscription? Make sure you enable the IPS intrusion feature in the firewall. Just by renewing the firewall subscription solved our DDOS attack.  If you're an enterprise, you have many options.  If you're a little guy like me, renting a VPS or a dedicated server to serve a small website, cost can quickly become prohibitive. First, plug out the network cable from your server. Now, check your server services are back to normal behavior by looking at the performance monitor and task manager. If not, scan your server with malwarebytes software to make sure your server is cleaned. This step will usually ensure your disconnected server back to normal again.  Speak to your server provide and see if they can issue you with another IP address as a temporary way to access the server or see if the server has remote console access (like you are sitting in front of it).  From here you can see if it is a single IP address and block it from the site or a distributed attack. The goal is to change your TCP listening port quickly enough that a botnet can't keep up, say once every 10 seconds.  This is accomplished using a simple proxy program that performs the port roaming.  The sequence of ports is pseudo-random, but must be based on the server time.  And the algorithm for calculating the server time & port must be hidden in your client javascript/flash code.  The program should also modify the firewall as it changes listening port, and the firewall needs to be stateful.  If someone is interested, I will upload my node.js script that works with Amazon, to GitHub. CDNs are expensive.  To keep cost under control, serve large files (large images, audio, video) directly from your server instead of through the CDN.  However, this can expose your server IP address to attackers. Note an attacker can still attack your TCP server port.  If it is a web server then consider using something like nginx that uses non-blocking IO and can handle large number of connections.  Beyond that there is not much you can do beside ensuring you run the latest version of the server software. If you're running a web server, consider putting it behind a CDN like CloudFlare or Amazon CloudFront. We learnt that we should renew security subscription (eg. firwall or anti virus) and not take it lightly. DDOS attack is happening everyday and can happen to small business too. Hope this helps.  Private clouds usually are expensive enterprise solutions, but Amazon VPC costs next to nothing to set up.  However, Amazon's bandwidth in general is expensive.  If you can afford that, then you can setup Amazon VPC's Security Group and Network ACL to block traffic before it arrives at your instance.  You should block all ports except your TCP server port.",4
"To my knowledge, this can actually be a limitation in the hardware, so you might have to get a new keyboard. Does it work correctly in other computers and/or with other operating systems? This is a hardware limitation, as the other answers suggest.  I used to have the exact same problem gaming with an older logitech keyboard that had a 3-key rollover.  Cheap keyboards frequently have this problem. Almost all consumer keyboards have combinations that don't work. It is unfortunate, but you just have to find the dead combinations and avoid using those keys for gaming. If you want to ensure that you never have to deal with this issue again, I suggest purchasing a das keyboard, which has N-key rollover.  In other words, you could be pressing every key on the keyboard at once, and all of the keystrokes would still register. For anyone still having this problem, I've run into a similar issue recently, and I found out something interesting. How many keys I could push was actually affected by the USB port I was using. I found this out after moving my computer, and apparently plugging it into a different port than normal, where two button combinations that normally worked no longer did. After switching to a different port, the problem was resolved. However, even cheapo keyboards typically support 3-key rollover in which case you should not be able to hit a jamming combination with just two keys. If that's what's happening you have either a hardware fault or a spectacularly rubbish keyboard.",4
"Another thing that may be destructive in a game like you specifically described is if someone creates an AI bot that listens to the packets and computes an optimal decision based on the state of the game. This could be impossible to detect. The important message to take here is that replays can be used for cheat detection sometimes and not protection. Even so, it may be hard or impossible to detect some types of cheats. It also begs the question what is considered plain cheating and what would be taking full advantage of tools that most players may not consider..  On the other hand, there is a competition going online in a deterministic single player game supposedly protected from cheating but player A again records her keyboard input until she loses, then she plays back her keyboard input and continues from the spot before she lost. She then repeats this process to allow her to basically save her state.  Can anyone tell she is cheating? Yes but it will require someone to check and compare different playthroughs, even then it would be hard. In a strategy game with keyboard shortcuts, player A creates some advanced keyboard macros that allow her to build up a base faster than any human player using ordinary controls. Is that cheating? Perhaps. But who can tell the difference? Perhaps a machine doing some statistics. Is the implementation here worth it? Generally I think the conclusion is that when you are working to detect or prevent cheats (like any other security related matter), whatever technique you are using, is being used on people and they may outsmart your or simply pick another way you haven't thought of to cheat the system. Human and especially cheaters, are very resourceful.",1
"Uninstall chrome and remove the chrome folder from ~/Library/Application Support/Google/Chrome/ and install again. As soon as I saw ""Keychain"" in the above comment it was clear ...went to keychain ...typed the root word for the site with the incorrect and unsaved updated passwords ...opened the keychain for that site -erased incorrect password and replaced it with correct one ...clicked ""save changes"" -tried logging in to site ...new password was there. The desktop is synced with icloud in latest osx update. If your browser icon isn't on the desktop it is going to adversely effect the browser. Open finder and drag the browser icon to the desktop and leave it there. Browser should be fine now. Chrome does prompt and offer to save passwords. However every time I click Save, it doesn't actually save. The Manage passwords - Saved passwords is an empty list. All within Mac OS and no need to get into anything potentially messy such as terminal or re-installing Google Chrome xoxo :)  With an hour of search and trial it got fixed. Initially I thought it was an OSX issue. Because I recently had an El Capitan upgrade and it gave me a hard time with KeyChain. I have other devices, iOS and OSX, none of which has any problem of password syncing. Shouldn't have stopped there. It turned out to be a problem of user profile after all. The right place to edit is actually here. Clicking the X deletes the current user and apps. No worries, signing-in again reinstall them back in a blink. All my saved passwords in account got synced as well. Happy autofilling ever after.",4
"You didn't have access permissions to create the necessary objects in AD. What account were you logged in with? Have you confirmed that your Active Directory is actually working? Can you join other computers to the domain? Have you tried running DCDiag and Netdiag to validate the configuration?  Can we get some more information on this? Below are some questions that came up for me after reading our post.  Struggling to get MSMQ installed in Domain Integration mode on Windows 2012 (Azure). So far, I've provisioned a brand new Windows Server 2012 (R2) machine on the Azure platform and installed the Active Directory role and promoted the machine to a domain controller. There are also some specific things you have to do to get MSMQ working in this mode on an actual domain controller. Here is an example for 2008 on what needs to be done.  Once the AD was in place, I then added the MSMQ feature, along with the Directory Integration add on. I had similar problem, able to resolve after adding the account to delegate control to create objects at Active Directory Users and Groups -> Right Click on your DOMAIN -> Delegate Control Did you replace 'DOMAIN' to omit your domain name or did you really try and create a domain named DOMAIN? That seems like a generic message and might not be the issue.",4
"This looks pretty straight forward, if the distance is less than 20, return a bool. However, if we follow the Single Responsibility Principal, this function is actually doing 3 thing. Calculation, determination and returning a result. Let's split it up: See how easy this code is to read? This is the sort of thing you can do with Python (or all code). Obviously this is only a small part of your code and you have a lot more to change, but I can already see some code duplication right below the if statement we just improved: I also see a lot of comments which state the obvious, for example, above ""set up screen"", and below:  As we've updated that, we can now go back to the large if statement. Given a collision, we update the pong, so let's extract that into a separate function: Okay, thanks for fixing up the code. Now, to make it more pythonic, we need to move all the statements (not functions) into the entry point, which is a  As we're trying to keep the variables in the main loop the same, we need to pass them into the function and get them out again after the state has been modified. I don't like the name ""change_pong_direction"", but for the moment it will do. You don't need comments in your code if you've written code which expresses what you're doing. The comment # check for bounce and redirect it is useful, as the code doesn't explain in clear english what it does (so :thumbsup: there). Now we have a calculation function that only performs a calculation, and the isCollision function returning True or False. I also renamed your function into snake_case, this is preferred when using Python. Here we see four things - first is the check for collision, next is a modification to the x and y, then we update the score, and write something to the display. etc. The code still runs as expected, and now we can look at what needs improving with the code. My IDE says you have 3 imports which are unused. I've ripped those out. See how the string has been embedded into the function? This (might) make it easier to introduce a ""display"" interface later on, if the score_pen changes to something else, you can easily inject the message into a different function. I've actually added another variable as part of this function, just to make it a little clearer. Of course you can improve it, but it looks like there might be an opportunity to adjust the 10 at a later stage. Such as for games that last longer than a minute, to increase the per-score points. Anyway, continuing on... we need to now write the score to the display: Now we get a little deeper, and see all the if statements. A famous programmer said (in many ways) ""every if statement is a function waiting to happen"". Let's look at one chunk of your code: Let's look at the scoring function now. As mentioned above, we are actually doing several things. Incrementing the score, and changing the display. Let's separate those commands into functions.",1
"Note, though, that for --add-drop-database to work, you need to specify the database(s) to back up using either --all-databases or the --databases YOUR_DB_NAME command line option to mysqldump.  You ""can"" make a backup of a single database with just the database name on the command line by itself, without the --databases option preceding it... that also sort of works but is really not a good way to go, unless you intend to restore the backup into a different database name than the one you got it from, which, thanks to the way that option works, you might do even if it wasn't your plan. Using --databases YOUR_DB_NAME or --all-databases automatically adds CREATE DATABASE and USE your_db_name; statements to the top of each schema in the file.  This means you don't have to specify it when you're restoring, and if you forget or get it wrong, the data still goes to the right place -- the ""right"" place being the place the file says it should go. The --add-drop-database option adds the DROP statement to clear the way for the new one, immediately before the CREATE and USE statements. For the benefit of anyone unfamiliar with magic comments, this isn't a comment.  It's a statement that will be executed by MySQL 4.00.00 and higher, disguised as a comment for backwards-compatibility.",1
"Side-Note:  If you are using symlinks, you should make sure both the linked directory and the actual symlink itself have the appropriate ownership / permissions.  To target a symlink with chmod / chown, use the -h flag and, if targeting a directory, make sure the trailing slash is not present (eg. /var/www/html/linked_dir and not /var/www/html/linked_dir/ - be careful if using tab-completion as it will automatically add the trailing slash) Note: WordPress usually contains a hidden '.htaccess' file, you will need to manually apply the chmod to that, and any other hidden file, as recursive chmod / chown will skip them. SELinux contexts - These you can check with ls -Z.  To turn off SELinux enforcement you can run setenforce 0 - This is good for debugging whether SELinux is the culprit or not, however you should turn it back on when done testing using setenforce 1.  If SELinux is the problem, then likely what you'll need to do is change the context of the files with something along the lines of chcon -R -t httpd_sys_rw_content_t directoryname  Permissions - They're listed as rwxrwxrwx.  the middle set of rwx is the important part as it controls access via group (so is what matters for myuser).  You will probably only need read/write permissions so a chmod -R 664 directoryname would probably suffice (read/write for apache user and group, read-only for everyone else).  You can check the permissions with ls -l.   Ownership - Sounds like you've got this right.  Owner:Group on CentOS7 should be apache:apache.  For your user you will have to add apache as a supplementary group.  You can check this with id myuser, if apache does not show up in the list of groups, then run usermod -a -G apache myuser.",1
"WAMP seems a bit overkill just to host some files... Windows comes with its own HTTP server,  IIS. You can install it using ""programs and features"", ""turn windows features on or off""; there you will see ""Internet Information Services"".  If you install Apache by itself on windows, you dont need php or mysql....In my experience apache has been the best thing to host your own files.  I would like to access a folder of mp3 files on my local Windows machine through http:// addresses. For example, typing http://localhost:9999/songs/test.mp3 into my browser would play test.mp3, which sits in a specified folder on my C: drive. What is the very simplest way to do this? (Background: a program I'm using wants me to enter the URLs of these files, but assumes they are remote and accessed over http. It doesn't accept URLs of the form file://C/Users.... So, I'd like to give these local files addresses that makes them ""look"" remote.) Configuration can be done using the provided graphical tools. An option would be adding a virtual directory to your instance, that points to the directory that holds your files.",3
"You may want to consider a real Document Management system. These days there are hosted web-based ones available such as DocuVantage which are very cheap and take no time to setup. At my organization, we are loading up an instance of (DocMgr).  It's free, runs against Postgresql, keeps revisions of documents, OCR, indexing, relevance searching.  It can easily be tweaked to run against MySQL.  The project has been stagnant for a while, but development is picking back up. I might be able to point you in the right direction a bit better if I was aware of what distribution you use. With these systems you get more than just file storage. You get version control (can go back to last version), security so you can control access, custom metadata and searching on metadata, full content search of the documents, can tie approval processes to documents, can scan documents in the web interface into the system, can get notifications when documents are viewed or modified.  I don't know which of Tahoe distributed filesystem or OpenAFS is the most adapted to your setup. Also, if it is only for documents sharing, you should perhaps take a look at KnowledgeTree or AlFresco. You could quite easily setup samba on your linux file server and share the directories that way. It would allow you to mount the shares, control permissioning, and not drastically change the way you are currently setup. Here is a link to the current official how to guide to setup Samba: http://www.samba.org/samba/docs/man/Samba-HOWTO-Collection/",4
"After the restart, everything should be peachy keen. If, like me, you just want to rely on your root router to provide WiFi, be sure to disable WiFi on your TP-Link Router. If, instead, you want to use WDS Bridging, there are some additional instructions to follow at https://www.tp-link.com/us/faq-825.html, which is where I got some of the instructions above. (For example, my Google WiFi app showed the Router LAN address as 192.168.86.1, and the DHCP Address Pool starting at 192.168.86.20 and ending at 192.168.86.250. So I chose 192.168.86.2 for my Archer C5.) Remember that if you ever need to change your router's settings again, you'll go to the new IP address that you configured instead of the original one. I just got this working. I just bought Google WiFi in hopes of improving the WiFi networking, but I have several devices on my network that still need to be connected via Ethernet. Devices like printers didn't show up on the same network as the WiFi when I just used the Archer C5 as a downstream router. This is basically the same idea as David's answer, but with a little fleshing out.",1
"I've recently taken an interest in analyzing commit history to find files with a lot of churn. The idea being that high churn files are likely targets for refactoring. Right now it's a simple console app that returns the number of times a given file was committed. (Renames are not currently followed. It's a known area for improvement.) The basic design is to have a visitor walk the commit graph and raise events for a listener to gather information about them. Currently there is a single listener that diffs each commit with its parents and caches those diffs. This should be flexible enough to create another one that, say, collects committer stats though.  Well, having your abstract class rely on its inheritors like that makes it pretty much useless. This should be declared like this.  This is the class performs a diff that is roughly equivalent to a git log --stat and caches them for later use.  Git's commit history isn't a tree, it's a graph and we don't want to visit any commit more than once, so this class needs to keep track of the commits it's already visited.",1
"Probably your IP address really isn't changing. Most times a ""dynamic ip"" is simply not static. For example, I have a ""dynamic"" IP address but it only changes every couple of days. The only way to have a true ""dynamic"" IP address is with dial up. There isn't one central program or system for IP based banning.  Some sites will ban a single ip, or if there is an especially annoying trouble maker they will ban an entire IP range.  Most likely though if you have a truly dynamic IP and are still getting re-banned the moment you log in, it isn't your IP which is giving away your identity it's a cookie stored on your browser saying you are banned, or a flash cookie, or maybe something which was cached. Unless you're connecting your computer directly to your internet, your home IP is different from your computer IP address. Your house will usually have an IP address dictated by your ISP while the IP addresses inside of your house are dictated by your router or switch. When someone bans you by IP they are banning your home IP address (which only changes when your IP wants it to, or when you re-lease your home's IP) not your computer IP address. The reason for this, even when you have a dynamic IP address, is that DHCP (the mechanism used to request a dynamic address) clients and servers both remember the last IP address each client has been assigned.  When reconnecting, clients (such as your router) will typically request that they be given the same address again; servers will generally comply with this request if the address is available.  If the client does not request a specific address and the server recognizes it, the server will frequently offer the same address again anyhow. If you haven't already verified that your external/public IP address is changing, then I'd tend to assume that it isn't.  Although there are exceptions (such as Om Nom Nom's ISP) where cable/DSL providers will assign a new/different IP address every time you connect, my experience has been that the public IP for any broadband connection will always receive the same IP address except in unusual circumstances (e.g., the ISP reconfigures their infrastructure) or after spending a long enough time disconnected that the address gets assigned to someone else. They don't ban your numerical address, they (often) use the other form of address, which is more like a domain. As shown in the link below, the ip can be turned into an address, this is similar to what IRC does. IRC is the best example, a type 2 ban will include some * in it, but will generally only affect one person. Another type of ban will ban the whole isp. If you've confirmed that your external IP actually is changing, then I'm going to have to agree with Daisetsu: Assuming web-based chat, you're probably being blocked based on a cookie that the site has set.",5
"If you use an enum, how will you describe the other differences between a Robot and a Zombie? Will you have a variable to hold a reference to the graphics representation and another to hold a reference to the behaviour? If so, do you really need an enum and hard code all your enemies? Your question shows lack of experience. Which is not a bad thing, because we've all been there and you're taking steps to improve your situation by asking around and trying stuff. Kudos on that! Let's say we make a game. The game has items, enemies, allies, etc and there are multiple of them, like zombies and robots as enemies. Do you think it would be better to make one parent class called ""Enemy"" (and the corresponding cpp/hpp files) and making a class for each enemy (zombie or robot in this example) and making 2 files per enemy or would it be better to make an enumcalled ""type"" in the Enemy class and having all child objects in that one class? Plan a bit more what you want from your game, with all the edge cases, then try to implement it with what you're most comfortable at first. Then take a look at what went wrong with the way you chose, and figure out a way to improve it for your next game.  If you use a parent class and inheritance, what is so common to them now that there is the need for a parent class? And if you use that, and you turn out needing a friend Zombie, how will you hack your way to avoid code duplication and have your zombie act as a friend and as a zombie? And if you decide to have a hoover enemy Zombie, will it inherit from Enemy <- Zombie? If so, how many inheritance levels are you ready to have before you consider you're really entangled in your architecture? And after you're done with your first implementation, you should take a look at the concepts of entity-component design. Maybe you don't need all of what's in there, but it could help you figure out ways to design your next game! When programming, there is rarely a single best way to do something. It always depends on the situation, on the requirements and and the implementation.",2
"I need to prototype a solution using Amazon VPC - what's the least expensive option available to create a VPC gateway on our side for the test lab? What router do you have available in your test lab?  Early in our evaluation of VPC I was wondering the same thing, and realised that our router (which is a Juniper, running ScreenOS) is on the Amazon approved list, so it was straight-forward; more-so that I would have thought, anyway.  Failing that, you're back to  Wes' answer above, i.e. software solutions like OpenSwan. Otherwise try getting a Mikrotik router to work. They can also handle the local routing nicely, as well as more compelx scenarios. I have 3 in use so far (office, data center, external project office) and will add nother 2 next month (another project office and a data center we build) and the prie / power combo is VERY hard to beat. I realize there are probably free VPN gateways (Vyatta comes to mind) but being that I'm not a VPN ninja, just looking for the easiest, nearly cut-n-paste way to get the connection set up, then just add the routing entries on our LAN to get started. Open source IPSEC solutions like Open Swan running on any old linux machine will definitely do the trick. There have been several people on the Open Swan mailing list that have posted questions about doing this and eventually succeeded. Expect a complex setup process though and you'll need to be/become familiar with some lower-level network details (or pay a consultant) to be successful.",4
"With UDK, you have script-level access, not native. As such, you are somewhat limited in the modifications you can perform. Additionally, UnrealScript is extremely slow; as such, it's difficult to optimize any product you do end up creating. Can you release a triple-A title on it? No. That being said, you're not going to be using it for that. As for Unity? It's a great place for a beginner/someone that doesn't want to have to delve into the complex details of an engine. Additionally, it's more flexible. With the ultimate goal of educating yourself without having to dive into C++, I'd highly recommend Unity. id Tech 3 will give you C++ access. That being said, it's much older technology, the tools aren't as robust, etc. Personally, I've never used it; but, it's not something you're going to build a commercial product with (unless you're looking for something scaled back. Check out this list: http://en.wikipedia.org/wiki/Id%5FTech%5F3#Projects%5Fbased%5Fon%5Fthe%5FGPL%5Fsource%5Frelease). Overall, it's not very well crafted for anything that drastically doesn't match Epic's product line.",1
"I would be looking at the resources availible on the servers when the slodowns happen, is there a spike in RAM or CPU use, is the Database having trouble keeping up with requests etc. 2Run Virtual VM against the java process and monitor the memory usage.  (if you cant run VirtualVM, then run jconsole.exe . Things don't magically change. There has to be a piece missing from the picture you've presented us.  The first thing is to determine if this is a resource issue, or a problem with the application its self. Has your application become more popular and being used by more users, concurrently than it used too, how many users are connected when the problem occurs? Are users using the application more often, or perhaps leaving it connected all day. Secondly, could it be Java related, have you updated the JVM recently and has this caused a problem or conflict with the application that is slowing it down? Is the application multi-threaded and has there been any increase in user load?  Sometimes threading deadlocks in an application do not appear until you get into just the right load situation, and therefore they can be a real pain to diagnose.  How long has the application been in production?  Has there been a significant increase in the database size in that time where specific malformed queries might be tying the system up?",4
"I use it for personal pc to pc file transfer(On Ubuntu Hardy). one known issue (and I can bear it with) is it can't handle Unicode(non-ASCII) well.   The answer is simple: thttpd. It wins hands-down in performance, memory usage, and security. And by security I mean it's secure by virtue of competent design and not doing anything stupid, not that it has all sorts of ""security"" bells and whistles. It's also near-optimal with respect to size and simplicity. It will allow you to pretty much do everything you require and is usually very easy to set up,you can even configure the port to use 80 if that is a requirement. You can also try HFS also called HTTP File Server ( http://www.rejetto.com/hfs/) While its is a windows application, it is said to work well using WINE. Its very low maintenance and even complete novices who don't even know what a File Server is will be able to use it without much effort. It supports drag and drop functionality and you can even control which IP addresses can access the files and supports users accounts and download speed control.  Please ignore if you are not open to alternatives, but the fact that you are willing to install a HTTP server with no preferences shows me that you want to get the job done of transfering a file more so than how it is done -  (I know this question is old, but it's a top result on Google and the existing answers are anything but light-weight.) If you choose a port lower than 1024 on *nix, you might need root privileges, otherwise bind() will fail. I'm not sure if they meet some of your more off requirements like serving single files, but you can do that with a wrapper that puts symlinks in the http root. If you do this with thttpd, however, you'll have to disable symlink restrictions. It's light-weight, download the source, and write a Python script(just copy the  quick start part on the home page is enough, 5-6 lines, set user name and password, home directory, etc.)",5
"Another issue I see with the aggregation strategy is that the code repeats itself a bit: TryParseInternal checks exceptions.Any() to decide how to handle the out parameter, and then its caller checks esr.Any() to decide how to handle the out parameter. If you are attached to the aggregation (I personally don't see the point) then this might be simpler if the inner method returned ParseException instead of List<Exception>. I'm confused. The existence of TryParse tells me that there are important validity constraints on the number, but there's a public constructor which doesn't validate its arguments. It seems that I could easily make an instance of PhoneNumber for which Parse(phoneNumber.ToString()) doesn't round-trip. I wouldn't say that restricting phone numbers to exact three equally long parts is a good idea as there is no standard format for them. I sugesst to just match the digits and other allowed separators and capture only digts: In fact, I'm even more confused: PhoneNumber doesn't override ToString() or any of the identity methods (GetHashcode, Equals, operator==, etc). Why not? How many parts a phone number has it's purely a visual representation and should be implemented either by the UI or alternatively by ToString(format). Apart from the parsing logic I find the PhoneNumber class should not have its three PartX properties but a single property Value that stores all digits.  Which prompts me to say: code to the interface, not the implementation. If a method can return IList<T> then that's preferable to returning List<T>. This aggregation strategy lets you down (and so does your test suite) when I pass null for codeline. Instead of the ArgumentNullException or ParseException which I would expect, I get a NullReferenceException from Regex.Match.",2
"I'm currently constructing and maintaining a number of MS Access databases intended to replace paper document systems where I work. As part of this work I'm using simple username/password combinations in place of a signature on the document and would like to keep the username/password combination common between all documents, to this end I created a separate database containing just the user information that's common to all the different documents. However for each document database I'd like to have a 'permissions' table which has a set of Yes/No fields defining what permissions a user has pertaining to that particular document, obviously I'd like to maintain referential integrity between the permissions table and the users table. All the databases in question are stored on a shared drive. At first I thought that in WhateverDoc.mdb I could link to the tblUsers table in Users.mdb and then create a one-to-one relationship between the username field of tblUsers and tblPermissions but it seems like I'm not able to enforce referential integrity when I create a relationship with a linked table. In the table design for tblPermissions, select the User field and then in the Lookup tab for the field options set the following options. Is there a workaround for this? I'm open to solutions involving VBA but thought I'd ask here rather than StackOverflow as it's not specifically a programming question. This prevents the User field in tblPermissions from containing a value that is not present in the Username field of tblUsers due to the ""Limit To List"" option being set to yes for the appropriate row source.",1
"It would imply that 3SAT has (non-uniform) circuits of size $n^{O(\log n)}$. Then, every language in $NP$ (and the polynomial time hierarchy) would have quasi-polynomial (i.e., $n^{O(\log^c n)}$) size circuits. Problem. Fix a function $f:\{0,1\}^n \to \{0,1\}^n$.  Given $y \in \{0,1\}^n$, find $x \in \{0,1\}^n$ such that $f(x)=y$. So, if your technique can do better than the Hellman time-space tradeoff on some cryptographically secure $f$, it would certainly be news. In the cryptographic world, the best known algorithm for this problem does a precomputation (depending only upon $f$) that requires $2^n$ time and $2^{2n/3}$ space, and outputs some advice of size $2^{2n/3}$; then, given $x$, it can find $y$ in $2^{2n/3}$ time, using the advice string of size $2^{2n/3}$ from the precomputation.  You can adjust the space vs time tradeoff, to use an advice string of size $S$ and take time $T$, as long as $S \sqrt{T} = 2^n$.  As far as I know, this complexity is believed to be the best possible, for algorithms that do not take into account any of the internal structure of $f$.  In particular, it is likely to be optimal when $f$ is a cryptographically secure hash function.  (This technique is known as the Hellman time-space tradeoff.) I don't know whether your result -- if valid -- would be a non-trivial advance, but here is one sort of problem you could test it on: If $f$ can be computed efficiently (say, by a small circuit), your result implies some sort of solution to this problem. Even if it took $2^{2^n}$ preprocessing time to produce a data structure of size only $2^{O(\log^2 n)}$ which could then correctly answer arbitrary 3SAT queries of size $n$ in $2^{O(\log^2 n)}$ randomized time with high probability, 3SAT would have quasi-polynomial size circuits, using the known translation of randomized algorithms to circuits. This would not improve the known algorithm time bounds because of the preprocessing, but it would still be extremely interesting as a non-uniform result. What do you mean by ""to within an error that can be adjusted to an arbitrary amount""? Is the algorithm randomized?",2
"The server runs Windows Server 2012R2.  There are two VM's.  One is a AD server, the other is a remote access server.  There are two networks.  The first is an external public IP connection provided by the building.  It is behind a firewall but all ports are open.  The second network is a local one to the office, 10.201.51.x.  I created a virtual switch for the internal network and gave all the servers static IP's.  Internal network works fine. I would like to take all the traffic from the external connection with the public IP address and route it to the remote access server.  I used to do this with a cisco router but we don't have that here. Inside your VM Settings -> Network Adapter -> select your virtual switch. If you need to assign it to a specific MAC-address, if internetprovider requires this, click on + and in Advanced Features you can select Static and fill out the MAC-address. On this new virtual switch: Give it a name, click External Network, select NIC #1 and uncheck ""Allow management operating system to share this network adapter"" (When you do this remotely you will lose your connection). To dedicate NIC #1 to your remote access server you should open ""Virtual Switch Manager"" -> ""New virtual network switch"" -> ""External"" -> ""Create virtual switch"". Is it possible to create a virtual switch that does this?  If not, is there a way to do this with Windows.  I have looked around but the docs on the web are more confusing than anything.  Is there a place that gives instructions for this in Windows?",2
"Also if you run distiller, you can see the compression settings for all distilled PDFs. I'm just guessing, but maybe acrobat applies these settings when saving your changed file. This includes stuff like you see in the PDF optimizer section... whether or not to embed fonts, downsample images, compress tags, etc. Sometimes PDFs contains unused objects (and even unused images and fonts). That objects may be removed when PDF is re-saved.  After highlighting, I save the file (using a different filename), and now the file change from 4MB to 3MB...  is it just compression?  Or making the images have lower clarity? (thought I cannot see any difference).  What is the reason?  If it is just compression, then why wasn't it done before, as winzip technology is quite mature more than even 10, 12 year ago. I had some PDF files and just try to open it and do some highlighting using Acrobat 10 (also called Adode Reader X)... acrobat automatically applies certain optimizations that shrink file size, unless you tell it not to. For example preferences --> general --> ""save as optimizes for web view"" checkbox. If you update your question with links to original and updated files I'll try to investigate what exactly led to such reduction of the file size.",3
"Is this a mostly correct understanding of how logic flows between pieces of a game? Are there other common ways of handling this (for example, Main updates Game directly and Engine is used as a service by Game for other things?) Is this a decent high-level understanding of the rendering flow between the Engine and the Game? Is this general design suitable for audio and physics systems as well (Engine collects 'primitives' from Game and delegates processing to subsystems)? I'm dabbling in writing my own somewhat basic game engine from scratch, and I wanted to ask about some common design configurations for connecting the various parts. I'm particularly interested in the relationship between the ""engine"" and the ""game"", and the various ways in which information and function calls cross this boundary (the ""control flow"", for lack of a better term). Anyway, I realize how big this question is, but the gist of it is this: What are some common designs for connecting the various parts of a game engine and what are the pros and cons of different approaches? How does the flow of logic updates differ from the rendering flow, physics solving, object creation, etc.? Which approaches are used by popular game engines (Unreal, Unity, IdTech, CryEngine, Source, Godot, etc.), and have popular approaches changed over time? But now this seems to complicate the relationship between the Engine and the Game. It used to be that the Engine 'owns the Game', updates it, and retrieves primitives from it, while the Game had no knowledge or dependency on the Engine. But now the Game needs to be able to call functions on the Engine as well. Now there are a few ways to resolve this; making the engine a singleton, creating back-pointers to the engine, etc. But I was thinking about a solution where the Engine passes a reference to some kind of EngineUtilities object into the Game and its Entities that gives them some kind of limited interface for calling Engine functions. Kind of an attempt at dependency injection, I guess. Finally, in my design, where the ""Engine has a Game"" as a ""Console has a Cartridge""; updating the game's logic and collecting of all of its renderable/physics primitives each frame. I've run into a situations where the Game depends on the Engine for certain tasks. For example, the creation and destruction of certain Entites/Components/Assets! Where things seem to get more complicated and where I get more confused are situations where it seems like everything isn't 'flowing in one direction', as it happens with the logic flow described above. For example, here's my understanding of the ""rendering flow"". So, for example, this is my current understanding of 'control flow' when it comes to updating or 'ticking' gameplay logic (pseudocode ahead): This is pretty simple to understand, as it is a straight-forward chain of Update() calls starting from the main loop, then the engine, then the game, and finally propagating all the way down some structure of entities (and/or components) until everything in the entire system has performed it's per-frame logic. This is a 100% linear relationship, and as such each piece only needs access (pointer/reference) to the next piece in the chain. At least, that's my current understanding of how the control flow works when it comes to things like graphics rendering, audio rendering, physics solving, etc. In these situations it seems desirable to encapsulate these tasks within their respective engine subsystems, so that as much of the related code as possible exists in a single place. Of course the Game world and Entities determine the content of the game, including what needs to be seen, heard, collide, where it is, what it looks like, etc. But it should be the Engine that is responsible for actually taking those things from the Game and rendering/solving/processing them.  It makes sense to me that the Engine should be responsible for allocation and deallocation of system resources (like memory) as well as loading assets from files. As such, in order for the Game to add a Sprite or Mesh, for example, to an Entity, it must be able to request the creation of a Mesh component (or something similar) from the Engine. The Engine will then create that object using a custom allocator, load the asset from a file, do any other preparation, and then deliver the result to the Game.",1
"Hopefully someone knows of a simple solution to this problem. The only limitation I have is that this must run on a CentOS box, but I'm pretty open to any tools and languages. I'm trying to find a solution that will allow for a user to connect to a server via SFTP using a provided username/key combination (stored in a Wordpress installation's MySQL database), without needing to constantly keep two or more separate systems in sync. This is the MySQL query that gets two columns that have the username of a user and the api key, which the user would type in to their SFTP client as their password: You can replace the backends of PAM and the Name Service Switch (NSS) with a MySQL backend. You will need pam-mysql and libnss-mysql. As far as the file system goes, there is a folder which has the same name as a user's username for each user, all in the same directory. This user's folder may or may not already exist. When a user makes an SFTP connection to the server, they should be shown only the contents of that folder once authenticated.",2
"Now I use geeqie, eog (eye of gnome) or gthumb, it depends on the actual Desktop Environment. And I do all editting in GIMP. It is not ideal, but it is quite good. Although I have been doing some research of this problem too, I have not found anything as good as Irfan. That is still an app I envy to the Windows users. I believe that GTK+ and KDE apps have fully configurable key bindings for anything in their menus, so you shouldn't get too hung up on that if you find something with the right functionality. It isn't updated much lately and the keyboard shortcuts are different, but feh is an incredibly fast image viewer. The nearest alternative of IView for linux is XnViewMP --- not open but powerful. Still the best alternative on the market. Easy download and install, just unpack the tar.gz to the default subfolder in your user folder and run xnview.sh. You must only ensure that you have all the compatilibity libraries needed. I read that some people had an installation bug because of no libpng15-compat packages which includes libpng12.",4
"Otherwise, you just change cygwin.bat to run zsh -l -i instead of bash --login -i and it will run as a login shell.  I was able to set the SHELL environment variable in Windows to /usr/bin/zsh and it worked without any other changes.  I just changed it for my Windows user, not globally. Just thought I'd add this answer in case it helps anyone who's looking for it. If you use chere (which adds a ""Bash prompt here"" option to the right-click context menu of any folder), you can set it to zsh (""Zsh Prompt Here"") with: Assuming you're interested in changing the shell used in mintty/Cygwin Terminal, it first checks the SHELL environment variable, then the (now nonexistent) passwd file, then falls back to /bin/sh, which is what it seems to be doing by default. If I read your question right, you're looking for something else than what  chere  supplies (which, granted, is pretty cool in its own right). The current version of Cygwin doesn't have an  /etc/passwd  file, and the system I'm working on has Windows account information in a domain database out of my control. Consequently,  chsh  is no longer supported. Of course, if you want to run multiple shells from startup, just create a set of .bat files to load different shells. (sh, ksh, csh, fish etc) I also found that bash is not hard-coded into the  startxwin  script, nor is it hard-coded in any .bat file. Turns out you don't need to fiddle with .bat files at all. Unless you're using cygwin as a multiuser environment (in which case use chsh as you would under a standard environment.) If you want to use Cygwin shell inside the IDE Intellij, then use following setup inside settings/tools/terminal I felt I should update this to provide info on doing this without chsh but still doing it on the Unix end. Edit the /etc/passwd file and replace occurrences of /bin/bash with /bin/zsh. (This is effectively what chsh would do, but this way you'd do it for all users in one go.) Setting an environment variable probably varies for different versions of Windows.  Try searching for ""environment"" in your Windows Control Panel.  For me, under Windows 7, it was Control Panel  System  Advanced System Settings, which brings up the System Properties control panel/dialog, then the Advanced tab, then the Environment Variables button, which brings up the Environment Variables dialog, then create a new user variable named SHELL with the value /usr/bin/zsh.  Then OK back out of all of that and start a new mintty.",5
"For this, I have set up a sync pair with Synctoy 2.1 (left folder=Images on laptop ; right folder=Images on desktop) and it works manually. I have found this link which deals with setting up a scheduled task. I tried fiddling a bit with that but no luck so far. The reason why I want a trigger of the type ""when desktop connects to the network, run SyncToy on laptop"" is because I don't want a task that will start with very little chance of success (as the desktop is not online often). The option of a trigger of the type ""at session startup on the desktop, launch SyncToy and pump the pictures from laptop"" is not preferred either since it will hog resources at startup. Since I don't use the desktop on a reliable basis (i.e. online time are rather random) I cannot set up a task at a specific time either. I do not want a solution of the type ""Home Network"" as I want to physically transfer pictures from one computer to another. After a bad experience with lost pictures, I would like to automatically back up photos from the laptop to the desktop.",1
"Does your server support remote ISO mounting such as HP's iLO? if so then I believe that'll be the simplest method, otherwise a PXE/kickstart is the the way to go. FAI (Fully automatic installation) for ""large"" unattended installations. A little setup has to be done, but worth it. FAI uses PXEBoot with DHCP, TFTP, NFS on a debian/ubuntu system. You can customize the install for every machine in the network using a class concept. It also integrates with CFEngine (and possible with toher management tools like puppet. I didn't try it yet). I think what you're searching is TFTP. Many BIOSes allow you to boot via TFTP. All you need to do is setup a TFTP Server and provide and image of your distribution. If you're wanting to install lots of machines over the network you should also investigate kickstart. Look into PXEBoot as the others have mentioned, but if you're looking to do the network installs on a larger scale and a good system to maintain all those machines (updates, installs, etc.) then take a look into XCAT (http://xcat.sourceforge.net/).",5
"Consider the problem of randomly shuffling the string $XX..XY..YY$, where each block has length $n$, with a circuit consisting of probabilistic pairwise swaps.  That is, all $(2n)!/(n!)^2$ strings with $n$ $X$s and $n$ $Y$s must be equally probable outputs of the circuit, given the specified input.  Let $B_{2n}$ be an optimal circuit for this problem, and let $C_{2n}$ be an optimal circuit for the original problem (randomly shuffling $2n$ elements).  Applying a random permutation is sufficient to randomly interleave the $X$s and $Y$s, so $\lvert{B_{2n}}\rvert \le \lvert{C_{2n}}\rvert$.  On the other hand, we can shuffle $2n$ elements by shuffling the first $n$ elements, shuffling the last $n$ elements, and finally applying circuit $B_{2n}$.  This implies that $\lvert{C_{2n}}\rvert \le 2\lvert{C_{n}}\rvert + \lvert{B_{2n}}\rvert$.  Combining these two bounds, we can derive the following result: We see that the two problems are equally difficult, at least in this sense.  This result is somewhat surprising, because one might expect the $XY$-shuffle problem to be easier.  In particular, the entropic argument shows that $\lvert{B_{2n}}\rvert$ is $\Omega(n)$, but gives the stronger result that $\lvert{C_{2n}}\rvert$ is $\Omega(n \log n)$. Anthony's $O(n^2)$ algorithm can be run in parallel by starting the next iteration of the procedure after the first two probabilistic swaps, resulting in $O(n)$ runtime. The paper [CKKL99] shows how to get 1/n close to a uniform permutation of n elements using a switching network of depth O(log n), and hence a total of O(n log n) comparators. Start with the switches $(0,1),(2,3)$ with probability $1/2$. Reducing $0,1$ to $X$ and $2,3$ to $Y$, we are in the situation $XXYY$. Apply the switches $(0,3),(1,2)$ with probability $p$. The result is A similar idea works for $n=6$ - you first randomly sort each half, and then ""merge"" them. However, even for $n=8$ I can't see how to merge the halves properly. As a side note, the set of probabilities $p$ which can conceivably help us is given by $1/(1-\lambda)$, where $\lambda \leq 0$ goes over all eigenvalues of all representations of $S_n$ at all transpositions. A previous comment already pointed out a result saying that O(n log n) switches suffice, but the difference is that in switching networks the elements being compared are fixed. with Random Transpositions"" shows that 1/2 n log n random transpositions (note: there is no ""O"" here) result in a permutation close (in total variation distance) to uniform.  I'm not sure if precisely what is allowed in your application lets you use this result, but it is quite fast, and tight in that it is an example of a cut-off phenomenon.  See Random Walks on Finite Groups by Saloff-Coste for a survey of similar results. This construction is not explicit, but it can be made explicit if you increase the depth to polylog(n). See the pointers in the paper [CKKL01], which also contains more information. Our next move is going to be $(0,2),(1,3)$ with probability $1/2$. Thus we really only care if the result of the previous stage is of the form $XXYY/YYXX$ (case A) or of the form $XYXY/YXYX$ (case B). In case A these switches will result in a uniform probability over $XXYY/XYYX/YXXY/YYXX$. In case B they will be ineffective. Therefore $p$ must satisfy",5
"If you have a 2008 DC you also gain the advantage of all the new group policy niceness - it works very well, especially if you have Windows 7 clients and non-priv users. Mostly print services performance is not so much of a concern. It would make little sense to have a dedicated print server unless your printing system is taking such a load of print jobs that it would be either bottlenecking or starving other running services otherwise, or you need some kind of administrative separation so you could give ""the printer admin guy"" a local administrative account to manage everything by himself. You can do all of this with 2003x86 but if you're going towards virtualizing things and you have a machine which can double as a print server, then use it as an excuse to just do the  move over to 2008(x64). You're going to do it eventually. I'm not sure I'd bother dedicating a server to this - but that depends on your environment. I've worked in environments where dedicated servers did make sense due to the sheer number of queues. You can do everything you need under 2003 however if you install a windows 2008 X64 box as your print server there is the advantage of forcing you to do things correctly. You will have to get drivers which are compatible with Windows 7/2008 and you will have to get both x64 and x86 drivers working if you want to share them out to both types of clients. There is not an awful lot which has changed at the core, but there are some new features which might come handy in some scenarios. The ""What's new in Print and Document Service"" document lists the changes in more detail: Placement of services is more of a thought exercise to come to your own conclusion. My guess based off of the number of printers and the number of servers, is that printing is a low priority service that only a few people manage. If that is the case, it really doesn't matter where you put it. Hosting print services has more to do with your computer environment than the technology available to you. Putting any service on a dedicated Hyper-V instance makes sense when you want to isolate it from other services and don't want to buy more hardware/licenses. Not sure if this is an option for you, but there is also hardware based print servers. Basically a small piece of hardware that you attached to your printer port, then plug in the ethernet cable on the other side. The only advantage of using a Windows server for this is that it auto loads the driver on the client, seems hardly worth the price.",4
"2) It deprives the system of the opportunity to use that data again. If the same program runs again, having the data already in RAM saves disk I/O. Pages of memory in either of these pools are free for the taking for existing or new processes.  Once there are no free pages in either the Free or Zeroed pools, the OS will start to forceably page out memory from other processes to re-use pages of physical RAM as needed.  There are 2 sets withing this group: Standby and Modified pages.  A Standby page is one that has alread been paged out to the page file and can be re-assigned to a different process (or within the same) quickly.  A Modified page has a dirty flag set and needs to be saved to the page file before it can be re-assigned. Yes.  The memory pages are re-assigned into the Free pool, where they can be allocated to new processes on demand.  A very-low priority process in the OS will eventually zero out those pages, adding them to the Zeroed pool.  You can watch this going on with Process Explorer (though they get zeroed out fast enough it's hard to see). 1) It's a completely wasted effort. If the computer makes the RAM free and it isn't needed soon, the effort of making it free gained nothing. If the computer makes the RAM free and it is needed soon, the effort of making it free just forces the system to make it used again, resulting in doubled effort. (The system could just have left it discardable and switched it to another use without ever making it free.) I depends what RAM you mean. If you mean any RAM containing data modified by the process, then yes. Those values could never, ever be used again. So the RAM must be marked free. If you mean RAM containing portions of the executable file, no, that RAM is not reclaimed. It was already marked discardable, since the system can always read the data back from disk if needed anyway. Freeing the RAM immediately would suck for two reasons:",2
"Little documentation has been published about the update to netsh and there is little discussion about it on the entire Internet (Google search dhcpstaticipcoexistence gives only 5 results at the time of this answer). But this feature is real. There is a new property called dhcpstaticipcoexistence in the netsh interface ipv4 set interface command, which can be set to enabled and this interface can be configured with multiple static IPs along with a DHCP-configured IP. However this has to be done with the netsh add address command, not yet possible via the GUI. Finally DHCP and static IPs can be configured to co-exist on one NIC. This feature has landed in the Windows 10 Creators Update (1703). I used Win IP Config version 2.7.2 from 2007 running on Windows 10.  Set your NIC to DHCP, Run Win IP Config as an administration (Right Click icon instead of double click for the option).  Then click IP+ and type in the static ipaddress you want.  I now have one DHCP and two static address.  Now I can do my embedded system development and access the internet at the same time with one NIC!",2
"So, my question is about these ""predefined"" options.  I cannot find any definitive documentation which describes what these actually are.  Some of them are, again, self explanatory (like ""Local Subnet""), but others are mysterious (like ""Play To Renderers"").  And even the more obvious ones could have subtle permutations (what if there are multiple subnets?). If the Predefined Set option is selected, there are several choices:  ""Default Gateway"", ""WINS Servers"", ""DHCP Servers"", ""DNS Servers"", ""Local subnet"", ""Intranet"", ""Remote Corp Network"", ""Internet"", ""Play To Renderers"". Where is there documentation (ideally from Microsoft) which describes all these ""predefined"" options?  A nice extra would be documentation for a way to add something to this list (even using something esoteric like a plugin or custom DLL). In the built-in firewall for Windows (Windows Firewall with Advanced Security), the definition of a firewall rule includes a Scope, which is a set of IP address filters which apply to the rule.  There are three basic possibilities:  IP Address/Subnet, IP address range, and Predefined set of computers.  The first two are self explanatory.",1
"You want to accept new commands in a timely manner but not absurdly early.  e.g., if a character is animating movement along 12 steps, accepting new input as the animation is completing the last step may be acceptable. Better to just have a clear animation (and a clear UI) so the player knows when to enter more input. However, evaluate your game mechanics and If you're able to develop the three approaches, run a playtest process to each one, see the reactions, ask questions to your players and see the results. If it is infeasible to speed up the game when there is buffered input, then you should either ignore input or buffer exactly one keypress (first or last doesn't matter much). Logging all user input for movement could create problems. An easy work around to that problem would be to put a cap and simply stop logging input if there is too many logged already. Process the input immediately. Start the animation and ignore further input for gameplay-affecting commands (but let the player inspect objects or do other things that aren't gameplay commands). Once the animation is complete (if it ends abruptly and has a clear point) or nearly complete (if it has a ""softer"" ending), allow new inputs. In a concept I'm working on, the player can move from one position in a grid to the next. Once movement starts it can't be changed and takes a predetermined amount of time to finish (about a quarter of a second).  However, I would simply ignore input while they are moving, or just ignore the input oof certain keys. I recommend, for turn-based games, that you take your second approach: use all the keypresses you got, in sequence. Furthermore, if you have keypresses buffered like this, speed up your animations. This allows the player to quickly move through parts of the game they're replaying without it getting tedious. Even though their movement can't be altered, the player can still press keys (perhaps in anticipation of their next move). What do I do with this input?  The difference between these two cases is whether a key which is pressed and released before movement becomes possible will still cause another movement. I recommend trying both to see which feels better. You should make sure that as soon as a movement is completed, if a key is currently pressed, the character will immediately start moving in that direction. This ensures that the game does not require the player to have exact keypress timing to move efficiently, which in my experience is very frustrating. If your game mechanics aren't strictly affected by this, ignoring input would be the best solution in terms of Occam's razor: ""the simplest solution is probably the best one"". Also be absolutely sure to communicate the input state to the user via the UI. If a button can't be clicked, draw it differently to signify this. Change the mouse cursor. Highlight objects when moused over differently. Make it very, very clear when you are ready to accept new input and when you aren't. 100% absolutely unquestionable clear.",5
"Really, a laptop is a consumer device and should not require a certain usage. While it is true that battery life can be extended by using it in a special manner, normal usage is usually fine. Every Macbook I know has only something like 5% of lost battery life in its first two years of usage. Lithium batteries like to periodically charge and discharge. It's best to keep them between 50 and 80 percent. Battery life will start decreasing if you're constantly running on 20% power. What kind of batteries are used in the new Macbook Pro?  Should the battery be periodically fully discharged?  What are the best practices for this battery? The only thing that is not recommended is to have it plugged in all the time for several months. However, if you actually do this, you don't seem to need the battery anyway, so there is no need worrying about it. And even if you did this, you can ""revive"" the battery by purposefully charging and recharging it a few times. In general, Apple's battery site will have the full best practices on how to properly treat and care for your batteries. In the case of all their newest devices, following good practice on the batteries is especially important as they are not generally considered user-serviceable anymore. (Whether or not we agree with that practice is moot; Apple locked the batteries inside the cases no matter how you slice it.)",4
"To test this premise, before you plug the cable into either pi, try and stop the service on both of them: I'm guessing this is because of hijinks caused by ifplugd, which runs as an init service by default on raspbian.  The intent is to configure ethernet automatically when a cable is plugged in, since this is an event that the normal NetworkManager system can't detect. I have two rpi's both of which have a wifi dongle. I can vnc into rpi but if I connect the two rpi's via Ethernet cable, vnc fails. Is there a way to hook the two rpi together via ethernet ports and retain access via wifi? Make sure it is really stopped with ps -C ifplugd (it shouldn't list anything).  When I noticed this app on my first pi, it seemed very pesky and I had to completely uninstall it to prevent it from coming back. Yes, put a static ip address for eth0  in /etc/network/interfaces on both RPis. The network has to be different than that of the WiFi devices.  If the wifi goes down and ifplugd isn't running, you probably have to reconfigure or disable NetworkManager. Plug in the ethernet cable and see if the wifi goes down.  If so, check ps -C ifplugd again (you'll need a keyboard and screen plugged in to debug this, obviously).  If not, problem solved.",3
"In places like schools and universities they register users based on their mac address, Tools like arpwatch have a database of static ARP entries. It is best to audit your network space prior to building an MAC address database. This is run on the NAT/DHCP gateway. I'd look at http://search.cpan.org/~jhthorsen/Net-ISC-DHCPd/lib/Net/ISC/DHCPd.pm which seems to have modules for reading both of those types of files. DHCP should not be depended on for network user management. What happens if someone picks a static IP?  I've no idea if or how well this works but it appears to collect some of the data you require so it could be a start. In our case, we have a perl script that reads a specially formatted source file and generates NIS, DNS (forward AND backward), and DHCP tables automatically, distributes them, and notifies the serving daemons.  This means we have one stop to clean up after ourselves and/or add something new. You are cleaning up your DNS (and, if applicable, NIS) when old computers are decommissioned, right?  So add a note to that process to clean up DHCP reservations at the same time. If you mean dynamic allocation of leases, you don't have to do anything.  The ISC dhcpd tries to hold on to previously assigned leases as long as possible until the lease pool is empty.  It will then scavenge from expired (and therefore currently unused) leases. I think you could put together a pretty simple perl script that would compare /etc/dhcpd.conf and /var/lib/dhcp/dhcpd.leases Are there any good FOSS tools to do an audit of reserved IPs used in a Linux/UNIX ISC DHCPd environment?  I'm trying to ensure that we do not have stale IP addresses reserved when old MAC addresses are pulled.",5
"Will OpenVPN play nicely with other vpn clients? Would we be able to use multiple (different vendor) vpn clients simultanouesly? The great advantage of openvpn is that it works through all routers as it doesn't use an unusual IP packet type. IPsec and pptp suffer from this. I don't have experience of Cisco's vpn. Is OpenVPN the right tool for the job? It seems as though using open vpn in ""bridged"" mode will give us all we want. OpenVPN has been a breeze to set up most of times and can be set up to work on port 80 tcp for extra connectivity, but requires installation and maintenance of another app. Also, if You will want to bridge some subnets over VPN and You want hardware routers to do that, they will not support OpenVPN, but will support IPSec. We would like full name resolution (wins/netbios and dns) as if we were attached to the office network. Nobody else has mentioned it yet -- unlike commercial VPN products, you don't have ""different vendor VPN clients"" with OpenVPN.  All of OpenVPN is open source, so people just port OpenVPN to whatever platform needs support.  So you only have one OpenVPN client -- THE OpenVPN client.  No vendor games to make your live unpleasant. The nice thing about PPTP is that every major commercial OS supports it out-of-the-box, with nearly zero config. I'd reccomend OpenVPN in bridged mode for the clients. Use routed mode to connect two networks. You still need to ship keys etc. to the clients, but we built a shell script that makes a .zip that you just give to the user. To cut it short, use VPN if You are an SMB and just want Your end-users to connect from home. Use IPSec if You are a blooming enterprise (and switch to IPv6) :) If You plan to deploy any serious VPN in enterprise You'll have to deploy a CA infrastructure. OpenVPN and IPSec both require it. It is not complex, but its security should not be overlooked. I use PPTP to a monowall box to connect to our ""server private vlan.""  We played with OpenVPN but getting work done was more important than twiddling with making the vpn work. Currently we do this with a lot of ssh tunnels. We are looking into switching to a vpn solution, which hopefully should have less client side setup needed like setting up host files etc. Also, ask Yourself what sort of firewalls You expect in the middle? IPsec works over UDP which may not be available from certain locations, like home networks with nat. But it is supported at OS level by Windows.",5
"if you're serving dynamic content and it's not practical to simply have two servers giving content simultaneously then your other option is to have multiple records on your DNS anyway and configure the backup server to throw ICMP port unreachable to clients that try and connect to it; if at any point the main server goes down then you simply remove the port 80 block on the backup and traffic will start coming in. The problem is that everyone thinks of HTTP, when HTTP is by far, nowadays in 2011, the exception and not the rule here. Most browsers a capable of trying another server if one fails. (See: Web Resilience with Round Robin DNS) Could you give more information on why you want a ""backup A record"" and how and in what circumstances you'd like to go to the backup. Now content DNS servers are located by a special type of resource record of their own, NS resource records, which don't have priority and weight information.  Equally, SMTP Relay servers are located by their own special type of resource record, MX, which has priority information but no weighting information.  So for content DNS servers there's no provision for publishing fallback and load distribution information; and if one is using MX resource records then for SMTP Relay servers there's no provision for publishing load distribution information. In short, I wouldn't do it for a site, because there are better ways to mitigate whatever risk you're thinking of, but you'll need to describe that risk if you want suggestions on how to mitigate it. You can have one cluster IP address backed by several servers with VRRP or CARP. Backup server takes over the address when primary server fails. There are no backup A records, but there can be several A records which are given out in random order. The only other (budget) way you're going to be able to do it is setup a separate machine (or two) to perform NAT on requests, thus if a webserver dies, you can simply remove the NAT rule for it. However, SRV-capable MTSes now exist.  (The first was exim, which has been SRV-capable since 2005.)  And for other service protocols, unencumbered with the baggage of MX and NS resource records, SRV adoption is far more thorough and widespread.  If you have a Microsoft Windows domain, for example, then a whole raft of services are located through SRV lookups in the DNS.  That's been the case for more than a decade, at this point. Everyone seems to think that you are talking about WWW servers, even though you explicitly wrote  The oft-overlooked truth is that HTTP service is the exception and not the norm when it comes to this.  In the normal case, yes, there is a mechanism for publishing information to clients via the DNS so that they properly fallback from primary servers to backup servers.  That mechanism is SRV resource records, as used by service clients for many other protocols apart from HTTP.  See RFC 2782. Also, it would be helpful to know the relationship from a network perspective between the primary and backup hosts. With SRV resource records, clients are told a list of servers, with priorities and weights, and are required to try servers in order order of priority, picking amongst servers with equal priorities according to weight, choosing higher-weighted servers more often than lower-weighted ones.  So with SRV resource records, server administrators can tell clients what the fallback servers are, and how to distribute their load across a set of equal-priority servers. The other option is that you only put one A record in your DNS server, and the DNS server (or something anciliary to it, like a monitoring script) keeps an eye on your site's main address, and if it fails then the DNS server's A record gets changed to your other site.  This means that only one site will be getting traffic at a time. There are two things you can do here: If you put multiple A records in your DNS server for a given name, then they'll all be served to clients and those clients will pick one from the set to connect to, meaning that traffic will be ""fairly"" evenly distributed amongst all sites simultaneously.  This isn't really what you seem to be describing, but it's a common situation (although I don't trust it, for a variety of reasons). The downside to this second strategy is DNS caching.  Anyone who got the old site address will be SOL until their DNS cache entries containing the old address get purged.  This means that you have to keep your TTLs low (increasing the load on your DNS infrastructure, although that's rarely a practical problem), but there's still the problem of ""rogue"" DNS caches, which don't honour TTLs.  These are a massive pain for anyone who ever has to change DNS entries, but they're a million times worse for anyone who needs to change DNS entries ""often"" (hopefully your site isn't going down several times a day, but still...)  Basically, anyone behind one of these misbehaving DNS caches will see your site as being ""down"" for an extremely extended period of time, and just try explaining to them that it's their DNS cache that's at fault...  Eugh.",5
"The router is running a dhcp server, and I would like that wifi clients get their ip directly from the router without any ""routing"" in the ubuntu box. You will need an extra interface for the machine itself tough. Or you can look for tricks that will leave the bridged interface usable by the machine as a normal ethernet interface. So, you basically want your Ubuntu box to act as a repeater or media converter, otherwise known as a bridge.  Try adding something like this to /etc/network/interfaces: I am quite sure it is not possible for clients to get IPs from router without ""any"" routing in ububtu box. The box will not route traffic from WiFi to LAN and back all by itself. Read http://www.linux.com/archive/articles/55617 for a fine introduction to wireless access point configuration under Linux, including securing and bridging. Note that not every chipset can run in AP mode, so your mileage may vary. You can bridge the ethernet interface and wi-fi interface, and all the traffic will pass unmodified. Your router will see DHCP requests from wireless clients and answer them.",5
"The idea seems just fine. Depending on what kind of adapter you use for the 3 x 2.5"" drives eat might be an issue. You should get one with active cooling (ie a fan installed).  Now, I have a 500gb HDD I took out of the laptop a few months ago, it had an issue with the g-sensor and locked up often when the laptop moved suddenly. I figured I could put that in the server and not have issues with the g-sensor. The replacement was a very similar model, same brand, except with better firmware where the g-sensor actually works. I'm thinking if I put the SSD in the laptop, I can RAID1 both of these 500gb drives in the server for backup purposes - just do a nightly backup sync with them. I'm scared to RAID them, though, because they're not truly identical. I decided I want to RMA it, and put the replacement in my laptop. The performance is actually needed there. However, I'm extremely weary of its reliability. As for backup, I would recommend you use some sort of software like CrashPlan (witch is free as long as the backup is taken to your server). I would also recommend taking a complete system image of your laptop with free (http://www.runtime.org/driveimage-xml.htm) or bought software like True Image or Ghost. Taking a complete image every month is enough for most users, if you run crashplan or the like.  I just built a home server a few months ago with four 2tb drives for storage of media. It was booting from a 120gb Vertex3 SSD. That drive failed, now I'm realizing I don't need a SSD for the boot drive in a server. I should agree that SSD is better in laptop, than in home server. Weary of SSD reliability? In case of writes count or overall reliability? These drives are all going in a 5.25"" bay in the server, the adapter has space for four 2.5"" drives. I was thinking I'd just pick up a WD3200BEKT laptop and boot from that in the server. Am I going to run into heat issues with these 3 drives? Will the vibration effect each of them? Is there a better way to go about this situation without wasting drives? Is this all a bad idea?",3
"As Fractalizer said, this is an issue with the setuid bit on sudo.  A detailed explanation of how to fix it can be found here: http://ubuntuforums.org/showthread.php?t=219767 First, you copied to a NTFS partition, which has different permissions, so yo are likely to lose some information in the process, although I have never tried it. Recently, due to some messy stuff with master boot record, I have to re-install my Ubuntu. Before doing that, I back up all folder (exclude root, bin, sbin, tmp, media, mnt) to a NTFS partition. There's a pretty good chance there will be other issues beyond this program.  So you may find other things blowing up once you resolved the specific problem with sudo.  Unfortunately, cp'ing to an NFS share isn't going to retain all the permissions you'll need for a fully functioning system.  Especially in places like /usr/bin.  sudo command must have setuid permission bit set on it's executable. But in Ubuntu you currently cannot become root. I think you can boot from emergency disk and set this bit on sudo using chmod. The best option is to use tar (or equivalent) and I guess you mistyped the command with its options. A simple way to do a backup is something like After installation of Ubuntu, I copied back all the folder using a nautilus (running by sudo nautilus). After that, I reboot my computer. And boom, now I cannot run sudo any more, my network services cannot run. When I run sudo from a terminal, I ge ""must be setuid root"" error. In ubuntu, root account is disabled by default, I don't know why all these files is no longer under ownership of my account.",4
"If you really tried everything and you are sure it didn't reset (did you try to ping the original IP address 192.168.1.1 or 192.168.2.1 (?) being on the same subnet?), then I suggest you ask for a replacement: I was unable to un-brick this router. As a last resort I'd opened the box to attempt a reset by draining the cap holding charge on the nvram. This also didn't work. As a result I have voided the warranty. So we purchased a new router. Not Belkin this time! This has worked for me with several different brands (although mainly Linksys) -- hopefully it will work for you, too. The reset switch in Belkin F5D7230 is not very easy to push. If you are having problem resetting it it's best to open the 2 screws under the sticker at the bottom of the device and open it. Then very carefully without touching any electronic parts inside with your fingers use something to push the reset switch for few seconds. I assure you the router will reset. Once it's reset the IP to access it will be 192.168.2.1 and for password just press enter.",4
"Well, system logs like cron.log are not really meant to be thumbed through regularly. If a jobs cannot be run, or produces output on stderr, you will get mail anyway. I realize that I can use grep or a log reader program to search through, but that isn't really what I'm looking for. I'd like to just be able to ""cat cron.log | less"" and thumb through without log spam from processes I don't care about (and I also realize that this process would be important on most web servers, but this isn't primarily a webserver and doesn't even allow connection from outside the company). Specifically, I'm trying to keep a nice clean cron.log under Ubuntu Server (hardy LTS). In addition to the 4 or 5 cron jobs that come pre-installed, I've added 4 of my own jobs (using the cron.d method). Now, one of the automatically installed jobs is the PHP session expiration script that runs every hour. If you want positive notification that your cron jobs have run, have the important jobs write a separate logfile, like important.log. Then you get a logfile with exactly the interesting information, in the format you like. Any recent syslog-daemon (like syslog-ng or rsyslog) supports filter-functionality. Just edit your rsyslog.conf or syslog-ng-conf to ignore entries for the process name /USR/SBIN/CRON which contain the string CMD. Since this executes every single hour and produces a comparatively lengthy log entry, it accounts for 90% of my cron.log. If I'm thumbing through the log file to make sure my daily backup scripts and such got executed, they get swallowed by all these entries. It needs to be executed, sure, but frankly I don't care about when it executes. It runs every hour and outputs an entry similar to the one below.",3
"The way I've broken this up is into manual groups of 40 containers, but even those groups take longer (at least on first run) than 3 hours. Additionally, this solution isn't really scalable. We add customers all the time, I don't want to have to manually maintain the lists. Was wondering if others have run into similar problems and if there are other ways I might chop this up. I'm looking to ways to break up a workload in a sensible fashion and didn't know if there were best practices or ideas around this. I have several hundred numbered containers, some with thousands of files in them, so the work at the rate it seems to be copying will take something like 15 days. My script is limited to running for 3 hours due to limitations in Azure's Automation environment.  Since the containers are numbered, I've considered some sort of modulus division for breaking up the workload, I could do that as 10 different jobs, this is gross, but doable, but probably will break on a not too long timescale once we have enough activity to slow things down. Finally, I could just move all of this to a VM and run the workload from there, but I was biasing towards using the available platform functionality. I've written a script that iterates over all of the containers and blobs. I need to check if each blob exists in the backup container. If not, copy it.  We run on Azure services and I'm backing up their blob storage. There doesn't seem to be anything offered directly on Azure for accomplishing this. Storage accounts offer redundancy, but not backup.",1
"Even if we consider a dialog that's not related to closing, it usually means that some process is underway, it hadn't completed yet, and the user must decide which way to go. It cannot be ""simply aborted"", because aborting is also an action that the user might not meant. Usually, a dialog box is displayed when the program needs the user to guide some action. Closing an unsaved document is excellent example: a dialog offers to save the changes, discard the changes or abort closing and return to editing. The program intentionally refuses to close without answering this question because closing will force SOME action to be taken. The program can't decide on it's own to eg. discard recent edit, or on the contrary, overwrite correct version with cat-on-the-keyboard typing. Today, most dialogs are not modal in the technical sense (the program remains responsible), but it's still easier to make them modal in a wider sense of logic flow of the program. There is no obvious right answer here. You may have accidentally corrupted your document (for example, the cat walked over the keyboard) in which case the answer is ""No"", or you might have spent hours typing in changes in which case the answer is ""Yes"". It also simplifies design of the program, as it's creators don't have to create ""a safe way out"" of every function. As to why it still is that way, people grew used to it, developers programmed with that assumption for decades and more importantly non-programmers used those dialogs in their office automation scripts and Microsoft is nothing if not a strict adherent to the goals backwards compatibility. I occasionally get the error that Word gives when trying to close a document with another dialogue box open but I've never been able to figure out why it happens. Behind the curtains, the program (in our case it's MS Word) creates a ""X button click event handler"" when creating the window. When there's a dialog window, Word records that. Then, in the X button handler, when the X button clicked, it checks for recorded open dialog windows. If there are some, the handler aborts the close operation. If there are not, it terminates the program and the OS cleans up the memory taken by the program. This is how it works. Several of those if killed abruptly without returning had unpleasant side effects, sometimes even outside the now-dead program and there was not a way to escape from them politely in all cases if they were doing something at a system level that required user input.",5
"In SQL Server 2012 or later, this can also be done with the FIRST_VALUE and LAST_VALUE windowed functions, but the execution plan may be less efficient. Also, these windowed functions are not aggregates, so you need to write the expression so that it returns the same value for every row per group, then apply an arbitrary aggregate. For example (using non-deterministic ordering just for variety): Without an ORDER BY clause the row chosen (per group) by FIRST and LAST is essentially arbitrary. The important point is that the values chosen by multiple FIRST and LAST functions will come from the same row. The row numbering can be done with ROW_NUMBER. Within the OVER clause, the GROUP BY columns go in the PARTITION BY section, with deterministic ordering provided in the ORDER BY section. You will need to write a subquery or use a Common Table Expression (CTE) to filter row number to 1. It is essentially impossible to duplicate this exactly, since the Access behaviour is not precisely defined; it's not possible to predict which row will be chosen as FIRST or LAST in all but the simplest cases. Your third choice is to write a SQLCLR user-defined aggregate (UDA). It is not currently possible to guarantee deterministic ordering with these, but the implementation might more closely match what Access does. You would need to be careful that all UDA results were computed by the same operator, to ensure the results of multiple UDA calls all come from the same source row. This does not say anything about the behaviour of LAST when a GROUP BY clause is present, but from testing it appears that FIRST and LAST return values from the row encountered first or last within each group. That said, if you can improve the query semantic to have a deterministic choice for FIRST or LAST row within each group, a general translation would be to number each row (ascending or descending per group), then choose values from the row numbered 1. This last point means you cannot just replace FIRST or LAST by MIN or MAX (aside from the different semantics) because the minimum and maximum will generally not be from the same source row.",1
"In the cases I've had of this (i.e. ""I never got the message!""), I've always stopped at ""I can see on the server that it was delivered to your mailbox.  There are things that could have happened at that point, but I can't be sure what might have happened if you didn't see it.""   We had the company president ""storing"" messages in his Trash folder once...  Right up until the day his assistant emptied the Trash.  Then we spent some time with both of them helping them figure out a folder structure that they could both live with. If that's the case then I personally would probably just stop at this point and show the email was deleivered to the server and leave it at that.  Trying to prove users are lying never ends up good. Their response... ""Well I don't know if I was FOR SURE sitting at my PC between 4:45pm and 5:00pm... someone could have come along and deleted it!"" Aside from the obvious (DON'T save things in your TRASH folder) I was able to prove it was NOT conspiracy by grabbing screen shots and the proxy log for that computer.  The user CLEARLY (accidentally) deleted their own Trash folder. (web based email client) One last anecdote: the Vice President's assistant at the time didn't trust her boss to keep the right messages, so we set up a mailbox that duplicated all his messages for her to work with.  She'd keep everything, even if he deleted stuff. If your logs are showing the mail was delivered to the SERVER then I think the only step beyond that (if you choose to do so) is to look in the user's mailbox, etc. At this point its very important to just ""state the facts"".  I'm not saying YOU did it... I'm saying that YOUR PC issued a deleteFolder command at exactly 4:47pm.  So, whoever was sitting at your desk at 4:47pm did it. I recently had a case where a user ""stored"" emails in their ""Trash"" folder (don't ask why) and then lost over 600 of their ""stored"" emails when the trash folder was deleted.  They INSISTED someone logged into their email account and emptied the trash. One thing you need to check is if the user has any Outlook rules that automatically moved or deleted the message.",2
"Firefox does resume downloads. The problem is that many servers refuse to resume downloads even if there's a break in connection - the time period differs from server to server. And as pointed out by others, Firefox can and does resume downloads. However, resuming a download just means asking the server to continue sending data. If the server refuses, there's nothing FF (or any download manager) can do. In your case, the site probably provides download links which are only valid for a certain time. By the time the connection fails and you try to restart, the link has expired, hence the download cannot resume. So, definitely a problem with the site. ( TED's servers immediately comes to my mind, if there's break in connection for about 5 seconds, the download is ruined and I have to restart the download from start.) That's become the download links are autogenerated, are valid particular combination of parameters ( perhaps some session id ? I'm not sure), and are not persistent. Once the control of the download has been handed over to the browser, the links expire, and hence you get the error message. I have FF and I can resume downloads. Be aware that the server itself must be configured to allow resuming, so I imagine that's your issue. I'd also imagine that all servers that display countdowns or similar behaviour won't allow resume. That's not correct. If a server doesn't support resume then download managers cannot resume downloads. Few servers inspect the HTTP headers for the referrers, and block if the referrer is not a browser. I guess while transferring the download, the header gets mangled and the referrer becomes the download manager, instead of the browser. This is definitely a problem on your end.  Firefox resumes downloads just fine from servers which allow resuming.  Try this out, go to some website serving fairly large files directly, with no redirects or countdowns.  Start the download, then close Firefox.  Reopen Firefox and your download will resume.   The problem with your case that you're trying to resume a download from a server that doesn't want you to.  Firefox can't do anything about that. Well, that's probably your problem. The download fails because your network connection drops, and Firefox runs into a timeout. You should probably try fixing your WiFi network problem.  My suggestion: Try IDM. It's one of the better download managers that I've used. If the problem still persists, my suggestion is to use an alternative link to download, or maybe switch to a wired connection while downloading.",4
"Help for SSMSE is not installed by the product. If you press F1, MSDN Online help opens. Pressing F1, you are able to access all F1 help topics, but you do not see a table of contents or an index for the SQL Server 2008 documentation. To add SQL Server 2008 documentation to SSMSE, install SQL Server 2008 Books Online from SQL Server Books Online Download Center. After installing, in the Contents pane of the Books Online, you can use the SQL Server Express filter to hide the content that does not apply to SQL Server Express edition. Interactive Resolver, which lets you resolve conflicts manually during on-demand synchronization in Microsoft Windows Synchronization Manager, is not supported in SSMSE. In SSMSE, Object Explorer only supports Database Engine instances. The Connect to Server dialog box prevents connecting to other services, such as Analysis Services. I do agree with others, that SSMS Express would be the way to go if you just need to run queries.  Some of our analysts manually run jobs on the server, though, so that was out for us. You cannot update the full-text catalog by using SSMSE. You can use sqlcmd utility or Windows Scheduler to update the full-text catalog. I feel your pain.  There is not a stand-alone installer for SSMS (outside of Express, sigh).  Might be overkill (unless you have a lot of analysts), but one option would be to create an unattended install of just the Client Components.  We only have a few people that needed SSMS without any server components so I just ran the install for them myself... You can use SQL Server Management Studio Express (SSMSE) for that functionality.  Below is a list of the limitations that exist when using SSMSE vs the ""Full"" SQL Server Management Studio (SSMS) from SQL Server Management Studio Express If you have the 2 disk set instead of the DVD or MSDN subscription, just use DISK 2 , it only has the tools Because SSMSE is a subset of SQL Server Management Studio, all objects that would normally be enumerated for a specific Database Engine Object Explorer tree are still visible in SSMSE. If it is an object for an unsupported item, only the refresh command is available. You may be able to extract the setup files from the installer to just do the Management Studio install, I'm not sure what dependencies it has. You can download a standalone installer for SQL Management Studio Express, but I believe to install the full version on Management studio you need to use the SQL setup routine, which obviously has all the other options.",4
"Coming to your question about pi-calculus, I think pi-calculus is still a bit too new to be finding applications in language design.  The wikipedia page on pi-calculus does mention BPML and occam-pi as language designs using pi-calculus.  But you might also look at the pages of its predecessor CCS, and other process calculi such as CSP, join calculus and others, which have been used in many programming language designs.  You might also look at the ""Objects and pi-calculus"" section of the Sangiorgi and Walker book to see how pi-calculus relates to existing programming languages. Wide-scale industrial adoption of a language is a matter for sociologists to study, and that science is even more in its infancy. Market considerations are an important factor  if Sun, Microsoft or Apple pushes a language, this has a lot more impact than any number of POPL and PLDI papers. Even for a programmer who has a choice, library availability is usually far more important than language design. Which is not to say that language design isn't important: having a well-designed language is a relief! It just usually isn't the deciding factor. Process calculi do have practical implications. A lot of computations out there are distributed they involve clients talking to servers, servers talking to other servers, etc. Even local computations are very often multithreaded to take advantage of parallelism over multiple processors and to react to environmental concurrency (communication with independent programs and with the user). This is just fine from the POV type theory, but it's awkward when programming. The reason is that programmers end up managing not just their function calls, but also the call stack. (Indeed, encodings of lambda calculus into pi calculus typically end up looking like CPS transforms.) Now, typing ensures that they will never screw this up, but nevertheless it's a lot of bookkeeping foisted onto the programmer.  The science of programming language design is very much in its infancy. Theory (the study of what programs mean and of the expressivity of a language) and empiricism (what programmers manage or don't manage to do) give a lot of qualitative arguments to weigh one way or another when designing a language. But we rarely have any quantitative reason to decide. Are research advances needed to make better software? After all there's a billion-dollar industry out there that can't tell the pi calculus from a pie in the sky. Then again, that industry spends billions of dollars fixing bugs. From my point of view, the No. 1 purpose of theory is to provide understanding, which can be in reasoning about existing programming languages as well as the programs written in them.  In my spare time, I maintain a large piece of software, an email client, written ages ago in Lisp.  All the PL theory I know such as Hoare logic, Separation Logic, data abstraction, relational parametricity and contextual equivalence etc. does come in handy in daily work.  For instance, if I am extending the software with a new feature, I know that it still has to preserve the original functionality, which means that it should behave the same way under all the old contexts even though it is going to do something new in new contexts.  If I didn't know anything about contextual equivalence, I probably wouldn't even be able to frame the issue in that way. Now, the natural type discipline of pi-calculus is some variant of classical linear logic. See, for instance, Abramsky's paper Process Realizability, which shows how you interpret simple concurrent programs as proofs of propositions from linear logic. (The literature contains a lot of work on session types for typing pi-calculus programs, but session types and linear types are very closely related.)  This is a tricky question! I'll tell you my personal opinion, and I emphasize that this is my opinion.  You say that ""the true end goal would be to use the theory to actually build a PL.""  So, you presumably admit that there are other goals? There is a delay between the time some theory stabilizes enough for an innovation to be usable in a practical programming language, and the time this innovation begins to appear in mainstream languages. For example, automatic memory management with garbage collection can be said to have been mature for industrial use in the mid-1960s, but to have only reached mainstream with Java in 1995. Parametric polymorphism was well-understood in the late 1970s, and made it into Java in the mid-200s. On the scale of a researcher's career, 30 years is a long time. I do not think pi-calculus is directly suitable as a notation for concurrent programming. However, I think you should definitely study it before designing a concurrent programming language. The reason is that pi-calculus gives a low-level --- but importantly, compositional! --- account of concurrency. As a result, it can express everything you want, but not always conveniently.  This is not a problem unique to concurrency theory --- the mu-calculus gives a good proof-theoretic account of sequential control operators like call/cc, but at the price of making the stack explicit, which makes it an awkward programming language.   I like to search for practical implementations of process calculi in the wild :) (besides reading about the theory). However, I said that the natural type discipline of pi-calculus is classical linear logic, and this is the source of the difficulty in using it directly as a programming language. In most presentations of classical linear logic, the (linear) function type $A \multimap B$ is not a primitive. Instead, you encode function types using de Morgan duality $A^\bot   B$.  Explaining this comment requires thinking a bit about types. First, useful programming languages generally need some kind of type discipline in order to build abstractions. In particular, you need some kind of function type to make use of procedural abstractions when building software. So when designing a concurrent programming language, my opinion is that you should design your language with higher-level abstractions than the raw pi-calculus, but you should make sure that it cleanly translated into a sensible typed process calculus. (A nice recent example of this is Tonhino, Caires and Pfenning's Higher-Order Processes, Functions and Sessions: A Monadic Integration.) Process calculi are still at the stage where the theory hasn't stabilized. We believe that we understand sequential calculations all the models of things that we like to call sequential calculation are equivalent (that's the Church-Turing thesis). This does not hold for concurrency: different process calculi tend to have subtle differences in expressivity.  Will they ever be needed is never a worthwhile question in research. It is impossible to predict in advance what will have long-term consequences. I would even go further and say that it is a safe assumption that any research will have consequences one day we just don't know at the time whether that day will come next year or next millennium. For a while now, I have been very interested in programming language theory and process calculi and have started to study them. To be honest, it something that I wouldn't mind going into for a career. I find the theory to be incredibly fascinating. One constant question I keep running into is if either PL Theory or Process Calculi have any importance at all in modern programming language development. I see so many variants on the Pi-calculus out there and there is a lot of active research, but will they ever be needed or have important applications? The reason why I ask is because I love developing programming languages and the true end goal would be to use the theory to actually build a PL. For the stuff I have written, there really has not been any correlation to theory at all.",5
"At my office we are planning to deploy widescreen LCD monitors in portrait mode. We bought one for testing (Samsung T220 unpivotable, so we adapted a pivotable support for it) and it doesn't have a good viewable angle in this mode, despite the specifications (170/160). In portrait mode no viewing angle is good as it is in landscape mode, even when you are staring it in a 90 degrees angle, one of the sides of the screen is brighter/darker than the other. Anyone know of any LCD widescreen monitor (preferably 22 inches 1680x1050) that is good for such mode? I have a 1200x1600 (portrait mode) 20"" and while I enjoy (and depend) upon the vertical size, it still feels a wee bit cramped horizontally. I also found that running f.lux highlights the problem much more on panels with bad viewing angles. We run some nice dell units with swivel stands, but they look terrible when rotated. LG are doing some very cheap LED-backlit monitors at the moment, and the viewing angles appear to be excellent. I'll try spinning one and let you know the model number if it looks good.",3
"I would argue that you need two AD forests. Your ""back office"" AD forest should really be separate from the AD forest that runs your application. You can certainly have a replica DC from the application forest within your physical headquarters location, but I'd strongly caution against a two-way trust between the forests. I certainly wouldn't want both domains to be part of the same AD forest. I REALLY do not suggest anybody else try and do this in a production system for anything long term.  I'm documenting it and working on reengineering the network as part of our overall plan to get rid of this. Making them Domain controllers however has been a problem.  They get all the way through setup and then fail with an error when they try to setup all the replication with RPC server is not available.  I know the domain is prepped correctly, Last week I did all prep work to promote a new server to DC to replace an older machine which went through fine. I got it to work due to the time crunch but it is a complete hack and I am working on rebuilding the whole network in the next few months to eliminate the need for it.  As it turns out the domain lookup is via the server IP but replication is by looking up the GUID.  Running repadmin /fix /s:DC1 gives you a report where you can see the GUID it is attempting to connect to, this is also found in DNS under Forward Lookup Zones, domain.loc, _msdcs. You would be best served with a site-to-site VPN to allow the DCs to have transparent communication and to allow them to register their NIC-assigned addresses in DNS. IPSEC on public IP addresses is a possibility, too. I added to the Hosts file (c:\windows\system32\drivers\etc\hosts) on the new DCs both the post-NAT address of the server and the GUID and I was able to get them to replicate successfully.  This is what I had to add in to make it work for each DC that was on the other side of the NAT: My organization is in the process of setting up a new network at a hosting provider for a very large application project.  Since the whole system uses Active Directory we are planning on using a pair of Domain Controllers there that replicate to our headquarters over VPN.  These DCs would also serve as backup for our existing system so we could lose connection or power entirely at our headquarters and the remote system could stay up and still let people log in. Our hosting provider has setup 10.180.87.0/24 as our subnet to use with them.  But because our internal IPs are 192.168.1.0/24 and they already have it in use they require us to NAT over to 192.168.50.0/24.  This part wasnt a big issue and easily setup with our Watchguard firewall appliance. You don't want to do this through NAT. You could construct a nightmarish scenario where you manage the DNS records for the DCs and hand-register the NAT ""outside"" addresses in the DNS. With the right port-forwards through the NAT you could probably make it all work. I suspect the NAT is the issue and servers are trying to set themselves up with pre-NAT addresses.  Our provider wants us to remap our IP scheme to fit with their network and I'm not exactly thrilled with the idea.  One option we are considering is creating a server and network on 192.168.50.0/24 and using intersite replication to go from 10.180.87.0/24 to 192.168.50.0/24 to 192.168.1.0/24 (although that is probably gonna be messy network config-wise to figure out.) But Im not completely convinced if that would solve the issue.  A DMZ is another option but need to look into it more before I try to get our provider to set that up.   Has anybody had experience with a similar setup or alternatives for getting DC setup on a remote connection? First sign of trouble was when I put the two servers on the domain and they couldnt connect or find the domain at all.  I ended up putting a post-NAT address of DOMAIN.LAN into the hosts file on both servers.  They were then able to locate and join the domain.",2
"CBL's software is a bit pricier than similar competitors at about $100, but the only commercial one I've seen with that much granularity in the controls and decent support available if you need some help getting the settings worked out. 1. If a drive is failing, reading it at all is very likely to make it fail worse. So it is quite possible (some would say ""close to certain"") that some/many/lots of sectors that were good before you read the disk twice via the procedure are now bad. The smart thing to do would be to have a good drive and do both steps above actually recovering data. IF you do this, of course, you might want to use ddrescue's ability to try extra hard to get the data off of hard-to-read sectors. run chkdsk /r in an elevated cmd prompt to locate bad clusters and recover readable info. This may improve your backup attempt reliability. Backup your files using simple copy afterwards. If this fails you can try data recovery methods that retry the read, however if chkdsk could not read the bad sector, you could repeat the chkdsk /r and try again. Multiple chkdsk /r and attempts to copy data is a good way to repeat attempts at bad sector data recovery. If chkdsk manages to read the bad sector once, it will write the data to a good block. Repeated chkdsk /r will continue to improve file integrity as long as the bad block can be read just once. If the data is gone, give up! I realize that the original question is very old, but I had this problem recently and thought that others might want to know the answer to the original question. It may be a good idea to copy all your data from the drive as much as possible then do a low level format followed by a repartitioning and slow/full format to allow bad sector re-allocation within Manufacturer system and NTFS bad sector list. Quick format won't mark bad sectors as bad. When this tool encounters bad sectors, it keeps track of them in a separate text file (one bad sector per line, so you can simply count the number of lines in the file to determine the total number of bad sectors), which is also used as a cross-reference to find out which files used those sectors. If the drive continues to give you problems, drop it from a very high location, this will prevent you from wasting any more of your precious time on an obtuse hard drive. As per usual, your mileage may vary, but based from various user testimonials, it works as advertised. I use it personally to maintain my disks. As no one ever actually answered your question, the following not-exactly-lightning-fast method may be the quickest way to get what you are looking for.  You may want to control and monitor future bad sector increments by periodically running chkdsk /b and monitoring for dangerous bad sector increment rates using sector scan software. Alternatively after the first pass you could try spinrite as suggested by happy_soil to see if it can refresh the bad sectors but get the bulk of the data off quickly first as this level of failure is often caused by failing heads, pre-amps or cache in the drive circuitry.  f this is the case and the faults aren't in the media, every second of ru time counts.  Once you have recovered data or given up and written it off, you could restore the drive to normal use again, but keep important data backed up somewhere else. In a case like this you can set it to 3 retries instead of the default, 20 or 30 I think.  By adjusting this down to 3 you should still catch all data on weak portions of the drive without wasting crucial time on files that may already be beyond software recovery.  Then when you've captured that round go back and select only the files that failed in the first attempt and retry a few times gradually increasing the number of retries to 10, 20, 50 until you get everything or the drive goes completely flat lined. If the drive shows signs that it's stable, it could continue working normally again for a long time. I recently reviewed CBL's new data recovery software and although this drive is technically still running, one of the features it had that I found worthy of mention was the ability to select the number of retry attempts for bad sectors. ddru_ntfsfindbad's manual says that you CANNOT run it on the original bad drive UNLESS the file system is/was NTFS. So you're ok in your case, but it will almost certainly be faster if you run it on a ddrescue-recovered drive and not the original.  And if the bad sectors are in certain filesystem metadata, you really will need to do this. chkdsk /b clears bad cluster list and rescans/updates bad cluster list. The bad cluster increase rate may remain stable and the NTFS bad cluster list should keep it under control, the drive may be safe to use again. Remember though that if all the hard drives factory allocated spare clusters have been mapped to bad clusters, then the drive can't map clusters in future and may be heading for eminent failure. Remember though that NTFS monitors bad clusters independently though, so this may not be the final end for the drive.  While SpinRite won't exactly do what you want, it will try to fix and recover data that are situated on bad sectors. When it comes to bad sectors on a disk, if there is no backup then what I do is get a backup image of it using a tool called Drive Snapshot:",5
"If your data are at all important, it's better to hand the disk over to an expert than to try to do it yourself. The low-level recovery tools I've just suggested can be tricky to use. You can easily make matters worse or simply fail to recover data if you're not an expert. To maximize your chances of success, power off the computer immediately. (Don't even use the shutdown option; just pull the plug.) Any continued use of the computer runs the risk of overwriting more of the data you want to recover. Also, this type of operation is inherently risky, so you should have a complete low-level backup of your disk before you begin. This means you'll need a spare disk that's at least as large as the one you want to recover. Patrick is basically correct, but there are tools you can use to try to recover your data yourself. Depending on how you partitioned your disk, TestDisk may be able to recover some of your partitions, although they may have some damage. The PhotoRec utility, or similar Windows-only tools, are more likely to be able to recover a subset of your files. In both cases, the fact that you've re-installed the OS means that the chances of your recovering all your files is minuscule, even if you hand the disk over to a specialized and pricey shop.",1
"If I can ReShare it from the server it will be ""Printer on Server"" for everyone. (It'll also mean I don't need to change the hardcoding when the replacement networked printers arrive). Can anyone think of a way of Re-Sharing a Shared Printer? When I connect the shared printers to Server 2008 R2, it comes up as a network printer, and settings take affect on the host (not the server) so I can't Re-Share it. A printer is a logical construct that ""maps"" to a physical device. There's nothing stopping you from creating a second printer, with a Share name of whatever you require, that ""maps"" to the same physical device. Why bother sharing it twice? Just install the drivers and create a TCP/IP port and map the second computer directly to the printer, or are you traversing networks here that is limiting direct TCP/IP connectivity from the second computer to the printer? The reason I need this is I have a number of workstations that run software in which I have to hardcode the printer name. In an emergency I have to use USB printers shared from a workstation, which also needs to print. So on the workstation the printer name is ""Printer"" where as on the other computers it's ""Printer on Workstation"", so I can't hardcode the printer names. Instead of connecting to the printer on the server, go through the Add New Printer wizard on the PC and create a new Local Port, and use: \servername\printershare as the local port name. Then select your driver, and the printer will be usable as whatever you name it.",4
"Most virtualization software can use this .raw file as the disk - I haven't done much with XEN, but VirtualBox and QEMU will both use it fine - HyperV and Virtual Server will need it converted to VHD format. I would expect XEN to use it. LICENSES: If this is Windows you are going to have license issues (perhaps not technical but definately legally speaking). I have checked google, and nobody else seems to have tried this as yet. Any suggestions would be great. ec2-unbundle: http://docs.amazonwebservices.com/AmazonEC2/dg/2006-10-01/CLTRG-ami-unbundle.html This is a raw disk image which is what Xen uses. NOTE: You will likely have driver issues, you didn't say if this is windows, linux or other. It will likely detect new hardware. The .img file, from ec2-unbundle, is the file that typically uses extension .raw on some virtualization platforms - It's just the raw disk contents byte-for-byte. A quick google for xen and raw shows this may be the command (but a lot of useless posts came back so not sure): I have an AMI which was created from a base AMI on Amazon. I would like to bring a copy of this internally to run for development. I have downloaded the .manifest & part00-50 files (approx 550mb) from my S3 bucket. Unsure how to convert into my local Xencenter however.",3
"I have dual core processor of Intel 2.2 Ghz, 15 GB free on system drive and 3 GB of RAM. Will Sharepoint Server 2010 install on my machine? Or do i need to again format my PC? I am fed up with Sharepoint. After much effort i installed Windows 7 (64 bit) for my desktop. Now when i see the Sharepoint Server 2010 requirements, it has a unpractical requirement of 80 GB free on system drive, 4 gb of RAM and 2.5 GHz per core. You actually need 80 GB but it doens't mean that you need free 80. The requirements state that you have to provide ""double"" space on the hard drive of you RAM. In you case - 4*2 =16Gb plus if you have hibernation file - add some more. Sharepoint isn't a toy, it's for the enterprise. The requirements aren't impractical - if you are running a Sharepoint instance foro your business you want to application to perform appropriately. Microsoft sets this minimum system requirements so their PSS don't get flooded with ""My Celeron server has 20 users on Sharepoint and it runs slowly"" calls. And according to this Windows 7 is ""optional"" for the Single server with built-in database and front-end Web servers and application servers in a farm configuration. Sharepoint Server 2010 should install and work fine on your computer. If you are the only user then Sharepoint will run appropriately for you within the resources you give it. And here is a remedy to get more space once you have SharePoint 2010 and a health warning that you are running out of the space",3
"If you want to see ""dangerous"", that would be those options or programs that rewrite the partition tables!  Grub had (or maybe has) a command like that some years ago, and I wiped out a laptop the first time I tried it. (I did fix it, but it took a long time). The key to your project will be (in grub, anyway) that there's a keyword default that you can use to designate which partition you want to boot from.  I think just moving that keyword around will change what you boot.  You might  also think about booting from a thumb drive as your ""temporary"" boot partition. The happy news is that modifying the configuration (in grub, a text file) is a pretty safe operation. You can even save backup copies to fall back on.  And grub itself can be worked from a command line if you get into a sticky situation. The nature of booting means that the only things available at boot time are the boot program and its configuration.  Therefore, if you are going to modify anything, it will have to be somewhere in the bootloader.",1
"There is no particular problem with using the trigram technique I wrote about in Trigram Wildcard String Search in SQL Server with multiple columns, dynamic SQL, or 200 million records. The code below shows one way to extend the basic trigram search to work with three string columns in the same table. For brevity, it does not implement the trigger logic shown in my article to keep the trigrams synchronized with the underlying data. If this is required, the modifications needed follow much the same pattern. If n-gram search is not suitable for you, and you can tolerate some latency, an external solution like Elasticsearch might be worth looking into. The comments on that article show people using the basic technique with multiple columns, much larger tables, and even with encryption. The modifications needed are normally quite straightforward, assuming the person making the changes understands the basic underlying implementation shown in my article. Note though that trigram search requires at least a three-character substring to search for (as the name suggests). If you really need to perform two-character searches (as in the question example), more modifications would be needed, and you would need to carefully assess the costs and benefits with your data. Without further information, it would seem that a two-character match on hundreds of millions of rows would produce a great number of matches.",1
"Aaronson is working within a particular context, but if you take his statement in an absolute sense, you're right to be skeptical. However, the concerns of a complexity theorist preclude much worrying about such 'exponentially' constructable quantum states. In any 'quantum experiment,' if a state evolves from a particular pure quantum state in a set amount of time t, the expectation value of its observables will be very close to some state that can be constructed from a circuit with a polynomial number of gates. That is, the complexity theorist paradigm of 'experiments deal with polynomial time constructable states' is realistic for experimental physics in all contexts that I'm familiar with.  I have trouble understanding this statement. From the definition of PromiseBQP I don't see how PromiseBPP = PromiseBQP means an efficient classical algorithm to estimate the expectation value of any observable in quantum mechanics. Any ideas how this connection can be made? This is technically imprecise. PromiseBQP and BQP, as you no doubt understand, are concerned with sampling states that are constructed with polynomial-sized (uniform families of) circuits. And it's true that if you're talking about 'estimating the expectation value of any observable',  this seems to include ANY quantum state which may generally require an exponential number of gates to construct. This question is regarding The Equivalence of Searching and Sampling by Aaronson. In page 4 he makes the following statement,",2
"I have tried labelling /var/git/.ssh/authorized_keys as sshd_key_t, but this has no effect. Thanks in advance! I had a heck of a time debugging and figuring out the right context.  audit.log didn't show much.  I also double checked the /etc/selinux/targeted/contexts/files/file_contexts.homedirs and it didn't auto-create the context. I would like to connect a user account I have created for git in /var/git using authorized_keys. However, SELinux prevents this with the following AVC message I don't use Git so I could be wrong, but if I get your problem right, you probably created the user entirely by hand (i.e. editing /etc/passwd), not letting the system know that /var/git is a user home directory. It normally puts the relevant directories in /etc/selinux/targeted/contexts/files/file_contexts.homedirs. For instance, I have the same setup but with /Var/svn, added with useradd, and here's an excerpt of that file, added automagically: I'm new to SELinux administration, but I understand that this message states sshd is not allowed to read anything under /var. How can I relabel to allow it access to /var/git? (without putting sshd in permissive mode)",3
"But that leads me to my question: Why is there no attempt to build a smaller micro-Rj45 port which could be used in constrained spaces? While I bought my last Tablet I noticed that no Tablet (and most smaller notebooks /convertibles) has a Rj45 port. Which I finde quite dissatisfying since I like to use RJ45 in numerous places. Smaller ethernet ports actually do exist.  The XJack was a connector used on PCMCIA modems and network cards that folded up and retracted inside of the card when not in use.  Instead of using a housing that completely surrounded the RJ45 connector, the cable connected at an angle and the jack had just enough material to make an electrical connection and hold the cable in place.  It looked odd and was awkward to use at first, but it worked, and it didn't take up much space. There's actually a few elements to this. Firstly invariably, ethernet is an additional, add-on chip,  rather than something that's part of an SOC. Without a compelling reason for it manufacturers probably prefer to save a few dollars by leaving it out when not needed. In theory you could use an alternate port design, I seem to recall a zenbook model with a hinged full sized ethernet port, HDMI could do ethernet in theory, and there's a zenbook model that seems to use its displayport as a ethernet adaptor(For some reason asus experiments a lot with this sort of thing) I think a reason could be that Rj45 is simply too big for a Tablet - it is downright massive compared to micro USB /HDMI etc. Type II PC cards are only 5mm thick, so the XJack would have been no thicker than that.  That's plenty thin, even for today's thin devices.  I think the reason that tablets/phones lack ethernet ports is not a matter of the connector.  As other people have mentioned, it's the internal circuitry that device makers want to avoid.  The extra cost (and probably more important, power consumption) associated with adding ethernet isn't worth the benefit.  Using wired ethernet with a tablet is also contrary to their being marketed as ""mobile devices"", since you end up tethered to the wall. Tablet's main aim to acquire mobility. So, Wifi or 3G is the most suitable connectivity method for tablet devices. Tablet manufacturers may not interested in giving a solution for your problem because that is not average tablet user's demand. The above answers from Journeyman and U-map are correct that a general lack of demand combined with the size of the connector and, to a lesser extent, the cost of the Ethernet PHY chip are among the primary reasons for leaving these off of most tablets and thin laptops. However, especially where tablets and other devices with small batteries are concerned, power is also an issue. From my experiments on development kits, I've found that shutting down the Gigabit Ethernet PHY saves about a Watt. While that's not a big deal for most laptops, it's a much bigger deal with tablets (and especially phones.) Also, the fact that USB-attached Ethernet NICs are cheap and readily available for the relatively small subset of users that actually want to use wired Ethernet on their mobile devices makes the manufacturers be even less worried about leaving it off. I think there is a difference between Tablets and other types of computers like Desktops, Laptops and Servers.  Practically in most situations where you're using a small, ultraportable device, you probably would be using wifi rather than ethernet. If you absolutely must have it, there's always the old stand-by of using a usb ethernet adaptor. Its really an economic thing, more than anything else. There's not much demand for a non poweruser for ethernet in devices too small or slim for a full sized port, and a non standard port would not be likely to catch on",5
"In each static method I instantiate an instance of the DatabaseMySQL class. In each static method I set the correct static $table name to be used in the SQL queries dynmically by getting the name of the class and appending an s to it. So if my class was User, that would make the table name users.  However I am still not clear about a few things. I am posting my code here in a clear and concise way and hope that people can point out both the good practices and the bad ones. I will list all my questions at the end. My next file I have is my class that all my other classes inherit from.  I have called it MainModel. I have no idea if this follows convention or not. Let me start with a little introduction. I am in the process of learning OOP in PHP. I have also researched Design Patterns but have not yet fully grasped the different types of concepts. I am at the stage where every few months I realise that I am not doing things the correct way and have to change my style. This is so frustrating. Therefore I would like to find out the correct way of doing things once and for all. I have tried to fully read up on Stackoverflow about the following topics: In my constructer I have placed an optional array which can be used to insert some variables as properties of an object as it is created. This way everything is done dynamically which leaves me to the final stage of my project. My inheritance classes. This is my Database class file. This class allows me to create as many instances as I like of this class and it will reuse the already instantiated PDO object stored as a Static property of the class. It also fetches the data from the result set using PDO to get the data as objects of a specified class. To summarise this file. I use static methods like find_by_id($int) that generate objects of the called class dynamically. I am using Late Static Bindings to gain access to the name of the called class and I use the $stmt->fetchAll(PDO::FETCH_CLASS, $class_name) to instantiate these objects with the data from the database automatically converted into objects.  Please do not close as a duplicate, I have honestly searched through almost every question on the topic but I still want to know a few things which I have not been able to clarify. Sorry it is so long but I have tried to organize it so it should read well!",1
"Am I missing something?  If your goal is a ""digital dice"" hardware for use with physical games, then no pc is required.  On the other hand, if your goal is a PC game, it already has a perfectly good random number generator, and has no need of your hardware, and even However, I'd recommend using a micro controller for communication. Something like the Arduino. Many have USB connections and will allow you to interface with your existing circuit with a few changes. Arduino also has plenty of helpful tutorials. Additionally, a micro controller will allow you to easily expand to other types of control inputs and hardware outputs.  So, assuming this is just a cool science fair project, not a serious attempt to build hardware to be bought, sold, and used, I would think about packaging it as a USB device. Or you could connect a button and 6 diodes to the port and let the computer do the rolling and lighting. Parallel port is nice, but hard to find on a modern pc, and windows has made its programming rather difficult since XP or so. You could do an arms-length integration by using a web cam and image recognition to decide what number was rolled.  Get fancy and call it an ""optical air interface"" to make it sound official. FYI this is how those lava lamp random number generators work, so there is good precedent.  http://en.wikipedia.org/wiki/Lavarand You'll have to build an additional or different circuit to communicate with the PC. Something you make yourself would likely be over the Parallel port.",5
"The issue here seems to be that you don't have root access on this host to change the system startup scripts like /etc/rc.local.  The idea of using a @reboot line in your user crontab is worth exploring.  I haven't tried that as a normal user but it should work. The other idea is as you say to run something in your user crontab every few minutes, check if your process is running, and restart if not.  For example: I guess I could have a cron job run every minute, or 5 minutes, and have it somehow check if the process was already running (not sure how to do that). I am assuming you have no access to sudo. These leaves you with just one option, adding a script in crontab that checks if the process is running, and if it not, then start it up. You also gain a kind of availability enhancement (eg. if your process crashes, it is started up again). be very careful with this sort of thing - if you make a mistake you will end up starting an extra copy of your program every 5 minutes which could eventually cause a lot of problems. Depending on your distribution (you did not mention it), you have a pretty wide area of choice: chkconfig, rcconf, /etc/(rc.d/)rc.local, symlinking startup scripts in init.d\rc.d - but all these require root privilege.   Would that be the best way or is there another like .startup script in my home directory that I could put a script in?",3
"ZFS is awesome tech, and I highly recommend it. But you're going to need to revisit your structure here in order to be able to correctly use it. Namely having ZFS create the logical volumes (vdevs) from the disks directly. You should DIRECTLY attach all drives to a box running ZFS. Get a SAS HBA and connect the drives to the ZFS capable box (eg runing OmniOS or SmartOS). You can then share the space via NFS, SMB, iScsi ...  The reason ZFS on top of HW RAID logical volumes is a VERY BAD idea, is because ZFS requires block-level access to actually properly function. Yes, it will be usable, but functionality will not be complete until you attach drives directly to the OS via an HBA or direct SATA connections. One example is that in the configuration you're proposing ZFS cannot reasonably protect your data against changes to the data below (on the other side of the HW RAID controller), and as such cannot guarantee the safety of your data. This is one of the PRIMARY reasons ZFS is used, in addition to it being super duper fast. It sounds like there's a lot more reading you need to do on how ZFS operates before you can accurately understand what you've proposed it, contrast to what really should be done instead.",2
"Try selecting them by dragin the pointer around them, just like you select text and then press CTRL+C.. Go to Word, and Paste.. Should work. Right click and Copy might not work in all cases as described by John that they maybe behind some web-based authentication. Bring the web image and/or text in the web page to the center of the screen.  Press PrtSc.  Open Paint in Accessories and paste with Ctrl+v.  Select the required area and copy by either Ctrl+c or by right clicking.  Then go to your word document page and paste with Ctrl+v.  Your job's done.  That's it!! From time to time I want to copy and paste a portion of a web document (viewed in both IE Explorer 7 and 8) into MS Word 2007.  The selected text copies and pastes fine, but I am left with only place holders for the images (png).  Right clicking the image and clicking copy, then pasting into MS Word doesn't work either.  If I paste the image into MS Paint and copy it from there, I can paste it into the Word document.  What gives? The ability to copy text and pictures into Word from a web page depends on the browsers layout engine.  Today there only seems to be a choice between Trident, Webkit and Gecko and only Trident will enable you to do this.  I found that an earlier version of Word enabled pictures to be copied at the same time as text.  I think it was Word 2002.  Otherwise only the placeholders for pictures are copied and each picture has to be copied and pasted into it's respective placeholder. I found that other word processors like Open Office scramble the formatting of the original web page during the pasting operation. Another factor is the view you're using in Word.  Some views, like Draft, don't show images normally.  Try switching to Print Layout view. There are some options in Word 2007 that you should check.  For instance: Advanced > Show Document Content > Show Picture Placeholders.",5
"As far as I can tell, the most native way to shut down the database is using pg_ctl stop -m, where the -m takes one of three options: smart, fast and immediate. I would like to shut down the database as gracefully as possible - while I'm fine to kill of open idle connections, I'd rather not kill it in the middle of any in-progress transactions.  I'm hoping to find some kind of mechanism that allows me to block new connections, block any further transactions from existing connections, and allow existing transactions to complete. Does anyone have any suggestions for more gracefully shutting down PostgreSQL than killing off all active connections?  Are there SQL commands I can run before firing off pg_ctl stop? One of the issues we've run into is that when extending the filesystem using fsadm under a running PostgreSQL, it determines that the target is busy and won't umount the volume.  This is expected. I'm working on automating as much as possible the mechanics of resizing an EBS volume on which we store all databases.  We're going to host a large number of large-but-infrequently-used databases on a single EC2 instance, where the secondary EBS volume hosts PGDATA (aka all the data files), whose configuration we've embedded in this script here.",1
"When a Linux box gets an ATA error, it syslogs it with a message identifying the disk as ""ata%d.00"". How do I translate that to a device name (e.g. /dev/sdb)? I feel like this should be trivial, but I cannot figure it out. The ${x} in host${x} refers to that first number in the [0:0:0:0]. So for me ata1 refers to host0 which can also be represented in SCSI form as 0:*: Since I use SATA, and only one drive is on each port I can deduce that ata1.00 = sda.  All of my drives are .00, I suspect that if I used a port multiplier, my drives would be given .01, .02, .03 etc.  Looking at other people's logs PATA controllers use .00 and .01 for master and slave, and based on their logs if you have ataX.01, the .01 should be mapped to the ""ID"" in the host:channel:ID:LUN folder from the /sys/dev/block/ listing. If you have multiple ataX/ and hostY/ folders in the same PCI device folder, then I suspect that the lowest numbered ataX folder matches the lowest numbered hostY folder. This is actually quite tricky. While it's safe to assume that ""the scsi ID"" is ""the SATA ID minus one"", I prefer to be really safe and inspect the unique_id which I assume (based on this post) is the SATA identifier. Depending on your system, it might be divined by wandering around sysfs.  On my system ls -l /sys/dev/block reveals that 8:0 (major:minor from /dev entry) points to /sys/devices/pci0000:00/0000:00:1f.2/host0/target0:0:0/0:0:0:0/block/sda  Likewise, ls -l /sys/class/ata_port reveals that ata1 points to /sys/devices/pci0000:00/0000:00:1f.2/ata1/ata_port/ata1 which is on the same PCI sub-device. The easiest way is to review the kernel log from boot, since the drive device names are mixed in from various sources (eg USB drives), or are assigned based on type of device (ie cdrom may be scdX instead, and everything has a sgX).  In practice, unless you have mixed different kinds of buses (eg SATA+USB) the lowest numbered ata device is going to be sda unless it's a cdrom device. So in one line per drive you have sdX device name, size, model, s/n and the pci and ata numbers. The sdc above coresponds to a USB SD card reader with no card inserted. Hence the ---- in place of real information.",5
"As you have access to a Windows system, download HP Library and Tape Tools. L&TT can run a wide variety of tests and report things which may be set up incorrectly. Tell us about your actual hardware setup. Looking at a local HP Ultrium 960 drive, I'm getting write speeds of ~2,000 Megabytes/minute (33Mb/s) across a mixed data set (with hardware compression ON). This is with an HP ProLiant DL380 G6 server and an HP-branded LSI Ultra 320 SCSI HBA.  3Mb/s is slow so there probably is a hardware problem as the other answers suggest. However, even when this is fixed, from a single server (depending on hardware and what you are running) you might have trouble writing to the tape fast enough to prevent shoeshining. Shoe-shining refers to the tape drive stopping and rewinding due to an empty data buffer or inconsistent incoming data stream. This was a problem with older DLT drives. LTO drives shouldn't experience shoeshining. The LTO format/standard was partially designed to eliminate this behavior. HP drives, in particular, have a variable write speed to help reduce this effect. The Ultrium 960 should shift down to 27Mb/s as a minimum streaming speed. One thing to try is using a cleaning tape and see if this helps. Otherwise, I am guessing that the write head is broken, creating many write errors that get detected by the verify-after-write functionality.",4
"Personally, I find OpenVPN to be the perfect tool for things like this.  It's very easy to setup and configure, runs well and Linux and Windows, and it's open source.  Hard to go wrong.  Also, I've used it heavily at multiple companies for production work, and had great success with it. You could use the Service Bus in Azure .NET Services to securely relay your messages between your cloud instance and your intranet web services.  We're using this to call internal web services from Windows Azure.  The service is obviously much easier to use if you're using .NET web services (using built in libraries), but the service is supposed to be vendor agnostic via a REST API. I'd setup a VPN between your Amazon EC2 instance and your local network.  That will allow you to keep all of your communications private and secure, while also allowing the EC2 instance to access devices behind the firewall. Amazon just launched Virtual Private Cloud service which enables you to connect your existing infrastructure to a set of isolated AWS compute resources via an IPsec connection.",3
"Also, you can try to use bootrec tool (included on windows, and use with other windows setup) with the option RebuildBCD to try to rebuild. Another way is using EasyBCD. I never tryed these two before, so use with caution and always make a backup of your stuff before doing anything! So for helping you making it work, if you have any way to resize the partition down, you can install another windows in parallel. This SHOULD make windows detect this Windows 2008 installed on another disk and create a menu entry for it on the new windows. So it would work on the same disk. After that you can edit the bootmanager menu (Open msconfig using execute on  Start Menu) and remove the new windows entry. So you would have a direct boot for your Windows 2008 without the other HDD. If its only the MBR that is missing, the Windows 7 Master Boot Record recovery would work fine as GRUB does. On Windows 2008 (As up of Windows XP) sometimes when you install, it creates something like ""boot partition"" where Windows put all boot related stuff on it. If that is the case, you may have a problem because no aditional space will be available, and it may not work with grub.",1
"In comparison, when we explicitly disable TLSv1.2 and TLSv1.1 in our client, thus making TLSv1 the preferred option, handshake succeeds. Question: is it the server's duty to choose out of the list of TLS/SSL protocols the client supports? Can I say to the server maintainers that it is the server that misbehaves, not our client? However, straightforward connection fails with the server terminating handshake. Debug output we get (on the client side) looks like this: The server (Oracle-HTTP-Server 11.1.1.7), however, only supports TLSv1 and SSLx. Additionally, it has a very limited selection of supported cipher suites, but there is still one suite in common that we can use with TLSv1: SSL_RSA_WITH_3DES_EDE_CBC_SHA (a.k.a. TLS_RSA_WITH_3DES_EDE_CBC_SHA). To me, it looks like the server doesn't even try to fall back to TLSv1: as soon as the client declares TLSv1.2 as preferred, the server bails out. We have to connect with our application (written in Java 8) to some very old server using HTTPS. Being Java 8, our client supports TLSv1.2, TLSv1.1, TLSv1 and SSLv3. Of course, it prefers to use TLSv1.2, as it's the newest protocol. We also tried to connect to another legacy server that doesn't know about TLSv1.2 and TLSv1.1, but here connection works as expected: even though the client says it prefers TLSv1.2, they agree with the server to use TLSv1, as the best common protocol:",1
"I ran into quite a few other errors (A plethora of ""Invalid Credentials,"" ""Access Denied,"" and other access related errors). I will update this later to cover them as well. This is why I added the LDAP group on my server and the obMemberOf attribute. I then used it on the sssd client as my ldap_access_filter (i.e., anyone who has attribute obMemberOf set to the DN for the development group has access to the system. I wonder whether you can configure sssd to use a flexible ldap filter and lookup different (non-default) ldap attributes in this case. Running 'cacertdir_rehash /etc/openldap/cacerts/' seemed to fix things. It created a symlink (the certificates hash with numeric suffix) that points to iRedMail_CA.pem I have a few servers set up and I'd like to centralize access for users using LDAP. I have my primary server that is hosting email using iRedMail, and there already happens to be an LDAP database that was set up with iRedMail. Now, I'd like for my user accounts to be tied to their email accounts (for example, changing their email password also changes their password on servers they have access to). I've done some searching (DuckDuckGoing?) for how to use iRedMail's LDAP database as a user authentication database for UNIX accounts, but I've yet to find anything remotely helpful. Anyone ever done this have any tips? If you modified iRedMail LDAP schema file, you should pay attention to sync this schema with upstream.",2
"You will probably have to change your critical value to something other than 5 to get the same kind of coverage. According to Wikipedia, you'll want the new critical value to be $5\sqrt{\frac{\pi}{2}}$ if your data are iid Gaussian.  The estimator for the mean would be the truncated mean, and the Wikipedia page for trimmed estimators mentions how to get a decent estimator for the standard deviation from the interquartile range. How can i fix this problem? Is there another method that i can use to identify thresholds without this issue? An alternative that might be more difficult to implement, but is probably more statistically appropriate, is to use trimmed estimators for the mean and standard deviation. With trimmed estimators, you throw away the most extreme values in your data (the proportion of which is specified beforehand), and estimate your statistics on the remaining data.  Instead of mean and standard deviation, you could estimate the median and mean absolute deviation. The median is immune to outliers, and the MAD should be at least more robust than the standard deviation formula.  What doesn't work with this approach is that if I have more than one anomalous value in a day, they are not detected. This because error, mean and standard deviation are too much influenced by anomalous values.",2
"This is a micro-optimization. The point is that there's no reason to do it, in anything resembling normal operation. It could easily hurt you if your usage pattern changes. XP can only see 3GB of memory. When my machine already has that, what's the point of a page file? It's reserving disk memory which it can never access. Unless it disables some RAM. So you've spent your hard-earned, on top dollar memory, only to find that XP is dating disk instead. This might not bother you if you're not experiencing BSODs, and you can certainly recreate the pagefile quickly enough, but why not be prepared? I run 32 bit XP with 4 GB memory (I know, XP only sees 3GB of that but it's interleaved and very fast) and would like to make two points: Run a few progs, Word, Excel, Chrome, IE Ex, watch a movie, stream some music or TV. You'll soon get a message that you are running out of virtual memory. This proves that XP uses the pagefile even when it doesn't need to. Check the task manager performance tab (alt ctrl del). Peak will be well below total physical memory, so why is XP using the pagefile? And why does XP think you're running out of memory? I believe that if you don't have a pagefile, then in case of a BSOD, Windows won't be able to write the minidump. This means that you won't be able to analyze the problem by using the appropriate tools. The pagefile is an anachronism from the days of DOS and Windows 98. If you've got enough memory, get rid of it.",3
"I would say the variable 'spent' is your target variable. I'd suggest you to analyse the distribution of each feature and target to check if, eventually a linear regression could be appropriate. What I would suggest you to do, howeveer, is to concentrate on the distributions of parameters to check for eventual unbalances and make groups to make estimation robust. You might try different other methods however, like a decision tree regression, which does not require any assumptions as it doesn't estimate parameters. I would approach this problem in a way that you try to predict what specific ads are bringing more conversions. The ratio between the money spent and the total_conversion or approved_conversion would be your target variable. Assuming spent is time spent on the page, I'd say that approved_conversion is your target variable. Your customer does something (e.g. buy something, register for a newsletter...) on the site (1) or doesn't (0); and one simple classification algorithm to predict that value (0 or 1 with a certain probability) is Logistic Regression. See ISLR Book Page 131.  The target variable could be spent but if you want to optimize what you're spending I think that only make sense when you put it in relation with what you're trying to achieve which I guess are customer conversions.  If your model's output is an ad that people might like I would say ad_id is your target variable and you're building a recommendation system. For example, if Person A likes item Ad1, and Person B shows similar feature values (age, gender, etc.), then you could recommend Person B the item Ad1.",4
"Is there anything better than VNC server for OS X? Also I need something that has a client (viewer) for Windows machines.  The reason VNC is so slow is because it does little more than stream a screencast of your desktop and forward mouse and keyboard commands. RDP and NX both send instructions on how to render the display (kinda like SVG vs. JPG--JPG is, in fact the format that VNC uses to send the screenshots). I think TeamViewer does the same process, though I don't know for sure. The only drawback is that on free licenses it limits your connection time if you use it too much. I don't have this problem because we have a premium license. You can use it on cross platforms (with feature limitations of course). An old SuperUser thread pointed  me towards the $80 iRapp, which seems to work rather well but is rather expensive (well, not by Mac users' standards, but...). It's an RDP server for Mac OS. I love Logmein for this as I have access anywhere and I don't need to worry about forwarding ports.  Might seem like a bit much if you are only accessing it over a LAN, but still usable. VNC just feels sluggish and inefficient, and occasionally has display glitches. On windows, I can use Remote Desktop (RDP / MSTSC), and on Linux I can use NX Server, both of which are much better performing than VNC.",4
"Server security is a multifaceted issue. Since many processes on your server CAN be compromised in some way, it is important to run your updates, eliminate unused services and then do security on what is left. You can have a rock solid apache, mysql, ssh configuration, but if you are running a Wordpress server that has security holes, you are going to get hacked - repetitively. You can grep through looking for exec commands, base64_decode, but if you don't understand coding, then you may not see where you are vulnerable.  fail2ban may be a good starting point since it will teach you what to look for in log files to know if you are getting hacked. Good security takes a serious amount of effort and simply copying commands from the internet without understanding them won't help you.  If you must run insecure apps because you are a startup and simply can't afford the alternative, make sure you sandbox your environment by utilizing separate VPS servers and make frequent backups. Then practice your restore process till you know you can get everything back up and running.",2
"If you're not connecting to your back-end server using SSL, it may fail to detect you're actually using SSL and be configured to force you to go to SSL (hence the infinite redirects). You may want to look at similar mechanisms as what Jetty does with its forwarded option (to be able to tell the back-end it's behind a proxy). Rails might be able to interpret the X-Forwarded-Proto by default. In this case, add this to your Apache config (in the SSL virtual host): If the backend server uses out-of-date self signed certificate, one more option is needed (if there is no access to the backend server): Alternatively, if you really want to connect your reverse proxy to the back-end server using SSL as well (mostly useful when that server isn't where the Apache Httpd reverse proxy is), in addition to https://backend-server-address, use the SSLProxy* directives to set up the CA certs, as documented in the introduction to the mod_proxy documentation. In particular, you would need to configure SSLProxyCACertificateFile and make sure the certificate of the backend server is issued to the right host name as seen by Apache Httpd (i.e. localhost in your case). Don't bother with HTTPS between your front-end and your back-end server, there's little point in enabling SSL for a localhost connection. To add to vbartoni's answer, it seems that from Apache 2.4 and up, there are different defaults and a new directive. I had a similar problem (the same error logs), except that the proxy was forwarding https traffic to another host.",4
"2) Summed over all data images to build a histogram over the pixels - I counted the number of times each pixel gets a 1 value in the dataset this question might seem a bit odd. I was doing some self-studies into information theory and decided to do some more formal investigations into deep learning. Please bear with me as I try to explain. Even if this is a very ""back-of-the-envelope"" calculation, shouldn't a neural network capable of learning the labelling be at least in the ballpark of 95 neurons? Why do we need, for instance, a neural network with 21840 parameters to get 99% accuracy? (considering the one in PyTorch's example for MNIST: https://github.com/pytorch/examples/blob/master/mnist/main.py) 7) So, as a rough estimate (and with care) we could say we would need a neural network of 95 neurons in order to be able to encode the labeling of this MNIST training set (190/2) 6) According to David MacKay in his Information Theory book, we could interpret a neural network as a noisy channel and consider each neuron as having a 2 bit capacity. Although he does state to use this idea with care. Chapter 40 of his book http://www.inference.org.uk/itila/book.html)",1
"The example above is just a bit of a simplification, I know for the above situation I could just use GetButton instead of GetButtonDown, but for a fighting game, you can imagine that the necessary inputs to buffer would get significantly more complicated. Is there a way to store inputs for a few frames, so that I can read a button press a few frames back? So, instead of using Input.GetButtonDown(""attack""), it'd be something like InputBuffer.KeyBuffered(""attack"",12) to see if the button was pressed within 12 frames? Basically, if the fighter is in the neutral state, pressing attack should lead into an attack. It uses Input.GetButtonDown(""attack"") to get the button, and if it detects a press, transitions into attacking state. Now, let's suppose the fighter has just landed, and ends up hitting ""attack"" a few frames before the land animation ends and he transitions back into neutral. As of right now, nothing would happen, since he's not in the neutral state where the check is, and when he enters the state, the button press is no longer happening. When the time comes for you to act on the input, you can simply pop from the queue and proceed according to what you find. This assumes that you care about every input (and the order in which they arrived). If you only need to consider the first input made during the attack, you don't even need a queue/buffer. As suggested in the comments, you can buffer frame inputs for when you're ready to process them. In your update method: So, I've got a fighting game with some pretty tight input windows, and I'd like to buffer the inputs for a few frames.",2
"That's my development server, though, which is under some load.  I'd expect higher numbers if the array was idle, like I suspect yours is.   http://www.1337admin.org/windows-server/windows-server-2012-r2/dell-h200-raid-controller-speed-hack/ Just for some perspective, I don't exactly know if your buffered reads are ""slow"" but I put mine up for comparison. On a PERC H200 card, caching is forced to be disabled for all physical disks configured into a virtual disk, regardless of the drive type and default drive settings. The default cache policy on a physical disk is Enabled in SATA drives and Disabled on SAS drives. When physical disk caching is Enabled, disk I/O performance is improved, but a power outage or equipment failure might result in data loss or corruption.  Enable cache directly from the firmware. YOU MUST be running on a battery backup or if power goes out you could be hosed by enabling the cache. Below is a tutorial explaining the full process to do so. If the server has battery backup you can enable basic drive caching on the Logical Volumes without risk. I have 4 1TB Western Digital Enterprise 7200 RPM drives on a 3Ware 95500-SX and my hdparm results are The long answer: The H200 is the old SAS 6iR with SATA 6Gb/s support. It doesn't have the usual features you'd see on a RAID card (battery backup unit, onboard caching, RAID5/6 support). The cache determines how fast your RAID array is (along w/ the # of spindles and type of drive), so no cache = slow performance. Add the 7200RPM SATA drives (which are slow compared to a 10-15k RPM SAS drive) and that's the level of performance you can expect.",5
"First one (windows container) is a classic container which shares the same MS Windows Server kernel and not very secure. However second (hyper-v container) uses hypervisor to provide isolation and security. Is it going to be possible to run Linux (Ubuntu, Debian, CoreOS, etc. ?) containers using the Hyper-V Container technology ? Using Technical Preview 5 (TP5) of Windows Server 2016 now, the general availability (GA) is going to be any day soon as announced at Ignite conference few days back. I cannot find a definitive answer anywhere (lots of contradicting information and very general). With the new version of MS Windows Server 2016 it's going to be possible to run containers in two modes: I have searched everywhere but cannot find an container image with Linux for Hyper-V Containers. Only two available which I'm able to find are Nano Server and Windows Server 2016 which are provided by Microsoft.  Sine release of Windows 10 Anniversary Update few months back Microsoft has included Windows Subsystem for Linux.",1
"It's probably also worthwhile making sure that all of your PHP scripts are chmod 555 and directories are not owned by the same user as the files.  This will allow CGI to execute the scripts but will mean that if a flaw is ever found in one of the PHP scripts it won't be able to modify itself or any of the other scripts and it won't be able to create a new script in any of the directories.  There are still plenty of things an attacker could do if they found a flaw in a PHP script but anything you can do to make it harder for them is worthwhile. If you want each site to be running as it's own user, the correct PHP 5 Handler setting is 'suPHP' rather than 'CGI'. Once you change this you should see that your whoami reports the individual users.  If you change the owner of the files in your document root to be ""apache"" or ""www-data"" (whichever one your server uses) then that same command will return the new owner of the file. Apache itself is most probably running as the apache user but thanks to SuExec, PHP scripts that are owned by root will run as root.  That is why your whoami command returns the root user. Note that you may have to run EasyApache and select ""Mod SuPHP"" during that process to recompile Apache / PHP for this option to be available first. This is done under:",2
"Have to wonder - why so much of your equipment is dying - environmental?  Electrical(surge?) accident (drops?) or other?  Your solution would have to incorporate protection against this. So now I'm asking you for a recommendation of a portable solution (max size is 2,5"" hdd) that would be (nearly) 100% reliable. Which will not break. At least not easily. 10GB storage is enough.  and rigorous in regards to the what and the when and the where, stick to it; and you've got something that's kept me (a klutz of dimensions) knowing that my that is safe... - 8 gig with all that data on USB-stick, hash and double/triple-check at will; one kept at each parent's house (this is of course something one has to re-adjust as one sees fit, in case no-one is to be trusted, two separate bank vaults would be my guess...hardcore though...) At one point in time I've moved all of my data (documents, programs, etc.) into one 8GB encrypted file container (TrueCrypt) and moved this file onto one of the most expensive pendrives in its range (the 16GB Voyager GT mentioned above) hoping it will prove to be 100% reliable solution. Unfortunately for me, it broke. You mention having to sync 8 gigs of data each day - is all of that data really changing every day? Could you perhaps have separate encrypted volumes for stuff that rarely changes (no need to sync every day, perhaps every week or month) and stuff that changes daily? I don't think you will find something realistic that is more reliable than a USB drive, so in order to increase your reliability you have to implement some form of redundancy.  Be ultra-prudent (parents do not live in same town as me, or each other (again....adjust as see fit)) Also, have you considered a remote solution - RDP into a system at home or possibly keeping things on the cloud?  There are things like Microsoft Live Mesh.",4
"All of that aside, you need to remember that even if you can get signal to your clients, your clients' signal still needs to get back to your access point, which will be much more problematic of a problem to solve than we cover in the first paragraph. If you only had a single client, you could install some directional, high-gain antennas to solve the problem, but that doesn't scale beyond a very small number of clients sharing a very small physical space. You're fighting physics here, and you're not going to win. Various regulatory bodies limit the Tx power of wifi gear, so there's not a whole lot of room for improvement there. Remember that RF power drops off according to the Inverse-Square Law, so even if you were able to double the Tx power of your AP, it's going to make very, very little difference in the RF as received by your clients. And that's ignoring the fact that it's going through stone, not clear air. So, what's the answer? Install cabling. You've tried power-line networking, and have found out what nearly everyone else does: it's a nice toy, but is horribly unreliable. You're just going to need to bite the bullet and install cabling from room to room.",1
"Second, external fans are nowhere near as thermodynamically efficient as internal ones where a laptop is concerned.  If your system is already hot, you're not going to see a 90/10 -> 10/90 reversal.   The heat is originating inside the machine from the power supply or battery, and you're not likely going to offset this very much by applying cooling externally. If you are in a very hot area where the air is hot, cooling the air in the entire room will likely make more of a difference than applying a cooling pad. As you can imagine, keeping the laptop on a surface such as legs or a bed or blanket will insulate the device causing air to not flow and increasing heat build up. As you can imagine even placing a solid surface between the laptop and the blanket will improve airflow and cooling, and a cooling pad could be useful here as well. If you're talking about a purely heat-conduction-based cooling pad, then this is even more the case, although obviously the first issue won't apply. Cooling pads are more for you and your legs than for the internals of the laptop. Unless the cooling from the pad is penetrating enough to lower the temperature sensors inside your computer then it's not going to change the speed of your internal fans. The closest a cooling pad would come to making a difference would probably be on a model like the MacBook Pro or MacBook Air where the aluminium case is part of the heat dissipation design. Though even then you'll only be able to do so much to pull extra heat off of the metal faster. First, I assume you're referring to a pad with built-in fans, in which case those fans are going to be making noise anyway. I don't have any problems with performance, but I was wondering if employing a laptop pad could drastically cut down on the spinning of the internal fans. I like complete silence, so I'd buy a laptop pad if it could, for example, reverse the above ratio, so the computer would be silent in 90% of the time and the fans would spin only in 10%. I have a laptop with a Core 2 Duo processor and the cooling fans are spinning most of the time. I'd say the computer is silent in 10% of the time and the fans are working in 90% of it. The cooling pad would be likely to reduce the fan-spinning, but with no air circulating in the tight spaces of a laptop, you can't expect the fans to stay off very long--that Intel chip is generating lots of heat (depending on which one you have, up to 35 watts) and all that energy has to go somewhere.  The fans push it out of the case.  Conduction with the cooling pad can't cool any part of the laptop except the very bottom, because there isn't room for air to circulate naturally.",4
"Lastly, the Smart Card table will have multiple smart card entries that I create, and I need to be able to assign 1 or more to a company, then, the company administrator can assign one or more of these to a user belonging to that company. And just to confirm, this will be a back end SQL database on my server, accessed by online web app AND end user desktop applications written in C# That user (company administrator) can now log into their company account online, and register multiple users (in their company). I am designing a back end database that will handle registration and login. I need to be able to deal with different account and authentication types. Just to clarify my needs. When a user registers the company, they become a company administrator, but they could also set up other administrators and/or users, so that why I have them all in different tables. I have added further tables as each user must have one or more forms of ID checked, along with being provided one or more access codes that will provide a range of access levels (physical and computer resource based) The issues I have are below, and hopefully my design solves these but I am not sure if I am on the right track. Imagine a user registers their company online (on my website), the company details go into my company table, and their details go into my user table. Each user could have either a password, or a smart card to log in. I have the relationship for password and smart card tables but this means there will be many null objects. Either a user will have a password or a smart card. The main issue here, that is different from other database design for user login is that a user could have a password to log in, a smart card, both or a combination of other, and I am unsure about null values etc.",1
"I bought a new hp laptop from e-commerce site ""Snapdeal"". Model is ""HP 15 ba025au"". It has AMD Raedon R4 integrated graphics. Since it had free DOS pre installed, I immediately installed Windows 10 pro 64 bit. I had already downloaded drivers from HP site. So after W10 installation, I installed some drivers such as AMD HD graphics, Realtech sound, Realtech wireless LAN. Everything was going fine then I tried to lower the screen brightness, I tried from hotkeys i.e. f7. But nothing happened, the screen brightness didn't go down. I tried from power control option. It shows the value like 25%, 50%.. but brightness remains constant. I googled the issue 'n find one solution that is to update the driver under ""display adapter"" under ""device manager"". But when I went to ""display adapter"", I found  my amd graphics driver is not listed there. I can only find ""Microsoft basic display adapter"". Then I realized my graphics driver was not working. I reinstalled it but the problem persisted. Then i read somewhere graphics driver doesn't work if graphics card is not there. But my notebook has the aforesaid graphics card. Then why it is not listed in device manager? How can I fix this issue so that my brightness control works properly. Please help me. Thanks in advance. If there are further issues with the display after changing the driver, then it may be possible that your graphics card has an issue, which is why the driver may have defaulted to the Microsoft one.",2
"As a note, I used paragraphs for all the work, but it is possible that you intend to use a manual return, or new line, between the quoted and unquoted sections of text. In that case, where you want to add a new line use ^l (small letter ell) rather than ^p in the 'Replace with' box, and if you need to find a new line, use ^11 (eleven) in the 'Find what' box. Next is a decision point for you. Some of the quoted text can be at the beginning of the paragraph, the end of the paragraph, or inside the paragraph. If you don't mind looking for extra lines later you can do it all at once by using On the Word ribbon, click on Find and select Advanced Find.... In the dialog box click on the More button which makes the dialog box larger, with more options. Check the box for ""Use wildcards"". This will allow you to use Microsoft's version of regular expressions. If you want to know more about the power of wildcard searches in Word, you can see the Microsoft MVP's page Finding and replacing characters using wildcards. The final point is that in Word for search and replace the paragraph symbol  cannot be used, but they give you ^p as a replacement in the 'Replace with' box. That is the carat ^ (Shift+6) and a small letter p. In the 'Find what' box you have to use the numeric equivalent, which is ^13. The title, with ""~~~"", is unclear. I think what you want to do is find strings of quoted text and place each one in its own paragraph, separate from what might be before it or after it. A regular search and replace is not up to handling the task. Regular expressions, however, can do the job in a snap. While it is not nearly as good (powerful) as what most are used to, Microsoft does have a complex system of wildcard expressions that look like, sort of, regular expressions. Your objective is easily within the power of that system. That will match any one character, like the ? wildcard does, which is not a double quote. To make it match more we use the count control { and }. It can either be used to indicate how many to match, {4} match four characters, or set a range {3,10} match at least three, and as many as possible up to ten. If the maximum is left off the range it acts like the * and matches as many as it can, {5,} match five or more. For this case we need to match at least one, and as many as we can, so it is {1,} In this case you want a double quote followed by anything that is not a double quote and then another double quote. The tricky part is the text between the quotes. the symbols [ and ] form a group of characters to match, but it only matches one character. We could use a range like [A-Za-z] for all capital and lowercase letters, but there could be other characters, such as numbers and commas as well. Instead we can use the not symbol ! and the double quote to match any character that is not a double quote. Like this, [!""]. Put all together, the pattern we want is ""[!""]{1,}"". The last part is to make it so that we can use what was found in the search as part of the replacement text. To do that we add ( and ) around what we want to keep, like (""[!""]{1,}"") and then we can use it in the replacement part as \1, meaning 'the first group found in the search pattern.' There can be more groups, but we don't need them for now. for the entire text of the file. But I'm not sure are there any good strategies for this line change for ""~~~"" contents using find and replace, or any other method.  On other possibility, if you know that you do not have any blank paragraphs, is to use the first option and then find and remove the extra blank paragraphs: On the other hand, if you want to avoid the work of removing extra blank lines, and are willing to use multiple (3) replacement commands, you can search for each of the possibilities with the appropriate replacements. Problem is, there are lots of parts, where no space or no dots before or after """". This makes it hard to use find and replace option for this work...",2
"Before installing all these products, I suggest that you uninstall any doubtful codecs package that you may have already installed. After having verified that .mov files do work correctly, you can afford to install a well-known package such as the latest version of K-Lite Codec Pack. The only video player that's guaranteed to play .mov videos correctly is Apple's QuickTime. You can download it here (be sure not to download the version with iTunes). The .mov videos are in a proprietary Apple format whose secrets are jealously guarded by Apple. Many Apple codecs have non-Apple implementations which are pretty good, but not perfect, which is probably the problem you're encountering. You can either use the QuickTime player to watch your .mov videos, or you can still continue to use Windows Media Player. But in this case you'll need a product that makes available to WMP all the QuickTime codecs currently installed. A free product that does that is available here : QuickTime DirectShow Source Filter.",1
"In this case, you could have a method countPrimesUnder that would iterate over the data structure and count the primes.   The only even prime is 2.  If we eliminate that first, then we can check for numbers less than 2 or divisible by 2 at the same time.   If you can multiply two factors to get a product, at least one of the factors must be less than or equal to the square root of the number.  You can see this by trying to multiply two numbers greater than the square root together.  If everything is positive, the product will be greater than the target number.  So once we've checked the square root, we don't need to keep going.  If there was a pair, we'd already have found half of it.   Putting comments on separate lines makes it easier to see that they exist when quickly scanning the code.  And in this case, it avoids unnecessary scrolling.   We could tweak the isPrime method a little more and likely make it a little faster.  However, we have other options that don't require isPrime at all.   If you know the maximum number that you have to check and need to check many numbers, consider using the Sieve of Eratosthenes or similar.  That's the most efficient way to find all the primes in a range.  So you'd first find all the primes and then just query a data structure thereafter.   We don't need the prim variable, as we can return false as soon as we find a reason.  If we get to the end, we can return true.  This has the same effect as your code with less scaffolding.",1
"I've had mixed success with it - some applications work fine, and report 256 colors; however Emacs (the main reason I wanted 256 colors on TTY) doesn't seem to work; there's a bit of a discussion about it here, but it doesn't really get resolved: If you find a TERM that works, modify the getty lines in your inittab and add the proper TERM at the end: Your 256-color RXVT/xterm/URXVT/etc are defined by terminfo and the setting of the TERM variable.  In most Linux distributions your virtual terminals are defined in /etc/inittab, and use a default terminal type that defines how many colors the terminal can use.   I've installed this and use it for console apps that require 256 colors (vim due to my configuration).  From memory these sequences look something like escape [ 01;34m  There are codes in the 30's for foreground and I think 40's for background. Note the baud_rate,...  port [term] there at the end.  That matches up with the command in the /etc/inittab line (/sbin/getty 38400 ttyN) -- note there's no [term] argument, so we get a default (run echo $TERM just after you log in on a virtual console to see what your system uses).  From the manpage again: The ANSI escape sequences for setting the foreground and background colors of text are what's limiting things here. Have a look at fbterm - it's a replacement framebuffer that can run in 256-color mode (you just run it as root from a TTY) kmscon is a way to get you 256 colors and video without having a windows system (X, Wayland) installed.",4
"My version of scvmm does not list ""Legacy Adapter"" as  a network cary type, however, so I can't get it to work without sideloading a driver after install. http://www.microsofttranslator.com/bv.aspx?ref=Internal&from=ru&to=en&a=http://blogs.technet.com/b/abeshkov/archive/2011/03/17/hyperv_5f00_debian.aspx I discourage anyone using hyper-v modules from mainline kernel in older kernels. The notable exception are the MS-provided ones for RHEL and SLES which are backports of later versions and work quite well. I've been able to have some moderate success with network drivers, but only when the vm was created on a hyperv console, not scvmm.  In HyperV, selecting ""Legacy Adapter"" for network adapter type works.. I was able to get tcp connectivity to my vm.  I did not stress the connectivity, or test it's stability, but it worked out of the box.  I found a link on the web that gave me this method (currently uncited, but I'm sure you can find it..) In order to get Hyper-V modules install on fresh Debian 6 in Hyper-V you need to compile your own kernel. This is how i did it Just as reminder: I have been playing with the Hyper-V modules for linux starting 2.6.33 and had freakingly bad results with hv module back these days. If you are using a mainline kernel, you get better results starting 2.6.39 where lots of changes whent in to get them quite stable.",3
"Also, while I have this question here, is storing layered maps in 3D arrays a good way to go about things? Meaning the first level is an array of layers, the second is an array of tile columns within the layer, and the third is the array of tile rows within the column. To answer your ""bonus"" question: Yes a 3D array is acceptable until some other requirement comes along to contradict that. Although if you go down this route, be sure you are storing a proper 3D array ([,,]) not arrays of arrays ([][][]). So in your case, if you are sure that a single ""Fringe"" layer will not be enough, the next solution - without considering any other requirements - is to support any number of fringe layers. I'm creating a highly flexible 2D tile-based map editor in XNA to make some of my future projects easier (will work for side-scrollers and top-down games). This editor can work with any sized tile. I'll be working with fairly detailed tilesets and will want enough Fringe layers to be able to render a wide variety of fairly complex scenes (e.g., forests with lots of trees overlapping).  There is a concept in software architecture called the ""Zero, One, Infinity Rule"". This basically states that you should support either zero of something, one of something, or any number of somethings. You should (almost) never write an application that supports (for example) 3 of something. This editor is based on a standard system. It has pre-determined layers such as Base, Fringe, and Collision. However, I've run into a bit of a problem. I don't know how many Fringe layers to use. On a side note, in terms of memory layout, you can either use single or double arrays for the individual layers, then have a list or an array holding that, making a double/triple array.  Well what you should do is not worry about that and design it to be flexible. Perhaps even allow the users to choose how many they want. But in reality, don't even bother until you have made your game or at least some sort of design doc. That way you can know what you actually need and can design around that. Like many have said, don't design engines, design games.  One other important thing to note - one place where YAGNI may be worth ignoring is with your file format. If you choose to have just one layer now - but think you might add more layers later - be sure that your data is stored in such a way that you can store multiple layers without breaking old files. I'm also considering having multiple Base layers to enable things like a player walking both over and under a bridge. So my question is this: Is there a ""right way"" to approach this situation, and if so, what is it? Just lots of Fringe and Base layers? If you were a map editor, how would you expect/want this situation to be approached? Of course, part of the problem is that you're committing a fairly major game development sin by creating technology for a game that doesn't exist. So any requirements are purely speculation. It is almost inevitable that you will over-develop some areas and under-develop others.",3
"Usually, you set some threshold (e.g. I want to redistribute 85% if all energy) and before processing the next Shooter patch, you do a simple check for the running total of the distributed energy (and quit the loop). This is usually few orders of magnitude faster (for a very minor implementation cost) than the reference brute-force method. Also, contrary to popular misconception, you do not have to distribute all energy to get great results. Look into Progressive Refinement method, which only processes the largest emitors, thus allowing to converge to 'close enough' solution much sooner. Of course, to get true benefits of Radiosity (e.g. color bleeding) it's best to redistribute as much as possible (given the resources available). Radiosity, by definition, handles only the diffuse component. You cannot 'limit' Radiosity to diffuse, because it already is handling just that diffuse component (remember - the diffuse lighting is just one (albeit popular) application of the energy distribution).",1
"What are you testing the link to link endpoints for speed? I mean, are all your links you're testing between 1 Gbit? You can only get 1 Gbit between two or more machines with gigabit network cards. The router will never go above 100 Mb since you said that's what it at most supports (so your wireless connections will be stuck going through a straw connecting to a firehose anyway). It is possible the linksys is interfering; it looks like we had an issue where someone had a small hub for testing some systems hooked up to a Cisco switch in our MDF room and it caused all sorts of problems where the switch was confused because of that hub having multiple systems coming off one port. Why? I don't know, I didn't handle the issue, just had the report after the fact, but I know it caused at least one room to have huge connection and performance issues between their lab and the NAS in another building. The best way to test that is disconnect the linksys from the switch and see what 2 or more wired workstations are getting for performance in transferring files and pinging each other after a reboot or two (and restart the switch) to flush cached tables and such. You might also want to test your wiring to the switch to make sure the wires and connectors are secure and of high quality. 1. Go to hardware section (Control Panel => System => Hardware => Hardware manager OR right click on ""My computer"" => Properties => Hardware =>Hardware manager). May be another names - something similar For Gigabit speeds the requirements are that both the switch and the network card or other switch port that it's connected to supports Gigabit, and that the cable be Cat5e or higher. For gigabit in a business I prefer pre-made and tested cables as a simple connection test with a cheap cable tester is not enough to verify that the cable has the signal integrity to support gigabit speeds.  However I have my house wired with Cat6 Gigabit and the end points are punched into Cat5e patch panel and wall ports and all cables are hand crimped to the computers. If all of these things are true and you're not seeing the link negotiate to 1 Gigabit then you'll need to trouble shoot the issue to rule out a problem, in order of likely hood:  bad cable, bad NIC in computer, Bad or mis-configured port on the switch, general switch issue.  This last one would be pretty rare if the switch seems to be functional otherwise.  If you have all of the requirements, you can troubleshoot the connections some with a sniffer like wireshark to see the initial negotiations when you plug in the cable. This is limited in use, and may not even work. The best troubleshooting method is an expensive network tester. Fluke makes some of the best ones of these, there are many others. You might want to contact a wiring vendor to help you troubleshoot this as it would be cheaper to pay for their labor than to purchase the tool. Just make sure they come with a quality tool. Re: How do I see the auto-negotiated speed and how can I manually force a speed? That seems useful for testing: If your workstations have gig cards and you can verify that they're working properly, you can try manually setting them to a gig connection full duplex, disabling the auto negotiation.",4
"You will lose all the data on the second drive. Make sure you have backups of the system first, in case something goes wrong, then move the data from disk two to disk one, and see if you can create the RAID volume. Is this a good idea? If yes, I am just not sure on how to install the RAID 1 (as software) to start using the secondary disk as a mirror of the first. Could you please help me here? Normally it's easier in logistics, to make sure things go smoothly, to make a backup and then wipe the volumes and create the RAID from a clean slate then restore your data to the new volume, but you might be able to create it without accidentally wiping the drives. Make backups first. You can install a RAID volume utility that will do the mirroring during runtime, but to get there you'll need to copy the entire disk (not only files but the whole disk in raw like with dd command in unix).  I would like to temporarily use that secondary disk as a mirror of the first, and move the backups to the main disk (which will obviously then be mirrored to the secondary disk too). I have a Unix server running with 2 disks with the same capacity, but 1 is being used only for backups storage.",3
"So one of the things I tried was installing from the recovery mode. And it took about 2 hours to get this message: When the update started the screen went completely black, so I thought it needs me to press the power button, which I did. After a few minutes the log in screen showed up, which was absolutely unexpected of course. So I tried to fill out my password, but it did not take it. I tried it about 20 times, and I am pretty sure the password was right, but it did not let me in. I tried a lot of stuff to fix it and install the update, but nothing worked. I also have an external hard drive with Catalina installed. And it works fine when I connect it to the Mac via USB. I also tried to replicate it from the external drive to internal. Well first of all it posted a lot of errors(which I do not remember), but after I somehow managed to copy the drive Mac failed to boot from the internal drive. It boots from external drive, but the same copy does not boot from the internal. You may think that the drive is corrupted, but I checked it with diskutils, and it shows it as a completely healthy drive.",1
"Click on your each screen, and see the actual resolution and how it's even connected to your computer. First of all. You should not be using windows to control anything resolution related. You should be using the Nvidia software to control that. When you finally do install (or update) the software, you will have way more control over each screen and what it can display.  How can I resolve this discrepancy? Could the display be at a different resolution and not the claimed 1920x1080? I've presently got three displays connected to Windows 10. My video cards are NVIDIA GeForce GTX 1060. According to the Windows Advanced Display settings, all three displays are 1920x1080. However, it looks like one display, which is an LG TV, is actually a 'lower' resolution. When I move any app window to that display, it takes up a lot more of the screen space. But looking through any scaling settings I can find, it looks like everything is 100% scaling. My guess is, one of your screen's isn't capable of 1080p resolution or your settings are wrong. And you should fix them in the nvidia software.",2
"I bought CAT7 gigabit ethernet cables with RJ45 plugs (S/FTP PIMF), which should be downwards compatible with CAT6A, CAT6, CAT5E etcetera. Furthermore I can see my laptop appearing in the router webinterface's network connection tab like so: And the port status tab shows that everything is normal as soon as I bypass the CAT6A jacks with the orange cables (I'm not sure if its really CAT6A cables, I can not read the imprints): However, if I connect the FRITZ!BOX LAN1 port with one of the ProSAFE switch's ports (in this case port1), and then connect the switch with one of the CAT6A ports and furthermore connect my laptop's RJ45 with the RJ45 outlet in the room the CAT6A is leading to it says ""unidentified network / no internet connection"". Using this setup, I can neither ping/reach the router (192.168.178.1) nor the switch (192.168.178.2) from the laptop that is in whatever room the orange cable leads to (I have tried different outlets in different rooms). I have had broken cables before but the ones in the picture aren't bent as badly as the cables that stopped working properly after being bent too much. Did the electrician do a sloppy job or am I using the wrong hardware? How do I connect the four rooms (with a dual outlet of RJ45 each) via the CAT6A jacks in the mediabox? I have a mediabox with eight CAT6A jacks (at least CAT6A is imprinted on them) preinstalled. The ethernet cables (orange) lead to different rooms in the appartment. Even though CAT6+ is harder to break than earlier versions I have noticed that the cables are bent really hard, like you can see in this picture: The ""FRITZ!BOX/Homebox Cable"" router has only four RJ45 jacks. My goal is to gather all the CAT6A jacks in a single local area network (also enabling WAN via the router ofcourse), using a NETGEAR ProSAFE GS108Ev3 switch. What I have noticed is that the switch's port that connects to the CAT6A jack is always indicating the LED that says ""10M connection active"", even if there is no client connected anywhere. I am not sure if thats normal. When I connect the router to the switch with CAT7/RJ45, and then connect my laptop directly to the switch using the same type of cable I get a 1Gbit uplink (IP via DHCP) and I am able to surf the web and ping the switch/router or visit the webinterfaces of these two devices.",1
"Could it really be one of these? Or something even further back? The most recent Windows Defender update for Skeeyah.A!bit came out two months ago, so it should have infected me no more than two days back when I got the alert, right? Could I have contracted the trojan via some Windows 10 vulnerability? I should note that my other family members use standard Windows accounts, only I have administrator rights. Thanks for shedding some light on this, I'm totally clueless! Any website could have been infected by malware and when you entered into one of those there was a silent download that infected you. Simply as that. It could be that you had a port open and someone scanned it and sent malware through it. You could have been infected before, or some of the devices into your same network could have been infected. Windows Defender just informed me it had placed the trojan Win32/Skeeyah.A!bit in quarantine. It was first found yesterday and now again today. How did I contract that? My download history reveals nothing noteworthy imho: By the way... windows defender wont do much to help you... you better start using better security measures.",2
"Someone is hosting the DNS for the domain now.  If you look at the actual registration for the domain, name servers must be provided.  When you register the domain, generally the registering company assigns their own name servers.  There should be some way for you to add the CNAME entry using that mechanism. That being said, you'll have much more flexibility running your own DNS server - you can create your own subdomains for example. Depending on your name registrar you might want to park the domain with them since they might offer tools for that. For real-live, actual DNS hosting, I have been loving ZoneEdit for more years than I can remember.  Your first 5 domains are free, and the next are a small one-time cost.  You can manage everything over the web and they do basic email and web hosting. I'm starting a small web design shop. We does not have much money so we would like to keep our costs as low as possible. So we are looking at free DNS services like zoneedit.com and such... which seems promising but we are concerned about security/reliability issues. So I thought I'd ask here first. I suggest the NS FREE package from Zerigo. It should be sufficient until you are profitable enough to upgrade. We use their more expensive packages with much success.  All we need is a @ourcompany.com mail address and a few web pages to showcase our work and we are looking at Google Apps which gives enough functionality for us. However, to verify ownership of the domain to google apps, there are 2 possible ways: DynDNS has been rock solid for me for years. I have had no reason to complain and I recommend it. Their documentation is excellent, and there are tons of clients there, both for Win32 and for unixy OSes. The way I read their Acceptable Use Policy, usage in your scenario (which is commercial usage) is OK with them.",5
"The long answer is ""it depends"", specifically on how you use your system, and how much you want to invest in managing the layout of data for performance yourself. If you care about the non-bulletproofed lifetime of MLC chips, ISRT can make you fly fast and keep your data safe. I don't have hardware to try yet, but am battling the boot vs. Smart Response question too.  I'm waist deep in the manual for an ASUS P8Z77 board, and it seem to say that you can partition a large SSD with one partition is a logical drive for Smart Response, and the other is a separate logical drive. The manual's a bit cryptic, though so I'm not sure. It can also change what data is fast and slow as your use of the system changes, again without you having to change anything. That transparency and responsiveness has a significant benefit to you, because odds are you don't want to constantly invest effort in profiling and maintaining that yourself. In general, though, something like ""Smart Responsive"" is going to be better: it uses the SSD as a cache for data on the hard disk, and can adapt to store the most frequently used data on the fast system without any user intervention. If that's really possible, then presumably you could specify the second partition as the page file location.  This is important to me, as I commonly work with very large database and flat files simultaneously in multiple processes.",3
"Unfortunately, there is no simple way of finding pages in a raw Postscript file. That is why %%Pages convention has been created (Adobe Document Structuring Conventions). I have a duplex printing emulation system written in bash.  It prints the odd pages first and then the even pages.  It needs to know if there's an odd page count so it can eject the last odd page that doesn't have a corresponding even side.  It also uses page counts for reporting purposes. I found this little snippet somewhere, it will process the document very fast and print out the page count. This can help if exiftool do not print this meta-data information because the document was not generated correctly: I didn't know how to do this correctly, so I wrote code that looks at the end and, if necessary, the beginning  of the postscript file searching for ""%%Pages:"" which is followed by a page count.  This works on almost everything except files printed by the Opera browser. The following Ghostscript command will reliably count the pages in your PostScript file -- but it can be rather slow, because it requires the file to be completely interpreted (run), as @afrazier already stated in a comment: Postscript files tend to be rather large with a lot of non-human-readable content, so I haven't yet spent a lot of time pouring over the ones that come out of Opera.",4
"In a game like HoMM, having a large stack of powerful attackers you'll be able to kill many other stacks in the game with a single attack. Melee counter-attack means that even defenders that you roll over still whittle away at your offensive stack, reducing its offensive power over time (unless you expend resources replenishing the stack). Some games, like Duelyst, let ranged attackers get a counter-attack in when being attacked by ranged attackers. This is rather subjective as its quite arguable whether you'd want to encourage the player to have many smaller stacks or fewer larger stacks. Ranged attackers can't get hit by the melee units they attack because it makes sense. Imagine you have a sword and someone shoots an arrow at you; they're too far away for you to hit them with the sword. Ranged attackers are typically used as big-damage/wimp: they stay far so that they don't get attacked. That's why they don't suffer that penalty.  Continued from above, melee-counterattack means that it matters less which player attacks first, as both players' units will get a chance to attack no matter the outcome. The first attacker will no longer be able to eliminate the opponent's strongest attacker in the first turn without consequences (well, except for ranged attackers, but that plays back into the first point). This reduces the first-turn advantage to mostly being about picking the battlefield positioning, which is more strategically interesting in a tactical game. It's a basic question of a certain type of ""realism"". It's hard to imagine getting a melee hit in on an enemy and them not getting involved at all, because you have to very close to them to hit with with something like an axe. It makes sense that the unit getting attacked gets a swing in. What I mean by ""composable"" is that since counter-attack is such a simple rule, there are many ways to combine it with other rules to get a combinatoric growth in strategic depth without requiring much extra work. For instance, make a defender unit that does 2x damage on a counter-attack, or make an offensive unit that does 1/2x damage on a counter-attack. You'll frequently find this element in simpler designs: you can combine simple rules far more effectively than you can combine complex rules, and the result is often equally simple and easy to understand. Melee counter-attack is widely understood and easy to incorporate into a game's design. It's easy to teach to new players which makes the game more appealing to a wider playerbase. It requires little additional/special artwork or animations. It requires little additional code. This is a two-parter that is predicated on accepting the prior three points. Basically, assuming you want the additional strategic impact provided by melee-counterattack in the first place, it's just easier to use melee-counterattack than to come up with a different solution. This one is specific to other game details. An example again would be HoMM: a single unit on the battlefield is actually a stack of creatures rather than a single creature. Bigger stacks have more power. Without counter-attack, it becomes much easier for multiple small stacks to overwhelm a single large stack, even though the large stack possibly represents more overall individual creatures. The counter-attack evens out the economy of attack actions with the smaller stacks; 5 small stacks ganging up on a larger stack do not get 5x as many attacks, possibly nulling out the supposed numeric superiority of the larger stack. Do you build and use units that have strong melee attacks but die more easily because they're in melee or do you use units with weaker ranged attacks that live longer because they can stay out of melee? In short, if you accept the previous three points, using melee-counterattack to achieve them is just probably far easier overall than using something else.",3
"Your basic problem is a complete lack of a methodical approach to cabling. The use of such absurdly long patch leads, as shown in the photo, just makes it near impossible to be properly organised. Then, I use 6"" patch cables to connect the panel to the switch.  The benefit is that everything is nice and neat, and organized...  Then, any infrastructure related cabling (connecting switches, etc) I run out to the side of the rack and then up/down to where it needs to go... or.. make or buy (if you can) some cables that are just long enough (but not so short that you put strain on the plugs - give it an extra inch or two for wiggle room). Our cables tend to be the long ones, but they are all pulled to the side, kept together and the other end pulled in - like you have a box of cabling at the side of the rack to store the main cable, pulling the ends out. That's tidier in many respects as you can still get your hands in there to change them, the cables aren't in the way. This is the main reason I can think of why very short cables would be a disadvantage. Similar to what you show in your pic, but a lot more disciplined - no cable gets added that isn't first put with the bundle on the side. Don't forget to label both ends though! Surely 6 inch cables are just too limiting, get the longest ones you can and loop them together to keep things tidy. One of the most important things to consider is how you are going to know which cable connects to which end points, without having to trace that cable. I use a MS Access database for that, because it's easy to use and I can produce reports in any convenient format. Several of those reports are printed out and hung on the rack. Any cabling changes get updated on those sheets and later entered into the database. As to the cables themselves, I prefer to make my own to whatever length is called for. If you don't want to do that I suggest getting a bunch of different lengths and using the appropriate one each time. There's certainly nothing wrong with 6 inch cables in and of itself, if that's appropriate to the situation in which they are used. The main downside with a cable as short as 6"" is that you may end up with problems if you use it to directly connect two interfaces, but for a patch between a panel and a switch (or two panels), it should be just fine. Personally, I try to put the switches as close to the patch panels as possible.  Typically, I'll put a 48 port switch between 2 24 port patch panels.  So it goes like: Before you touch another cable sit yourself down and give serious though to the result you want, rather than how to fix what you have. If you don't have a final objective everything you do to that cabling will still end up messy and hard to work with.",4
"Pooling is more often used to get around memory allocation issues. If you are making a bullet-hell game then creating and destroying all of those bullets is going to be a costly operation. It would be better to create a bullet pool, and when a bullet is destroyed it is just removed from the logic and draw loops, and returned to the pool. Once it is shot again it is re-initialized and added again to the logic and draw loops. If you are trying to use this to precache objects, I would take a different approach. If you are drawing three balls there is no reason to load its texture and shape three times. If your object and renderer are separate (makes life way easier) then you can just have the renderer draw the same ball in three different locations. You can take this a step further with sprites. You can load a single sprite sheet for all of the animations of an object (or multiple objects) into a texture and just draw a section of the texture each time. Each object does not need its own texture. I am asking since the game still tends to stutter once in a while when I grab some models off the pool to ingame-runtime (although it is already ingame)",2
"So far I've seen people mention the  and  tags, but there are other DOM things that are vital to JS/HTML game development, including stuff like adding EventListener callbacks to the  element for things like the 'mousemove' and 'mousedown' events. For example, this takes the first canvas element it can find and sets it up so that when you click on the canvas, it calls a function you've defined called ""shootAtClick"": You can see it in action in this example where I've extended the Akihabara engine to accept basic mouse control. (Press Z to begin, use arrow keys to move, click the mouse to shoot in the direction of the cursor.) Overall, I'd recommend <canvas> tags over straight DOM manipulation (e.g. making each sprite its own <div> to make hit-testing amazingly simple). It's easy to get DOM manipulation wrong and badly performant, whereas traditional 2d development strategies work well on an HTML canvas.  You might as well ask what C++ stuff you need to write applications.  That depends.  Tell us more about the project, and you might get a useful answer. Web sockets as well may have browser specific differences, but socket.io looks like a promising library that provides a convenient abstraction. Just to throw more resources out there, check out RaphaelJS. It's a very nice SVG drawing and animation library that has a nice API. Don't forget to check out the demos. HTML and JavaScript are honestly fairly bad platforms for game development, but that won't matter until you've decided on a kind of game.  The answer would be radically different for a first person shooter, an RTS or a puzzle game. Chrome has a couple of significant audio bugs, such as being unable to play short audio files, and playing many simultaneous sounds can crash the entire browser. The main thing is to understand the browser differences, using libraries to abstract and provide legacy support as needed. Audio compatibility is a pretty major component that requires different sound files for different browsers: http://html5doctor.com/native-audio-in-the-browser/ Though cross browser compatibility is getting better and more uniform as time goes on some of the newer components are still not supported entirely equally.  Notice that you're starting to get vague, hand-wavey answers rattling off random technologies that seem neat.  That's a warning sign that you haven't asked a question with a particularly legitimate answer. You should be aware of the Audio and Video tags. Web Storage is pretty critical if you need to save a lot of data for saved games etc. Local storage may have some browser specific differences, and for legacy support you'll definitely need a library such as jStorage or YUI Storage Lite. In addition to some of the game-making frameworks that are out there, you should be aware of the Burst Engine, which is a JS framework for displaying SVG-based animations in an HTML5 Canvas:",5
"Alternatively, you can use the shortcut Ctrl-D to duplicate the selected object as this doesn't touch the clipboard. I have two Inkscape windows. The setup is pretty simple. In the first window I draw a filled circle and a filled rectangle in it, with the circle set on top of the rectangle to show that the area around the circle is transparent (that is, you can see the rectangle ""under"" the circle, see screenshot 1, left). In the second window I just drew a filled rectangle (screenshot 1, right). The underlying problem is that with current Inkscape packages for Mac OS X the pasted objects are indeed pasted as bitmap images (with a solid white background), instead of being pasted as vector copies of the original vector objects. This is a rather contrived example but is readily reproducible. The real graphics I am working with have a bunch of objects all in a single group, but I have the same results. I feel like I'm missing something. The circle no longer behaves like a circle at its destination. Instead, it acts kind of like a bitmap. I'm definitely not using the bitmap copy feature. Please adjust the X11 preferences for the pasteboard as described in the Inkscape FAQ: Copying and pasting in Inkscape creates pixellated images instead of copying the vector objects (and as seen in this screenshot). The changes will take effect immediately (no need to restart X11/XQuartz or Inkscape), but objects copied and pasted earlier will have to be redone. Copied vector objects are pasted as bitmap images, and no vector information (style, path) is available in other Inkscape commands either which expect vector data on the clipboard (e.g. paste style, paste size, 'Shape: from clipboard' in pencil/pen tools, 'Paste path' in the path effect editor, etc.) I had this issue while trying to select multiple text boxes and copy them: the background between the text went opaque white. Instead of copying, I tried duplicating, and that gave me exactly what I wanted: a copy of all the text boxes in the same positions relative to each other with no background connecting them. When I copy the circle from window 1 to window 2 the transparency around the circle is lost (screenshot 2). I'm looking for a way to copy objects from one window to another without losing the surrounding transparency.",4
"The real thing to remember here is that Amazon does not need to turn a profit on this venture. This is another one of Amazons ""Loss Leaders"" where there goal is too get you to subscribe to Amazon prime and then once you are in their system they can start turning the real profit.  TL;DR: It's hard to say exactly what Amazon is doing but we can be confident that devs are being well paid whether or not Free Time is independently profitable.  1) Time Based As you guessed it possible that amazon is paying apps based on how successful they are in time. Although this seems somewhat problematic as it promotes bad design practices where the goal is time sunk into the app instead of fun. This means that a usual system used in other subscription mediums would fall apart here. 2) Commission/Licence Most likely amazon is paying a set amount for access to each game each month, or commissioning specific studios for amazon exclusive titles. This would put them in line with how Netflix runs their subscription model. We can even run a hypothetical example of the inverse with this very product. So let's say you subscribe to Amazon Music (which is almost certainly losing Amazon money) and to make Amazon Music cheaper you get Amazon Prime. Now that you have prime you say what the heck and get Amazon Free Time, now Amazon is collecting more money from you originally at near no extra cost thanks to the digital nature of Free Time. 3) Some Weird Combination Big tech companies these days love their proprietary algorithms so it's totally possible that in addition to base monthly price they reward any number of things. This could include playtime, how often an app is opened, how much a customer comes back to the app, and other data driven methods. While 2 makes the most sense to me from the lack of information on the subject out there it seems very possible that this is the answer.  While I'm sure your looking for details it doesn't appear that there are many details about how exactly devs are paid (and it might even be obscure the the devs working on the projects) I can give you some speculation.",1
"On the manual you have an example of how to use prepared statements with the .net connector (the second one is the C# version). You can use those or MySQLCommands, in this case it doesn't matter much (more on this below): Please note that this is dynamic SQL and prepared statements or filtering will not be effective against SQL injection. If there is a table in your database called ""my_prefix ; DROP TABLE Students;--"" it is your responsibility to sanitize that. Using the language/connector commands, as you probably want to do, is usually the best option for maintainability, unless you have an extremely large number of tables to be dropped. I have found this question answered but not specifically for C#. The answer seems to be to execute the following statements tailored to my specific database. However, I can't figure out how to use these statements in C#. Please note that using the mysql syntax for prepared statements to generate dynamic SQL is an ugly hack and should not be recommended.",2
"I started to recover my /dev/sdq1 when I was not familiar with HDD recovery. Well, I was careful enough and recover 85% in about 500 runs. I've found out that contents of /dev/sdq1 start at 32256 byte of /dev/sdq with fdisk. I dded these bytes at the beginning to separate file begin.raw and now running cat ./rescue/begin.raw ./rescue/fixed.vdi > ./sdq.raw to prepend them to recovered data from /dev/sdq1.  Below is my script I use to recover data. Sometimes I had to detach HDD physically because my usb hub does not support complete power off on its ports. P. S. I have Logitech USB 2.0 hub but it does not support full poweroff on ports either with uhubctl. Maybe there's other tool that can work with it? Afterwards, I've found out that I need /dev/sdq (without 1) contents actually to convert them after to .vhd with VBoxManage convertdd z:\fixed.vdi d:\disc.vhd --format VHD and after I can attach and read that .vhd in Windows, because VBoxManage does not support partition conversion and requires full harddrive raw data as an input. That means I should have started ddrescue with /dev/sdq and not /dev/sdq1. Is there an easier way to continue runs of ddrescue to that new ./sdq.raw with old rescuelog? I'm not sure that --output-position will fit here. (and how does it work exactly? -- This is not clear from docs how output-position will influence on rescuelog interpretation).",1
"A) Daily server reboots are wrong. Other than that, the proper way to restart is shutdown -r now, which properly shuts down services on the server and then reboots. but after these scheduled reboots, some services don't automatically start. The ones I have noticed already are OpenVPN and PostgreSQL, but I expect there to be more. Firstly, I noticed this after adding the scheduled reboot but as I recently upgraded from 15.10, it could be that something went wrong during the upgrade. B) Using reboot to reboot the server does not shutdown services, so those might end up in unstable state and therefore refuse to start. The correct behaviour depends on the init system used. It is not perfectly clear which one you are using. Since you upgraded to Ubuntu 15.10 systemd should be the default, but you are mentioning /etc/init.d and this would point to System-V style init scripts which are a bit different.  There might be an issue regarding the autostart of the daemons with which you are experiencing problems.  First you should check that the service has the correct systemd init script under /etc/systemd/system/multi-user.target.wants/{service}.service",3
"The VARIABLE thread_cache_size says how many ""threads"" or ""processes"" to keep around.  20 is a reasonable value for *nix.  0 is fine for Windows. With the query in hand, get SHOW CREATE TABLE for the table(s) involved.  And EXPLAIN SELECT ... to have some clues of how it operates.  Then show us those things; we can probably help you. You do not mention what language your application is in but if you are connecting on every page requested you could add a lot of speed simply by using a database connection pool. That way you application will not need so many connections and will not have to connect on every request, giving you extra speed simply because you don't reconnect every time and also reducing drastically the number of concurrent connections it uses, which seems to be your problem. I am not sure why this is happening, or whether this is normal. I tried to adjust the 'max_connections' variables but still no luck. Has anyone ever got this problem as well? How can i optimize this ? Threads_created -- again, find out the ""per second"" value.  It, too, should probably be under 1/sec. I have only seen problems like this you mention on Windows machines. Most of this problems are also highly attenuated simply by moving your db to GNU/Linux (or BSD). Connections / Uptime is ""connections per second"".  If this is less than 1, then the rate of making connections is OK. Back to the 'real' question...  If MySQL is ""slow"" or ""high CPU"" or ""high I/O"", then locate the query that is likely to be the villain.  SHOW FULL PROCESSLIST; might catch it.  The slowlog, with long_query_time=1 will catch and record it. I have a website that runs perfectly normal, until today it becomes very slow. After a few hours debugging, it all points down to database.",3
"There was another issue: the cron launches the firefox without env variables, so I had to define the display where to work.  If there is a running firefox, the call to firefox just contacts it, transmits the print request and exits immediately. The print request is executed asynchronously in the running instance. I don't know if there is a way to get a notification when the printing is finished. which should get you at least some delay - it waits until there's a non-zero-length output file before continuing.  Hopefully this will slow it down enough that subsequent calls will use the existing firefox and not think they have to spawn their own. A workaround (untested) would be to create a fresh profile in your script (unpack a prepared zip under a random name). You can run multiple instances of firefox as long as they use different profiles. Erase the temporary profile once firefox exits. This assumes that the printing extension doesn't leave the firefox instance running; if it does, you could hack it to exit firefox once it's finished. The problem is that inside a bash script firefox is called many times in a few seconds instead of being called once at a time and waiting to finish. When you call firefox without specifying a full directory, it will run a wrapper script which does an exec of the actual executable. This may be causing your problem or the executable may be doing a double fork which causes it to run in the background without using &. I don't know how you can prevent this.",4
"That being said, if you're going to try to use this system for what we're all assuming you're using it for, you've got much bigger problems than this is going to solve. If the user leaves the workstation, they should either lock it or log out depending on their plans. If they log in or unlock another workstation you'd know just by looking at your central authentication records. If they're lazy, give them smartcards, wireless dongles or some other password-replacement method of authentication. Adjust timeouts for locked (and unlocked idle) workstations to have them automatically locked and then later logged off if the user forgets. Wireless dongles will lock when out of range. Smartcards will (if configured to) lock or log out when pulled. Most password-replacement techs will prefer at least a pin cod as well to (re-)authenticate. A script to send a ""user away"" message to the server upon an inactivity timeout seems by far the easiest.  Coupling that with a screensaver that requires authentication would also make it effective at verifying that the user is the correct user. Of course this method can't tell the difference between the user actually being away from the desk and being at the desk but not using the keyboard/mouse (reading documentation perhaps,  watching an automated presentation, or discussing matters, ...). To get it working on all platforms, you'd need some magic though. There's always Jabber and other conferencing solutions like that Cisco platform - but what you need most is something that is completely and seamlessly integrated with the client OS and user authentication, lock/unlock as well which is a tad more work... at least if you want it reliable and mandatory (easy with Communicator for Windows). The part that won't work is the requirement of ""identifying a user without them authenticating themselves""... ^^ Any Instant Messaging and Presence platform will provide presence and notification information in scriptable form. For Windows there's Office Live Communications Server and Office Communicator. Assuming everyone carries a cellphone and they have bluetooth enabled, you can install software to perform actions when the phones come into and go out of range. (~30 feet) Under Windows there is an API call that returns simple data on keyboard/mouse activity. This is what IM applications use to automatically detect when the user is away or idle. Would that suffice for your needs? There is no doubt an equivalent for other OSs too. Sounds easy to achieve together with some simple user policies. Whatever mobile device they have they need to authenticate on those as well of course.",4
"I noticed that on one of the forms I have a control that is querying the table that is being moved to the backend. I'm not sure if having each user's local copy of the front end query the backend table just for this one control is worth it. I want to minimize db corruption, so should I just remove any sort of querying on the backend table for small stuff and only allow users one major write to the backend table? Or does this not matter? I have an MS Access database that was being used by 30 people. They all use it at different times of the day, but within any 8 hour block of time there might be 10 people using it. This database was not being split between front end and back end. The main process was that a user filled out a form and then VBA code would write this information to a table.  If by 'querying' you mean the control is simply running a select statement against the data table, it is doubtful that would corrupt any data.  If your control's query is manipulating data (add, delete, change) then it could very well be corrupting the data. Thirty users on an Access DB is challenging.  You may want to look into using a more powerful DB for the backend.  If you don't, be sure to repair and compact your Access DB nightly. I now want to make a new version of this database where the front end is split from the back end. This is because there were numerous times where the whole database became corrupted and all our data, queries, and forms were lost.",2
"This will hopefully reduce (though sadly, probably not entirely eliminate) overheating due to fans getting bunged up, and in the event that one does fail might reduce the impact and cost of each unit that fails. In any case, as you know its a hostile environment perhaps it might be worth having a regular (3 or 6 month?) preventative maintenance cycle for these machines too, where they get cleaned. In your shoes, I'd consider a low cost, low power (ideally fanless) terminal server client of some kind and use them with a terminal server/citrix or VDI backend. @RobertMoir's option is probably the most cost-effective.  ""Plan B"" would be some ruggedized tablets or laptops (The tablet units might be more appropriate if your users need to move around the warehouse while accessing data...) Take a look at Axel terminals. I use the M70 pretty heavily in warehousing and industrial cooler environments for RDP/Citrix and terminal emulation solutions. The devices are built for the conditions and will outlast even traditional thin clients (from experience). You are looking for ""Industrial Computers.""  Systems designed for ""harsh"" environments.  Most the time they are indeed fan less if you can get away with it.  However there are high performance machines able to handle large range of temperature, dust, vibration, humidity, etc. Ruggedized laptops would be a good option (though pricey). You might also consider using cheap nettop devices: they often run fanless, and could be contained away from the monitor in an enclosure that had a fan and simple filter on it.",5
"Here is my scenario. I have Comp1 which has VPN installed on it & working fine. I have made Comp1 as remote enabled as well. Now i want to log in to Comp1 ( remotely) from Comp2. So, in your scenario, from Comp2, RDP into Comp3 (on same LAN is Comp1), and from Comp3, RDP into Comp1.  Now, from Comp1, start your VPN connection.  You are now connected to the VPN by way of Comp1, through Comp3, from Comp2 lol.  Convoluted I know, but it works in a pinch. Instead, while remoted into the home computer, I VNC yet again to another computer at my house (meaning, this VNC connection is through the LAN, and not subject to the VPN) and use this second house computer to establish the VPN to corporate. Without starting the VPN, remote desktop works fine from Comp2 to Comp1. When i connect to VPN on Comp1 the remote desktop from Comp2 breaks. I'm at a remote location to set up a tunnel back to the corporate network.  I need to get on the corporate network to set up the other end of the tunnel.  I VNC (or RDP) to a computer at my house.  If I used the computer at my house to connect to the corporate VPN at this point, I'd knock myself out of the VNC connection.",2
"Our company also switched from GroupWise to Outlook and we used the 'send to others but don't include me' feature all the time. Outlook does not have that specific feature, but the functionality can be replicated (with 6 extra clicks). We're using Outlook 2007 in an Exchange 2007 environment.  Is there a way for someone to schedule a meeting for a group other people without that meeting appearing in the scheduler's calendar? My company is coming from a Novell GroupWise background and many people have apparently been utilizing this feature for some time and are looking for an analog in the new environment. You can set someone up as a delegate, they can have access to your calendar and or email and send in your absence.  Tom is out of the office.  Tom calls Evan and asks him to schedule a meeting for Tom and Dick for this afternoon.  Can Evan create a meeting request that includes Tom and Dick but does not include Evan? There's no requirement that the meeting appear to originate from Tom or be on the behalf of Tom, Evan just shouldn't be among the attendees and it should not appear on his calendar. Method one is to setup a delegate that can schedule meeting on your behalf. GroupWise did this too and it isn't quite the same. Ideally you want the person (Evan) to not have the meeting in their calendar, and for the other person (Tom) to be shown as the Organiser, which will also allow them to update the meeting, see the list of who accepted or declined etc. Method two is to schedule the meeting as if you are attending and after it is sent, switch the status to 'free' on your own schedule. It accomplishes almost the same thing as the GroupWise feature but takes a few extra steps. Only way to do that is for Tom to allow Evan to have full access to his calendar by changing Calendar permissions. This means it requires setting up in advance, and is probably only appropriate for trusted team members not for everyone to do for anyone else (standard example would of course be an executive and their personal assistant). There is also a bit of a trick to this: Evan should first set this up as a simple Appointment in Tom's calendar so Tom is the organiser and sole attendee. Then ""invite attendees"" to turn the appointment into a meeting. If you start out creating a meeting, Evan will be the organiser and an attendee and would have to change this round.",4
"Why are you using two different if statements to run the same line? If you need to run it in either case, use or, not an elif. Never use except without giving it a specific exception to look for. If you made a syntax error like a typo then you'd actually not notice because of that except and you'd probably think there was a problem with the JSON. If you're looking for errors from invalid JSON, that raises a ValueError, so raise that instead. You should read PEP0008, it's the invaluable Python style guide. One such suggestion is to structure your modules so that all the plain import statements are first, separated from the from _ import _ lines. It's neater and easier to read. If you're going to have a comment under a function declaration, you should make it a docstring so that it can be useful to others reading your code (as docstrings are accessible programmatically through the interpreter).  Your receive function has a lot of unnecessary comments, I'd strip them out especially in places that are pretty clear from the actual code. In particular:",1
"Many clustering algorithms exist, I would say that the most popular is K-means however spectral clustering and Gaussian mixtures are also frequently used. As always, each algorithm is best suited for a specific type of dataset, it is up to you to choose which is best suited, or you can just try all of them and see which is best.  In general it does not make much sense to cluster features. In an ideal world for your features to be the best they can be they should actually be independent, thus there should be no relationship between them. Typically when we talk about clustering it is clustering the instances. To attribute some associative labels to a subset of the instances based on the similarity of their feature values. I will paste the entire algorithm at the bottom of the answer for a quick copy and paste but here I will go through its different parts so you can see how it works. The algorithm goes as follows: first we initialize some centroids within the range of our data. These are the red dots in the image below So now we have 1500 instances (records) in 2D space. This can be extended to any number dimensions. 2 is easiest to plot. First we will make some artificial data. These will consist of $n$ 2D Gaussian clusters with a given mean and variance. Here $n=5$ and we will have 300 instances per Gaussian distribution.  What we see here is that each instance is grouped into a cluster with similar attributes as itself. For example if the dimensions of our data represented height (x-axis) and weight (y-axis), then we can group people into 5 different BMI indices.  Now we will update the position of the new centroids by finding the mean position of all instances which are closest to the given centroid in each dimension.  We then repeat this process until the centroids no longer move significantly. Usually if the difference in position for all the centroids is less than machine epsilon we consider the algorithm to have converged. I will describe a homebrew version of the K-means algorithm such that you can understand what is happening under the hood and perhaps you will see why we cluster instances and not features. To use the algorithm use the following, where data is artificial data we made above but can also be any numpy matrix where the records are the rows and the features are the columns. Each member in our population belongs to a specific BMI index which is clustered based on two measurable attributes he possesses (features), his height and his weight. Here you can find a list of clustering algorithms with their respective usecases. Always use the libraries when you want to implement standard algorithms, they are highly optimized. But for education sake it is good to look at what is happening.",1
"As per Diago's answer, you can completely opt-out of Windows Error Reporting. But also note that you can opt-out for specific programs. If you frequently experience hangs in a few specific applications, it would be preferable to click ""Select programs to exclude from reporting"" instead of disabling Windows Error Reporting. I would also recommend contacting the vendor's technical support (if possible) and complaining about the hangs. Software shouldn't hang on a regular basis. Instead, you can go to Service Control Panel (Start > Run services.msc), find Windows Error Reporting Service, and set it to Disabled. Note that this system only works if the vendor actually creates a WinQual account and examines the crash data. In general, there's no easy way to determine whether they actually take advantage of WER data other than asking. But if a vendor takes advantage of WER, submitting WER data about a specific crash increases the chances that it will be fixed in a later version of the application. In Win7 when I close an unresponsive program using the ctrl-alt-del mechanism, it pops up a dialog box saying ""Windows is checking for a solution to this problem"" and of course it doesnt know the solution to the random crash. Why is opting out for specific programs better than opting out for all programs? Sending Windows Error Reporting (WER) data to Microsoft uploads it to the WinQual database that software vendors (not just Microsoft) can query to find out what crashes in their software are most prevalent. Submitting this information may help software vendors identify the most important crashes in their software and fix them.  Crashes have an associated bucket ID that identifies where the crash occurred. Hangs used to all go into the ""HUNGAPP"" bucket (as of Windows XP), which ends up being pretty much useless because it doesn't distinguish between different hang reasons for the same application. Windows Vista (and 7) supposedly collect more useful information about hangs. Having looked at the WER data for software that I work on, I still don't think that the information being collected about hangs is as useful as the information collected about crashes. However, disabling WER entirely just because hangs are annoying seems like throwing the baby out with the bath water.",3
"I know that this can be done through GP Preferences client side extentions (which means deploying and validating the deployment of that CSE installer, which I'd prefer to avoid) and through a GP deployed start-up script (which feels a little clugey, and will take forever to apply).  I'd much rather create a central admx or adm template that I can use to manage the settings I need. http://msmvps.com/blogs/cgross/archive/2008/12/16/installing-group-policy-preferences-client-side-extensions.aspx The above link also includes a GPO startup script to install it if you don't have WSUS, but I agree that it's a kludge. Below is another example script. http://social.technet.microsoft.com/Forums/en-US/winserverManagement/thread/5f9f5658-2eca-4e10-9ab2-9c3ee048d9af I've got a 2k3 functional level domain environment.  I'd like to use group policy to deploy a few registry settings. GP Preferences are so ridiculously useful that I'd hesitate to work around them. Plenty of work has been done in deploying the CSE easily. If you have WSUS already, you can download Feature Packs and then approve the updates for the Client Side Extensions. Group policy in a lot of cases is just a happy interface for a lot of different registry settings.  Can I just take a random admx file and modify it for my needs?  What are the syntax pitfalls I should be concerned with?  Is there a reference guide that I'm just missing?",2
"4) At work: set up a reverse tunnel to the moniker above, at first with ssh, and just check it works; Is there a way that I can configure a set of SSH tunnels between my Ubuntu VMs and the external Digital Ocean server which will make my Ubuntu VM at work addressable via SSH from my Ubuntu VM at home? I am willing to make this work by setting up a SSH tunnel from each Ubuntu VM to the (addressable) Digital Ocean server (with the SSH command originating from the Ubuntu VM).  5) now download autossh for your distro, a small wrapper which uses port 6521 (that's why!) to check that the connection is still active, and if it is not it kills the running instance of ssh and starts a new one.  This sets up a passwordless ssh connection (see the use of the cryptographic key **id_rsa) without terminal (-N), in protocol 2 (-2), using port 6521 to check that the connection is still active, and redirecting to my home port 22 all that is sent to port 8400 at work.  I would like to SSH from my (unaddressable) home machine to my (unaddressable) work machine so that I can run software on my work machine from home. All of the solutions I found on the internet require that one of the VMs be network-addressable, but none solve the problem when both machines are unaddressable. I'm willing to cough up a few bucks to rent a cheap Digital Ocean server to bridge the communication between the Ubuntu VMs.  If they'll allow it, you could try something like hamachi (I use paid logmein hamachi) to have both machines exist on a private VPN with just each other.   I have Windows PC behind a NAT/Firewall at home, and a Windows PC behind a NAT/Firewall at work. i.e. Neither PC has a publicly visible IP address. On each of the PCs, I am running a Ubuntu VM on VirtualBox.  If you prefer to use the digital ocean server you refer to over Hamachi and the like, I suggest you look into OpenVpn. Set up the d.o. server as a server and the pc's as clients and make sure you allow clients to reach each other and you are done. You then reach your job pc through its vpn ip-address. 2) Set it up, either on your router or on your pc; this way, even if your ISP changes your IP address, the moniker above always points to your home. The site no-ip.com has instructions on how to do this. where I now have to use the passwordless key for work. I have found that this connection is always, always up. Truly satisfactory.  My IT folks at work will not let me SSH from outside into my Windows machine or the Ubuntu VM running on that machine, nor will they open any ports to make my machine accessible, nor will they give me a publicly visible IP address for my Windows PC or Ubuntu VM. But I can SSH out (as I often do to manage Amazon EC2 instances).  to execute the executable file auto in /etc/rc.local automatically at boot, as myself instead of root. I can now connect to my work pc with the command:",4
"The activation is done by a marker in the BIOS (SLIC), a Digital OEM Certificate (*.xrm-ms) and a generic OEM product key. Something like a bootable copy of Acronis True Image will do this, or linux's dd can do so too I believe.  On your currently installed OEM Windows search for the OEM.xrm-ms, make a backup of it and run a tool that can extract the currently used OEM key. Now save this Key + OEM.xrm-ms to an external drive and do a clean Windows 7 install. After the setup, copy the OEM certificate to C:\Windows\System32\OEM, run %SYSTEMROOT%\System32\SLMGR.VBS -ilc %SYSTEMROOT%\System32\OEM.xrm-ms, next enter the OEM key with this command: %SYSTEMROOT%\System32\SLMGR.VBS -ipk KEY. Now you have a clean Windows which is activated via OEM key. Make a system image of it or create a setupcomplete.cmd which does all the steps on your own and added it to the Windows DVD so that the setup does all the steps on its own. The ""cleanest"" way to do this in my eyes would be to make a disk image of the entire disk as it stands. Then you can restore it to EXACTLY as it is today at any point in the future. https://forums.lenovo.com/t5/Welcome-FAQs-Knowledge-Base/How-to-create-quot-Factory-Default-Recovery-Disc-quot-with/ta-p/274167  Otherwise look for a similar tool in your laptop there must be one if the Windows came pre-installed. There's a default application software provided by the manufactures on the PC's with pre-installed Windows like for Acer there is Acer Recovery Management,for Dell there is Dell Backup and Recovery Manager and for Lenovo I think there is Lenovo ThinkVantage Tools or OneKey Recovery.These tools are used to create factory restore disks for future use,previously I think they used to provide recovery media with the PC's but now they expect you to create one using the above mentioned respective tools.I am posting some links see if it helps:",3
"If some processes do come up as running, but not reported in /proc, you probably do have a problem any way you look at it. Essentially, when you run ""kill -0 $PID"" you are sending a nop signal to process identifier $PID. If the process is running, the kill command will exit normally. (FWIW, since you're passing a nop kill signal, nothing will happen to the process). If a process isn't running, the kill command will fail (exit status less than zero). When your server is hacked / a rootkit is installed, one of the first things it does is tell the kernel to hide the affected processes from the process tables etc. However it can do all sorts of cool things in kernel space to muck around with the processes. And so this means that  So, if you're here until now, the method is to kill -0 every available process in the system (anything from 1 -> /proc/sys/kernel/pid_max) and see if there are processes that are running but not reported in /proc.  What are the tell-tale signs that a Linux server has been hacked?  Are there any tools that can generate and email an audit report on a scheduled basis? a) This check isn't an extensive check, since the well coded/intelligent rootkits will ensure that the kernel will reply with a ""process doesn't exist"" reply making this check redundant. Other monitoring systems such as Zabbix can be configured to alert you when files such as /etc/passwd are changed. This will give you a quick indication if any of your main server files have changed in the last 2 days. Here's a bash script that implements all that - https://gist.github.com/1032229 . Save that in some file and execute it, if you find a process that comes up unreported in proc, you should have some lead to start digging in. b) Either way, when a hacked server has a ""bad"" process running, it's PID usually won't show under /proc. Tripwire is a commonly used tool - it notifies you when system files have changed, although obviously you need to have it installed beforehand.  Otherwise items such as new user accounts you don't know about, weird processes and files you don't recognize, or increased bandwidth usage for no apparent reason are the usual signs.",4
"For example, I have a local webserver known as COMPUTER8, which handles some requests. However, when using Google DNS, the DNS fails to return the local IP address.  BTW, you typically set up servers with fixed IP addresses. This is precisely because DNS servers cache the IP addresses of servers. If DHCP changes the IP address of your server, the DNS server wouldn't know. Is there a good way to use Google DNS and still be able to use local servers? I presume the local servers obtain their DHCP IPs from the router, so would it be possible to set the secondary DNS server to that of the router?  SO, the typical setup is that you have your own DNS, and hook that up to some upstream DNS, which may then forward the requests even further. Your own DNS is probably smart enough to cache many names, so www.example.com will be requested only once. Your own DNS thereby also speeds up webbrowsing, it's not only resolving local names. The solution is to create a local DNS server as default DNS for your machines on your local network, and on that local DNS configure google DNS as Forwarders. So the result is a merged DNS with local and web entries I would like to avoid using hosts or specifying hard IP addresses due to the dynamic nature of the local network.  DNS is very much a federated system. Google's DNS don't know the IP for some.internal.sever.at.example.com. However, the Google DNS does know how to contact the DNS server for *.example.com and forward the answer.",3
"Or in other words, that specific bit in the bitmask cannot be set, and is returned after calculating the permissions on the user object. We run a multi-directory environment (AD and OpenLDAP) and perform password synchronization via an internal webapp. This works well because we've disabled users from changing their own password via OpenLDAP and AD could only be accessed by the few services that require AD. If you're having issues with setting the security, ensure that you have Advanced Features enabled in ADUC (View --> Advanced Features). I am not sure how you can achieve this using program but in Active Directory User and Computers. Select User and go to properties. There is Option of ""User cannot change password"" option. However, we are now looking into allowing PC's to attach to the AD domain. Initially, I believed that disabling password change for users would be as simple as changing the initial userAccountControl LDAP attribute we assign during account provisioning. This proved to not be as simple as I assumed. We currently use Python and python-ldap for account provisioning (code below), Per Microsoft docs, we set userAccountControl to 66048 (Normal account and don't expire password). I tried changing it to 66112 (66048 + Disable user password change) but AD did not retain that value and instead, recorded it as 66048. What you're looking to do is deny the SELF pseudo-user access to change the password. The best way to do this would be configure permissions on an entire OU to restrict password changes. From here, move all of the relevant user objects into this OU and ensure that the user objects are inheriting their permissions from the OU. Also linked from that document is http://msdn.microsoft.com/en-us/library/aa746398.aspx, which describes how to programatically adjust permissions on user objects. Similar to maniargaurav's solution, but you can do this programmatically using PowerShell (if it's Server 2008 R2 or using the Qwest AD cmdlets link text) or VBscript. If you're using Python now, you should have no issue enumerating all users and doing a script such as the following (from Scripting Guy at MS) link text. Has anyone done something like this before? I'd prefer to accomplish it either by using Python or a set-it-and-forget-it setting on AD.",4
"AFAIK, nobody's created any other layer-3 protocols to work with TCP above them, but there's no reason you couldn't. There are examples of communication systems in the military using TCP but not IP since the comm path is a serial-type connection that doesn't get routed thru routers, etc.  If you look at the a TCP packet before it's headered with IP fields it seems easily possible to not use IP if your ""routing"" protocol is different. Still, TCP and IP are two separate things and completely and intentionally independent. The fact that TCP does not require IP is immediately apparent with the fact that TCP can run unmodified on both IPv4 and IPv6, which are two completely different protocols.  You can replace IP with something else.  In fact, that's exactly what you're doing when you're using TCP over IPv6.  TCP is still TCP, but the IP is v6 instead of v4. You can pair anything else that works with either protocol, but these two are so complementary it is just a yummy reliable way to transfer data and fill the tummy with internet data. It greases the tube to allow other dry foodstuff and data handshaking alike to support this pairing. But in no way is it exclusive.  The reason why TCP/IP is such a common abbreviation (as opposed to, say UDP/IP or SCTP/IP) is because the two protocols were designed together, and in the original paper by Vint Cerf and Bob Kahn, the two concepts were combined together into a single protocol. Soon thereafter they were divided into IP to provide routing and TCP to provide flow control, multiplexing, error-detection, etc. It wasn't until six years later that UDP was introduced to provide a ""lightweight"" multiplexing layer without the rest of the overhead involved with TCP. The answer is no! For example there is an old RFC describing TCP over IPX: http://tools.ietf.org/html/rfc1791 For those with short memories, IPX was the Novell Netware protocol: http://en.wikipedia.org/wiki/Internetwork_Packet_Exchange With a little work, you could create a competing protocol to IP that would serve the same purposes, but it would probably have to contain most if not all of the same features, and would probably end up looking a lot like IP anyway. You could argue that extensions to IP (such as IPSec) are effectively alternate layer 3 protocols, so there you go.",5
"An analytic proof could go like this: assume the distance matrix did not characterize the graph uniquely, then you could build two graphs that are different from the same matrix... If you are given two sets and they are identical, does this imply the graphs corresponding to each set are also essentially the same? I think the following example with four points answers your question (though it gives multi-sets of distances). See Reconstructing Sets From Interpoint Distances by Skiena, Smith, and Lemke for more information. Now imagine I put the elements from the upper triangle into a set, such that you no longer know which pairs of points produce the distance values. This is essentially the set of edge weights, with no information regarding the relationship between the weight and the edge the weight belongs to. A distance matrix uniquely characterizes a topology in a given space on some give elements given some property such as distance assuming a few ground hypotheses... Let $D$ be the distance matrix between all pairs of points, such that entry $D_{ij}$ corresponds to the distance between points $i$ and $j$. This matrix is of size $n \times n$ and the upper-triangle contains the distances between unique pairings of points. In your case, the distance matrix is defined from a given vertex to the other. If the matrix is symmetric with 0-diagonals, you have a simply connected graph. If you have diagonal elements, you are allowed to use self-loops and if the matrix is not symmetric, you have a digraph. There are $n$ points in $R^2$ (i.e. the 2D real space). We can think of them as a complete graph where edge weights correspond to the distance between points. Additional information on the subject can be found in terms of spectral craph theory, which only defines isospectrality, not isomorphism.",3
"If you have access to N different training algorithms and you can use cross validation to optimize them and choose the best one ( the one that leads to a very low bias error), then there is a high probability that you don't need to use stacking. But stacking is very beneficial when all these N algorithms can not lead to 0 bias error (if you have variance error use bagging), it is a case that may confuse the cross validation algorithm. The other thing is using cross validation to optimize algorithms may lead to different algorithms that have similar biases, which reduces the benefits from the stacking technique. After you finish generating the meta data you can now use all the data the 5 folds to train the base learners. The higher the convergence between the final set of base classifiers and the ones you used to generate the meta data the better. This is why the less the data you have the larger the k you need to go with and vice versa. Here is one last note to think about: using cross validation to optimize base learners my not be very beneficial, but why? You can use the 4 folds ( training data) to optimize the base classifiers. You can also find the best hyper parameters by applying cross validation on your training data, re-train using all training data ( the 4 folds) and then test using the last fold to generate the meta data. you need to know that stacking, is one of the most tricky ensembles and this is why it is not well studied in literature in compare to bagging and boosting. In the other hand it is proven that it is very important, especially for practical purposes.",1
"(If I point firefox to the image via mapped-drive rather than through Apache firefox brings the image up just fine.  So it does seem to be Apache at issue) You could try the following directives in your Apache httpd configuration file to see whether it is due to problems using the sendfile-systemcall or MMAPing :  Bringing the image up across the network through a mapped-drive reveals the entire image.  (so the image(s) itself seems to be okay).   I am bringing up an image directly through firefox (no PHP or other scripting code) and it appears that Apache is returning either a truncated image or a corrupted image.   If I hit ""refresh"" (in firefox) I get about 5 more lines of the image.  And if I hit refresh again I get another 5 lines. What operating system is this on ? Is the file you are serving local to the server apache runs on, or is it also accessed over the network ? I have seen those two be the culprits before, though only if there was something funky going on with the storage subsystem. It is usually a bad idea to disable these, since it eats into performance.",2
"Then you call WD, and they tell you to call Dell, and the guy who sold you the RAM is long gone as well. By the time you've sourced top-quality third-party parts, the price gap may be substantially narrower than you anticipate. You usually get the same warranty on an HDD by buying off of Newegg, and better pricing for RAM and HDDs. The only thing you lose on this is extra money, once. What you gain by purchasing certified hardware is total support for the entire system. For the most part, I buy the more expensive components from the manufacturer (in our case, that's HP).  Sure, for hard drives, I could buy top quality, bare drives for less, but then I'd have to get carriers for them, mount them, and keep a few spares available since I wouldn't be able to call up and say ""ship me a new one."" If you're buying high-end servers from a supplier like HP or Dell, you will need to know a good sales person that can give you some decent discounts, as you may end up saving more than 15% on the total price from the one on the online configurators. If you know how to build servers, you're going to want to buy all the parts yourself from a good supplier and assemble it yourself. It may end up saving you hundreds and even thousands of dollars in some cases. Just imagine, you bought a server from Dell, and then some drives cheaply direct from WD and maybe some Kingston RAM from a guy on the corner Other server manufacturers might actually tel you your warranty is void because you installed uncertified hardware and they are no longer responsibe for the damage it may have done. I only know for certain about Dell :) Then something happens, you call Dell, and they tell you that unless you remove the unsupported stuff from the server, install some supported RAM and HDDs, and then fail diagnostic tests, they cannot help you, because the stuff you have in the server was never tested to work inside it.  Basically, you're paying for: warranty (and knowing that you'll get a spare for the life of the warranty), and convenience. One particular manufacturer that I like is SuperMicro. They are excellent when it comes to bang for buck, and they actually make motherboards, power supplies and chassis which are very good.  If you're buying ""high end servers"" why would you want to dilute the value of your hardware by putting in generic parts? Part of the value of buying expensive hardware is the support you get from the vendor and the extensive testing their hardware engineers perform to certify hardware. Really that's what you're paying for. Some manufacturers tend to use their own firmware on certain parts to make sure you can't add on your own drives to a server.  Some people are under the impression that Dell or HP actually make the stuff inside. This is untrue, as they only make chassis, and assemble parts from other manufacturers, sometimes re-branded parts like the PERC raid cards from Dell.",5
"One of my requirements involve writing some of the information to a database (ie, execution time, size, URI, remote IP).  For that reason I'm apprehensive about piping to a script and keeping a database connection open.  So I'm leaning towards a nightly analysis.   I would recommand to don't use a pipe because foreach request you will lost a lot of time in apache to pipe to the script and wait the end of the script to free apache ressource. You could try using mod-log-sql to log to a database on every request. My preference is to a something nightly when you have lower load, but it depends on how up to date you need the data. So, if for a reason, you database become very slow to do INSERT, you could have all apache threads/process to be waiting for your script to finish your job and can't be used to process new user request I have a need to analyze logs generated by Apache2 webserver.  I'm considering either piping the log to a script that will just wait on the stdin for input, or analyzing the logs nightly with a batch job.",3
"There's only a couple of ""gains"" that I can think of off the top of my head. First off, security. If for some reason a login had to have elevated permissions on the instance but there are sensitive databases then that could potentially be a reason to have multiple instances on the same box.  What will I gain and what will I lose?  Tagged sql-server-2012 because it's the most pertinent, although generally the question is across SQL Server versions. As for losses, you are having multiple instances having to share the same resources. Sure you can easily isolate, but you're still taking away max specs if it was instead one instance.  For a use-case of 9 databases spread across 3 time zones, is it advantagous to spread the databases onto multiple SQL Server instances on the same server? So let's say you have a OLAP and OLTP environment but somehow on the same server, let's say in cases of multi-node failover.  Now you are supporting both nodes on 1 server, by setting affinity masks and total amount of RAM per instance,  you ensure that customers can still process data (and make $$$) while reporting takes a back seat. I have been advised by MS to install multiple instances of SQL server to work around spinlock problems due to Always On. It has not helped with push locks in Clustering Services though, as they sit below SQL Server instances. Another one would be the requirement for different SQL Server versions to support different applications, if you didn't have other available servers.",4
"A file is simply a seqence of zeros and ones and it dosent matter if a file is encrypted or not and when you change a file the file will only change some zeros and ones but not all. Google drive should be able to only upload file changes and not the whole file. I use dropbox with an 500mb true crytp container and when i did some testing the file wasnt uploaded in full size when i changed something in the container. There a different methods and file hashes so that the programm hasnt to upload the whole file when you change something. What 'doesn't work'? You've given nothing to go on here, so I'm going to have to guess to try to help you. Taking the psychic debugging route (warning: prone to just as much error as any other psychic), I'm going to guess that the Google drive app is upset about the fact that the place it's supposed to put it's stuff is missing. IF that's the case, then you'll probably want to disable auto-start on the Google drive app, and start it manually after you've mounted the volume or whatever where you want the drive stuff to land. You could also try just killing the drive app, then mount, then restart the drive app.",2
"I am thinking of setting up a raspberry pi inside a private network to act as a firewall for all incoming/outgoing traffic in that network. Is this possible? I am guessing that if so it would limit all traffic in the network to the throughput the pi is able to output (and therefore in most cases not very useful), is that correct? From everything I've read, the sharing of the USB bus between the wired Ethernet and USB adapters puts a definite bottleneck on performance. So yes, you can definitely use the RPi as a firewall, but performance may be disappointing depending on your needs and Internet speeds. I may still use it as my hotel room travel router where Internet speeds tend to be limited to 5 Mbps or so. I did some informal testing with a RPi in bridged, routed and NAT configurations using a usb wifi adapter to connect clients to the RPi, and the wired 100 Mbps Ethernet to my Internet router. Connecting directly to the router with wifi, I get 60 Mbps download speeds. With the RPi in the path, it drops to 10 Mbps or less. This was without any iptables processing on the RPi. The idea would be to have a router connected to the network and connect the pi (along with all other devices) to the router and configure the router to allow only connections to and from the pi which in turn would allow or deny them access to other network devices. I'm a big Raspberry fan - but in this case I would recommend to use the 'BananaPi R1 Router Board' which gives you the right hardware for your task. This is mostly for educational purposes really rather than a serious attempt at something like this.",3
"This latency-induced factor is the main reason for things like Content Delivery Networks being used by smaller sites that don't need the additional capacity to handle their user's requests. It's also true that driving a network interface uses CPU resources, and so a server doing a lot of network IO will also be using some CPU.  However, these days, that CPU load is probably very small and you'll run out of network bandwidth before you run out of CPU cycles to process it. You see, even a server on a 100Mbits/sec Internet connection and a client on a 25Mbits/sec fiber-to-the-home Internet connection may not see transfer speeds of 25Mbits/sec for a single tcp session. The latency between the client and the server will likely impact the speed at which the tcp sliding window can move forward based on the rate of the receipt of acknowledgement. See this article about tcp perfmance: http://www.cisco.com/web/about/ac123/ac147/ac174/ac196/about_cisco_ipj_archive_article09186a00800c8417.html Lastly, there's an awful lot of network infrastructure usually between a client and the server, and it could all potentially impact the rate of download. It's also true that lots of network connections consume other resources on a server, including memory, so lots and lots of long running downloads will tie up other resources and may or may not result in performance bottlenecks. It partially depends on what you mean by ""server's speed"".  The server's network connection speed will absolutely have an impact on the transfer rate of data.  It's likely the server has a much faster network connection than the clients, but enough clients could saturate the link and result in slower downloads. There is always a bottleneck somewhere. Determining a maximum network transfer speed is a matter of locating that bottleneck. Possible bottlenecks in their order of likelihood are as follows.",2
"For Windows XP there is a PowerToy allowing different Desktops which could be populated with different Program Windows. I would be surprised if there is no solution for other windows versions. In Linux this is pretty standard. You could switch between the Desktops by an hotkey which is pretty convenient. I would say this is the best solution i know about. Both AMD and NVIDIA have Virtual Super Resolution (or Dynamic Super Resolution) configurations now in the latest drivers. Many monitors support many resolutions.  If your monitor is capable of doing that resolution, then you might be able to get it to scale up a bit.  But as far as exceeding a maximum resolution, the answer is usually no.  The hardware generally lacks the ability to sync to a resolution that is not supported.  If you are looking for an possibility to show simply more Programs virtual desktop_s_ are the solution. It's not the size of your monitor that determines its maximum resolution, but the technology built into it. I would recommend finding the latest drivers and utilities for your existing video card, which may have desktop stretching or other capabilities. If that doesn't help, you'll need to get a new card and/or monitor. You could use a virtual desktop; this would allow you to scroll around to support more virtual resolution.  But apart from that, no.  Your LCD has a maximum resolution.  By definition, that means it cannot support a higher resolution.",5
"If you are wanting to copy a block of formulas keeping all references the same, you can press Ctrl+` (backquote) to show formulas and then copy and paste by clicking the icon on the Clipboard task pane (activate using small arrow on clipboard section of Home Tab). If this is not what you are trying to achieve, a simple example would help. If I try to follow the steps to cut and paste from a new worksheet I find that all references in formulas stay fixed when copying to the new location including relative references. In fact in Excel 2010  i find that after cutting and pasting formulas, the first row and column contain links to the old sheets but other rows and columns reference the new sheet which looks like a bug?? I just found a partial workaround. It's not as general as the copied-worksheet-and-cutting-range workaround in the OP, but it can be much faster if you just need to quickly copy something. What we all really want here tho is a way to temporarily disable the absolute references while we copy. There could be an option to allow you to hold alt when you paste to ignore every $, like they just weren't there, but they actually still are once the paste is completed. Due to the total number of columns, XFD# is the last valid cell reference. Make sure the new references generated don't overlap an actual reference in your range of cells or that reference will be broken in the last step. Choosing a letter from a column you never reference in your block of cells to be copied will ensure you do not break any formulas. As an example, if you replaced $ with 'h', $b$42 would become hbh42, which is still a valid reference that can now be copied and will update automatically!  You can try replacing the $ with 2 letters in some cases, but that has a number of problems of it's own, so it would be easier to find a 1 letter replacement that won't overlap an actual reference. The following will work with less complexity then writing your own macro and achieves the final result. Yes, I know I'm not using the Absolute cell reference but as shown in OP's example you don't need it.",3
"See ""Basic"" Approach under Retrieving the Source Code & Building/Compiling the Modules on this page: https://www.linuxtv.org/wiki/index.php/How_to_Obtain,_Build_and_Install_V4L-DVB_Device_Drivers I've never tried if it works on the Raspberry Pi, but it works on a regular PC. It may be difficult to record and display the result video at the same time because the Raspberry Pi is very slow, but if you send the output directly to a file it may be fast enough if the USB ports provide enough bandwidth. Something else to check is if the device uses too much power: if you see a little rainbox in your screen, you should connect this device to the Pi through a powered USB hub. I'm the guy who wrote this wiki page and created the fix. I pushed the code to the Linux kernel so the device is detected if you are using version 4.13 of the kernel or later. The device was tested with a Play Station 2, DVD player and a Nintendo 64. It worked for all of these devices except the Nintendo 64 (the video frame froze when this device was connected). You can run uname -a to find your kernel version. If it's older than 4.13, you may want to build the latest version of the media drivers for Linux. You'll only need to compile the kernel module, which is faster than building the entire kernel.  Once you run the script that downloads the latest tarball, you should be able to find the fix in em28xx-cards.c, which is the same as displayed under Making it work in the wiki page that you have linked.",1
"Is this possible? e.g. see screenshot link below, I just want my client to see/access Transcript folder, nothing else. http://i.stack.imgur.com/kEHfW.png One approach: The 'match' feature in sshd_config allows you to specify rules based on group membership or username. Now their need is when user login to sftp server either via winscp or some other client, they should be able to see only their own folders in home directory, the other folders should not be visible to them. Also in their home directories they should not see any files or folder which start with dot(.). Thanks everyone, finally this link helped me to achieve my task. http://rmtheis.wordpress.com/2011/07/03/setting-up-an-sftp-site-on-amazon-web-services-ec2-creating-an-account-to-share-with-a-third-party-and-restricting-that-account-to-allow-only-sftp/ The display of hidden folders is a client-side issue and not something you can really influence server-side.  My client needed sftp server for sharing files, so I created sftp server on a amazon ec2 ubuntu machine and added different users.",2
"CloudFront insists that the TLS negotiation between itself and the Origin be trustworthy.  A factor commonly overlooked is that TLS (SSL) certificates do two things: provide for encryption of the connection (obvious) and provide for authentication of the server -- attestation that the server is authorized to be a server for the requested hostname, and is not an impostor (less obvious).  This is why you get browser warnings if the server's certificate's subject and/or subject alternative name doesn't match the hostname in the browser's address bar. ...then it works, with one exception: the dzczcexample.cloudfront.net domain can't be used in the browser to access your origin through CloudFront.  In some configurations, this is desirable, because you don't actually want your content accessible via that second entry point. If you set the Origin Protocol to HTTPS Only, the traffic between CloudFront and the ELB will be secured by TLS, and you can configure the actual origin domain name either way.  The Cloudfront-Forwarded-Proto header will indicate the protocol used between CloudFront and the viewer (since in this configuration, the load balancer will always set X-Forwarded-Proto: https). Note also, if you want to ensure that only your CloudFront distribution can talk to your origin, you will want to configure a secret Custom Origin Header in CloudFront, and  the Origin needs to reject requests that lack this value.  While it is possible to use the ELB security group to allow access only from the CloudFront address space, that space grows fairly often and you need a way to keep your security groups updated to allow new address ranges... but it is my opinion that this provides a false sense of security, since anyone can technically create a CloudFront distribution and point it anywhere, including your ELB.  Using a custom header avoids this. CloudFront can use any Internet-accessible hostname as its origin -- the origin doesn't have to be inside AWS.  You could, for a random example, use a Google Cloud Storage bucket as an Origin in CloudFront.  The integration between CloudFront and the Origin is a loose one -- by that, I mean that CloudFront has no special awareness of ELB.  It just resolves the hostname via public DNS and makes connections. CloudFront allows a limited exception to this requirement, allowing certificate validation to succeed as long as one of two conditions is met: Otherwise, your best bet is indeed to do what you have suggested -- mapping a hostname in a domain that you control onto the ELB in DNS, and configuring that hostname as the origin domain name in CloudFront. The first condition isn't possible if you use the ELB hostname as the Origin Domain Name, which is why the CloudFront console prompts you to whitelist the Host header for forwarding if it sees that your target is an ELB.",1
"I prefer to move it to /tmp because I remove the file from the web application but keep it just for if I removed the wrong file or I just want to know how is built the file. In addition to his answer, I would like to say that when attackers infect your server they usually (not always) create a file with execution permissions. Allowing www-data to write on public directories like wp-content/uploads don't prevent the attacker to write a php file and call it from outside. In this case, and using apache2 as a webserver, you can add a .htaccess file in your public directory to disable the php execution. Here is the .htaccess file that I place in wp-content/upload to avoid php execution in public directories. Moreover, set the right permissions are ultra-important. I'm one of those who think that apache user (httpd, www-data, apache...) must not be able to write anything on the server but wp-content/uploads or whatever public directory you have configured. This prevents of plugins or core to be modified by attacker, if the attacker access to your server with apache user. I like to use this script to set Wordpress permissions.",1
"Since you only get dot-names (and not MIB-defined object names) in the response, snmpwalk probably guesstimates what the object type is. So, I'm wondering whos fault is this? Does the remote device send the variable's type in responce? Or snmpwalk has to guess the type? As you can see some variables are of a STRING type, others are Hex-STRING. The correct output would only have Hex-STRING types. You can pass snmpcmd options to snmpwalk such as -Oa or -Ox to force variables not having a display hint to be interpreted as ASCII or hex respectively; or you can use -d to dump the raw packets. The explanatory text for -Ih is quite informative, too: The best place to find fine detail on the SNMP standard is of course the relevant RFCs; the O'Reilly book Essential SNMP is probably worth a read, too. This mailing list post explains things; per RFC 2579, MIBs can contain a DISPLAY-HINT in their definition of variables. This defines how the data might be displayed; e.g. as a number to 2 decimal places, an ASCII string, etc.",3
"Same issues.  Completely out of ideas, even checked using openssl the new .pem files and they do expire correctly in 90 days (they are from letsencrypt). His issue is mine to a T, and all things he tried more or less I did too.  The difference was his was solved because he had nginx running.  In my case I have no such reverse proxy server.  So I just cannot get Apache to see the new certs I got using certbot (that was a whole other issue, certbot auto renew didn't work gave errors and so I did a certbot cert only apache and pointed appach ssl-certs in etc/httpd/extra  to there. Alas nothing, no change, websites still report the expired ticket.  Which was in another folder /live/apo.nmsu.edu-0004 which I moved to tmp.  So not sure how apache is still picking all that up. I guess you should check what certificates you have in the files pointed to in your Apache configuration, especially that you seem to have some symbolic links involved. Tried everything else like he did.  Moved the folder the /etc/httpd/extra/ssl-certs and ssl-certs-proxy were pointing to to /tmp, and had those files point to the new .pem location:",2
"My thought on whether it's the Right Thing To Do or not is that there really is no guarantee of confidentiality on a work machine. Right? The expectation of privacy on work equipment is something that varies from country to country, and even locality to locality. In the US there is large precedent for any activity on work-supplied equipment being fully and completely auditable. That's not always the case elsewhere. In Europe, where worker protections tend to be more robust, I wouldn't be as confident as I would be in, say, Alabama.  Depending on the contract you have with employees, it can be a risky area to go spying on them.  Generally speaking, if you suspect them of wrong doing then it could be legally permissable; and if it is exclusively on work related activity, then it is usually fine. Hidden/administrative shares are easy enough to create, they have a $ on the end of their name.  See this knowledgebase article on the subject. Disregarding the morals (as this is a technical site!) the way to do this would be to create it with a share name and a $ at the end of the share name (which would hide it). Going back to the technical question you asked, you can share a folder on a network with restricted access. Other people can see the folder is there but they will not have access to it unless specified. Yes it's possible. I'm wondering if this is technically possible (I'm the system/network administrator so I can pretty much go anywhere, do anything). I need to share the folder with one other staff member. Again, secretly. As a side note I think the situation leading to this is more of a people problem better solved with a people solution rather than a technical solution. This is more of an ethical issue rather then technical issue. The question you need to ask yourself is, are you violating company's policy by ""secretly"" sharing someone's document with another staff member. It's often normal for someone in management position to ask for this type of request. But it's quite another when it's a colleague in the same level. Personally, I think you cross the boundary of your job responsibility and not to mention company's policy. I'm sure most companies are frown upon this type of practice. I've been asked to secretly share a staff member's My Documents folder on her XP Pro workstation with another staff member. As others have pointed out, such shares are easy to create. However, they're also easy to discover if someone knows how to look for them.",5
"If you notice that another website hosted at a server at the same data center is loading faster, then perhaps bandwidth isn't the trouble.  Your browser could be rendering your page more slowly than it renders the other page. For a large page, yes, it will show slower. X bytes take longer going through a 10mbit line than through a 100mbit line. Deciding on speed by when a page is rendering isn't a good technique.  The content of the page affects the rendering time.  There are a variety of ways to actually time the transfer, but again, that's probably telling you more about your Internet connection than your servers. Note that the transfer speeds that you notice are limited by the slowest link in the chain between you and the server.  If you are testing transfer rates by loading a page in your browser, you're probably being limited by whatever network speed you have available through cable/DSL/whatever. For investigating the performance of your page, take a look at http://developer.yahoo.com/yslow/, there are other similar tools available. Lots of webpage requests are going to be small, so latency becomes a significant factor because it is the initial delay in the request and delivery of a page. It is probably because the new line has higher latency.  I recommend you go read ""It's the Latency, Stupid"" (Not calling you stupid, that happens to be the title.) That you use an average of 5mbps is irrelevant - you do not talk of averages here. The question is not so much the average, as the spikes, and the spikes can not be handled that fast on a 10mbit link than they can on a (reasonably fee) 100mbit link.",3
"For now, you can use an emulated WebSocket that relies on Flash. gimite has one implementation, along with a Ruby backend. (But the backend of WebSocket is so simple you'll probably be writing your own server for it IMO. I'm really not at all sure what the point of tying that into Apache and the obsolescent mod_python is.) I've been reading quite a bit about this now, and feel like it's time to put it to the test. Only problem is: There aren't any native support out there. I know it's a bug for both Firefox and Webkit, which means it won't be long (hopefully) before they're in the nightlies. And I know Apache is (kinda) working on websocket support, but they seem to have been beaten by a 3rd party extension (experimental, as they call it). So. To the question. Has anyone gotten to the point where they've successfully built their own Firefox or Webkit-browser with websockets, and gotten a server running that supports websockets? If so, could you post a quick how-to. I think it'd be a bit premature to put WebSocket support in the browser just yet, the API isn't finalised. I've been trying to build Firefox, using the patch from the bugzilla, but it keeps getting rejected. I haven't looked at the 3rd party extension to Apache yet, but if you have, let me know (:",2
"I've studied the behaviour of the DNS proxies inside home routers extensively (including the DG834), and never seen a problem that was query type specific. I'm assuming your mail traffic runs on standard SMTP port 25?  If so, you'll want to set some NAT routes for traffic on that port to/from your mail server, and setup your router to use your ISP's DNS servers.  That should accomplish what you are looking for (assuming you want all mail traffic to go through your mail server). I would expect this behaviour to be caused by a bug in the firmware, or perhaps a very obscure setting but I can't guess at a reason. You've already tried to debug with Wireshark so I'll assume you have some knowledge of networking, and have already pinpointed that the issue is actually with your router. I don't see anything in the product manual that states that the router supports DNS, so I don't know what you are expecting it to do. This is quite unusual. I've never seen a home router that treated some DNS record-types differently from others. I am leaving the mailserver program configured with the Google public DNS server addresses.  It's a workaround but I am satisfied with it.",4
"Once that is complete you will need to create a Proxy that uses this credential. From the SQL Server Agent > Proxies > Operating System (CmdExec) folder you will need to right click and choose New Proxy... Use the credential you created earlier. It looks like you need to configure the second step of your job to run as a domain account with permissions to write to the \KWS2-WEB-SERVER\Share\Reports\Uur share as well as read access to the C:\Reports\Energie\Uur folder. You first will need to add a credential to SQL Server. From the Security > Credentials folder you will need to right click and choose New Credential... Fill in that information and click OK. Basically your .bat file runs when you run it manually because you're the owner.  It trusts you.  When you run the .bat as a different user, the cmd will actually display a warning and ask you if you trust it.  Your job is hanging because the cmd is waiting for your answer.  Adding ""--trust-model always"" will skip this question and allow the .bat to finish. Typically when a job like this hangs it is because the command execution is waiting for a response.  Because any command execution run from SQL Server is run as a shell, and you can't see the shell, it can't get a response.  If I were you I would try running the bat file manually and see if you are asked for a response.  If so correct that part (so that it doesn't ask for a response) and try again. If running it yourself doesn't help then try running it through xp_cmdshell and see if you get any output back.  It may still hang though. Now you can configure your second job step to use the Proxy you just created using Run As: drop down of the job step page.",3
"NLB is a load balancer, not a reverse proxy. IIS has it's own ROUTING mechanism that can do the rest (but would require a balancing server in front to do the routing). I am wondering how NLB compares in terms of features with a reverse proxy such as nginx. I do appreciate that they are implemented very differently, however nginx can route requests to web servers based on factors such as HTTP methods, headers, etc... So I am wondering if such a thing is possible with NLB. If you need to balance based on higher layer information like request-type and whatnot, you are better off with haproxy or some other dedicated load-balancing software... far more flexible and nicer to your network.  NLB is useful, but only within a narrow band of applications. I am currently investigating Windows Server 2008 clustering features (for fail-over and load balancing purposes). I am thinking of using NLB to balance HTTP requests to 2 or more web application servers. Is it possible to configure NLB to route traffic based on the type of HTTP requests or does it only operate at the IP address / port level?  NLB is a hairy beast and has some unique network requirements that can drive a switched network into the ground if you don't organize things correctly.",3
"Use of revision control would also fit into the title I proposed, and obviously that can be done with almost any language. A somewhat different idea is to double-down on compilers-related stuff -- JavaScript is actually a somewhat common target language (e.g. see languages like TypeScript or CoffeeScript that transpile to JS). For example, you could start by transpiling some homespun language into JS (or a simple language like lisp), then ask them to write an interpreter for their custom language, then finish off by asking your students to write a compiler generating webassembly. There's a lot of existing tooling/scaffolding you could probably use here to try and smooth out some of the rough edges. And to keep things simple, you'll have the student implement all of this in JS (or maybe TypeScript?), because why not. Alternatively, you could instead turn this into a mini compilers or parsing lesson: when writing React code, you can optionally embed HTML-like expressions (JSX) in your code. You could maybe explore having your students write a pre-processor for JSX: parsing and manipulating AST representing HTML-like expressions is probably going to be easier then manipulating ASTs representing full-fledged programs. (The one complication is that JSX is intermixed with JavaScript. So, to simplify this assignment, you should probably provide a regular JS parser the students can hook into/invoke.) For example, one project might be to guide your students through writing a simplified version of libraries like React or Angular -- probably after having your students use those libraries directly so they know what they're trying to emulate. This can be an opportunity to teach about some of the more interesting programming paradigms like functional reactive programming. (React was originally written in Ocaml, so you could maybe dovetail things there). All programming languages are tools. Whether one is a programmer, a computer scientist, a student, or an instructor, the tool status of programming languages remains constant. Selecting the right tool for the job, out of those which are available at the time, becomes an exercise in itself. The second half of the exercise is how to use the tools you do have to achieve your objectives, even when the tool available is not the one you would prefer, nor even in the top tier of the tools best suited for the task. You can still use JavaScript as a tool to teach some concepts - educational concepts. Use the learning of JavaScript to teach them how to learn. Both how to learn in general, and how to learn a new coding language. Should they get into a career path that never sees the need for JavaScript code, they are still very likely to encounter the need to learn a new language at least once in their career, if not in their further education. Perhaps you can also include education on how to evaluate a language for its strengths and weaknesses, or a segment on how, and why, new languages are created. One big difference between Java and JavaScript is that Java has enormous standard libraries, and JavaScript has very little. In consequence, JS forces you to either reinvent the wheel or learn to use a package management system. I think I would hire the people that learnt the educational languages, and so have a better understanding. I can then train then in the language that I use. JavaScript just might not have anything about it which makes it better than others available to you for covering, or demonstrating, any of the ""core programming or theoretical CS concepts."" Such a position, however, does not disqualify it for use as a teaching tool. Neither in your situation, nor in a general sense. But I would recommend against forcing the use of a language in a way that isn't natural to its design. While Java now has functional elements, I wouldn't use it to teach about FP as a paradigm. Frankenstein built a monster, of course. No need to repeat it.  The main disadvantage of these ideas is, of course, that it'll likely take a non-trivial investment of time to pull off. If you have only a few weeks to cover JavaScript, you'd likely need to significantly scope these project ideas down, or just not use them altogether. As the others have said, you'll probably want to avoid gratuitously welding two unrelated CS topics together, or avoid giving your students the ""frankenstein"" experience. As stated, the choice to move JavaScript into your core curriculum is ""for vocational-readiness reasons,"" rather than for some concept it demonstrates uniquely well. Perhaps, then, it ought to be used as a tool in the same direction. Not being a certified prognosticator my predictions are not worth much. Nevertheless, I think JavaScript will be a work-place tool for some time to come. Teaching it to your students is likely to be of value to them, even if there is no shining spot in theoretical concepts for it. Using such an approach enables inclusion of JavaScript in the curriculum and offers multiple objectives in the ""vocational-readiness"" direction. It also avoids following in Dr. Frankenstein's footsteps. Javascript, on the other hand is another OO language. It is different from the usual candidates, however, since it is prototype, rather than class, based. But that is a detail that an experienced OO programmer can become familiar with outside of any course.  But if you want to have students learn a new language, especially one for which they also grok the basic paradigm, a project course is a good way to do it. Students build something significant in a new language. If you design the project you could also require aspects that they haven't seen much of before, such as concurrency.  I see little value in the panic to teach what will be used at work. They will get this experience when they start work, or if they do a project using these tools (after learning to do it properly). And work will not teach them what they missed at school/college/university. More broadly, I think teaching your students how more complex libraries or framework work is likely to be a more valuable and long-lasting skill compared to just teaching whatever language or library is trendy today. I don't think it particularly matters which library or framework you're implementing: the main value comes from the act of exploring some non-trivial implementation, and from ""peeking"" under the layers of abstraction we're already used to. And if you can give students exposure to JS (or whatever other language) at the same time, great. It would take a lot of work to set up, but you could even do a kind of open group project where the class is divided in quarters for the first half of the practical side, with each group given a spec for a library and instructed to publish their implementation to an in-house repository. Then in the second half each individual must develop a project which uses the four libraries. Wrap up with written feedback on why they chose the dependencies they chose for the second half, and read out selected feedback to the class. The goal would be to learn by experience the value of documentation and encapsulation. Maybe a bit of an orthogonal answer, but let me try to refocus your thinking. I don't think that teaching a lot of languages, especially if they are similar to one another in some way is a big advantage for students. Giving them experience with different paradigms (ways of thinking) on the other hand is a big advantage. While I prefer Java (for its libraries), Ruby is actually a more pure OO language as, is, of course, Smalltalk and its successors. Scheme or Haskell can be used for FP, of course, and one has actual syntax.  But, many universities have a Language Principles course in the curriculum in which students study the fundamental ideas of languages without necessarily building a compiler for any of them. An older book covering this is by Ravi Sethi. You need to be careful that you focus on ""principles of"" and not ""examples of"" languages, however. But it is a good place to take a meta level view of languages and paradigms.  One idea which might be a bit too trade-schooly but which I offer in case you disagree: the practice of software engineering. One interesting idea might be to ask your student to try re-implementing different parts of the web ecosystem, which would naturally involve using JavaScript.",5
"That being said, a ReadyBoost drive might help alleviate some of the I/O bottlenecks and improve performance slightly in some of the more intensive applications - like Visual Studio - if your tasks often involve fetching data from the hard drive (like during a compile/build). it needs such more ram. my minecraft can maximal give 20FPS if i am lucky and on the lowest settings (FPS = Frames Per Second (in your game) ) I would suggest using the Resource Monitor or Performance Monitor and keep tabs on how much pressure is on your CPU, RAM and Disk I/O. You might find that you are really CPU limited, or maybe if your disk read queue length is constantly above 1.0 you will get lucky see some benefit from the ReadyBoost drive. Also, you are probably hitting your pagefile pretty regularly, you might want to check and make sure it is properly defragmented with a tool like PageDefrag from the Windows Sysinternals group at Microsoft. AND IF you wanna use readyboost only for minecraft, just google(youtube) it (""how to speed up your minecraft"" like names). It is actualy posible. i didnt test that but i think it will work better than standard using readyboost ;) The sad news is that a ReadyBoost drive will do nothing to help your out-of-memory situations like running games, Visual Studio or Virtual Machines - it doesn't get added into the available RAM pool. ReadyBoost really functions as more of a disk cache. but when i used a simple 4GB (precicely 3,86GB) usb, i have on the lowest settings 5enter preformatted text here060FPS!!!. but thats only the ram, for grapic like games you need also a good grapic card. (I have Intel pentium inside CRY!)so it can also be your grapic card. If you're at 2GB of RAM then you're most likely not going to see much benefit from ReadyBoost. AnAndTech had an interesting article regarding this. See below for some performance charts. and you have to know: readyboost wont use it for minecraft but for the extra system cache and other High-priority stuff. but becouse the RAM becomes free by the readyboost, the minecraft, other grapic games or (high-ram-use)programs have more ram for them self to produce better quality or speed!(or both). I have an HP Pavilion s3000y desktop. The only problem is, the motherboard only supports a maximum of 2GB of RAM, which is not enough for some games or other memory hog applications (like Visual Studio or running VMs).",4
"I'm particularly interested in what others who has walked the same path as us feel. For the record, the game is a spin on the real-time strategic genre with a classic 16-bit look and feel. I would suggest reduction. Generally break down game development into series of tasks, not abstracted under Marketing or R'n'D umbrella but actually tasks. It seems to me like the only problem with what you're doing is that people will feel like they're being undercut and that you may be 'weighing' the money in your favor. Otherwise, it seems doable, albeit complex. It's somewhat similar to what's some of Agile methodologies use, when features are broken down to functional and non-functional tasks and each assigned points depending on time this task will consume. A friend and I are building a game together and having a great time with it, and we hope to sell the product at some point. The prospects for actual earnings maybe be practically nil (and we aren't getting our expectations up), but regardless, we wanted to get some of the hard ""profit splitting"" questions out of the way. You're a small team, hobbyists, and all expending effort towards just one game to be sold and not even founding a studio and therefore you need to act like a Volunteer organization and everyone's help is important. I'm going to suggest the simplest method of all.  Take a small cut off the top as founders, 10% between the two of you maybe.  Split the remaining 90% equally between anyone who puts in a solid amount of work, including you two in the mix. If you're uncomfortable with that then offer a deferred salary to be taken out of first sales and remove the percentages entirely to make it entirely a business transaction instead of a shared effort. I've personally only experienced working on a project(s) with one other person, but I think I can still offer my two cents. Once you start playing games with ""your job is only worth 75% of what this other guy did"" with a deferred payout on a hobby project you're only asking for bad feelings when what you really need is team spirit. So this example, one person can find and filter candidate for example for programming role, but I guess he won't be interviewing the, existing programmers will interview candidates for programming roles, and existing artists will interview potential artists. This is more accurate way of dealing with tasks I think. For example you already broken HR into: hiring, legal, distribution. You can move further and break following actions: Finding candidates, filtering candidates for interviews, interviewing candidates, handling contracts. Originally, my feeling was that we are both putting a full time effort into the game and would split the profits evenly. I am a programming and his role would be art and music/sound. Game design and concept generation is something we are both doing together. This has become complicated, as we are now looking at ""hiring"" a few other interested parties and we want to split any profit up fairly. We were looking at perhaps hiring another artist, and perhaps a sound person. We are not sure at this point what level of commitment our hired hands will be willing or able to commit. Our idea was to make a very large list of all tasks that need to be done across all categories (programming, marketing, art, sound, research, design, etc) and assign weighted values to each task based on the time requirement, difficulty/value of skill set required, and importance to the project. The classic formula tends to go something like a 40-40-20 split if there's a programmer, artist, and sound designer. Although even this is just the 'general' thing, and by no means a rule of any sort. Then, we can assign tasks to our hired hands and as they complete them, they would earn that percent of any profits that came in. My hope is that my original partner and I would divy up the ""remaining"" profit (the money our hired hands don't earn) equally when all is said and done. There's still a lot of details to work out, but our feelings were to divy things first into broad categories as follows:",4
"Try completely wiping your iPod on the device itself then reconnect. There may be something corrupt that the PC doesn't like. I ran into a problem like this where my company added a network drive that was the same drive letter the ipod wanted to be. If it is able to sync on the Mac then my guess is that the iPod is not formatted to work with iTunes on a PC. They use different formatting and normally will prompt you to format it to work correctly on the Mac. If you have enabled disk use then a portion of the drive is Fat32 and that would explain why the PC sees it as a camera but iTunes can't read the library directory correctly. Not sure if you already did this or not. You can try to restore the iPod on the OSX machine, then see if it'll work with the iTunes install on the windows machine. This will lose all data currently stored on the iPod, so make sure to backup anything that isn't synced through iTunes before hand. Your Post says Windows 7, so this may not apply, but in XP I recently had to goto the Control Panel, Cameras and Scanners, and change the default action associated with the iPod from open with ""Microsoft Office Scanner"", to ""Do Nothing""  This solved my daughter's issue iTunes freezing during a sync.  Don't know if this crosses over to Windows 7 though. It may work to reset the iPod back to factory defaults (since it is sync'd already with the Mac and you can pull the pictures off it manually, you shouldn't lose anything). I believe using the ""Erase all content"" in the settings will do this as well as holding the home and sleep buttons together. Once this is done then it just needs to be connected to iTunes again to be setup and reinitialized. At this point Win7 iTunes may pick it up and set it for sync.",5
"Kaspersky Internet Security is the consumer version. It doesn't support server OS's, so Windows 2008 R2's right out. ESET NOD32 Antivirus 4 is sold on single server basis. I just spoke with sales and have installed a trial on the one server that needs AV. Their web site is really confusing though. I thought the same thing until I called them.  We like Trend Worry Free (hosted on their server or yours), we can add all of our clients to one control panel and manage changes from there, no AV servers at client sites.  This may help with the single user since it can handle multiple sites. I always liked ClamWin.  May or may not meet your requirements though.  It doesn't scan everything all the time, so performance is better, but that might be something you need.  Interestingly enough, I see it performed horribly on an AV ""roundup"" test a few months ago, but the previous year it was a top performer.  Not sure about the status at this time though.  I'm pretty suspicious of those tests though. Personally I like AVG, and you can buy it on a per machine basis.  The only downside is that from time to time when they update their scanner it requires a machine reboot so you have to schedule those for convenient times. You'd be looking at their Kaspersky Business Space Security suite, in order to get the Kaspersky Anti-Virus for Windows Servers product, which supports Windows 2008 R2  Unfortunately you normally can only buy these in bundles of 10+ licenses, and will start at around 250. Kaspersky might be able to do you a deal though.",5
"We have one virtual machine acting as munin master and all other virtual machines are munin nodes (version 1.4 on Ubuntu 12.04 LTS btw.). So my idea is to create another munin virtual machine just like the one I already have and so my question goes about if it's possible that all virtual machines only poll once the virtual machine and send their data to two munin servers. If you have any custom plugins, which store data, and expect it to stay the same for 5 mintues untill the next poll, you will have to fix those modules. Same for plugins that alter data somewhere on the drive*. The idea is to both not stress virtual machines twice just to gather the same information and at the same time be able to shut down (for maintenance, or because of an accident) a monitor virtual machine and still be able to see the graphs. Just fix allow directive regex in /etc/munin/munin-node.conf on your nodes to allow both servers to access the munin-node.  *eg. if your plugin counts the number of lines in the log file, and then empties it, the second munin poll, immediately after, will report a value of 0 lines in that log file.  Otherwise, most (cpu, load, temperatures, etc.) plugins work without problems. We had the same setup when migrating from munin 1.x to 2.x with cgi and new OS, when we needed the old one to work, while we tested the new one with production data.  I'm in the process of making everything on our server setups redundant, and so comes the monitoring.",2
"Because typically, networking gear goes into its own racks, and servers go into their own racks. The network rack will often have patch panels in it, also on the front, so that the cables all just go into cable management - on the side of the racks and/or across the front of the racks. My assumption is that is makes it easy to plug/unplug cables, and to watch the blinkenlights. It does make the cable management a bit more awkward though, since I'll have to feed all the cables from the back of the rack to the switch ports on the front, either via the side of the rack or by leaving some vertical space between the devices. The reason is most environments (think business office rather than server closet) use switch ports primarily connected to patch pannels.  These, of course, run to office / cubes.  Very often all wall ports in a building are not live, but will be connected based on need.  By having both the switch ports and the patch panel ports facing front, making changes as people move is easier than reaching into the back of the rack. In server closets, individual or small cluster of racks, as has already been mentioned, people will sometimes choose to mount the equipment reversed.  This is fully supported by the manufacturers who generally place mounting holes for ears both front and back. They do make some switches with the power and ports on the back and led's on the front. They are usually called AV switches but they function the same. Example... luxul.com Since I have little experience with racks I wonder why the switches have their ports on the front of the rack, while all the devices that connect to it (SAN, servers, tape, firewall) have their ethernet ports on the back. The same holds true for large data centers, where primary switches will be centralized with distribution out to individual servers or network equipment housed in racks throughout facility.  Again, easier to change as needed by having everything front facing. I've ordered some rack hardware from Dell. A 42U rack, a 3U EqualLogic SAN, two 1U PowerConnect 5424 switches, three 1U PowerEdge servers, and some other stuff such as a tapedrive, firewall, UPS, etc.",4
"It sounds like your IT department suffers from what pretty much every IT department in a large company suffers from. Building little castles and defending them with your life, not letting others in, being bossy, etc. As someone who deals with other people's IT departments every day, I see it all the time. And it's frustrating. However it is the IT departments job to ensure that YOU can do YOUR job properly. They need to make sure that you have what you need, when you need it. If you have a piece of software that the business needs to keep running, they have to provide a platform for it to run on. That's their job description. But you need to make sure that they have the information they need to do their jobs. The basic fact is though that the change needs to happen from within the IT department, and it has to be initated from above them. And if you can get IT to realise that they are not a force unto themselves (as that most of them do not generate income for their businesses this can be quite a slap in the face), and that they are there to support the existing staff and enhance the business, then you'll find that your questions become irrelevant, because everyone will be playing happy families. So, the IT department are very justified in not putting this random computer on their server network. If you had come to me, in my organisation, told me you're starting a new project, I would have given you three VM's: Dev, Live and Staging. You would have full admin rights to Dev, and we would discuss what you needed to do your job for the other two. If you needed full admin rights to them, and could justify it, then you would get it. We have our VM deployment down-pat. VMWare makes this incredibly simple - it only takes about 5 minutes per VM to get it deployed. If you came to me with a workstation class machine loaded to the hilt with consumer grade RAM, consumer-grade HDD's, consumer-grade PSU and consumer-grade RAID, I would refuse to put it on the server network too.",1
"That said, if the drive does die, this gives them grounds to deny a warranty request if you mention doing that, and pretty much covers them if there's data loss. ""It says so in the manual, you were using it wrong""  It most probably won't but it indeed can. Any electromagnetic event has a chance, unexpected ones especially. This has actually happened to me once about 15 years ago - my laptop mainboard has got semi-fried after using a USB hard drive (a seemingly healthy one, I still use it occasionally and it's ok, no incidents have happened to it or been caused by it since then), USB ports became USB 1.1-only and memory quirks (observable with memtest86 and causing occasional unexpected behaviour during the computer usage) began to happen. I've had to apply warranty to replace the mainboard. My understanding is that the USB standard says that if a device is active, then electricity may be flowing to and from that device, and the device is allowed to count on the idea that the electricity may continue.  This is a key reason why operating systems have support to ""safely"" turn off a USB device; the OS tells the USB device to start expecting that the electricity may become unavailable. In practice, the risk seems small for USB-based thumb drives, because people do that all the time.  In practice, this DOES occasionally damage the thumb drive.  When I was actively working tech support for dozens of companies (simultaneously; I worked for a company that other companies used for tech support), I don't ever recall hearing of the computer being damaged.  So it must be exceedingly rare, but I think that the potential does theoretically exist. Some USB implementations have done an extremely poor job of handling unexpected device disconnection.  Samsung has no way of knowing whether its drive might be used with such an implementation.  Consider the following sequence of events: This is still called ""hot pluggable"", because the computer remains hot: you're not required to shut down the entire computer. I have personally seen a usb drive fail to mount on unix after being pulled out of a Windows machine without ""Safely Removing"" it. Putting it back into the widows machine and doing a safe remove fixed it. Its unlikely you will damage your pc, and somewhat unlikely you'll kill your hard drive (though sudden power stops are bad).  Even on a really sloppy USB implementation such a sequence of events would be unlikely to occur, since everything would have to happen at certain specific times, but there's no telling what could happen if events unfolded as described.  Since it's possible that the USB implementation might work perfectly and reliably if nothing is ever unmounted unexpectedly, but an unexpected dismount could cause unbounded disaster, improperly dismounting of a device should be considered a possible cause of disastrous failure on some systems.  Since Samsung has no idea whether users of its drive might be plugging them into such systems, it errs on the side of caution by avoiding any assumption that they aren't. ""while file transfer is in progress"" which means that the file structure would be in flux and could get corrupted if it doesn't have some kind of transaction system. Using ""Safely Remove Hardware"" will flush any delayed writes, wait until the transfer is complete and ensure no programs are using files on it. Any time you have electricity being transferred, you shouldn't just separate the electrical connection.  In theory, the electricity could jump a small amount of air, and might find a different electrical contact.",5
"Here is something I would suggest if you can do it. If you can get view the GVim on the system desktop then go to set the font that you want it at then do a: This seems to be fixed in the new version of Vim. (Verified with Vim 7.3d, available here http://groups.google.com/group/vim_announce/browse_thread/thread/9e72fb17d311d535# ) Only clue I can offer is that you can set another colon-separated field just after the hXX font size to indicate the character style: i for italics, b for bold. And leaving that field blank: The format of the commands that dggoldst has is correct.  I've tried several iterations of getting this to work.  The format works fine if you use: Curiously, this used to work, and only relatively recently ""broke"".  As for how recently, I can only say ""within the past month or so"".  I don't know what I've updated with the machine (just run Windows Updates when they're available to keep up to date). I get the exact same issue as dggoldst above.  adding the extra "":"" doesn't seem to do anything at all, unfortunately.  There's no ""r"" or something like that to make the font regular.  It'd be nice if you could see what setting the font via the ""Edit -> Select Font..."" actually sets things to.  There's clearly some magic that's happening behind the scenes.  Doing some more testing (removing ""set guifont"" from the _gvimrc, opening a file, typing "":set"" to see what's set, then clicking ""Edit -> Select Font..."", highlighting Lucida Sans Typewriter, clicking OK, then typing again "":set"", shows that guifont=Lucida_Sans_Typewriter:h10:cANSI) I suspect that there might be an issue with that particular font - the default font may actually be the italics version of the font, or the ""first"" instance of the font that vim is looking for is the italicized version.  My ""solution"" was to use a different font.  In my case, I'm using: This will show you the font you currently have GVim set to. Open you .vimrc file in your home directory (Ubuntu) and add the following line:",4
"If you don't know the network name of your computer, you can find it out as follows: right-click on the This PC folder (or Computer) in Explorer or some other file manager, and choose Properties from the shortcut menu. On that page, look for the 'computer name' text. That's the name you need to enter instead of 'laptop' in our example. Be sure to use the name of the computer that has the printer attached to it, not the one from which you are trying to connect to the printer through the network! I am trying to share my USB printer on my main desktop machine to the network so that anyone on my network can access it.  Previously, I would just go to the printer properties and enable it in the sharing tab to 'Everyone' but there is no sharing tab for the printer. Finally got it to work on the computer where I wanted to print from (My Laptop). So after opening the Devices and Printers window and clicking on Add a printer at the top, click on 2nd option ""Select a shared printer by name"" then click on ""Browse"". Next screen allowed me to navigate to and select my Network printer that was hooked up to my Desktop via USB.  Next, Windows will probably ask you to select the printer make and model, install its drivers, etc. Keep providing the information that the wizard requires, according to your specific printer. When done, Windows should add the new printer to the computer, and you should be able to use it to print files. The procedure described above worked for us, hopefully it will work for you, too.",3
"The class C address space was the most commonly available of the historic address classes. This address space was intended to provide addresses for small networks with a maximum of 254 hosts.  Another example is the assumption of the mask by some routing protocols. When some routing protocols receive an advertised route, it may assume the prefix length based on the class of the address. In this case, 255.255.255.240 is a mask for a network of 14 hosts. A tool like whatmask is very helpful here: Class C address blocks used a /24 prefix. This meant that a class C network used only the last octet as host addresses with the three high-order octets used to indicate the network address. Not all organizations' requirements fit well into one of these three classes. Classful allocation of address space often wasted many addresses, which exhausted the availability of IPv4 addresses. For example, a company that had a network with 260 hosts would need to be given a class B address with more than 65,000 addresses.  CIDR is useful in cases where you don't need a full Class-C network (254 addresses) (or something even larger). If you only have a network of a dozen or so hosts it's a more efficient use of IP address space. Class B address space was designed to support the needs of moderate to large size networks with more than 65,000 hosts. A class B IP address used the two high-order octets to indicate the network address. The other two octets specified host addresses. As with class A, address space for the remaining address classes needed to be reserved.  To reserve address space for the remaining address classes, all class A addresses required that the most significant bit of the high-order octet be a zero. This meant that there were only 128 possible class A networks, 0.0.0.0 /8 to 127.0.0.0 /8, before taking out the reserved address blocks. Even though the class A addresses reserved one-half of the address space, because of their limit of 128 networks, they could only be allocated to approximately 120 companies or organizations.  Class C address blocks set aside address space for class D (multicast) and class E (experimental) by using a fixed value of 110 for the three most significant bits of the high-order octet. This restricted the address block for class C to 192.0.0.0 /16 to 223.255.255.0 /16. Although it occupied only 12.5% of the total IPv4 address space, it could provide addresses to 2 million networks.  255.255.255.240 is an example of a variable length subnet mask, which is used with CIDR, or classless inter-domain routing. CIDR is a way to split Class-A/B/C networks into smaller subnetworks where you don't need say, a full 254 addresses (or 16 million, in the case of a Class-A). For class B addresses, the most significant two bits of the high-order octet were 10. This restricted the address block for class B to 128.0.0.0 /16 to 191.255.0.0 /16. Class B had slightly more efficient allocation of addresses than class A because it equally divided 25% of the total IPv4 address space among approximately 16,000 networks. The system that we currently use is referred to as classless addressing. With the classless system, address blocks appropriate to the number of hosts are assigned to companies or organizations without regard to the unicast class. A class A address block was designed to support extremely large networks with more than 16 million host addresses. Class A IPv4 addresses used a fixed /8 prefix with the first octet to indicate the network address. The remaining three octets were used for host addresses.  The unicast address classes A, B, and C defined specifically-sized networks as well as specific address blocks for these networks, as shown in the figure. A company or organization was assigned an entire class A, class B, or class C address block. This use of address space is referred to as classful addressing.  Even though this classful system was all but abandoned in the late 1990s, you will see remnants of it in networks today. For example, when you assign an IPv4 address to a computer, the operating system examines the address being assigned to determine if this address is a class A, class B, or class C. The operating system then assumes the prefix used by that class and makes the appropriate subnet mask assignment. Historically, RFC1700 grouped the unicast ranges into specific sizes called class A, class B, and class C addresses. It also defined class D (multicast) and class E (experimental) addresses, as previously presented.  When you are using a custom subnet you are using classless ip addressing. This allows a large block of IP addresses, like those in a class a network to be sliced up into smaller networks. You do this for a variety of reasons. Do a Google search for Subnetting Tutorial and you will find a ton of resources. Cisco's site has some very good games on how to perform subnetting as well.",3
"I think I'm agreeing with @Anton when I say use parenthesis when there is doubt. I'd say this is an issue of understandability. Depends on our ability to distinguish symbols, which is really good, and not on pairing symbols. It does also depend on our ability to apply precedence rules. My experience has been that after years of practice coding that's second nature. All in all, I advise to allow your coworkers to decide which form to use where, and to spend code reviews checking code for correctness, not for compliance to formatting standards. I don't see how wrapping a single identifier into parentheses is ever going to make things clearer.  On the other hand, I also don't see myself immediately understanding how a op b ? c : d parses for every possible operator op, or what a && b, c || d will evaluate to.  There's also plenty of cases between that, especially when you add operators with (imho) unexpected precedence, like bitwise &. You'll find, I think, that the second is much easier to read. This is because I'm using multiple types of braces (illegal, but bear me with here). As humans we can identify the different symbols very quickly, but we can't really line up matching symbols all that quickly. To take this example further, I intentionally line-break as shown because it's easier to edit. I could delete the entire line starting with ""||"" and I don't leave a dangling operator from the line above.   If readability is the goal, then I favor more visually explicit formatting, see below. Paired characters are hard to match. The longer the line and/or the deeper the nesting the harder it becomes. For a single logic operator you can argue this example is over-formatting. There is judgement involved. But with line-leading logic operators, the logic & logic structure just pops out with just a glance. Also, consider math. Mathematics has been around a lot longer then CS and they've adopted precedence rules and get rid of the parenthesis whenever they can.",3
"You can make a base Targeting state class/definition that Tree/Combat/etc derives from which can then allow some common code and even sub-states to be shared between tree cutting, combat, whatever since your targeting is likely going to be common. One option is as Nathan's comment, just identify when and where the values need to be cleaned up and do it manually there. Another option is to use a hierarchical FSMs.  You could have one ""parent"" state that is the Tree state or Idle or Combat or whatnot, and then a number of sub-states for Tree including Approaching, Cutting, etc.  Now the variables are only stored in the Tree state so they are cleared/discarded when switching from Tree to Idle.  The substates should have easy access to their parent's shared data. If you implement callback on the edges of a FSM rather than just the nodes, meaning chunks of code that are run on transition, then you can have a specific implementation of an edge from any of the Tree states to Idle allowing the idle state itself to be unaware of tree-specific details but allowing code to be reused for all of the possible ways into the idle state.",1
"I have a group that I use named staff.  It is a parent of a few nested groups.  I need to audit the group to list the users who aren't a part of that group or any nested child groups within it.  I have found scripts that will give me the users who aren't part of the Staff group, but it doesn't include any of the child groups so a user could be part of the child group but still be listed as not being in the parent group.  I don't want to have to script that for each child group of the parent. This query also uses the not (!) operator as you are looking for users who are not a nested member of the group. You can also run LDAP queries through ADUC/DSA by selecting ""Custom Search"" and the ""Advanced"" tab. The numeric OID for this operator is 1.2.840.113556.1.4.1941. You request this type of option in the LDAP search by placing the numeric OID between the attribute and the value you are searching for (attribute:OID:=DN). You can run LDAP queries with the PowerShell cmdLet like this: You can run a single LDAP query like that by using what it called the LDAP_MATCHING_RULE_IN_CHAIN operator. Two MS articles that cover this and some other details on LDAP search syntax that makes this possible:",2
"From my (limited) experience with GAE, while database reads are trivial, database writes are expensive. We've got a database of around 20,000 GAE entities; when measured, writes are taking around 0.3 - 0.5 billable seconds. Updating our entire datastore takes ~2.5 billable CPU hours. It's perfectly possible to use GAE to run an authoritative server, but given the cost, I wouldn't want to -- a cloud-hosted VM server would be more flexible and would likely cost less. Running the update on GAE would consist of a cron job launched every minute.  This would update all of the entities and save the results to the datastore.  This would be more CPU intensive for GAE. I am looking for an answer that persuades me to either perform the world update on the google servers OR an authoritative server that syncs with the datastore.  The main goal here would be to minimize GAE daily quotas. Running the update on an authoritative server would consist of fetching entity data from the GAE datastore, calculating the new entity states and pushing the new state variables  back to the datastore.  This would be more bandwidth intensive for the datastore. Clients of the game would authenticate and set state directly against GAE as well as pull the latest world state from GAE. For some rough numbers, I am assuming 10,000 entities requiring updates.  Each entity update would require: I am new to developing Games and this may sound naive, but somehow having 10,000 entities or more to describe a world sounds like a lot. I would rethink the database and the patterns that place those entities in your world. The Text and Blob datatypes are saved as binary by default although they are retrieved as a string.  I am considering different game server architectures that use GAE.  The types of games I am considering are turn-based where the world status would need to be updated about once per minute.",3
"But real tactical RPGs don't just find the best path on a flat plane and move there. They typically have limited move ranges and must climb up or down. If you've ever played Final Fantasy Tactics these would be affected by the Move and Jump stats. This is where I get lost. How do I alter the A* algorithm so that it finds the best path toward a target, but the path is only so many tiles long? How should I take height differences and jump stats into account? How do I implement jumping over a gap? This, however, finds the globally optimal path which not the best solution because real adversaries usually do not find the optimal path. It's highly unrealistic, sometimes to a point which is obvious to the player, and annoying (especially when the AI as such is basically invincible because it, too, chooses the optimum). Good simulations deliberately do not find the best path. A much better algorithm might be to do hierarchical pathfinding -- if nothing else, by drawing a straight line on the map, and taking 4-5 waypoints, then pathfinding from one waypoint to the next, considering only the node weights that are so far known, and setting all other node weigths to ""indifferent"". Alternatively, you can run A* on a coarser grid first, and then pathfind from one large node to the next (but I'm guessing that drawing a line on the map is just fine, too). Climbing and gaps are pretty trivial since they only modify cost. Pathfinding (and most of tactical AI) is all about summing up the cost on all to-be-visited nodes and minimizing that. An impassable cliff will have an infinite (very, very high) cost, slopes will have a higher cost than normal, etc. This is much more realistic (and also consumes a fraction of the processing power because the graph is much smaller). Yes, it can mean that a unit moves towards a cliff only to find out that it can't get across. That's fine, it happens to real adversaries, too. Next time, it won't happen again (because now the infinite cost is known). I'm messing around with writing a really poor tactical RPG in C++. So far I have a 2D tile map and just got the A* algorithm working based on the pseudocode in the wikipedia. If it helps, right now my map is represented by a Vector of Tile objects. Each tile has pointers to the North, South, East, and West tile, which are set to a nullptr if no tile exists there, such as along the edge of the map or if a tile is set to non-passable.",2
"I prefer using font sizes 14 on my title bars, message boxes and icons and font sizes 13 on my palette titles, menus and tooltips. However, as my laptop screen is too small, in order to make my apps fit on screen, I use font sizes 12 on my title bars, message boxes and icons and font sizes 11 on my palette titles, menus and tooltips. I don't know why I can't resize the window to make it larger than my screen in Windows (but it is possible in Kubuntu), therefore, some parts of my apps cannot be shown with my preferred font size. I am using Windows 8.1 on my laptop, which has a 15.6"" screen with resolution 1366x768. I measured the screen with a ruler and calculated its DPI, which is 101. Therefore, I have set the scaling to 105%. However, when I change to an external monitor, which is a huge one with resolution 1920x1080 and DPI 93, I need to change the scaling to 97% but when I change the DPI back and forth, my font sizes have get resetted. I expect after applying the file, the DPI settings and the font sizes take effect at the next sign in. However, on my laptop screen, after I applied the file, signed out and in, the DPI setting changed, but the font sizes were resetted to tiny, and I had to apply the same file, signed out and in again to get the correct font size. The situation is even worse on my external monitor. After I applied the file, signed out and in, both the DPI setting and the font sizes were resetted to their default values, which were 96 DPI (the physical DPI as measured by dividing the resolution by the physical size is 93) and font size 9, which is totally unacceptable. How can I write the .reg files such that the settings can be correctly applied with a single sign in? I have tried changing both the DPI and the font size by using .reg files. Before switching to my laptop screen, I apply the following:",1
"What happened here is, you told it to start signing the zone using only one key, the KSK.  Because there were no other published keys, it signed the whole zone with the one key it had available, the KSK.  (That's legal DNSSEC, but it's not the typical configuration.  If there had been an active ZSK, it would have signed the DNSKEY record with the KSK and signed everything else with the ZSK alone, but it had to work with what it was given.) Sometime later, the ZSK was published (but not activated), so it was added to the DNSKEY record but not used for signing.  Later still, the ZSK did become active, but the records in the zone are already all signed at that point, so named reckons there's no need for it to do any work right now.  When the KSK signatures get close to their expiration times, they should automatically be flushed out of the zone and replaced with signatures from the now-active ZSK.  Since you have sig-validity-interval set to one day, this should start happening sometime tomorrow. Anyway, for your purposes, you just wanted to make two keys that were published and active immediately.  You don't need to think about prepublication intervals until you roll keys. I'm testing DNSSEC with Bind 9.7.2-P2. I have a question regarding the first signature created over a zone that already exists. I'm using dynamic DNS. I create the ZSK with a Publication date previous to its Activation date. I restart the service and I can see that the key is published at Publication date, but it's no active later, when Activation date arrives. The timing considerations discussed in that internet draft are for when you're rolling from one key to another, to allow time for signatures from the previous ZSK to expire from caches.  It's not necessary to pre-publish the ZSK when you're signing the zone for the first time. I create the first two keys: one KSK and one ZSK. According to https://datatracker.ietf.org/doc/draft-ietf-dnsop-dnssec-key-timing/, the first ZSK needs to be published for an interval equal to Ipub, before it can be active.",2
"The other solutions are quite good, but also remember that man pages are just data and you can easily do almost anything with them in Linux. For instance, I need to know the meaning of the -o flag for mount. I run man mount and want to jump to the place where -o is described. Currently, I search /-o however that option is mentioned in several places before the section that actually describes it, so I must jump around quite a bit. When reading a Unix manpage in the terminal, how can I jump easily to the description of a particular flag? @piccobello's answer is great, but it was eating the colors in my man pages. Instead of piping to less (since man already uses less by default usually), I simply pass the modified less command to man: converts the page into a plain text file you can then manipulate. I keep a copy of the bash manual as text in my bin directory so I can just load it into my text editor to search for things and copy and paste while I'm editing scripts.  Although it wouldn't work for you in a terminal, I (with the help of a friend) even wrote a script that grabs a man page and displays it in a web browser so I can use its navigation/search features which are way better than less.  It's a bit kde dependent, but easy to modify.",3
"I am looking into setting up a sendmail server and I am kind of a newbie. I was just wondering, can sendmail be setup to act like exchange active sync?  The answer is no, but an explanation is warranted and perhaps that will help you in the next step.  Sendmail is an MTA: a message transfer agent.  (Exchange is another.). That is, it takes SMTP messages (aka email) and transfers them from one MTA system to another.  At some point an MTA says ""yes, I'll take responsibility for delivering this message to this user"" and puts it in that user's mailbox.  The mailbox is managed by a mail server: Exchange for example, or something that speaks IMAP.  At which point a mail user agent (your phone's email client, outlook, whatever) can log in and pick it up.  What your mail client speaks depends on the mail server.  Outlook can speak IMAP but when it talks to Exchange it speaks Microsoft's own protocol.  Webmail is basically a web front end where the web server does the mail client piece for you. Sendmail can only act as ""email Server"". However you can use another Products which offer such ActiveSync features like: For ActiveSync to work you need your client (phone) to stay in sync with your mailbox/contacts/calendar as held on your Exchange server.  Now you see the problem: sendmail can get a message into your inbox by delivering it to Exchange, but knows nothing about the rest.  You really need Exchange.  At which point, since Exchange is also an MTA, you dont need sendmail at all.",3
"If you are afraid of some changes done to the computer's hardware (like installing a hardware keylogger) that would compromise your privacy in the future, then the only solution is to never ever give the computer to anyone. It's about compromise, if you provide no hard drive, are you willing to wait for the technician to find a hard drive and install it and the necessary OS and software to diagnose it? Will s/he pass the cost of parts/labor to do this on to you? If you anticipate it going back again, remembering you have an extended warranty, will the technician have to do the same each time? It may be worth the up front cost to you to avoid greater costs down the line. How much effort and expense you're willing to go to should be balanced with the trust you have in the technician(s) and the sensitivity of the information on your hard drive. All the other answers are only concerned with the privacy of the preexisting data on the computer, which may or may not be enough for you. I'd recommend taking out your existing hard drive and replacing it with a cheap low capacity one that you either have lying around or can purchase for a minimal amount. Put your OS on it using the same key (if possible), as this hard drive won't be active in a computer at the same time as your original. You should be able to generate restore media from your existing hard drive. Then, put a simple password in place and keep it as free of personal info as possible.",2
"And it's possible to mix both even if exact, reproducible game logic is needed: Use delta time for inconsequential things like particles & UI animations so long as they don't affect game logic. If the need is simply to have a stable replay it's also possible to record the delta times along with the replay data to ensure the physics end up with the same rounding errors. It depends on how the rest of your game runs. Either use ticks or time, but choose one and use it everywhere. I prefer using ticks, and setting a max framerate. If the game slows down, so be it. I don't want any quantum tunneling to happen. It's simpler, and i like it. But you could use delta time, and get different framerates with the same game speed. That's up to you to decide. But whatever you do, make the choice once, and use that in your entire game. If you add time based cooldowns in a tick based game and the game runs slow, the cooldowns are suddenly relatively shorter than they're supposed to be, and vice versa. You can still use delta-times with tick-based updates for calculating the number of ticks for convenience. Fixed tick-based logic update is useful when you need to easily insure game physic & logic is 100% reproducible as delta-time updates will cause different rounding errors depending on the time elapsed. The bad side I can think about is that if the tick rate is increased, it would affect the game logic. While having a base update rate and a multiplier(that can be applied to everything that is tied to tick rate) would probably fix this, I am not sure if this is a good approach. In the past I've done cooldowns as a specific time in ms, but I was thinking isn't having tick based cooldowns better. This would be more accurate and efficent(not sure).",3
"The planner can't get a good estimate of how selective your join is going to be, and so can't make good decisions about it. I'm curious to see what it reports with this query (I do not know if results will be the same as original query): (it just separates the LIKE join from all the others) Based off this, I can see that the query looks normal except for the horrendous JOIN on a LIKE.  To me, it looks like the query planner is either getting confused on the number of matches it EXPECTS to get from that JOIN (and thus plans a sequential scan), or reporting accurately and your query is potentially doing some sort of cartesian product. The plan I see when I reconstruct your example is that it does a nested loop around a sequential scan on tmp_psm_seqs and a bitmap scan, using the index you created, on protein_seq.  That seems like a pretty good plan to me. Another option would be to make the LIKE operator look much more expensive, which should drive it away from using it as a filter in a cartesian join.  As superuser, you could do something like:",3
"I haven't actually ever tried it for a personal files, but you could transfer the file using the BitTorrent protocol which was designed for peer-to-peer file sharing. There are many free client programs available like Torrent to use. The documentation for the latter discusses how to create a private torrent file and send a link to it to someone else for sharing. There are also many tutorials on the web. You also could query IP from Websites like http://www.wieistmeineip.de/ but there are many problems with forwarding the adress/port through the router and your firewall. You can use SSH to copy files securely from one system to another. One must run a windows SSH server program such as freeSSHd, while the other uses an SSH copy tool such as PuTTY Secure Copy client. In order to know what your system's IP address is you could write a batch script send an e-mail (with blat) with the server's current IP address, then create a scheduled task to run the script every day (or every few hours even). I use a Windows XP PC and have a file of about 700mb, that I would like to copy to another PC (Win XP) over internet. None of the PCs have have a static IP from ISP. How can I transfer the file? I don't prefer uploading the file to a file sharing server and download it in the other PC. Is there any other way (like team viewer or torrent)? Teamviewer should be the best option(the fastest one) I see imo. Torrent would be very slow to transfer.",5
"if you dont want to change change your addressing scheme then best possible solution is the usage of NAT Bridging and Proxy ARP. OpenVPN should be capable of providing this with a host on each end, probably also any Cisco with irb configured between an ethernet interface and an ipsec tunnel. Renumbering is the right answer.  If you don't want to do that, then implement NAT in both directions.  This would allow you to keep the existing IPs in both locations.  However, each side would ""look"" like a different subnet from the other side. IMO the better solution to accomplish this is with VPLS or L2TPv3. Contact your carrier to see if they can provide VPLS or other L2VPN services. You will need double NATing to achieve this without changing IP addresses. You can not ping an IP that is in your LAN and expect it to reach somewhere else. As long as you are part of a LAN and communicate locally, your traffic to IP address in 192.168.2.0/24 will always go to your local network. As mentioned above, you will need two IP ranges lets say 192.168.102.0/24 and 192.168.202 that will be used for double NATing like this:",5
"A downside of an Enterprise solution is that a fee is required. However, maintenance and in particular the number of resources that is required to apply it, is often underestimated in my opinion. Maintaining Jenkins by Engineers from the department means that they have less time to develop code. At the moment, getting a CloudBees subscription will not solve our problems as we are currently busy splitting the monolith.  Whether you should get a CloudBees membership really depends on your situation. Do you have a monolith? Has all code been untangled. Do you have microservices with clear boundaries or are they nanoservices? Long story short, do you have a robust architecture? If false, then CloudBees will also not be the holy grail. I personally prefer Jenkins over CloudBees as we have enough Jenkins knowledge in our engineering department. However, if their is a lack of such knowledge or interest then it could be useful to use a Software as a Service (SaaS) product like CloudBees. In general, this means that the department will be unburden as the platform will scale automatically and the SaaS provider is responsible for keeping Jenkins up and running.",1
"It's not easy, and the science of game design documents is woefully under-explored in comparison to other software domains. You should not require yourself to add information that you think you need just because some template has it. Typically large games aren't built off monolithic design documents, but a distributed document system. Either a series of smaller targeted docs or a wiki. Personally I prefer the wiki approach as allows readers to quickly jump between related topics which maps well to system designs which are often integrated. A good paper on this is Requirements Engineering and the Creative Process in the Video Game Industry. There are no minimum / maximum. It should contain everything you need to start work on the game. Nothing more and nothing less. Having just begun studying design documents, I've come to the conclusion that no-one really knows what goes in them. Each list contains layout sheets (or character layouts for characters - front, side, 3/4), notes, mechanics notes for animation, etc. Whatever you need or might need.  The size and scale of the project documentation completely depends on the size of team and the scope of the project. The design document itself should be what you need to progress from pre-production to production. How do you turn the game design from this nebulous idea to something that can be created and executed? Too little detail and there's too much ambiguity, creating uneven results, too much detail and you limit possible creativity from your team. I'd say you should work out what the gameplay is, who your character is, and what the environment is like. But my advice would be to start making playable prototypes as early as possible.",5
"Of course you could make your own tent like product.. But that's a very expensive risk, the stuff in your racks could cost in the millions to replace.  Recently we had a rather funny incident: suddenly about a liter of water ran from above the drop ceiling right in the middle of the office. Turned out it was some pipe in the air conditioning that detached and released condensed water. Rerouting and stopping at the source would help. Maybe find a way to run a remote water sensor that can alert you if there's an issue? You could try running plastic over the tops of the racks...but I don't know what you'll do to ventilation or what happens if the heat warms the plastic up and releases some kind of gas. You can hang foam with wires or screw it from one side without compromising it's being water proof, and being rigid you can give it a little slope to divert disasters off to the least damaging side. Being light it's not hard to position where it will do the most good. I'd probably look at finding a way to run a trough around where the AC leak was and re-routing it. Anything you do near the equipment will affect heating/airflow/fire suppression/safety, and putting things in the ceiling that isn't made for it can be a safety/fire hazard as well. Regular maintenance of the AC unit/inspection of the unit may help mitigate the risk. Obviously that's not what anyone wants to happen above a server equipment rack. What is the easiest (I mean that materials should be easily accessible and the consrtcution should be easy to manufacture) reliable way to shield a rack from above to protect it against minor water leaks? Get a sheet of rigid 1"" insulating foam or something and make a cut-to-fit lid for your rack with at least enough overhang so that drips off of it don't splash on your equipment. I used to have a rack in an office below a deli. Seriously bad land-use planning, but whatever. Not to mention if you have fire suppression...blocking things like that from reaching the equipment, what would that do?",4
"Otherwise, have you configured Internet Connection Sharing on the troublesome computer? That may also have a similar effect... At that point, you will probably need a new router, but you can at least try on more thing: Try copying your settings manually, updating the firmware and then resetting it to factory defaults to make sure that it is not just a corrupted configuration or corrupted firmware. Then test one last time. If it does not work, you can head to the store. Since you have connected with two different network cards, one without a wire, you can PROBABLY rule out those network cards and cables (assuming you unplug the wire when trying to connect wirelessly). You still have signs of a network card broadcasting garbage since it is interfering with other computers, and goes away when you unplug it. You might try another wired NIC. I would also see if you can connect your computer directly to your Internet connection, and see if it continues to happen. If not, you know the router is the issue. Are you using VMWare or VirutalBox? If not properly configured, the DHCP server that those virtualization products provide can provide DHCP to the other computers / devices on your real network and causes the devices to send all requests to your computer rather than your router.",2
"Is there a way to shut it down without being logging into it, so I can get the SD card out without possibly corrupting the file system (by yanking the power cord)? More recent Pi models have two points you can bridge with a wire to reset the Pi.  If you hold it in reset it will be safer to pull the power. See https://www.google.co.uk/search?q=raspberry+pi+reset+pins&biw=1077&bih=592&tbm=isch&tbo=u&source=univ&sa=X&ei=eGL9VJC5FeWy7Qan1IC4Dw&ved=0CCAQsAQ If the your raspberry pi is connected to a display, and you are at the terminal login prompt, pressing CTRL+ALT+DELETE should start the reboot sequence.  You can watch the screen for the moment of reboot(screen blanking), and remove power.  This should be the safest method. Old unix's of the '80s had a user call ""sync"" that ran ""/usr/bin/sync"".  The command line login would return after the sync to a login prompt. So it's been a while since I've played with it, and I forgot my Raspberry Pi's login info (both username and password), and it's running. I've seen some instructions for resetting the password, but they require the SD card to be put into another machine. Add a user called shutdown that runs only the shutdown program in the ""/etc/passwd"" file.  In the field for the shell add ""/usr/bin/shutdown"".  You would probably have to add shutdown to legal shells file too.  If you are realy concerned about this you can attach a button to a GPIO pin and have some code that starts with the system monitor the pin and halt the pi when the button is pressed. Frankly unless you can see SD card activity (flashing LED) I'd just pull the plug.  Frankly, frankly even if I saw SD card activity I'd just pull the plug (I keep nothing important on the SD card).",5
"Single user will kick everyone off but then you will be the first one back on with the backup. Then when you are done set it back to multi_user. I write Minion Backup and this kind of thing would be very easy to do.  We allow you to run Pre and Post code before each DB, so for your SharePoint DB, you can just put it into single-user mode and change it back when it's done.  However, we can't guarantee that you'll be the one to get that connection. SQL does not allow to backup a database when in offline mode. You can also create maintenance plans in order to automate the job. So the first step of you job can be setting the offline databases to online mode and then the maintenance plan will do is work and then the third step will be to set your databases back to online. Second you might consider using Minion Backup.  It's a free automated backup script that is supposed to be both very powerful and easy to use.  The writers may very well have a config specifically for SharePoint or at the very least you can ask them.  http://minionware.net/#miniontabs|2",3
"Is there any way to define shortcuts for often-used values derived from CloudFormation template parameters? You could use a nested stack which resolves all your variables in it's outputs, and then use Fn::GetAtt to read the outputs from that stack I don't have an answer, but did want to point out that you can save yourself a lot of pain by using Fn::Sub in place of Fn::Join This construction or very similar is repeated many times throughout the template - to create the EC2 host name, Route53 records, etc. Instead of repeating that over and over again I would like to assign the output of that Fn::Join to a variable of some sort and only refer to that, just like I can with ""Ref"": statement.  You might use nested templates in which you ""resolve"" all your variables in the outer template and pass them to another template. For example - I've got a script that creates a Multi-AZ Project stack with ELB name project and two instances behind the ELB called project-1 and project-2. I only pass ELBHostName parameter to the template and later on use it to construct :",4
"The best way to explain it is like this: Imagine a factory that produces exactly one car per hour, on the hour. Say you decide to sample the rate at which the factory produces cars. You start sampling at 5:59 and stop sampling at 7:01. You see two cars produced, one at 6:00 and one at 7:00. You sampled for 62 minutes and 2 cars were produced. Thus you calculate that the factory was producing cars at about 200% of its rated capacity. In addition, you cannot compare top values against each other because top doesn't provide you a set of measurements of a single system state but a set of independent measurements each subject to their own set of conditions. For example, the per-CPU values can be computed using a completely different mechanism from the per-process values. The per-CPU values can be exponentially decayed while the per-process values can be the difference between two totals. So they can reflect measurements of the same type of thing, but using completely different methodologies.",1
"In the long run, thinking in terms of tiers and of ID numbers is more likely to hurt you rather than help you.  At the start, use ID numbers only for those things that don't carry their identity with themselves. Studies should carry their identity with themselves; that is, their names should almost certainly be unique. Patients (people) don't carry their identity with themselves; there are many people named ""John Smith"". Exams patient and studies would all relate through join tables (patientid and studyid inthe patient study table and examid and studyid in examStudy table and patientid and examid in the patient Exam table). This is becasue you ahvea many  to many relationshsip between these things.  It's not clear whether you need any additional columns to identify rows in PhysicalExam and in ClinicalExam. It doesn't look like it on first glance, though. Later, if there's sufficient interest and if I have enough time, I'll write this in SQL. Now as to the exam details this is where the EAV table comes in. It would include Examid, parameterName, parameter value) You would then inseter a record for each value you want to store. That way you can add and change them as the exams change.  They are harder to query this way but this is exactly the use case that EAV tables were designed to meet, frequently changing values that can't be knwon in the original design. Access may not handle this well, so you might want to consider a nosql database for this part. Your problem is most commonly solved using what is called an EAV table. This is because you will probaly be adding more and more exam typoes weach of which will have differnt parameters. First your problem is that you are using Access which is not going to perform well with the struture you need.  And of course you need to protect patient information according to HIPAA rules. Be very sure you are aware of them and how to prtect the data. Again Access may not be the best choice for that unless you don't intend to store client names and addresses etc at all. I would structure it so that I had all the common details about the patient in a patient table (and some related tables for things that change over time or that have mulitple values such as address).",2
"See http://www.faqs.org/rfcs/rfc1178.html for best practices on naming your host.  /etc/hostname is used to populate then name in in the kernel during starup.  Put the unqualified name in /etc/hostname and run 'sudo /etc/init.d/hostname start'.  Also add an entry for your hostname in /etc/hosts using the interface address in /etc/hosts. I believe the only thing that is supposed to directly accesses /etc/hostname is the startup script /etc/init.d/hostname.sh. Depending on the configuring in your /etc/nssswitch.conf file you can use it for domainname resolution before calling out to a different system, normally DNS. I believe it should be pretty safe to change the file.  Change the file, run /etc/init.d/hostname.sh and then restart any services you are running like apache.  If there are issues the fix should just be a minor edit of a couple configuration files. So, normally, the system will route all hostname queries through /etc/hosts first, before querying DNS. The problem you are encountering is likley in /etc/hosts.  This is where addresses are looked up.  There should be an entry there for your servers IP address.  Fallback is to use the loopback address, localhost@localdomain. So for what purposes does that hostname string get used, and how can I be sure that it won't affect any functioning systems? The mail server should use /etc/mailname to determine its name. This should be a fully qualifed address suchas myhost.example.com. While the hostname is defined in /etc/hostname, many processes will use DNS and/or the hosts files to figure out the hostname. So you might try looking at /etc/hosts and also check the nameservers listed in /etc/resolv.conf to see what is defined.  Your apache configuration may depend on the hostname being a certain thing.  When you setup apache did you use IP addresses in your configuration or the name that was in /etc/hostname. If you have a mail server setup it could also be using the name from /etc/hostname.  Though localhost.localdomain is what is in /etc/hostname, then I suspect your email system is partly broken already. If you are really paranoid you could copy the production system into a VM, make the change in the VM and then see what breaks.  If you don't already have a test environment now might be a good time to set one up. When I ssh in, it gives me a tab name of username@localhost.localdomain, which is counterintuitive, making it seem like those tabs are on my localhost.",5
"http://www.buffalotech.com/products/wireless/dd-wrt-1/airstation-extreme-ac-1750-open-source-dd-wrt-wireless-router http://www.buffalotech.com/support-and-downloads/faqs/configuring-web-filtering-on-a-buffalo-airstation I feel Option 4 is not well understood, but the Options 1-3 have a lot written on them in various places, so I'll comment on 4. I prefer the social solution, instead of the technology one. Which are programs like Covenant Eyes. Instead of ""blocking"" software this is called ""accountability software"". The basic approach is to use a router that has a lot of configurability, like a dd-wrt router, and set the router's DNS to openDNS's server, and add a command in the iptables to force all DNS packets to go to the router's DNS server. You can configure it in the router to filter specific computers (by MAC address). If you're trying to concentrate, say, for a week or two on some crushing work project, you can change the settings on openDNS to block pretty much all time wasting sites, give the password to the router and openDNS to your wife, and then have a single work computer on which you can't check facebook just one more time... or at least until your project is complete. These programs allow the child to view anything, but everything they see gets reported to you. The advantage here is then you have to talk with the child more, and have a dialogue about what they are viewing, and establish proper boundaries through a punishment / reward system, and through education. I offer this as an alternative, hoping it may help in addressing the root of the issue, rather than the symptoms. For mobile, you can likewise install the app, and use parental controls to block the default browser. Disadvantage: It only blocks one site, and as soon as the kid figures out it is the hosts file, he can just edit the file back again. Option 2-Sign up for opendns.com, configure it to block what you need. Set the kid's computer's dns to the OpenDNS servers. Option 4- Use a router than can lock down the DNS settings so that everyone on your network HAS to use openDNS. Disadvantage: You're only blocking individual sites, one at a time, and some people have a hard time confronting their router's settings. http://www.buffalotech.com/products/wireless/dual-band/airstation-extreme-ac1900-gigabit-dual-band-wireless-router Advantage: You can have total control over your network. There are some sites that are dual purpose, such as images.google.com, that you either won't be able to block or won't be able to differentiate between a safe and unsafe use, but in general, this has the best total control over your network. 4c) Buy a router that has a GUI option for ""parental settings"" that can force the use of openDNS or Norton SafeConnect. (Basically option 4b done for you). http://www.buffalotech.com/products/wireless/dd-wrt-1/airstation-extreme-ac-1750-open-source-dd-wrt-wireless-router",2
"This will give it a lower scheduling priority. Scheduling priority's range from -20 (highest priority) to 19 (lowest priority) the default for nice is 10 if the -n argument is omitted.. or maybe you want to enable binary logging in mysql and run full dump once per week / night, while copying bin-logs to safe location every 1-2hours. and.. read-only slave for backup-only purposes is an option as well. You could always try not running it during peak hours. Whatever priority you use, the dump is still going to require a lock on all your tables.  If you have a spare server around that can cope with the write load of your server, you can set up replication to that server, and then backup from the slave server. This also has the advantage that you can stop replication while you do the backup and get a consistent snapshot of your data across all databases, or all tables in one database without impacting the database server. This is the set up I always recommend for backing up MySQL if you have the resources.",4
"I found bug trying export Opera bookmarklets into Chrome : Opera bookmarklet should not be contain  empty folders and folders without name (otherwise while Chrome stops importing process after empty folder / folder without name position) Opera 35.0 appears to have no Import/Export option under settings, and they have deactivated Opera Link, so there is no way to do this except through other 3rd party software. Transmute does not appear to work. As far as I remember, Chrome installer has no Opera bookmarks import feature (supports only I.E. and probably Firefox). Try to use Transmute -- this tool supports all major browsers (without intermediate exporting the bookmarks to HTML from the browser) and some other formats. In addition, it features some other handy stuff which you may also be interested in. An alternative solution would be web based bookmarks.  The advantage to web based book marks is that they are always available, from any browser, on any platform, and never need to be imported/exported.  Foxmarks does have web based bookmarks, as do other web services.",4
"A big culprit of memory leaks in classic ASP are ADO objects that never get closed and disposed of properly. I'm hosting several ""classic ASP"" applications back-ending into MDB files w/ IIS 6 and not seeing this behaviour. It can work just fine.  Has anyone else had any problems with IIS 6 App Pools running away with memory utilization? I am using server 2003 and our one classic ASP applications (powered by an Access database). Once a request has been made, it leaks around 180 MB. I moved the app into its own pool to isolate it from the other sites. I do not think we had this problem with IIS 5 in Windows 2000, I could be wrong. Could it be that it is loading up the database? Would trimming out old data help? Thanks for your time! I'm guessing that you've got some code that is running away w/ memory (storing session variables that never get cleaned up, etc). I doubt that removing data from the database is going to improve matters, unless the leaky code is leaking in proportion to database size (i.e programmer is loading a large recordset and then parsing it script-side). Profiling / benchmarking classic ASP code isn't fun. You've probably got database connections being stored in session variables that aren't being closed properly. I'd look in that direction first. Do you see the memory usage increase over time?  More than likely it's a leak in your app code.  You can try to increase the frequency of process recycling in order to free up the memory as a workaround.  If it is a code problem then this is just a band-aid and you'd want to analyze your code for the problem.",3
"Go to Network>LAN on the side menu and change the LAN IP address of your TP-Link N router to an IP address on the same segment of the main router. This IP address should be outside the main routers DHCP range. Example: if the DHCP of your main router is 192.168.2.100  192.168.2.199 then you can set the IP of the TP-Link N router to 192.168.2.11 Use an Ethernet cable to connect the main router to your TP-Link N router through their LAN ports (any LAN ports may be used). All other LAN ports on your TP-Link N router will now grant devices Internet access. Alternatively, any Wi-Fi device can now access the Internet through your TP-Link N router by using the SSID and Password set up in the above steps. Connect your computer to a second LAN port on your TP-Link N router using an Ethernet cable.* Login to the TP-Link web interface through the IP address listed on the label on the bottom of your TP-Link N router (see below link for assistance): Go to Wireless>Wireless Settings and configure the SSID (Network name)which can be the same or different from the main routers. Select Save. Go to Wireless>Wireless Security and configure the wireless security. WPA/WPA2-Personal is recommended as the most secure option.  Once configured, click Save. Note:  After changing the LAN IP address a reboot will be required and you will need to log into the TP-Link N router with the new IP address.",1
"Just use a single network cable of the correct length. It's very hard to correctly crimp an RJ45 connector on a cable if you're not used to doing it. To directly answer the question asked, you only rewire one side to make a crossover cable. Making a crossover cable from a patch cable is simply the act of swapping the destination of two pairs of wires in the cable. Doing that to both ends will result in a (nearly) standard patch cable again (of course two of the wire colors would now be swapped but the functionality would be that of a patch cable). Even if Auto MDI-X didn't exist, (almost) all home routers have builtin switches and that means you'd use a regular patch cable to connect to it. IMHO, you should probably just get rid of all of your crossover cables because it does exist and it is almost universal at this point. Even if you only have a single device on your network (which I strongly doubt is the case), it's still much better to have a switch on your network than to connect your one device directly to a single port router. With a switch you can easily add new devices as needed without unplugging the old ones. With modern network equipment a cross-over cable is not needed; the ports are auto-sensing and detect whether a cross-over is required and adjust for that automatically. In the early to mid 90s there was very little need for crossover cables. They were basically only used when creating peer-to-peer connections between just two devices or connecting anything to a router. For the last 10-15 years practically every router, switch, hub or nic you can buy has Auto_MDI-X which eliminates the need for crossover cables entirely. This feature is also only needed on one end of the connection so even devices which don't support this feature won't need a crossover cable if they are connecting to something that does have it.",2
"Ideally, I'd want to be able to select both the Date and RoleType columns in their entirety and then spit out the results in a table that would have the different role types on the X and the different dates on the Y. I'm trying to write a formula that will basically count the number of different role types of humpback whales (MC, G1, G2, etc.) seen per calendar day. It's really laborious to do it by hand, but I'm getting hung up on the formula.  I'm going to try to make this as specific as possible, but I apologize in advance if I'm not clear! I don't do much logic coding in Excel.  The idea is basically to come up with a sum of the amount of times a role type was seen per day. So if the data looked like this: I'm working with over a thousand sightings, so I'm reallyy trying to avoid doing this by hand. The expression I came up with so far is: Where A3:A14 represents the date range being investigated, F$2 being the target date, B3:B14 is the list of role types being investigated, and ""MC"" is a (clunky, I know) manually entered designation of which role type is supposed to be counted.  I would want the results to come out as MC on 3/1/19 = 3, G1 on 3/1/19 = 1, MC on 3/2/19 = 2, etc. So I think it may be an AND statement situation, now that I think about it?",1
"The Media_Wearout_Indicator is what you are looking for. For 100 means your ssd has 100% life, the lower number means less life left. To check ssd life left on a (solid-state drive) ssd, you will need to install the smartmontools package. It contains two utility programs (smartctl and smartd) to control and monitor storage systems using the Self-Monitoring, Analysis and Reporting Technology System (S.M.A.R.T.) built into most modern ATA and SCSI hard disks. The ""fstrim"" command from ""util-linx"" will run through the filesystem and issue TRIM commands for all unused space.  On distributions such as Ubuntu this is disabled by default except for a select list of ""known safe"" drives from Intel and Samsung.  But the command can be run manually on any partition for any drive. On the Mac, check out the digilloydTools DiskTester. There are also some interesting data points there to see the effects of reconditioning on drive performance. Apparently the standard recommendation is to do a full-drive write of all zeros. I'm not entirely sure why this helps (don't a lot of writes eventually kill SSDs?), but it does seem to be endorsed by the major vendor SSD support forums.",4
"First and foremost, yell.  Loudly.  At your vendor.  For having a product not supporting the over-a-decade-old HTTP/1.1 protocol. I'm configuring our servers, and due to the nature of our load balancer, we can't send connection keep-alive headers. I'm trying to determine the impact of sending these headers to both the end-user and the server. Will either one notice anything? The impact of not having persistent connections is a major increase in loading time of resources.  With keep-alive, a single TCP connection can be used to request multiple resources; without, a new TCP session (with a new three-way handshake - and, if you use SSL, a new SSL negotiation) is required for each and every resource on the page. Keep alive will greatly enhance the performance on both the client and server side. If possible do not disable it. The load balancer should work fine with keep alive turned on. In practical terms, the impact will depend on the number of resources on a page, the round-trip time between client and server, and the number of concurrent requests a client's browser is making at a time (modern browsers run ~6ish by default).  Lots of resources per page and distant clients will mean a very noticeable increase in page load times.",3
"Make sure you create the new disk as a true clone of the old - which means making it the same size and ignoring any additional space. So your only hope is that your disk copy has been faithful enough to ensure that the array will still be valid when you put the new disk in. There is no guarantee I'm afraid. So you have used RAID 0 to get a speed improvement but have at least doubled the risk of a catastrophic failure. As you've also discovered, many RAID controllers are a lot more fussy about the health of a disk than a desktop or laptop controller. Unfortunately, you have fallen foul of one of the many problems with RAID. Especially with modern, high density, low quality (e.g. cheap) disks. In addition, everyone should remember that RAID is not a backup. You still need backups, ideally both on-site (for speed of recovery & in case your cloud backup provider goes bust) and off-site (for local disasters such as fire, flood, robbery, etc.). For future reference, I personally recommend only ever using RAID 1, 0+1 or 6 for home/small-office devices. Even then, I recommend disks designed for use in those situations & not standard desktop drives. I would also recommend matching drives. RAID 0 & 5 in particular are a recipe for disaster. The more disks you put in a RAID array, the more likely you are to get a failure and disk failures are very common in cheap drives.",1
"Compare the inventory with their licenses, take note of how the software is used, some licenses only allow certain types of use (MSDN licenses, for example). I'd start first with assessing their business needs and determining what software is required for their business functions.  Then determine what would be the hardware+software costs for new servers.  Then take a look at the hardware available and see if it's worth keeping.  In a similar situation I was able to get the client to dump 6 older machines and buy 1 copy of windows server datacenter edition.  This version allows for unlimited virtualization so seperating servers was still possible without the overhead. Direct savings over licensing wass about $3K Same goes for the laptops/desktops.  It might be cheaper to lease new OSes with the hardware than setting up a seperate openlicense.  If they want or need SA rights you can still enroll OEMd licenses. Consult your local license provider. They wont get slammed at running illegal software - especially if you're looking at legitimising them. If you're not too hard pressed on financial side, you could restart by volume licensing everything and discarding OEM legacy. You could also get Software Assurance, the same key for everything, home usage program, easy upgrades in number of licences etc. Prepare a report for them, listing their software and identifying any items for which you could not find licenses. Suggest that they locate or purchase licenses for this software, since not having them available is a legal liability. Microsoft has some licensing options specific to small business with as few as 5 computers, that allow them to spread payments over three years, finance the cost, etc.  The information is here: http://www.microsoft.com/smallbusiness/buy/software/buy-software.aspx#VolumeLicensing Consider their position too; If they're in Education or Non-Profit, they can get heavy discounts and get VLK (Volume Licensing). Alternatively look at ""Microsoft Open License"" ( https://partner.microsoft.com/UK/licensing/licensingprograms/ltvolumelicensing/vlopenlicense )",5
"I'm tired of all these corporate friendly looking desktop themes. Any cool ones out there that look like the ones we see on TV? I already looked at several of the top theme websites, and all the cool themes were either outdated or not very techie. Hackers on TV, you know, the kind that hack government networks by mashing keys, always have this super cool over the top desktop themes, that unlike their button mashing hacker skills, I hope the desktop themes actually exist. Well, if you run vanilla Enlightenment as your desktop environment, you're already pretty close. ;-) The sawfish window manager which used to be in Gnome (before metacity came along and made it all corporate and boring, or imposed some sanity, depending on your point of view) was great for themes.  I used to run a bunch of green-on-black xterms/emacs decorated with a spiky metal theme and it looked great (for it's time).  What eventually killed it for me  was the increasing likelihood of having a black-text-on-white-background web-page open on the desktop; these just do not look good in a dark desktop theme (way too bright).  What you really need to be able to do is theme web content too to match your dark hackerish look (OK probably possible through CSS overrides, and some sites let you pick a theme, but there's no standards.  Wonder if anyone's done a firefox extension ?). And yes, I'm talking real desktop themes, I'm not asking for a GUI to one button trace an IP to someone's exact location or zoom and enhance programs ;)",3
"I am sure the bugs are being worked on, and dedup might be in a usable state sometime soon, but please don't jump into using a brand new feature in a development release until you are sure its level of stability matches your risk tolerance. The L2ARC code seems well tested and stable, and if you workload involves a lot of random reads, it is very likely to help. L2arc has been in OpenSolaris for a while now. Dedup is already in the development builds of  OpenSolaris.  If you can't wait until the next release here are some directions on how to upgrade to the development branch. The easiest way to get dedup at present is to go to the GenUNIX site, download the latest OpenSolaris preview release on ISO, and install that. Choose one of the AI ISOs because the installer is better. It's coming either this month or next but only in dev builds for now, it may be in 128 but more likely a little after that. Sit tight, it'll be worth the wait :) The dedup code is not thoroughly baked at this time and I would recommend that you only use it on test machines without any important data. I run the development branch on my file server at home.  Which is currently build 129.  You have to be careful and wait a few days to make sure there aren't any gotcha's before doing an image-update but I haven't run into any problems doing this.  And if you do you just reboot in the previous BE...gotta love zfs clones.  I'm not using L2arc but I did play around with dedup a little the other day on a zfs volume.",4
"Also, AFAIK, Virtual Box only runs on Intel Macs, not PPC.  I think Parallels does too.  Microsoft's Virtual PC for the Mac ran on PPC.  if it's still available to purchase anywhere, it will probably cost a significant percentage of what buying a second-hand or even new whitebox PC clone would cost.  It also emultes PC hardware so you'd have to run an i386 linux rather than a native PPC linux. If you are tied to the idea of a VM, know that it's probably going to be ugly. Most of the virtualization platforms for PPC Macs aren't virtulization engines so much as emulation engines. Since there's no Intel-compatible hardware layer, they've got to completely pretend, which leads to incredibly slow performance. The only one that MAY offer some performance that I know of may be QEMU. There's a Linux-on-PPC forum  Given that you can buy brand-new 64-bit AMD or Intel headless whitebox clones with 4GB or 8GB of RAM and 500GB or more of hard disk space (and Gb NIC, IDE, SATA, etc) starting from about $500 AUD (about $400 USD) these days, you can probably get a 2nd-hand P4 with 1GB RAM for $200 or so.  or recycle someone's obsolete-but-still-overpowered desktop machine. Is there a reason you want to run it in a virtual machine and not run it natively? There are still lots of distributions that run natively on PPC, so don't feel like you've GOT to run it in a VM.  virtualisation is one possible method of doing what you want but, since the machine you're talking about is a G5 PPC Mac (i.e. quite old and slow by today's standards), AND it sounds like you're using it as your main workstation as well (""do other stuff"") AND it's intended to be a production server, IMO a far better option is to pick up a second-hand P4 or better PC and install linux on that. They have ""application stacks"", with things like Moodle, Joomla, MediaWiki, Wordpress, ocPortal (which looks awesome!) pre-installed, and ""infrastructure stacks"" -- a MAMP stack, a MAMP stack, a Django stack and a Ruby stack.  There is a lot of great stuff there. Good luck. If you can't get native virtualization and you don't want to run native linux, then I think having a production server running on an emulated platform would be a bad idea. Since you asked.   This does not answer the surface question, but, for running Apache with MySQL or PostgreSQL, take a look at BitNami Stacks.  They have compiled all the standard stuff you would want for a webserver (in a universal binary), and it stands alone without dependancies on your system's libraries.   it makes perfect sense to do so for ""helper instances"" and I do this on my MacBook.  If this is a strong requrement for you, consider migrating to an intel-based Mac. Getting linux installed and working on a real PC will be a lot less hassle than getting it running in a virtualisation environment on hardware that is both ancient AND a completely different CPU/architecture/platform (virtualisation sw, if you can find any for a PPC Mac will almost certainly emulate an i386 PC rather than run native ppc code) You are right.  It is not worth the pain to try to compile these things on OS X.  But downloading and installing these stacks are great.  [I haven't run any in a production system yet, but, with a pass over the config files, I don't see a reason not to.] depending on how much load your production server is expected to get, you may even get away with an old Pentium-II or something - you can pick them up for free, although they're less likely to have a built-in network card (will cost $20 or so) i would NOT run a production server instance on a desktop machine.    Too many outside parameters influencing its behaviour. Unless you have some compelling reason to keep OSX I would use YellowDog.  As a RedHat shop I can say that RHEL has maintenance issues with Apple PPC, and CentOS hasn't released a PPC version in a long time.  I instead used YellowDog and have had good luck with it so far.  I can manage it the same as I do other RHEL systems.  The only difference I have found is that since the kernel image names are different, any scripts that parse that must be modified.",5
"That being said, I agree with Ben S. If the only choice is sqlite (which I hope it isn't), NFS is the way go to. But a real DBMS is a much better choice. I am helping in setting up a web service in which user data will be stored in sqlite databases on a server running perhaps Samba, and one or more web servers will read and write to those databases on the backend. I would like advice as to which network file system to use in an all-linux environment, as well as any other thoughts This really is not what sqlite is for. It is possible but a much more robust system (and easier to maintain) would be a simple mysql installation instead of an entire fileserver. so, be very careful!  NFS in particular is well known not to follow POSIX behavior, especially about atomicity. NFS is probably your best bet. NFS is pretty configurable and will likely prove to be the best tool for the job. If you have any Windows in the mix, CIFS (Samba) is the way to go, but in an all-linux environment, NFS. I'd suggest a real multi-user database. SQLite is the best choice for single-user apps, not multiple servers.",5
"what you want is a NIC Team or Bridge.  Both technologies basically allow both your adapters to act as a single unit. Will this effectively cover myself so that if one method hiccups or momentarily drops, the other will maintain a constant connection? or will it still drop connections while it switches from one NIC to the other? This would definitely help you in the event that one NIC has a minor problem, however - be aware that this obviously wouldn't help if your internet connection or router has a hiccup. It is definitely worth googling to see if your hardware supports teaming - if not, setup a windows bridged connection.  This short guide will talk you through the process If you really needed redundant connectivity, you would really want to run the two nics on separate wifi networks on separate frequencies.  Perhaps you would connect one to a 5ghz network and the other to a 2.4ghz network, that way interference on one interface is not very likely to affect the other. Of course this all assumes that your problem is something about your wireless link.  If problems you have are upstream from the wireless ap/router, then the above would be wasted. In any case, if you are using wireless as your tags suggest, the far better solution would be to run a piece of wire. If you just added to identical wireless nics and connected to the same AP, then the issues that cause loss of connectivity are just as likely to connect both the radios on your client.  You wouldn't improve anything Given your tags I you are talking about a wireless network.  I suspect this will not help as much as you are hoping. Your next challenge is that most desktop versions of Windows don't support teaming natively, and very few drivers support any kind of teaming of wifi interfaces.  Without bonding at layer two, this would all be pointless.",3
"It's been a while since I worked properly with AWS, but you can script what you describe above. Creating a new image and changing the EBS config isn't that hard, at least not as far as I can remember. Once you have the script, you might be able to set up your FTP-client to execute it once an upload is complete.    My current process would be to update the main instance and then create a new AMI and map the load balancing to use this new AMI but that seems long winded, is there a quicker way to update a single instance and have the load balanced served instances use this updated main instance. Amazon has lots of documentation on how to set up the AWS command line tools, this one is for ELB: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/UsingTheCommandLineTools.html I am using AWS and have setup and Instance for my site. I have then created an AMI for this instance and setup load balancing to spin up additional instances when the CPU process gets to 60%+ which is working fine. I want to update the website but cannot update the AMI directly as it is just an image of the main instance.",2
"Although the drives seem to test out fine now, how likely is past exposure to heat likely to make them prone to failure now? I've read the report mentioned above and check operating temp ranges for samsung, WD, and hitachi hard drives. Based on research I have concluded that drives operating in the 30 degrees to 45 degrees offer the least likely hood of failure; A S.M.A.R.T. high temperature of up to 55 degrees is not a cause for any concern; and that Peek temperatures over 60 degrees would indicate a reduced drive life expectancy.   1) The south bridge that houses the IDE/RAID controller chipset is often cooled by a small heat sink only. They tend to run hot in normal conditions, so a rise in ambient temperature due to a lack of case airflow and lots of HDDs could plausibly cause data corruption. My personal desktop system at home has 5 SATA drives racked up inside. Recently my system started failing in odd ways like random kernel panics and I eventually traced it to random degrades on the RAID array. Sometimes I could boot, other times I couldn't and so on. After chasing software issues for a while I finally went to pull the drives and discovered the real reason they were failing: they were hotter than a barbecue on the 4th of July! The front case fan had seized up and the PS fan had a loose power connector caught in its grate so the inside of the case had been cooking. A couple of my external USB drives that I use for backup show maximum temperatures of 63 & 64 C respectivly ! I have now built a cooler for the external drive 2) RAM or CPU overheating is a common culprit of CRC memory errors, which translates to data corruption. Monitoring CPU temp and performing memory tests is essential when faced with data corruption. There appears to be some unknown factor at work that is causing abnormal failure rates in external drives.  While heat seems to be part of the problem it is not the whole answer.  I would advice all users of external drives to monitor temperatures closely whenever there is a change in the nature of the drives usage or environment.  This appears to be some ""unknown"" (not the usual) problem that leads to these drives over heating unexpectedly.   This is only my limited personal experience, but I have run a few drives in the upper 40C without issues over 2-3 years, since I was using a silent enclosure (cooled by a small fan to avoid reaching 50C). At these temps I would assume shorter lifespan and quick death, rather than random data corruption, but I could be wrong. In any case, anything under 40C-42C is just fine. What is a general safe operating temperature range for SATA drives? Should 37 be a concern or is drive damage not an issue until after a certain point? With this in mind I replaced the failing fans, added a couple more, upgraded the front one blowing across the drives from 80mm to 120mm and closed it back up. With it standing back upright again the temp range is now generally sitting at 32 on the bottom of the set and 37 at the top. The initial post stated that the user did not know how hot his drives had got - some discs record this parameter and it is accesible via the SMART information. Hard disc sentinal is one piece of saftware that reports this as maximum temperature in entire lifetime. My own testing indicates that a rise of up to 10 degrees is to be expected during periods of heavy access. Now I know that with my case opened an a house fan permanently cleaning out the cobwebs the drives run at 31-32. A quick test with no ventilation to replicate the failed state shows the drives ran up to the high 40s pretty quickly. I don't know how bad it was during the actual failure or how long its been like that. 37 degrees should not be a problem at all. Naturally, hard drives differ in their specs, some can run hotter than the others. You should check the published specifications of the drives that you have. For example, WD Caviar Black 1TB operational temperature is -0 C to 60 C. Of course, you would not want your drive to run 60, as it might reduce its life span. If your current S.M.A.R.T. indicators are fine and don't show uncorrectable sector counts, I would consider the drives safe for use.  I am currently rejecting the explanation of computer virus, user ignorance, bad USB protocols and the placement of the drives in an improper location. As a hold over, I found a house fan and got that sucker cooled off. It ran great with everything nice an chill. About this time I learned how to get drive temperature readings from S.M.A.R.T. Google published a very interesting study (PDF) about hard drive health and lifespan, based on data collected from their systems (many thousands of hard drives). That study says that:",5
"But I seriously doubt the font has any relation to Word 2010 at all. You're just seeing it in Word because Word uses the Windows font repository. You most likely already obtained that font from an external resource. This is a list of the fonts that come with Office 2010. If you need them professionally, they can be purchased separately. The fonts that come with Microsoft Office 2010 are installed into the default font folder in Windows.  A simple way to solve this problem is to embed the fonts you have used in the document when you save it. You can do this by going to File > Options > Save and checking the boxes under Preserve fidelity when sharing this document:  As such, they should be available in any other Windows application that makes use of the Windows font repository. Either way, it does not look like there is a font called bubble1, so it is likely a third-party font that you installed. Depending on the license for the font, you may be able to simply copy it from your fonts directory (%windir%\fonts).",3
"Answer 2: It sounds good to me, though I haven't looked at any lab comparisons yet.  I have started recommending it to co-workers for their home computers -- it's a lot better than nothing! ""Microsoft Security Essentials is NOT supported for businesses of any kind. Not only that, but it is licensed for consumer use only, and carries similar license wording to Office Home & Student. Commercial or for-profit institutions must pay for the software. AVG and Avast have similar licensing."" I will ask MSFT if he is right. For now, all I can say is the person who sent is a MS partner and usually has good info to pass along... MJ Just to address question #2 - I have been running MSE with religious updates and was infected last week with a GDI vulnerability attack that nearly forced a total wipe. I'm re-evaluating the whole thing. From benchmarks that I've seen MSE seems to give a very good level of protection, almost as good as any other anti-virus product on the market. Apparently you need have no fears on that account. Answer 1: Unfortunately, no.  The license specifically states in the first bullet point in section 1 I just want to bring some precisions regarding the licensing and correct what I think are erroneous statements from other answers (e.g. ""Microsoft Security Essentials is NOT supported for businesses of any kind""): Microsoft Security Essentials may be used in a business environment under the following condition:",5
"I have lost count of the number of times where I have documented something a few years ago which has come in handy later on down the track. The best thing is that as it is my own notes, I can write about certain ""gotchas"" which I encountered which I may forget some time in the future. On the other hand, Googling and bookmarks won't afford me this. Further to this, if I were to use bookmarks alone I run the risk of those websites not being around when I may need them in the years to come. There are the ""archiving"" websites, but I'd prefer not to have to rely on them if I didn't have to. Using the above two solutions I have all of my notes and all of my files with me everywhere I go, regardless of which PC(s) I use. I have found File Syncing to be a huge benefit as well. There are multiple options out there, most of which are free. As I don't trust the cloud with my data, I use BoxCryptor to encrypt my filenames and files. Having said the above, I'm definitely not against bookmarking. I have found a wiki entry with my own notes as well as useful external links to websites contained inside the wiki entry to be a killer combination. This way the information is all in one spot. If I'm worried about a useful website not being around when I need it, I use a HTML to PDF converter and attach it to the wiki too. I use Dokuwiki hosted on my webserver. My job requires me to be across a large range of topics, so having my notes easily accessible and available regardless of which PC I am using is extremely useful. I find a lot of people, as well as companies don't document their knowledge, or, do a poor job of it. I'd like to discuss what has been found to work well and not so well when it comes to doing this in the hope that we'll be able to assist one another.",1
"My problem is that I'm using the FEBE extension (which creates backups of my profile for recovery purposes) but it needs me to set a folder absolute path to tell it where the backup should be created so that, on windows, it doesn't allow a path without a letter and backslashes like ""c:\whatever"". But when I open firefox on linux, this path has no meaning and backups are not done. Is there a way to have one absolute path that's understandable by both OSs? Even if it doesn't point to the same place on each OS? FEBE also offers integrated cloud storage (thru Box.net) so you can access the backup that way from both the Ubuntu and Windows partitions.  Since you are sharing the exact same Fx profile, it doesn't matter which platform does the actual backup and posting to Box.net.  It's the very same data getting backed up, so you should only have one platform or the other perform the backup (not both). I have windows 7 and ubuntu on dual-boot and I'm sharing the same firefox profile folder on both OSs, which is located on the windows partition (which is mounted on ubuntu startup automaticaly). Note: I found two similar questions on this forum (though realating to different contexts) but none of the suggested answers work in this case.",2
"If your code blocks are converted to <pre> or <code> blocks in HTML, you would be able to open the HTML page and copy it into OneNote while keeping the formatting. <pre> and <code> is dropped, however. You can use some simple CSS to style the content of the tags, which I believe will be copied as well. There are a list of OneNote Command-Line Switches, but they're for OneNote 2007, and I couldn't find a similar document for 2010. /paste and /insertdoc either pasted in raw HTML or nothing at all. Opening the temporary HTML file and copying the rendered results into OneNote works for me, but it's a bit of a hassle. Do you have the ability to export the MD as HTML? If not, you can use something like Dillinger to do so. Another thing about pasting into OneNote is that it has never seemed to respect new lines.  I always have to go back through large pasted documents and add the newlines back so that it's not all one giant paragraph.  I have yet to find a solution for this. With PanDoc and a batch file like that, you can drag and drop a MarkDown file onto the batch file, and a temp.html file will be generated in the source folder. Open it up, copy/paste the results into OneNote. You might be able to automate something with PanDoc. It can accept MarkDown (among other things) and convert it to many other formats, including some that OneNote might understand, like HTML.  It's mostly command-line, so a quick batch file might be in order. You have to turn on Markdown in the ribbon.xml config (see bottom of link), open a text editor in admin mode to edit the file, and turn the tag to 'true'.",3
"There's a old feature of Cisco switches called ""Private VLAN Edge"" (see https://www.cisco.com/en/US/tech/tk389/tk814/tk841/tsd_technology_support_sub-protocol_home.html).  The feature is restricted to a single switch (or stack-of-switches), and it doesn't really work across multiple switches.  That should make all Intra-VLAN traffic visible to the device. That might be a bit more than was intended in the first place, so...  Now whenever a host with a MAC-address-of-interest connects to any of the ports, it's switchport gets mapped into VLAN B, and to reach anyone else on the subnet (other MACs-of-Interest included) it must go through the inline device. MAC learning and ARPing should be ""native"" for all systems involved. Port Macros assign host of interest to VLAN B. Connect both VLAN A and VLAN B to a pair of bridged ports on the router (as above, two interfaces & BVI, or better two-ports-on-switch-module & SVI).  If the inline device is just intended for analyzing traffic (and not for manipulating or blocking), a custom smart port macro (see above) might just be running a user defined macro that adds another line of ""monitor session 1 source interface 0/Y"", as soon as a MAC address of interest is discovered on a given port. Of course, you'd need a ""cleanup macro"" as well, that runs after ""line protocol down"" of a given switch port.  Think of it as ""hotel mode"", where you want to keep your guests' system from talking to each other, but everyone may talk to the router.  Connect the inline device to a SPAN destination port on the switch, and use VLAN B as SPAN source. Whatever talks on VLAN B will be visible to the now passive inline device.",1
"If you can't or don't trust the person you are giving access to data, than no technical controls can give you what you are looking for. Users will forever be the biggest security hole.  It is certainly more secure than sending a plain text message. Once a message leaves your servers you can not be sure that the transmission is secured with TLS thru the entire journey (Unless you set up a direct trust and force TLS between two endpoints).  You have to assume your message is in the clear once it leaves. With the encryption service - Microsoft allows you to encrypt the message and send to a recipient.  The recipient can read the message by accessing it thru a web portal or mobile application.  They have an option to sign in with a matching Microsoft ID (it must match the recipient address) or use a one time pass code that is generated and sent to the recipient address.  You can try to layer in additional features like TLS (but you can't guarantee it) for transport.  You can also be sure proper SPF, DKIM, and DMARC records configured (but these still rely on recipient honoring them) to help. Because you do not own and can not dictate the terms from which the recipient will receive and open the message you must TRUST that they are the person you are sending it to.  If the recipient account is compromised than they may be able to open the message.  This includes some man in the middle scenarios where the means to access the message (the portal link and encrypted message), as well as snagging the key.   If you want end to end encryption you need to rely on S/MIME or something like PGP. But even with these tools you can never be 100% sure of who has the private key or if the recipient was compromised.",1
"It is a multi-class imbalanced data and I have a small dataset to train around 700 PO descriptions. The number of classes is 7 and the class distribution is similar to exponential. One class is dominating.  How to vectorize one-line text data? I have used tf-idf including bigrams and trigrams but I am not able to get good results.  Using bigrams and trigrams is likely to generate a high number of features, but with a small dataset the traditional approach would be to reduce the number of features. You could start by removing the least frequent words/n-grams (e.g. less than 3 occurrences), and/or use feature selection with InfoGain. It might not be very accurate but at least you avoid overfitting.   My take is that TF IDF should not work since the term frequency and the IDF frequency will be very small.  Check my answer to this question. Nowadays there're many pretrained embedders to choose from. They'll give you fixed-size numerical vector of features. You don't even have to go DNN way, xgboost will work just fine.",3
"First, I open a cygwin bash terminal.  Then, I type startxwin and hit enter.  Two new tray icons appear.  One of them resembles the cygwin logo, but with a green X inside the black C.  I left click that tray icon and find Image Viewer on the Graphics menu.  When I click that, Eye of GNOME starts.  (And the 'auto refresh' feature works.) Linux software cannot run on Windows. Cygwin provides a way to compile software designed for Unix into something that may run on Windows. I've never used it so I can't say what the limitations are, but given the existence of CyGnome I would assume it's possible. And if yes, can I remove Microsoft's own apps(Windows Explorer, IE, Windows Media Player, ...) and replace them with GNOME 3 Apps? Alternatively you might consider checking out GreenGnome instead. It's a port of Gnome 1 to Windows. The project is long since dead, and was from the XP era. http://translate.google.ca/translate?hl=en&sl=uk&u=http://uk.wikipedia.org/wiki/Greengnome&prev=/search%3Fq%3DGreenGnome%26client%3Dfirefox-a%26hs%3DhgC%26rls%3Dorg.mozilla:en-US:official%26channel%3Dfflb . A long time ago I used this for a bit, and worked nicely. http://cygnome.sourceforge.net/ They started a Gnome 2 port project, but it died. http://cygnome2.sourceforge.net/ To avoid compiling yourself you could also check out CygWin Ports which says it has a Gnome 3 port, as well as Nautilus, and Empathy. ftp://sourceware.org/pub/cygwinports/portslist.txt . I've never used it so I don't know the specifics of getting it up and running. Just install gnome-flashback and xinit from the cygwin setup, GNOME Flashback shortcut will appear in your start menu in the Cygwin-X folder. I want to install GNOME Shell and its core apps and features (including Natilus, Evolution, Empathy, Multiple workspace, and more) on a Windows 7 platform. I installed these cygwin packages (and their dependencies) and I am now able to use Eye of GNOME 3.18 on Windows 7.",4
"After you are done you can just delete the virtual machine and even get rid of VirtualBox if you don't see yourself using it in the future. Also when you are installing on the virtual machine make sure that you don't allow it to have a network adapter. That way you can type in the product key and it won't count as being used as it will never verify it with Microsoft. Would Remote Desktop from another machine work.  I once even connected via remote registry to activate Remote Desktop Option 2: Use WAIK to create a sysprepped image with an unattended answer file.  Connect laptop hard drive to desktop to apply image, then put laptop hard drive back in laptop and start up It looks like I need to figure out how to install video drivers with sysprep (to fix Option 2) or automatically partition/format the drive in a vLite DVD (Option 3) I have a laptop with a busted monitor (I can only see the leftmost inch of the screen).  I need to reinstall Vista on this computer.  However, the external monitor is a function of the video card drivers; it is not hardware. Since you are going through the same setup on both machines you can see on the virtual machine what the current step is and how many times you need to click tab for instance to get to certain option. The virtual machine will give you an idea of what is on the screen of the laptop. You will be able to tell when the laptop is waiting for input as there will be no Hard Drive activity and hopefully the little bit of the screen that you can see will give you more hints. After install you will also be able to use the Vista virtual machine to figure out what commands and keys have to be pressed to enable remote desktop on the laptop. That way after installation you will be able to control the laptop through the network. Here is another thing to try while you wait for a better solution. Since you stated you have another computer I am assuming this is possible. Outcome: During testing (Virtual PC), the first thing the disc does is prompt me for hard drive partitioning. Outcome: Arrives at login screen.  I type in the password (without seeing anything, mind you), press enter.  The computer then does a full restart.  I have no idea why, I cannot diagnose, since the monitor isnt working yet (Video drivers havent been installed) All those responses are great.  I ended up solving it by using my vLite disc with video drivers slipstreamed in, getting a Virtual PC and my computer going with the same disc, and following along, tabbing and enterring where necessary.",3
"I/O and CPU contention are typically the causes of poor performance. As you can tell from this answer, there's no simple solution. Between top, vmstat and even iostat, you should be able to pinpoint what your system is doing. Second place to look is CPU contention. If yous CPU is constantly high between the user and system values (us and sy values in vmstat), you likely need more CPUs on the system. There are many reasons for this. Your first place to look would be anything I/O (disk) related as this is the slowest component of any system. If you run top when accessing one of the sites, and you observe that the value for %wa is consistently high (50%+ for a 2 CPU system), it's likely you're saturating your disks. You can run vmstat 1 as well which will give you a better breakdown of what your system is doing. The wa value (usually the second last column under the cpu heading) is the same as the one in top. You could be out of RAM and therefore swapping, or your database(s) could be hitting the disks too often. In this case, I recommend increasing the RAM on the VM.",1
"The Path-finding is working Perfectly. However (as expected) when the play completely blocks off all routes to the Agent's Destination, the agent simply moves to the spot closest to the destination that it can get to and sits there. I'm finding a serious lack of any information on this subject.. as far as i can tell no one has even tried to do this, or asked for help doing something like this. (at least not with Unity's Nevmesh.) I think A* is a little(and by little i Mean LOTS) outside of my skill to implement at present. Then using the usual shortest path search methods (Dijkstra/A*) you will end up with the path that uses the least amount of obstacle destructions. What i need, is to detect when that happens... Some kindof NoValidRoute() or something, I haven't been able to dig anything up in the Unity documentation or from google Searches. If this is the case you should recreate a new route as soon as the player clicks to place a tower. As ratchet freak said you can assign a much higher cost to passing through towers and you will find out this way if there is still a valid route (no high cost). If there isnt you just prevent the player from placing the tower. Make the cost of moving through/destroying an obstacle an order of magnitude greater than the cost of the entire path. The exact amount is not important just that destroying a single obstacle is more expensive than taking the long way around through every tile of your map. Once i have that, I need to determin which Navmesh Obstacles need to be destroyed to open the shortest route possible with the fewest number of Obstacles destroyed. (i.e. if a path can be opened with only 1 tower destroyed which tower would give me the shortest route.) I'm using Unity's Navmesh system. Each Enemy unit has a Navmesh Agent component. And each Tower has a Navmesh Obstacle component. I feel like there is a conceptual error: Do you really want to destroy the 'cheapest' obstacle, or rather preventing the player from placing this last, full blocking, tower. So I'm building a Tower Defence, with mazing (i.e. the player can place towers to make the mobs take a longer route to there destination.)",3
"In Windows 8.1, the right-click option is gone. What's an easy way to manage my wireless network profiles in Windows 8.1 (without being connected to it)? If at all possible I'd like to avoid third-party software for this task, and although I'm sure I can do this from Powershell, I'd like my convenience back. I found a tool for users to manage their network profiles (change priority, make default, forget, view properties, etc.) without using those command line methods. I believe this is a much more user-friendly solution! You can find it here: http://main.kerkia.com/Products/WinFi/description.aspx. See this article for more info: http://windows.microsoft.com/en-us/windows-8/manage-wireless-network-profiles My Windows 8.1 machine shows ""Forget this Network"" and ""View Connection Properties"" when right clicking the network name.  In Windows 8, I used to be able to right-click on a wireless connection in the ""Networks"" sidebar and click ""Properties"" to access the password info, the connection settings, etc. The right-click context menu would also allow me to forget a network on demand (with the ""Forget Network"" option).",3
"I want to know where Google Chrome stores its search engine settings for the Windows XP platform. The reason being; my search engine keeps being changed to SpeedBit and I've thought of just locking up the file to prevent Write access to it. It would only be read-only, on the system level so hopefully that should prevent the annoying search engine change that's happening. Under Ubuntu Linux, I found the search engine settings stored in ""~/.config/chromium/Default/Preferences"". Under Windows, it should be "" C:\Documents and Settings\%USERNAME%\Local Settings\Application Data\Google\Chrome\User Data\Default\Preferences"" I don't know what version of chrome you are in. But in the latest version of Chrome, open a new tab and enter you can create a folder with a shortcut to this folder and set the browser how you want and then save the folder User Data in the folder with the shortcut to the chromium folder and then copy and paste each time you want to restore your settings back. Works for me after I use cccleaner with Iron C:\Documents and Settings(your windows account name)\Local Settings\Application Data\Chromium""  and folder called ""User Data"" this is were it is located.  The command opens up your search engine manager. Find the speedbit search engine which must be listed there. Then hover over to the right of the window and you should see an ""x"" symbol. Click on that and it should remove speedbit from being your default. Then find Google,Yahoo or bing and hover to the right again. Before the ""x"" symbol you should see ""make this default"" button. Click on that and it should reset your search engine back to the one of your choice. I don't know whether this helps you or not. But this is what I was able to understand from your question.",4
"Not only does it solve a lot of problems with low level resource management, but it also helps to ensure that resources are loaded only once, and then reused if they are already loaded. Finally, I think a very useful feature is the in-game reloading of resources. Since the resource manager holds handles to all the games resources, you can build in a function which causes all the resources to be reloaded from disk and updating the game in real-time, which is extremely useful for artists/designers who work with your game. A resource manager can also cache resources, so that although the last reference to a resource has been dropped, the resource manager can choose to keep it in memory in case it gets loaded again in the near future. The resource manager would be programmed to automatically handle all memory management with your resources. Depending on how you design it - if C++ then make friends with your scenemanaging class(es) to ensure that ownership is properly handled. One of the reasons for having a resource manager is the sharing of resources. For example, when you call resourceManager.Get(""sprite.png""), if ""sprite.png"" is already loaded, the resource manager can simply return a pointer to the already loaded sprite resource rather than creating a new image and reloading it from disk. Another benefit is, besides caching and reference counting is that it can handle dependencies (model a needs texture b? I'll get it for ya!) and load order issues (model a needs to know what shader b requires? Let me load shader b before loading the model!) If the resource system is abstracted well, the underlying details can wary between file system, physfs storage, sql even...",3
"I think the sticky point will likely be getting (redirected) links on the test.example.com server working properly. Here's a pretty complete explanation. Does anyone have idea how to do this? And what should I do in my domain provider's controls, so that they can fetch data from my server, without changing URL. My PC has a configuration to work as a server. It has quad core processor with 8GB RAM. So I made it a server.  With apache, I don't believe there is a way to make this mapping arbitrary and ad-hoc; each domain is designed to represent a site, not a directory within a site. You would use name-based virtual hosting and then define subdomains with your DNS provider.  You must set up an additional DNS name and virtual host for each domain name (including subdomains) you wish to use. Wouldn't a reverse proxy set up in mod_proxy cover this? Although as pointed out for,  sanity's sake you'll probably want to run a separate Apache for each host header (www, test, etc.). Also, you'll want to confirm that your ISP is set to forward *****.example.com to port 80 on your IP number and not just forward www.example.com. But I want to forward e.g. www.example.com to folder C:/wamp/www/example and test.example.com to folder C:/wamp/www/test  At present, when I am calling my IP address e.g. 123.234.345.456, I get index file from C:/wamp/www/.",3
"What it the external/WAN IP address of your main router? If it's 192.168.something or 10.something or 172.16-32.something then you'll need to as your provider to put your modem into bridge mode. But from what I recall, the WRT45G is JUST a router, and there may be yet another device, the one that acts as the ""modem"" for your connection.  Is there such a device? Once you know you can get to your outside router on a certain port, yes you will need port forwarding on both in order to reach a host that is behind both. If so, you may want to talk to your provider about opening up those ports for you - some providers will block things by default unless you need them and ask for them to be opened.  OR, make sure that your modem isn't also being its own router and also in need of port forwarding. In your situation, I would've suggested one of the routers that allows a separate ""guest"" network, but you've already bought it, so that's that. I'm assuming you had the proper IP of the computer you were trying to forward to in those port forwarding settings? When you are trying to port forward -- and assuming the family router is also the modem, what the ISPs call a ""gateway"" device (just to confuse things) -- you have to do it with the outside-exposed router to the inside address.  You can't just do it on the inside router (yours) because the traffic will never get to it. Please note that when you say that your router has the same external IP address, that cannot actually be true.  If you are checking via something like ""whatsmyip.com"", you're just seeing the external presentation of your home's connection.  But with your router connected to the family router, yours has to have a ""wan"" address that is within the range of the LAN addresses that the family router hands out.  And you're not really gaining anything by hooking up your router to the other router, except for a separate additional wireless network that isn't buying you anything at all.",2
"Assuming i'm using classic slow softwares like adobe suite (photoshop, flash, autodesk) and some very simple soft like notepad, php and so on...! I would say it's good to have a large internal HD. And a large-ish external HD to backup all your files on. Disks in the 50GB range cost almost as much as a 500GB disk, there' just no reason no to spend the extra $20 and get 10x as much storage. You can do both. Partition your drive into 2. One for your Main Drive (C:) and the rest for your storage.  This way, if you system crashes, you can wipe drive C and start over without having to wipe your entire storage.  If i have a 500GB, i normally give 100G to my main drive.  It usually depends on how many apps you planning to install as this will be on the main drive.  The other drive will be where I store all my backup, pics, etc. I don't think hes worried about data protection at all. He just wants to know if a smaller hard drive will boost his data access speeds. In most cases this won't help at all and if it does, the difference would be marginal at best. The platters on these drives are the same size so a smaller drive doesn't mean less travel time. Just make sure you get a drive with a good cache amount and stick with the 7200 RPMS. You shouldn't see any slowness. The other benefit is that if you're really looking for speed, you can image your 32GB C: drive to an SSD, and boot off the SSD, giving you significant speed benefits as all the Windows activity (registry, swap, prefetch) is contained to the SSD. From reports from friends who've done this, putting Windows on an SSD gives the system more of a boost than anything else.  but this time i'm not sure i want buy a big Hard Disk! i was thinking to have a ""very small"" HD like 50GB as main one and external (big) for store all other stuff! The best mix I've found is having a big physical disk (for speed) logically chopped up into smaller logical partitions based on what you need. The main reason for this is backup and restore. If you have a 500GB C: drive, and your Windows boot disk gets corrupted  (based on the software you mentioned, I'm assuming you're running Windows), you have to restore 500GB.  do you think this is a good practice for improve performance/speed or i'm jast saying some stupid thing!? If, however, you format your boot disk as a smaller disk (I usually recommend 32GB, but it depends on what you're doing), then you can restore your boot/programs disk without forcing a restore of the other 468GB. In other words, you're more likely to do it.",5
"That process is a little bit more manual work than what you're probably currently doing, but not much, and you may find that solution is the simplest to implement overall. The other might be to use different folders.  When you have a folder called ""downloaded"" and want to watch a video, start by dragging that video into the ""current"" folder.  Then watch the video from there.  If you get interrupted, that's fine.  Then, after finishing the video (or during some sort of daily/weekly clean-up procedure), manually drag the watched videos into another folder called ""watched"". Also, what do you mean by ""watched""?  Do you watch them on YouTube, and make a copy while they are being watched, or download from YouTube in some other way?  In other words, when you say ""watched"", is that only watching via MPC-HC, or is watching by YouTube also something that is supposed to be counted? If the software you're using doesn't support the feature, you could modify the software (I believe MPC-HC is ""open source"").  This may be something many people consider infeasible, but gets to the core of answering the question about, ""how easy is it to do this?"" Some of these solutions might not offer a solution for previously-watched videos, but only be a solution for going forward. For the computer to present you with information, like what videos you watched, that information will need to be recorded somehow.  Some software may do this (with a menu option like ""Open recent"" which then shows recently-used files).  This would most likely be done by the movie playing software, although in theory there may be alternative approaches involving noticing what files you access (effectively monitoring your usage, similar to how anti-virus software can keep track of files that gets accessed). Is it ""possible""?  Well, sure.  Like academic test questions that use words like ""always"" or ""never"", the word ""possible"" is a highly charged word, which can be interpreted in the extreme.  What is more relevant to ask is, ""how easy is it?""",1
"Camtasia is really good for screen cast , but I think you should by Snagit instead, both from one company . But you'll get screen capture from Snagit :) There is a tutorial on their help site titled ""How do I record system audio? (The sound that comes out of my speakers)"". You may want to see if this can help you. I had the same problem.  The greyed out option changed however when I changed my headset from a typical 3.5mm input to a USB model.  I can now record from the system audio. Vista disable the sound and microphone options of your system by default you need to right click on your speaker icon (taskbar) select recording devices > right click and show disabled devices. This will allow you to turn on the option to record your computers sound. May want to make sure Windows is seeing your microphone correctly first.  In XP it's: Control Panel, Sounds and Audio, Voice. Also, I found Fraps to be a lot simpler to use than Camtasia. Then again, I'm not a very demanding audio/video capture user.",5
"You will be asked what you want to do, just like when you right-click drag/drop in Windows explorer to create a shortcut. One of the options is ""create hyperlink"" which is what you want. Job done. Wash, rinse, repeat for multiple links, or use multi-select in the first place (Ctrl+click or Shift+click) An alternative way to create the hyperlink which should get around this issue as well as being far quicker would be to simply use drag and drop as follows: (Aside: a normal left click drag and drop will insert attachments without prompting, and you can also do this with no email open: select documents, drag and drop to Inbox and a new email will be created for you with attachments on and with subject line already set which you can edit of course). Open Windows Explorer, navigate to a folder containing the thing you want to link to (a file, document or a sub-folder). Right click and drag to your open e-mail message - you don't have to have them open side by side, you can drag to the task bar on the message, HOLD and wait for the message window to pop to the front. (don't drop on the taskbar, this does not work)",1
"There are a few alternatives to using the Windows Update site.  One I would recommend is WindizUpdate.  It's designed for Firefox/Opera users so they can get Windows updates without using Internet Explorer, but perhaps it will let you update your machine while side stepping the group policy issue. You could also try Windows Updates Downloader which is a slightly more manual process but should do the job if Windiz isn't suitable Use the program KillPol (standalone .exe) to remove Group Policy and then you can visit Windows Update. You need to be only a local administrator and can also use the same program to re-apply GP. I'm offering the download from my Dropbox account because I have no idea where on the net to find this program. It was given to me by a coworker. I am in the administrators group for my local Windows XP machine and I would like to get updates via http://update.microsoft.com/[1]. However, this is prevented via the group policy: [1] Several installed applications are Microsoft based, but are not part of the machine standard (eg Visual studio). As such, I am not getting the updates for these applications. I could periodically go to the various application sites and look for hotfixes, but that is beyond tedious. Speak the the IT dept of the organisation. That policy setting is typically enabled when updates are managed centrally using WSUS, Altiris, etc.",4
"And as others hinted, you could should create a separate init script to help you manage this instance. The stock RH /etc/init.d/httpd script should act as a starting point.  Yes, it's quite simple.  You basically just have to start up the second instance with a non-default config file on the command line.  If you do a web search for ""apache multiple instances"" you should find what you need. Having 2 different config files gets you part of the way there.  In addition to the PID files that Kristaps mentioned, you will need to either listen on different ports (other than 80 and 443) or bind to a different interface. Copy over the config files under /etc/httpd to another directory.  Modify them, changing the ServerRoot, DocumentRoot and other path related variables in httpd.conf and others.  Then create a separate init script that call httpd -f  Another useful option of the httpd command will be the -t option to test the configuration file for errors. In a typical environment, you can create a copy of httpd.conf and then modify the following properties on the new file. Say for example if the name of the newly copied file is /etc/httpd/conf/instance1.conf, then you can start this new instance using the following command And depending on your implementation you may need to modify additional properties like LockFile (if you are running on a NFS)",4
"The weird thing is, the article is published, and the permalink is the right one, as you can see on this screenshot of the WordPress CMS: I've accessed my WordPress DB and I've search the article to see if there is something wrong there, but from what I can tell all the fields are OK, here's a screenshot: I've been ""playing"" with the system and I've deactivated the Custom Permalinks plugin, and now the searches inside the blog work, but they link to that ""strange"" permalins.  I have a WordPress blog I've migrated from another CMS, and I've being having a lot of problems with my permalinks structure: lots of articles give a 404, although they are there, somewhere, published.  but it does not work. In fact, if I try to use the short link (http://www.muycomputerpro.com/?p=5023) the article does not show either. The permalink (below the title) is correct (http://www.muycomputerpro.com/Actualidad/Especiales/2009-las-grandes-crecen-en-la-bolsa/) The site is www.muycomputerpro.com (MCP for short), and for example an article that should be found is: http://muycomputerpro.com/Actualidad/Especiales/2009-las-grandes-crecen-en-la-bolsa I really don't know what is causing this. The permalink structure should work (I'm using the ""Custom permalink"" plugin to preserve the old URLs that had a alphanumeric code at the end of the postname) and the permalink config on wordpress is ""/%postname%/"".",1
"If you're willing to consider commercial options, Aspera and Signiant are leading products for this sort of thing. They'll also be much faster than Rsync or anything SSH-based. I have had success with large file transfers with BitTorrent Sync.  I havent used the Linux version, but the Windows version works well.  It is designed to transfer large files and supports block level changes and resuming transfers. As a secondary preference, it would be desirable for multi-session support (even more so if it auto-scale sessions based on reliability).   I need to transfer a directory of large files (~1.5G each) from my laptop at locations with variable intermittent connectivity and high round-trip delay (e.g. From hotels in Asia to servers in Virginia).  Since the source is a laptop, it needs to be able to automatically resume infinitely until the file transfers are completed. The --partial option will cause rsync to resume interrupted transfers where they left off. You can also include --progress if you would like a transfer status indicator.",4
"2) split the object to different sub-objects (at triangle level) based on the material and issue separate draw call for each. This keeps the shader simpler, but has higher CPU overhead due to more draw calls. I tried passing a variable from the vertex shader to tell the fragment shader whether or not I wanted part of an object textured. That way I could write another shader for the other parts of the object. Unfortunately, what I have creates the undesirable effect seen here. 1) put the textures for the house to a single texture are just UV-map the object accordingly. If you would need to use different BRDF etc. for different parts of the object, you can use uber shader and switch in shader and use another texture to control which BRDF to use in which parts. The benefit is that you can render the entire object with a single draw call reducing CPU overhead with the expense of some GPU overhead due to the uber shader. When I rearrange the code a bit so that the bottom half of the house is textured instead of the top I get this In vertex, put  vPos as varying. Then instead of doing the height check in vertex, do it in fragment.",3
"2) If you use a big number of layers/neurones, then the model will learn  too much representations/features that are specific to the training data and dont generalize to data in the real-world and outside of your training set (overfitting).  Working with neural networks since two years ago, this is a problem I always have each time I wan't to model a new system. The best approach I've found is the following: Adding to the previous answers, there are approaches where the topology of the neural network emerges endogenously, as part of the training. Most prominently, you have Neuroevolution of Augmenting Topologies (NEAT) where you start with a basic network without hidden layers and then use a genetic algorithm to ""complexify"" the network structure. NEAT is implemented in many ML frameworks. Here is a pretty accessible article on an implementation to learn Mario: CrAIg: Using Neural Networks to learn Mario 1) If you use a few number of layers/neurones, then the model will just learn a few useful representations/features of your data and lose some important ones, because the capacity of middle layers are very limited (underfitting). The general approach is to try different architectures, compare results and take the best configuration. Experience gives you more intuition in the first architecture guess. Choosing the right number of layers can only be achievable with practice. There is no general answer to this question yet. By choosing a network architecture, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. In a DeepNN each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the problem at hand, this information can never be recovered by later layers. This is usually referred as ""Information Bottleneck"".",3
"I have two hard drives, one with just one primary partition (sdb) and one with several partitions (sda). My fdisk -l looks like this: I have Ubuntu 11.10 installed on sda5 and windows 7 installed on sdb1 (I'm not sure what's on the primary partition /dev/sda1 -- some old operating system I don't care about). I installed Windows first, then installed Ubuntu. I'm using grub as my boot loader, and I'm able to boot to Ubuntu (and using it now) but at first update-grub2 didn't even see Windows 7. After searching online, I found a common solution to this was to create an /etc/grub.d/11_Windows init script containing: I know a lot of people are having problems similar to this one, but I don't think this is a duplicate. My situation is a bit different and none of the solutions that are working for other people have worked for me. Here I set root to (hd1,1) to match sdb1. This added Windows 7 to the grub menu, but if you select it, it just shows a blank screen and hangs (it does not boot into Windows 7). I've tried a number of solutions given to people having similar (but not identical) problems as mine and can't get anything to work.",1
"Once the WSFC is running, you can then force AG, if needed. Perform a Forced Manual Failover of an Availability Group: Depending on the locations of your application servers, their configuration and their ability to reach a SQL server, then in theory you can have two nodes believing they are primary and having data changed at the same time. Once you fix your network issues and the nodes resume connectivity all the data changed on the original primary will be overwritten from the node where the fail-over was forced to. This can result in the loss of critical data. Once I've been involved in an outage where our mirrored servers lost connectivity. One of the things to worry about is making sure your applications are pointed to a single instance. In a network outage you can have all the nodes of an Always On cluster up but unable to communicate with each other. You force a fail over to a secondary and then as long as there is an outage you can have two primary nodes since the original primary won't know about the forced fail over. I've seen this situation once with SQL 2005 and mirroring. And we decided to not to force the fail over and let it stay unreachable. Reason being that in the worst case if we had to back up and restore to restart mirroring, then it would be a 2 day process for us with risks of the transaction log becoming full and not being able to expand the disk on which it sat.",2
"The basic trick of ClearType is to take advantage of the fixed, predictable sub-pixel elements of an LCD screen.  A Quattron screen will still have fixed, predictable sub-pixel elements, so we are good so far. By the way, Linux and Mac have the same trick; they cannot call it ClearType because that is a Microsoft trademark, but they can do the same thing. I imagine that a future update will add explicit support for Quattron displays.  In the meantime, if you don't like how it looks, you can always disable ClearType.  If you use ""Standard"" smoothing instead of ClearType, you still get pretty nice smoothed fonts; and if you have a very high-resolution monitor, you don't really need anything as tricky as ClearType.  (ClearType was a bigger win back when people had laptops with only 800x600 displays.) Now, if your screen were black-and-white, ClearType would be easy: just take advantage of the sub-pixel elements, you have a higher resolution display, done.  But the sub-pixel elements have colors (red, green, and blue... and now yellow) and the ClearType algorithm tries to pay attention to what the colors are, and balance them so that you don't get visible color fringes on your characters.  Until and unless ClearType gets a Quattron-specific upgrade, the ClearType algorithm will be trying to balance the colors assuming standard RGB, and the yellow sub-pixel elements might throw things off a little bit.",1
"Despite the fact the oficial repositories are sometimes behind the cutting edge, install software from YUM or other package manager is fast, clean and more secure. The Debian New Maintainers Guide is pretty helpful in this situation, specifically Chapter 9 - ""Updating the package"". - while it may seem scary at first, it can be as simple as I know I can think differently sometime ahead but right now I think this is the best choice for a regular guy like me. I don't believe you can't automate much further. There's a definite overhead that will only make it worth it for a respectable amount of servers. Instead of compiling from source and deploying, build (or find) Ubuntu packages for the newer versions you need.  Often you can take the build files from an old version and just use the newer source.  You can then maintain your packages like any other, and only worry about tracking configuration files. I came from 15 years of Windows and made a lot of research in order to discover what Linux flavor I'd choose to work with. I have just upgraded Fedora 13 to 14 and something I have to say is: mantain software installed from source is a real nightmare. The core problem you've got is that rather than having the distribution track security updates and apply them for you, you've got to handle that yourself.  You can make life a little easier, though, by subscribing to the security announcements list for your distro and filtering it for the updates you care about.  I have a procmail script that it kept automatically up-to-date by my package builder (any package in there goes on the ""let announcements for this package through"") and anything that passes the filter (meaning ""I need to consider a manual update for this"") drops into the todo list (ticketing system) for further handling. As others have mentioned, you should build your own packages. In order to keep up to date with the upstream sources, subscribe to their mailing list(s). Some have a low volume -announce list.",5
"If you work with Windows I highly recommend Bart PE. There are a number of antivirus plug-ins available for it, as well as just about every disaster recovery tool you might need. You can do it with Knoppix and ClamAV.  There's tools (sorry, I forget the name) you can use on Knoppix to access the registry, but it's pretty tricky to clean out the registry.  ClamAV will do fine at finding viruses in the filesystem, however.  But some spyware (and viruses, I assume) will put itself all sorts of places in the registry so that it reinstalls itself. Better option: back up the system, do a clean install, scan the files (documents, movies, music) that are still needed and copy those over.  Reinstall any software from scratch and don't copy any of the registry over. Either the Ultimate Boot CD or the Windows version.  I would recommend the windows PE based disk for cleaning windows systems. Hopefully, your friend has been performing regular backups of their system.  It is now too late to do a backup as others have suggested.  If you perform a backup now, you're potentially creating a virus infected backup because you cannot be sure which files have been affected by the virus.  If you were to restore data from a virus infected backup, you could end up back in the same situation your friend is in now.  :-( Once a virus is detected, I take the approach that the whole installation is now suspect and should not be trusted.  I don't think there's anything to be gained by trying to ""clean up the mess.""  Cut your losses and re-install. When a virus damages your system, there's only one way you can be certain that you've removed all traces of the virus, and that's to do a complete re-installation, including re-formatting of your drive(s). I've used SystemRescueCD successfully in the past. Also there is SLAX with the NTFS-3G module. Both allowed be to transfer data from one HD to another. Make sure you scan the files before putting them on another system.",5
"then logout/login. This chooses xim as the input method (ibus is the default). If you remove your ~/.xinputrc, then im-config can  list the input methods available, even if it fails to select one. However, we can specify the path to the script as the command, in which we will simulate pressing the same key combination. I noticed however that no matter what config I tried, the ibus daemon was running everytime after I had rebooted. In this askUbuntu answer, Kayvan Tehrani shows a setting that works for me in that setting. Set the XMODIFIERS to an empty string before running your application: The basic idea is this: in the system settings you can specify the command that is started by the Ctrl+Shift+U key combination. Running into this problem on Ubuntu 18.04, I've tried all suggestion above, but unfortunately none of them worked. I ended up uninstalling the ibus package.  I tested this in PhpStorm 2018.2 EAP Build #PS-182.3458.35 and I can say that it works, but with some caveats: However, I've failed to find any configuration panel under KDE ubuntu 14.04 to do the job, and the standard im-config utility seems broken. I finally succeeded by creating a file ~/.xinputrc that contains a single line: XMODIFIERS apparently changes the way xim and ibus works, and when it's cleared, it disables the CTRL-SHIFT-U combo, allowing it to filter into the current app. To simulate keystrokes, I tried the xdotool (apt install xdotool) and the xte (apt install xautomation). I struggled with this problem for two days. I tried all the methods listed here earlier. And it looks like I came up with a solution. But it is very unstable and has bugs.",4
"I 'm designing a distributed system and I deploy it on 1Gb cluster of Windows Server 2008 R2 But I have issue with the latency, the normal Ping takes 0.270ms and I believe this is too much, since all the machines connected locally via one 1Gbs switch. If your switch is either very dumb or mis-configured, it would use the store-and-forward forwarding scheme instead of cut-through. Changing that would also reduce latency somewhat. It is possible that your NICs are running with a reduced rate (maybe due to some power saving state) - checking your switch's management interface will provide more information about actual link speeds. Or your host's CPU on a whole is running with reduced speed (e.g. due to SpeedStep) or falling into deep sleep and needing some time to wake up. Deactivating the power management voodoo would eliminate that as a possible cause. If your hosts are virtualized, it would be rather normal to see varying/increased latencies due to virtualization scheduling. I do a lot of distributed systems design / operations and seriously, 0.2ms is not somthing I would orry about, unless designing a HFT system, and then using WCF in itself is a stupid move - HFT counds latency in Microseconds tehse days (note: HFT = High Frequency Trading). No, it will not. At the end., a proper designed WCF based cluster system will handle decent latency without problems. Yes, indicidual packes will take longer than zero time, but on the other hand proper message protocol design and work packet granularity in a cluster means that this does not mean machines do nothing.",3
"The server has very low memory, around 1GBs, and so the cache is not enough to improve the performance one on the large table. I've indexed the last 6 small columns, but it doesn't seem to be helping. One table containing the last 6 columns, and the other containing the blobs and extra data, and link it to the previous table with a foreign key that has a one to one relationship? I have a frequently accessed table containing 3 columns of blobs, and 4 columns of extra data that is not used in the query, but just sent as result to PHP. There are 6 small columns (big int, small int, tiny int, medium int, medium int, medium int) that are used in the queries in the WHERE/ORDER BY/GROUP BY. I'll then run the queries on the small table, and join the little number of rows remaining after filtering to the table with the blobs and extra data to return them to PHP. Please note, I've already done this, and I managed to decrease the query time from 1.2-1.4 seconds to 0.1-0.2 seconds. However I'm not sure if the solution I've tried is considered good practice, or is even advisable at all?",1
"Some of the other alternate WRT firmwares offer even more features.   I'm considering upgrading my router from Tomato to OpenWRT because I'd like to run OpenWRT will let me setup my WRT54GL as an OpenVPN server. Tomato is pretty much ""fire and forget"".  The bandwidth graphs are nice, and it's really quite easy to configure.  It lets you ssh in, if you want to fiddle with low level settings, but ever since I set it up, I don't think I've ever went back into the UI. The big one for me was that the stock Linksys firmware on my WRT54GL wouldn't allow me to setup a static NAT such that a service was available on the outside (internet) on a different port than it was actually running on the internal server.  Specifically, I wanted to allow SSH access to one of my home boxes on several alternate ports (long story), but I didn't want to have to setup SSH on my home box to run on those ports.  The default Linksys firmware would allow that, but Tomato does. 1) if you want to use features that are in Tomato (or another alternate WRT firmware) that aren't in the vendors stock firmware With Tomato, you can set your BitTorrent traffic to Lowest priority, your HTTP browsing to High and your Skype to Highest.  That means that your HTTP and VOIP traffic will always be serviced before BT, which is nice if you have a lot of downloaders in your house. I was easily able to configure several routers in a secure bridge and they're as stable as can be... to my knowledge they've never had to be rebooted. I think the biggest one for me is the Quality of Service support.  Sure, you can do it on the Linksys firmware, but it's extremely limited. Stability and ease of configuration were my two big reasons when comparing it to other open sourced firmware releases but the real gem was a relatively easy to configure qos implementation. Control.  Tomato gives you a lot of control over your router that you  don't (usually) get with the standard firmware.  To give a few examples: I mapped IPs to MAC addresses, so my each interface on my network will always have the same local IP. I think most people run one of the alternate WRT firmwares (like Tomato, OpenWRT, DD-WRT, Packet Protector, etc) just for the geeky fun of it.   I found that, before I switched to Tomato, whenever people used BitTorrent the network would slow to crawl (and be pretty much unusable).  Now with Tomato, the network is smooth as silk. I enabled passwordless SSH on my router.  Then is used an alias in my .bashrc so I just type ""router"" and I'm logged into my router via SSH. However, there are some extra features that these firmwares have that are not available in the stock firmware.   For example, others here have mentioned the ability to increase your radio output power beyond what's allowed with the vendor firmware.",5
"i have a Acer Aspire One D257. In this netbook the hard disk is defect so i bought a new one. Now i want to reinstall Windows 7. Im using an external DVD Drive plugged into USB.  When i uncheck this checkbox i get a lot more drivers, and some SATA Drivers but the also do not work. I get the same error message as before.  The laptop has the newest BIOS - 1.15, it is reset to factory default settings except that i enabled the Boot Menu prompt with F12. When i select the SATA AHCI Driver it does not find any drivers. When i uncheck the checkbox ""Hide drivers that are not compatible with hardware on this computer"" it shows one driver: Acer HWID (path_to\1.inf). When i continue with this driver i got an error message that says something like: No new devices found. Check if the driver files are on the installation disk. When i show him the Chipset Driver it sees a lot more driver. When i uncheck the checkbox ""Hide drivers that are not compatible with hardware on this computer"" it show some drivers: The Windows 7 DVD is staring, Win7 setup is starting and when it comes to Hard Drive options it says that no drive was detected and i should try search for drivers. It shows me this window: Screenshot from web  From the Acer Support Website i've downloaded the SATA AHCI Driver and the Chipset Driver. I unpacked both to a USB flashdrive in seperate folders.",1
"I have been using Symantec Corporate edition for years, and finally just updated to the latest version of Endpoint Protection, and I'm loving it. Its reasonably priced, and does very well, and isn't a resource hog either. We're currently using McAfee for our Windows desktops but to get updates and alerts with the latest version it looks like you need to be running their EPO server software. I'd like to avoid the cost of hardware and Windows licensing, and if possible to run just client-based antivirus. CA ETrust Enterprise AV you can purchase and run as few as one individual clients that download from the vendor's servers over the internet.  Cost is about $40 per client per year. Are there any desktop antivirus products suitable for use in an enterprise environment without a Windows server? Internal updates/redistribution mechanism is very simple.  Configuration is also customizable and very simple.  They have a free 30-day eval you can download. If you do, then any AV that gets its updates over the internet should be fine. If you were to set very aggressive caching for their update site (should be easy to track down with some basic logging), then the updates only need to be downloaded once, and can then be grabbed from the cache of the proxy server. I think it may be worth you while to grab eval copies of some likely looking AV products and install them on a test (virtual?) machine. Then have a look to see if they can be configured to grab the updates from a specific source. If they can, it should be easy enough to determine where their default downloads come from, along with whatever connection strings are sent to the source. Once you've found one you can work with, set one machine up to download the updates and configure the rest to get them from that machine. In all likelihood the download source settings will be stored in the registry, making it simply to use GPO or a script to propagate that information. It's a fair bit of fiddling about but the end result may be worth it.",5
"This is the difference between the UI and the in world objects. Your main menu is a screen space UI, the rest of your objects are in world objects, likely sprites. Now, in game, these will both look just fine. In the editor, it's going to display the UI using the UI size, except it's going to interpret those values as world units. In other words, it's going to convert pixel units into world units. That means your UI, which is 1080 pixels across, is now going to be 1080 world units across. That's more than 1000x bigger than your game objects. The reason they're vastly different sizes is just how the editor displays their size. The main menu (UI) has its size based on pixels. Maybe something like 1080x1920. The world objects have their size based on world units. Probably something like 1 unit across for those little circles (wild guess, impossible to tell with the information provided). So, what to do about it? You can change the scale of your main menu. This won't affect the on screen size, but it will change the in editor size. So, for example you could scale your main menu to .0075 on the X, Y and Z axes. It'll still be the same size in pixels, but will be scaled in the editor to be closer to the same size as your in world objects.",1
"Note that you are mounting the entire disk, whereas normally you would mount a slice, such as disk0s2. I know this thread is old, but I'm posting this for those that may stumble across it. I tried multiple different things including hdiutil and various software programs. However, most of these programs were sorting files by type and did not preserve name or the original folder structure of my hard drive. I tried EaseUS and it was the only one that was able to show me the files in the proper and original format. Incredibly happy with the results so give it a shot if you are having difficulty. Be aware that this helps if your problem is with a disk image similar to mine. Nonetheless, it should still work with partitioned disk images, see man diskutil. I have seen infrequent issues when Firefox has been used to download an disk image.  Very uncommon, but it has happened... I ran into a similar situation.  I did what simonair suggested and I received a message in Terminal that Volume mounted successfully.  However It did not help me access the mounted Volume neither was I able to confirm the mounted Volume.  Nor was I able to mount the image using Disk Drill or even scan through. Nothing succeeded.  Generally, if you are receiving ""No mountable file system"", that generally means that the disk image is corrupt or damaged....  Try re-downloading the disk image, or downloading the disk image from within another browser.... I have just had the same problem, with a disk image created by Disk Utility itself. In my case, the disk image is whole disk image, i.e. it contains a bare filesystem, no partition map whatsoever. Neither Disk Utility nor hdiutil attach disk.dmg would work, both complaining about the dreaded ""no mountable file systems"" issue. Here is what I did to solve my problem.  I see that in this post some suggested that we should convert name.dmg to writable file.  I used the command from Terminal to convert but was not successful.  INSTEAD alternatively, I used Disk Utility, attached the Volume (yes I got the same message again, ''no mountable system files'' however, Disk Utility nevertheless attached the image, which I was able to do before anyways.  This time, I selected it and then clicked New image, and then selected image format read/write, not the compressed option. I created a new image.  This time this image was writable however, it still would not mount using Simonair's solution.  However, this time I was able to scan it with Disk Drill with exact file locations and folder hierarchy as I had. Recovering all from thereon was a breeze.  Just remember: when you create new image of an attached volume and chose not compressed but read/write, you need to have a disk with exact amount of space that totals the total space allocated within that name.dmg file (not the amount of space the data takes).  Mine was huge, and luckily I had an external disk with enough space to do that and it took about 5 hours for about 400GB of disk allocation.    Good luck.",4
"The only change that has been made recently is to remove a couple of ciphers from their cipher suite; they specifically disabled TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA and TLS_RSA_WITH_AES_128_CBC_SHA on the server side. I've already confirmed, however, that this is not an SSLv3 problem. The site is PCI-compliant, so it's using very up-to-date SSL settings. openssl reports the following: What I can't figure out, is why Chrome is complaining about this particular protocol version and cipher suite; it seems perfectly fine to me. The site opens with no issues in recent IE (and the IE page info matches what openssl ios reporting) but fails in Chrome and Firefox. For one particular web site, which we really need to be able to communicate with, we are starting to get the usual SSL handshake error: Is there a way to find out what protocol/cipher settings Chrome thinks it got from the server, or why it decided they were invalid? TLS does not work like this. In TLS the client starts the handshake and includes all the ciphers it is willing to accept from the server. If the server does not find any overlap with its own ciphers then it will in the best case notify the client of this particular problem and in the worst case simply close the connection. To fix the problem you should configure your server to accept as much as possible secure ciphers. See Recommended configurations at  the Mozilla Wiki.  It might work with other browsers because these offer different ciphers to the server, i.e IE 11 on Windows 10 supports this specific cipher while Firefox does not support it. Both Chrome and Firefox do offer TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA which were removed from the cipher set of the server and therefore it worked before. In your case you get ERR_SSL_VERSION_OR_CIPHER_MISMATCH, which means that the server does not support any of the ciphers offered by the client. I don't know which ciphers you have configured at the server but SSLLabs shows you which ciphers are offered by Chrome. And ECDHE-RSA-AES128-SHA256 (TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256) is not one of these.",2
"Spend your time and effort on adding storage, providing an easy to use file repository for your users, implement mail quotas and educate your users - it'll be much cheaper and you'll save yourself a world of pain. If you're looking for a new mail server, then I believe that both Dovecot and Cyrus support single instance storage. Having previously run OpenMail systems which do exactly that I would very strongly recommend you find a different solution to the problem. There aren't many mailservers capable of doing this, they are niche products and they are very complicated. Exchange 5.5, 2003, and 2007 all are capable of using Single Instance Storage, which is pretty much exactly what you're asking for. It's per-database, but it's better than nothing. 2007 changed it to only work on attachments, and 2010 did away with SIS altogether due to some of the scaling issues and lack of overall effectiveness over time. Here's a blog entry on Technet explaining it a bit. As an alternative idea - you could use storage that does de-duplication. I think NetApp can do something like this. Although, it may still be cheaper just to buy more storage. Just my 2 cents.",3
"Players are also quite happy to buy into subscription-based services - most MMO's use this format, and are quite successful because of it. This subscription is not generally tied to your ingame actions, and you pay regardless of if you play. But, again, this is a well-understood cost to the player. There is no surprise here, they understand what they're getting. This is my own personal opinon on the subject, and some other people may dislike microtransactions for other reasons, but hopefully this will give you a first element of answer. Players are all too happy to shell out $60 for a triple-A title that has decent reviews - because they understand the cost that it will incur to them. They feel like they can appropriately weigh the cost against the reviews (and their preferences) to decide if they want to invest their time in the game. Most mammals act like this; our reward centers are focused on getting rewards more often, not necessarily in larger amounts. Similarly, we feel costs more strongly if we incur them more often. A pet cat who is fed multiple times per day is much happier than if the exact same amount of food was given to them all at once. If you ask a player for a lump sum at the outset, or silently bill them every month, they forget about the money. But if you constantly harangue them to buy more skins or loot crates, they are viscerally aware that they are constantly spending money in the game. This is bad, because even if you only ask for a dollar or two at a time, doing so a dozen times feels like you're asking for more than if you just asked for $30 up-front.  But microtransactions are more uncertain. Players don't know if they'll end up spending $3 or $30 over the course of a week or month of gameplay. There's no upper limit to the amount they might need to spend in order to play the game the way they want. There's also the issue that you don't know beforehand what will be locked up behind microtransactions when you buy the game. Of course you can make your researchs, but there's a difference between buying a game without microtransactions and knowing you can explore it fully without any constraints, and buying a game to later on realize some of the content is not available. And of course, there's the slightly different issue of competitive games where microtransactions can give you an edge. It's hard to not feel cheated in this case. Let's say you're playing a non-pay-to-win game, and you buy some hats or skins. The assumption is that because the game was free, but the skins cost money, that it must be costly to produce skins. But this is ludicrous, skins and hats are the easiest things in the game to produce! Players know this, and so when you demand $1 for a fancy hat, the cost is associated with the hat, and players feel a bit cheated because they know that the hat couldn't have taken more than a dozen or so man-hours to slap together, and it seems ludicrous to demand so much money for something like that. They are often associated with psychological tricks to make you spend money. One basic example of these tricks is create a fake currency (like the gems in your example) which act as a layer to detach the customer from the money which is actually spent. Another example would be to tease the player by giving him one free item, which will later on only be available through microtransactions. Much more devious methods exist.",2
"Hot swap hard disk trays are indeed intended to allow you add and remove disks without shutting the system (typically a server) down.  In a professional environment swappable and extractable backups are traditionally done with tape systems. But yes, you could use some hot swapping systems for backing up under some conditions. Taking into account that those are meant for replacing damaged components, e.g. disk or power drives. There is a consequence of being meant for replacing damages, not backups, and that is that this hot swap systems often take several hours to rebuild, in case of hard disk drives common RAID systems. This process works well and really works on its own other than remembering to swap a drive, but wouldn't be suitable for large arrays (like a SAN, for instance) or if you're packing multiple terabytes of data. I would consider purchasing a NAS device with its own RAID array to do your backups. We (the company I work for) has a NAS that all our backup jobs dump their data to on a daily basis allowing us to leverage incremental backups. The NAS is in turn backed up via USB3 to an external disk which is taken off-site nightly (we use two disks so one is always connected). Making back-ups to removable disks is a viable solution for small business but my recommendation would be to use an interface designed for removable storage like USB3, firewire or if you're fruity, thunderbolt and not hot swap disks. The primary purpose though is to facilitate easy replacement of broken disks or adding disks in RAID arrays. That means that the mechanical engineering is typically designed for incidental use and it will not stand up to the wear and tear of daily disk swaps.  In addition, it would mean that after removing the disk you would need to use the RAID controllers management tooling to present the disk to the your server as it won't magically appear.",3
"Thinking about organizing your data and processing it efficiently due to that organisation is the biggest single optimization you can ever make. The advantage of this is that your GUI (eg. buttons) does not necessarily get stuck if your logic is slow. User can still pause and save the game. It's also good for preparing your game for multiplayer, now that you separate the graphic from the logic. For example if you have 1000 projectiles and 1000 enemies the naive solution is to just check them all against each other. That said, you can still make it slow if you waste it. The problem is not so much the number of objects as the number of collision tests performed; the simple approach of checking each object against each other object squares the number of calculations required. Testing 1001 objects for collisions this way would require a million comparisons. Often this is addressed by e.g. not checking projectiles for collision with each other. For example if you list on each square of the grid what enemies are in that square then you can loop through your 1000 projectiles and just check the square on the grid. Now you just need to check each projectile against the square, this is O(n). Instead of a million checks each frame you only need a thousand. The other answers have handled the threading and power of modern computers. To address the bigger question though, what you are trying to do here is avoid ""n squared"" situations. Even Space Invaders managed dozens of interacting objects. Whereas decoding one frame of HD H264 video involves hundreds of millions of arithmetic operations. You have a lot of processing power available.",3
"Can this be done without using third party software such as Ghost,Clonezilla, R-Driveimage, DriveimageXML?  next you must prepare linux distro. this should boot, do reverse above - unpack image to the partition device and reboot. It's not easy to prepare. I am running Windows 7 (64Bit) and have created an image of my C: Drive and the image (WindowsImageBackup) is stored on my D: drive - What I want to do is to be able to click on a shortcut that will re-start my PC and automatically restore the WindowsImageBackup image to the C: Drive with no further user input. This is possible using linux. At first you should clean all unused disk space. Good idea is defragment partition before. You should clean unused space using many available programs for free use eraser.  boot any LIVE linux distro. make compressed copy using command 'dd if=/partiotnion/where/it/is|gzip >/partition/image.img.gz At the moment I use reagentc /boottore followed by the shutdown command to reboot the PC into the recovery environment which opens a dialog that requires user input to continue - I would like to automate user input process or discover if there is any switches or DOS commands that can give me what I need. I have Googled many times for an answer to this to no avail. I have experimented with BCD Boot, ReAgent.exe, boottore, UnAttend.xml, and various other re-imaging options but none have answered my query.",2
"in order to use that on any computer, you have to digitise the signal first, which is usually done via a framegrabber card in the computer. if you are getting an HDV-stream (which is really an MPEG-TS stream over ieee1394), there is a separte hdv1394src object: most other protocols you will found out there (e.g. GigE) are proprietary, industrial grade procolls (and don't offer ""camcorder"" hardware) that's where all the webcams come into play, probably some external analog2usb devices, maybe some ""camcorders"". bttv (bt878) and connexant based grabber PCI(e)-cards are very good supported on linux (others are probably well, but i never used them), and in my experience this is still the way to go if you need low-latency, but unfortunately these cards are getting sparse. industrial firewire cameras (mostly used for image-recognition and the like; there is no ""camcorder"" on the market using this protocol, so i only add it for completeness sake) used a protocol called ""IIDC"". Can I use a camcorder as V4L2src in gstreamer? Simply can I use a camcorder as a webcam in Linux? If so what models are sported? a DV-stream contains both audio and video, so in order to get an image you have to first grab the DV-stream  then split the two (audio and video) streams apart (aka ""demultiplex/demux them"") and then decode the resulting video-stream.",2
"One obvious resolution is to use only internal servers and eliminate external server group.  We use NAT and all internal servers have address from private ranges, eg. 192.168.1.0 The answer depends in part on the firewall you are using as some firewalls will take care of the DNS translations for you.  Whether you want to rely on your firewall to this degree is a big question (at least in my mind).  It's also unclear to me whether all/most firewalls offer such capability meaning you may find yourself stuck with what you have, or at least a subset of vendors once you've removed the external DNS servers. Currently we have a following setup. We have two domain controllers which also serve as  DNS servers, used as resolvers by local clients. We also have external autoritative DNS servers for exact same DNS zone, just for servicing outside world. This leads to a situation when the same record has to be entered twice on both server groups. We have a similar setup, but I have purposely kept the external DNS on different servers than the internal DNS for security reasons.  As soon as you move the external DNS to the same server as your internal which is also Active Directory, you have to open a hole up for resolvers to the same machine that serves your internal Active Directory.  If there is a flaw that crops up in the DNS service (as there have been in the past), then an attacker can potentially compromise your internal Active Directory machine.  By keeping internal and external DNS on separate machines, you do not have to open up anything through the firewall to the internal DNS/Active Directory box keeping it much safer IMHO. The question is how to avoid leaking internal addresses (that will resolve to 192.168...) if internal DNS servers start serving external requests? Finally, if depending on why you're interested in eliminating the extra servers you may want to consider outsourcing external DNS service. Having a zone duplicated intranet/internet is called ""split brain"" and you have outlined the pros/cons nicely. Now you must choose on the pluses and minuses. Hint; live with duplicate updates for the few records that have to be on the internet. Back to your current setup (and the hassle of doing things twice), it sounds like something that could very easily be automated.",4
"For example, on a Cisco switching infrastructure you should be able to use DHCP snooping to prevent this happening in future. Other switch brands may have similar features. You can track down the user using the techniques others have mentioned, but even better would be if you can prevent this from ever happening again. If none of these work then you will need to do a search by physically diconnecting one wire at a time from the switches until the problem goes away. One way is to get your laptop and do some ""war driving"" in the dorms.  In other words, roam around looking for strong wireless network signals coming from the rogue router. This will get you close to it, as you look at the varying strengths. If your switches are not managed its well worth upgrading, but if that's not an option just ping the IP from the earlier step: The procedures above will help you find the IP address they are using.  The traffic lights on the routers may also help for a visual indication. If you don't have a handle on where all the wires go it may be difficult to trace.  If you can pin down the port or ports that have th routers, consider disconnecting them at the switch. Most likely someone brought their router from home and connected the wrong port to your switches.  Most home routers provide DHCP and will happily give out addresses in the wrong range.  From what I have seen this is fairly common in dorm situations. I'm guessing that someone in a dorm room has connected a SOHO router instead of their computer, probably so that they can have wireless access.  What you can do is a series of steps to isolate the problem. Try posting a notice that anyone with their own router should connect only the WAN port to your network. By the way, if the administration and IT are doing a decent job then the students have signed some kind of agreement that prohibits them from doing this.  SO you can bring in the Dean of Students (of whatever he is called on your campus).",4
"In the chart above, it looks like all-but-one of the inputs are negatively correlated with the output. This implies that as these inputs increase, the output decreases and vice versa. I have tried the pandas code for trying to find out the correlation between the output and the inputs I am feeding. Here is the code:   Can someone please tell me whether I am on the right direction of what I am trying to achieve? What is the meaning of the above image? I was trying to understand which column will affect the result more in the positive or negative or neutral way.    You are in right way! Length of each box is actually correlation that you are looking for, positive, if box is on the right side from start (null) position, or negative on the left side. You can see that grey box have length 1, which is obvious because output has perfect correlation with itself. Regarding features: it seems, that they all have a small negative correlation (except input4, which correlation with output is slightly above zero), as they are all on the left side from the null.  In a correlation framework above, the biggest driver of the output is the input which has the greatest absolute correlation value.",3
"When you build your Keras model using the functional interface, you can also build additional models on any subset of the paths through the network by reusing the intermediary functions. Then you can train on just portions of the network (given that you have targets for the outputs). I haven't tried to train on sub-networks of a network, but I do use these intermediary models to propagate activations between internal layers. The Python package conx that is built on top of Keras will build these intermediary models for you, by the way. For more, see: http://conx.readthedocs.io/en/latest/Getting%20Started%20with%20conx.html?highlight=propagate_from#Propagation-functions I have a partial neural network with several layers of various types, with weights $\theta$; let's call it $F_{\theta}$; and the dimensions of input and output arrays are the same, implemented in Keras with Tensorflow backend. Within this framework, how do train a model of the form $y = F_{\theta}(F_{\theta}(x))$, or more generally some $n$-th iteration of $F_{\theta}$? In other words, I reuse an entire subset of the neural network, with all the same weights, so the full model is deeper but does not have more parameters than the shallower model.",2
"The processes with negative values don't seem to have much in common, and it's strange that PDU DISPATCHER and IP SNMP would be using the most memory. I've found an article suggesting you can subtract CISCO-PROCESS-MIB.cpmProcExtMemAllocatedRev and CISCO-PROCESS-MIB.cpmProcExtMemFreedRev (Cisco process memory usage) but that doesn't seem to result in sane values. The output of show processes memory on a switch shows the same results that I see via SNMP (insane values if the allocated - freed logic is correct), but it also shows a Holding column that looks like it has what I need. I wrote a script to snmpget all of the values to prevent timing issues, and sorted them by Allocated - Freed : I think it might be down to shared memory and the original OIDs pre-dating modern memory management in IOS. Often the values are the same (resulting in a zero) and sometimes the freed is greater than the allocated (resulting in a negative number)- though I think this is probably to do with memory being freed between when I pull the allocated results and when I pull the freed results. I can't find any references to Holding in the CISCO-PROCESS-MIB and I've had no luck searching on the internet. I can't find any other Cisco MIBs that contain better data, so I don't think there is a reliable way to get this information. I've just spent a good half an hour looking at this, and I don't think there is a way to get an accurate number from SNMP. This is the data available from the 3 tables in cpmProcess (there are many more OIDs, but they don't seem to be populated on the devices i was looking at)",2
"Incorrect, all Windows servers, even ones promoted to DC will have local users and groups.  However they are managed via the command line. The machine in which I can see Local Users and Groups is running Windows Storage Server 2012 R2. It is a Dell box of recent vintage. The machine in which I cannot see Local Users and Groups is running Windows Server 2012 R2. It is a virtual machine. Using set under cmd, I can see that the LOGONSERVER is identical. AS well, my USERNAME is identical, as is my USERDOMAIN. I have two machines with Windows 2012 r2. In one, under Computer Management->System Tools I see a ""Local Users and Groups"" submenu. In the other, I do not see it. How can two different machines in the same domain present two different sets of capabilities? One of the servers is a Domain Controller. Domain Controllers don't have local user accounts or security groups. I'm new at this Windows Admin stuff so I really need to wrap my head around Windows user access control. If it's inconsistent, I grow confused!",3
"I think you could use time series modeling algorithm as @Student_T said. Also you can make window time to find relation between new donate and previous donate and you can use amount, may be people with high payment and low payment have different behavior. first of all you should change your data in a way that fill gaps. I mean you should add data about month that a person have not any payment.  You can create several RFM types of features from this data set. Examples: Number of donations, time between donations, time since the most recent donation, time since the first donation, average donation amount, the amount of first donation, the most recent donation amount, etc. (I can provide more examples, if needed.) You could group together data of each user and run the algorithm for the user that is being asked for. Regression is the way to go if you want to know if a user will donate or not, to find the probability, try markov! person_id / month(or day or week or 3month) / count of payment / count of payment last month/ sum of amount that paid last month/ Is paid last month?/ Also, you could detect patterns in the dataset by plotting it and then figure out which algorithm to use based on what is the degree of correlation and the nature of the plot. In order to make this model generalizable, I'd recommend pulling several such cross-sections of your data (in addition to the October 2015 slice explained above,) Since your task is to predict the probability of donation during a four-month timeframe (Mar-Jun 2016), you can create those features (leading indicators) for each donor as of the end of October 2015. All leading indicators would be created based off of timeframes prior to that cut-off point. Your observation window is from Nov-2015 to Feb-2016. This is where your event flag (dependent variable) should come from: 1 if a donor donated (again) during the observation window, and 0 otherwise.  First you need to find in what kind of distribution is your data: linear, exponential, normal ... Etc.  then you should find whether your filed is useful and independent or not. and try to add other filed. and then build your model.",4
"Calling Math.random() many times isn't very good, try instancing a static final Random object once instead (look around on SO for valid reasons). In the above example, I simply used Collection.shuffle() because that's what a dealer physically does. You use bare Exceptions for different types of errors. Better is to create specialized exceptions or re-user exsisting ones, and add useful information.  You'll notice I'm using a custom exception as @RobAu pointed out. You'll have to handle it in the relevant parts of the program. The added value is that this Object would take care of many constraints like not drawing when empty whereas with a simple list you have the add, set & remove methods exposed. The Java convention says a main() method starts a Java program. Your function does so much more. You should extract these business actions to appropriate methods/objects. I recommend not relying on the exception but rather checking with hasCards() if the operation is legal. The exception should only be thrown if you thought you could draw, but can't for unforeseen reasons (also called exceptions!). In getUserInput(), you're creating a Scanner every time. There is no need for that, just make one once, and reuse it. You couldn't do it nicely with that Main class with only static methods. But if you make an AceyDucey class, you can make the Scanner a class attribute, like so: It seems the Dealer functionality replaces that object. I would rather have a Deck object with functionalities like restore(), shuffle(), draw(), hasCards() etc. Tip: When I see a static method doing stuff on its unique parameter, I instantly know it should have been encapsulated in said parameter.",2
"See this article for how to secure MySQL.  Similar (but distinct) advice applies to other database systems. Anyway, keeping my (possibly way off-base) assumptions in mind, if your current setup works for you without assigning public IPs to your DB servers, then why change it? Giving your DB servers public IP addresses will make them (potentially) accessible from the internet at large, which will introduce security concerns that you will need to address. Since it is very easy to get security wrong, my answer to your question is No, unless you've already got a well-thought-out security infrastructure built up around your servers. Of course, your database server must be properly secured.  You should be especially careful if it is accessible from the outside Internet. Sure.  TCP/IP can be used for anything, including connecting to databases.  For instance, you might be interested in the MySQL wire protocol. Best practice as I've known it is to stick a firewall between any server & the Internet, NAT the public IPs to private, and open the necessary ports ONLY.  Absolutely lock that server down as described, but at the least, this will save your DB server from having to process all the random port scanning & scripted attacks. I have a few instances running on the Amazon cloud. Some are DB-servers, some are blogs and one has my webapp. The DB-servers don't have apache or any other web-servers installed. So will it be safe to allot an IP to the DB-Servers?  If I understand mattdm's clarification to your question, then you are currently running DB servers in Amazon EC2 on instances that do not have public IP addresses. Not having any experience with any of Amazon's ""cloud"" offerings, am I correct to assume, then, that you access your DB servers from your other servers via some kind of Amazon-provisioned private network?",4
"A whole different approach would be to use NeoPixel LEDs.  These are LEDs that can illuminate in a variety of colors and brightness's with usually 8 bits for each of red, green and blue.  These LEDs can be ""chained together"" with no obvious limit to their length.  You could then ""snake"" a chain of NeoPixels either up/down or left/right achieving the matrix you desire.  Because NeoPixels are highly timing sensitive, I use a cheap Arduino as the driver for the NeoPixels and then drive the Arduino through communications from the Pi ... basically an Arduino becomes an LED driver module with the content logic coming from the Pi. There is an integrated circuit (IC) called the MAX7219 that is designed to drive 7 segment LEDs.  Specifically, one IC can drive 8 at a time.  It also has a second mode which allows a single IC to drive 64 LEDs in a matrix.  Putting it another way, in your example, one MAX7219 could drive 8 columns of 8 LEDs per column.  The MAX7219 is drive by a protocol called SPI.  Since the MAX7219 is an ""input"" only device ... one needs 3 pins to drive it.  A clock, a chip select and a data pin.  If you want to drive multiple MAX7219s you have some choices.  First, the MAX7219 can support daisy chaining to any length you desire.  For example, if you used 10 MAX7219s you would have an 80x8 array.  If you used 20 MAX7219s, you would have a 160x8 array. An alternative mechanism (one I don't necessarily recommend) is to have all your MAX7219s share the same clock and data line and use the chip select to specify which instance you are writing to.",1
"Ultimately, you must do your own due diligence to determine if you want to trust something and then take countermeasures (run in a sandbox, a virtual machine, etc.) to mitigate any unknown factors or miscalculations you made when deciding whether or not to trust. The point is, publishing hashes is useful for lots of special cases other than just downloading the file from the same site hosting the hash. I downloaded a huge amount of files, but discovered the use of md5 and sha as integrity checkers quite recently. From then I always prefer to check it for big downloaded files, even if I never found them to be corrupted.  I usually only bother to check if my download was interrupted and I resumed it later, because HTTP requests with a seek offset are not as widely used, so something silly might have happened. Verifying it with a hash from the same site I got it from is of little use, security-wise, as other answers and comments have said.  It can be more useful if you downloaded from a mirror, but the hash is from the original upstream.  Or if the filename is ambiguous, and for some reason you aren't sure you got the exact version you wanted. Checking MD5/SHA1 hashes is a good first step and you should do it when you have time. However, you must consider your ability to trust the hash provided. For example, if the the author's website with the hash is hacked, then the attacker can change the hash, so you would not know. If the hash you calculate is not the same as the hash provided, you know something is up. However, just because the hashes match does not guarantee the file is good. A better alternative for a software author to provide integrity and authenticity is through digitally signing the files being distributed. This attaches the authenticity information to the file and does not rely on trusting some website. If an author digitally signs the file, the only way for this to be faked is a compromised certificate authority or if the developer's signing key was stolen. Both of these cases are far less likely than a website on the Internet being hacked. The answer and choice one makes is going to be based on his/her risk tolerance and considerations of time and effort in verification. For security reasons, YES. Consider that a Tor exit node was found to be patching binaries during download, then remember that your ISP may or may not have the slightest morals, and that they are in complete control of your internet connection. In many cases, the download is a compressed file that won't decompress if it's broken.  If that's NOT the case, checking is not a bad idea.  I might check an ISO before burning it to write-once media.",4
"Also, in the event of someone trying to brute force attack the rdp port to find the password of a user which can remotely login: Check to make sure that Group/Account policies haven't locked the account out of remote access. (Should this feature be enabled) The only problem is the M### don't link to the MS articles on MS website, but on the (same) articles on EventID.net, so you'll have to figure out which MS articles they apply to (or become a subscriber, not sure if it's free, didn't check). I've seen multiple Windows 2003 terminal servers have this problem.  For me they have all been machines with more than one network interface or a multiport interface.  If you go to Terminal Services Configuration (tscc.msc) and go to properties on your RDP-Tcp connection there is a tab for Network Adapter.  Change this option from ""All network adapters"" to the network adapter you use for front end connections. I recently found that TS got problems with connecting when Advantage Validation Tool is not installed properly, Just guesting You may check that part of software on yours WIN2K ore they get automatic updates which is locked system because problems with WGAs",4
"iOS has WebGL capabilities, but currently they have disabled it.  I do however think it's likely they will enable it in the future, so HTML5 could be a good platform for you.  Also, because your looking at making puzzle games these are traditionally less resource intensive than other genres of games so the current performance should be good enough for your needs. HTML5 is definitely the best for cross-platform support. As any browser that renders canvas should be able to play your game. Although you'll have to be wary, as there are still some browser specifics.  As an example, here's Bejeweled in HTML5.  Try running this in your ipad browser: http://bejeweled.popcap.com/html5/  Do not rush, take your time, and learn. I also do not recommend stopping half-way to develop on a different platform.  There is no doubt if you want to develop a game on iPad or iPhone, it is probably best to learn cocos2D and obj-C. I would recommend a mobile development platform like Corona or Moai. They make the process a lot simpler, by handling all the low-level eccentricities of iOS and letting you do all the programming in Lua. My suggestion? Pick a random thing you want to do, develop it FULLY, then decide if you liked it, then consider some ideas: As far as engines to use, I'd look at either Cocos2d or Unity.  Both will get a lot of the heavy lifting out of the way and let you hopefully concentrate on making the game itself. We make the Construct 2 game maker engine which lets you create HTML5 games.  No programming is required but if you want to you can program your own plugins and behaviours in Javascript to extend Construct 2's capabilities.  If you're looking to make HTML5 games rapidly you may want to look at Construct 2 as an option.  There's an extensive free edition as well which is getting quite popular now. If you're making a game specifically for the iPad, I'd shy away from HTML5.  You'll get much better performance using a native app. HTML5 has a good future ahead of it and it's just starting to gain some good traction so now might be a good time to step in!",4
"I know you mention WebEx RemoteSupport as being too expensive, but we used the basic WebEx package initially, before upgrading to a corporate solution. PS I think that NetSupport created an online control of its software like LogMeIn, but i couldnt find it, maybe they've stopped. Actually it worked well but I understand that it's a more complicated solution than teamviewer, logmein etc.. (that could be used easily also for one-time-assistance) At various times the company I was working for already owned either or both, and leveraging for support was simple. It doesn't require firewall changes, doesn't require custom software, and even the more security conscious clients were generally allowed to run it without issue. So when the notebook was in our LAN we could reach the computer using the native control protocol while when it was outside our network its client automatically connected to our gateway and we could see and control it via internet just like it was inside. AFAIK it used HTTP protocol. A couple of years ago in my previous job we used NetSupport Manager (I think it was v8 or v9). It had a ""gateway"" function that could bypass firewalls. Actually it worked in this way: Personally, rather than using specific software, I've used both WebEx and GoTo Meeting to help remote clients. Add to that that multiple people can join the call and share the screen at once, and it becomes a pretty powerful tool. I could switch between demonstrating from my desktop to troubleshooting the remote one. Unfortunately I dont remember how much this software costed (they bought it before i joined their team) but i think it was not so cheap. The sales force used it too - it was a truly global, and cross discipline tool. That might help you sell it to management :) Check out Fog Creek Copilot.  It gets around firewalls and implements a custom VNC-based system.  Their pricing is very reasonable and I've never had any trouble with it.",3
"If other people are spoofing your domain to send spam, your domain can end up on blacklists or with a poor reputation at reputation services such as SenderBase.  SPF and DKIM both work to prevent this kind of spoofing, meaning that your domain's reputation is only caused by you. But spammers can publish their own SPF and DKIM records so mail providers won't use the presence of these features as a whitelist.  The large mail providers all either maintain IP address and domain reputation services or use commercial ones.  Hotmail uses SenderScore.  Your IP and your domain have neutral reputation with them however you may notice that they still list your domain as not having SPF records.  This may be due to DNS caching but it could also be because your SPF record is published using the SPF record type but is not also published using the more common TXT record type. Bounces are important.  Failing to unsubscribe bounces is one of the primary reasons for low IP address reputation.  Different kinds of bounces require different actions.  ""No such user"" should be treated differently from ""Inbox is full"" but even ""Inbox is full"" should result in an unsubscribe eventually.  When accounts expire at Hotmail, they go dormant for around 6 months and are then brought back to life as spam traps.  Anyone who ignores bounces will end up blacklisted by Hotmail when those bounces becomes spam traps. The simplest solution is to force your SMTP sender to use IPv4. Otherwise, you will need to add the IPv6 version of your MX to your SPF and make sure it has a valid PTR record. In your DKIM header, s=mail means that receivers will look up TXT records from mail._domainkeys.afspraakmanager.be to find your public key. You should either change your MTA to use s=default or add a TXT record at mail._domainkeys.afspraakmanager.be with your DKIM record. DMARC is useful for getting feedback on what IP addresses are sending mail claiming to be from you.  Even in small environments it can be common for mail to come out of two or three IP addresses.  Once you are sure your SPF records are covering all of the IP addresses that send legitimate mail, you can switch the policy to -all.  DMARC is simple to set up and, when I did so, I got my first reports within a few hours. FBLs give you feedback on how well your recipients are liking your email.  When they click their ""Spam"" button, the provider sends you a copy of the email to let you know to unsubscribe that user.  It can also work as an early-warning system that someone has compromised your server and is using it to send out spam.  Hotmail have an FBL program (which they call JMRP) however I have never managed to successfully sign up for it.  Hotmail also have the SNDS program which is very similar but purely for IP addresses.  AOL and Yahoo and many other places have FBL programs.  Failing to have a working unsubscribe link or even having one that's confusing and difficult to use can cause your recipients to use the spam button. Lastly, the best thing you can do to improve your IP address reputation is to send lots of email that people want.  Greater volumes of email with low incidents (such as a spam report or triggering content-based spam detection) will improve your reputation faster than the same ratio with a lower volume.",2
"Others have already mentioned reasons for mirroring (projecting via a mirror to fold the distance needed; or back-projecting), and for 180 degree rotations (set-up tablets, etc.) One can envision a circumstance in which the display mounting and associated brackets can only permit an inverted attachment of the panel. A kiosk with limited access may be one example, a display unit mounted at ceiling height with a mounting bracket that cannot be attached unless upside down. Many restaurants show their menus on rotated screen (portrait mode); same for airports Departure and Arrival lists. Newer ones are simple oversize screens with 90 degree rotated display. The short answer probably is: Because not all display devices are able to correctly report their orientation. I know probably not many do this, but for some time that I had my desktop monitor close to where my head's at in bed, I would rotate the screen so I could read or watch a movie while laying on my side. I've also done that with my laptop a few times. In the past monitors could be rotated on their mounts 90 degrees each way. Haven't seen that for a long time but it was possible with some older models and I admit that this would be helpful at work in situations when I want to see many lines of code at a glance. Though I'd like to have it autorotate like in smartphones, not to have to do it manually from the OS. Off the top of my head, I can imagine the following use cases for being able to flip/mirror/rotate the screen orientation: In the case of a laptop, I can picture a situation in which the keyboard is mounted inside a box with the display extending outside the enclosure which would require to invert the image for viewing in a normal orientation.",5
