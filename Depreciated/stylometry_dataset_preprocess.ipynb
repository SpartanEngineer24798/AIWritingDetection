{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16355,"status":"ok","timestamp":1686470403305,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"D7R2KKvF_yPr","outputId":"2d02afde-bc51-4ca9-df0c-368358b9ba0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#if running on Google collab, mounting the drive might be convenient\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File downloaded successfully.\n"]}],"source":["import urllib.request\n","\n","url = \"https://huggingface.co/datasets/Hello-SimpleAI/HC3/resolve/main/all.jsonl\"\n","file_name = \"all.jsonl\"\n","\n","urllib.request.urlretrieve(url, file_name)\n","\n","print(\"File downloaded successfully.\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import os\n","\n","current_directory = os.getcwd()\n","file_path = os.path.join(current_directory, 'all.jsonl')\n","\n","with open(file_path, 'r', encoding='utf-8') as json_file:\n","    json_list = list(json_file)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"oD06WVD-wsSb"},"outputs":[],"source":["#the data is extracted as a dictionary of a sequential number index and the raw text\n","#it is divided into data_ai and.json and data_human.json\n","#note that the dictionary is 1-indexed\n","#{1:\"Hello, this is an example text written by a human\", 2:\"As an AI language model, I cannot provide an example text written by a AI.\"}\n","\n","import json\n","\n","data_ai = {}\n","data_human = {}\n","\n","counter_ai = 0\n","counter_human = 0\n","\n","counter = 0\n","for json_str in json_list:\n","    result = json.loads(json_str)\n","    counter = counter + 1\n","    data_ai[counter] = result['human_answers']\n","    data_human[counter] = result['chatgpt_answers']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save the files as data_ai.json and data_human.json\n","import json\n","\n","with open(\"data_ai.json\", \"w\") as write_file:\n","    json.dump(data_ai, write_file)\n","\n","with open(\"data_human.json\", \"w\") as write_file:\n","    json.dump(data_human, write_file)"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1686475742749,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"b_MsBI-wAX0D"},"outputs":[],"source":["import pandas as pd\n","\n","#this datapath is from Google drive mount, change as you need\n","data_path = \"/content/drive\"\n","\n","#loading the data again from the saved file (not really needed if you run the whole thing in a single run)\n","with open(data_path + \"data_ai.json\", \"r\") as read_file:\n","  data_ai = json.load(read_file)\n","\n","with open(data_path + \"data_human.json\", \"r\") as read_file:\n","  data_human = json.load(read_file)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4136,"status":"ok","timestamp":1686470315451,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"oo5LHlkh-jzo","outputId":"071dd9d0-8338-4d28-d6a7-426e6f863330"},"outputs":[{"name":"stdout","output_type":"stream","text":["Installing collected packages: readability\n","Successfully installed readability-0.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["#packages required for lexical feature extraction\n","!pip install lexicalrichness\n","!pip install readability\n","!pip install nltk"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2153,"status":"ok","timestamp":1686470325841,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"S7li_C1J6gkn","outputId":"d69738ea-acee-4161-98e0-2498e8c667a0"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#importing packages (you may need to install more libraries as these were imported in Google collab environment)\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize \n","from nltk.tokenize import sent_tokenize\n","import re\n","import statistics\n","import numpy as np\n","import string \n","\n","from lexicalrichness import LexicalRichness\n","import readability"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":299,"status":"ok","timestamp":1686470331772,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"K1gWjn-tDsvT"},"outputs":[],"source":["#'spacial puncts array, needs to be provided separately\n","special_puncts = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686470334147,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"UBZRlVezBI5H"},"outputs":[],"source":["#functions to extract stylometric characteristics from a piece of text\n","#to do - check if word count, sentence count, and paragraph count is actualy deterimental to study or not\n","def word_count(document):\n","    tokens = word_tokenize(document)\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    return len(filtered)\n","\n","def sentence_count(document):\n","    tokens = sent_tokenize(document)\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    return len(filtered)\n","\n","def paragraph_count(document):\n","    tokens = document.splitlines()\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    return len(filtered)\n","\n","def word_count_sent(document):\n","    tokens = sent_tokenize(document)\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    word_counts = [word_count(sent) for sent in filtered]\n","    if len(word_counts) == 0:\n","        return 0, 0\n","    mean = sum(word_counts) / len(word_counts)\n","    if len(word_counts) < 2:\n","      stdev = 0\n","    else:\n","      stdev = statistics.stdev(word_counts)\n","    return mean, stdev\n","\n","def word_count_para(document):\n","    tokens = document.splitlines()\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    word_counts = [word_count(para) for para in filtered]\n","    if len(word_counts) == 0:\n","        return 0, 0\n","    mean = sum(word_counts) / len(word_counts)\n","    if len(word_counts) < 2:\n","      stdev = 0\n","    else:\n","      stdev = statistics.stdev(word_counts)\n","    return mean, stdev\n","\n","\n","def sent_count_para(document):\n","    tokens = document.splitlines()\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    sent_counts = [sentence_count(para) for para in filtered]\n","    if len(sent_counts) == 0:\n","        return 0, 0\n","    mean = sum(sent_counts) / len(sent_counts)\n","    if len(sent_counts) < 2:\n","      stdev = 0\n","    else:\n","      stdev = statistics.stdev(sent_counts)\n","    return mean, stdev\n","\n","#to do - check if total count is helpful to the classification or not\n","def total_punc_count(document):\n","    punct_count = 0\n","    for char in document:\n","        if char in string.punctuation:\n","            punct_count += 1\n","    return punct_count\n","\n","def special_punc_count(document, special_puncts):\n","    punct_count = 0\n","    total_puncts = 0\n","    for char in document:\n","        if char in string.punctuation:\n","            total_puncts += 1\n","            if char in special_puncts:\n","                punct_count += 1\n","    if total_puncts == 0:\n","        return 0\n","    else:\n","        return float(punct_count) / total_puncts\n","\n","def special_punc_count_sent(document, special_puncts):\n","    tokens = sent_tokenize(document)\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    punct_count = 0\n","    total_sentences = len(filtered)\n","    if total_sentences == 0:\n","        return 0\n","    for sent in filtered:\n","        for char in sent:\n","            if char in string.punctuation and char in special_puncts:\n","                punct_count += 1\n","    return float(punct_count) / total_sentences\n","\n","def special_punc_count_para(document, special_puncts):\n","    tokens = document.splitlines()\n","    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n","    filtered = [w for w in tokens if nonPunct.match(w)]\n","    punct_count = 0\n","    total_paragraphs = len(filtered)\n","    if total_paragraphs == 0:\n","        return 0\n","    for para in filtered:\n","        for char in para:\n","            if char in string.punctuation and char in special_puncts:\n","                punct_count += 1\n","    return float(punct_count) / total_paragraphs\n","\n","#relies on stylometric basics for readability and lexical richness\n","def readability_score(document):\n","    try:\n","        r = readability.getmeasures(document, lang='en')\n","        fk = r['readability grades']['Kincaid']\n","        f = r['readability grades']['FleschReadingEase']\n","        ari = r['readability grades']['ARI']\n","    except:\n","        return 0, 0, 0\n","    else:\n","        return fk, f, ari\n","\n","def lexical_richness(document):\n","    sample_size = 10\n","    iterations = 50\n","    lex = LexicalRichness(document)\n","    ret_list = []\n","    words = document.split()\n","    try:\n","        if len(words) > 45:\n","            ret_list.append(lex.mattr(window_size=25))\n","        else:\n","            window_size = max(1, len(words) // 3)\n","            if window_size > len(words):\n","                window_size = len(words)\n","            ret_list.append(lex.mattr(window_size=window_size))\n","    except Exception:\n","        ret_list.append(0)\n","    ret_list.append(lex.mtld(threshold=0.72))\n","    return ret_list"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"R1UtI2t3FlkR"},"source":["Still to do: https://huggingface.co/docs/transformers/perplexity\n","Perplexity is considered the most common metric, maybe worth implementing as features"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1686470340291,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"Y3fHUkJZCJds"},"outputs":[],"source":["#function call to call all the above feature extractors in a single go\n","#set verbose = True to understand what each number corresponds to\n","#to do - consider reporting words/sentence per sentence, and change network architecture to be able to process that\n","def extract_features(document, special_puncts, verbose=False):\n","    results = []\n","    results.append(word_count(document))\n","    results.append(sentence_count(document))\n","    results.append(paragraph_count(document))\n","\n","    words_per_sent = word_count_sent(document)\n","    results.append(words_per_sent[0])\n","    results.append(words_per_sent[1])\n","\n","    words_per_para = word_count_para(document)\n","    results.append(words_per_para[0])\n","    results.append(words_per_para[1])\n","\n","    sent_per_para = sent_count_para(document)\n","    results.append(sent_per_para[0])\n","    results.append(sent_per_para[1])\n","\n","    results.append(total_punc_count(document))\n","    \n","    special_punc_result = special_punc_count(document, special_puncts)\n","    results.append(special_punc_result)\n","    \n","    special_punc_sent_result = special_punc_count_sent(document, special_puncts)\n","    results.append(special_punc_sent_result)\n","    \n","    special_punc_para_result = special_punc_count_para(document, special_puncts)\n","    results.append(special_punc_para_result)\n","    \n","    readability_results = readability_score(document)\n","    results.extend(readability_results)\n","    \n","    lexical_richness_results = lexical_richness(document)\n","    results.extend(lexical_richness_results)\n","    \n","    if verbose:\n","        verbose_results = []\n","        verbose_results.append(\"Word Count: Number of words in the document\")\n","        verbose_results.append(\"Sentence Count: Number of sentences in the document\")\n","        verbose_results.append(\"Paragraph Count: Number of paragraphs in the document\")\n","        verbose_results.append(\"Word Count per Sentence: Average number of words per sentence\")\n","        verbose_results.append(\"Word Count per Sentence: Standard deviation of number of words per sentence\")\n","        verbose_results.append(\"Word Count per Paragraph: Average number of words per paragraph\")\n","        verbose_results.append(\"Word Count per Sentence: Standard deviation of number of words per paragraph\")\n","        verbose_results.append(\"Sentence Count per Paragraph: Average number of sentences per paragraph\")\n","        verbose_results.append(\"Word Count per Sentence: Standard deviation of number of sentences per paragraph\")\n","        verbose_results.append(\"Total Punctuation Count: Number of punctuation marks in the document\")\n","        verbose_results.append(\"Averaged Special Punctuation Count: Average number of special punctuation marks per total punctuation marks\")\n","        verbose_results.append(\"Averaged Special Punctuation Count per Sentence: Average number of special punctuation marks per sentence\")\n","        verbose_results.append(\"Averaged Special Punctuation Count per Paragraph: Average number of special punctuation marks per paragraph\")\n","        verbose_results.append(\"Flesch-Kincaid Reading Grade: Readability grade based on the Flesch-Kincaid formula\")\n","        verbose_results.append(\"Flesch Reading Ease Score: Readability score based on the Flesch Reading Ease formula\")\n","        verbose_results.append(\"Automated Readability Index: Readability score based on the Automated Readability Index formula\")\n","        verbose_results.append(\"Lexical Richness (MATTR): Measure of lexical richness based on the Moving-Average Type-Token Ratio\")\n","        verbose_results.append(\"Lexical Richness (MTLD): Measure of lexical richness based on the Measure of Textual Lexical Diversity\")\n","        \n","        return list(zip(results, verbose_results))\n","    \n","    return results"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":1989,"status":"ok","timestamp":1686475876783,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"JlXsaADPH5em"},"outputs":[],"source":["#calling the extract_features per every line and appending it to the 'features' array\n","#keeping a separate labels array, 1s for ai and 0s for human (this is the standard way for MLPClassifier module)\n","\n","features = []\n","labels = []\n","\n","for n in range(1, len(data_ai)):\n","    try:\n","        text = data_ai[str(n)]#[0]\n","        if len(text.split()) < 10:\n","            continue\n","        features.append(extract_features(text, special_puncts))\n","        labels.append(1)\n","    except:\n","        continue\n","\n","for n in range(1, len(data_human)):\n","    try:\n","        text = data_human[str(n)]#[0]\n","        if len(text.split()) < 10:\n","            continue\n","        features.append(extract_features(text, special_puncts))\n","        labels.append(0)\n","    except:\n","        continue"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1686475879285,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"e8icjCgPJKFw"},"outputs":[],"source":["#saving the labels\n","\n","features = np.array(features)\n","labels = np.array(labels)\n","\n","#np.savetxt('/content/extracted_features.txt', features)\n","np.savetxt('/content/extracted_labels.txt', labels)"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686475895590,"user":{"displayName":"Eddie Seung Hun Han","userId":"17068561500896983476"},"user_tz":-540},"id":"N4esPVQVLeYJ"},"outputs":[],"source":["#saving the standardized labels (this may or may not be a good idea considering that this would limit the performance to the dataset)\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","normalized_features = scaler.fit_transform(features)\n","\n","np.savetxt('/content/normalized_features.txt', normalized_features)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMGIZhhE+CyAMBcCB99LgRS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
